{"2024-01-03T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2401.01879v1","updated":"2024-01-03T18:39:13Z","published":"2024-01-03T18:39:13Z","title":"Theoretical guarantees on the best-of-n alignment policy","summary":"  A simple and effective method for the alignment of generative models is the\nbest-of-$n$ policy, where $n$ samples are drawn from a base policy, and ranked\nbased on a reward function, and the highest ranking one is selected. A commonly\nused analytical expression in the literature claims that the KL divergence\nbetween the best-of-$n$ policy and the base policy is equal to $\\log (n) -\n(n-1)/n.$ We disprove the validity of this claim, and show that it is an upper\nbound on the actual KL divergence. We also explore the tightness of this upper\nbound in different regimes. Finally, we propose a new estimator for the KL\ndivergence and empirically show that it provides a tight approximation through\na few examples.\n","authors":["Ahmad Beirami","Alekh Agarwal","Jonathan Berant","Alexander D'Amour","Jacob Eisenstein","Chirag Nagpal","Ananda Theertha Suresh"],"pdf_url":"https://arxiv.org/pdf/2401.01879v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01862v1","updated":"2024-01-03T18:09:33Z","published":"2024-01-03T18:09:33Z","title":"A Vision Check-up for Language Models","summary":"  What does learning to model relationships between strings teach large\nlanguage models (LLMs) about the visual world? We systematically evaluate LLMs'\nabilities to generate and recognize an assortment of visual concepts of\nincreasing complexity and then demonstrate how a preliminary visual\nrepresentation learning system can be trained using models of text. As language\nmodels lack the ability to consume or output visual information as pixels, we\nuse code to represent images in our study. Although LLM-generated images do not\nlook like natural images, results on image generation and the ability of models\nto correct these generated images indicate that precise modeling of strings can\nteach language models about numerous aspects of the visual world. Furthermore,\nexperiments on self-supervised visual representation learning, utilizing images\ngenerated with text models, highlight the potential to train vision models\ncapable of making semantic assessments of natural images using just LLMs.\n","authors":["Pratyusha Sharma","Tamar Rott Shaham","Manel Baradad","Stephanie Fu","Adrian Rodriguez-Munoz","Shivam Duggal","Phillip Isola","Antonio Torralba"],"pdf_url":"https://arxiv.org/pdf/2401.01862v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01854v1","updated":"2024-01-03T17:48:10Z","published":"2024-01-03T17:48:10Z","title":"Multilingual Instruction Tuning With Just a Pinch of Multilinguality","summary":"  As instruction-tuned large language models (LLMs) gain global adoption, their\nability to follow instructions in multiple languages becomes increasingly\ncrucial. One promising approach is cross-lingual transfer, where a model\nacquires specific functionality on some language by finetuning on another\nlanguage. In this work, we investigate how multilinguality during instruction\ntuning of a multilingual LLM affects instruction-following across languages. We\nfirst show that many languages transfer some instruction-following capabilities\nto other languages from even monolingual tuning. Furthermore, we find that only\n40 multilingual examples in an English tuning set substantially improve\nmultilingual instruction-following, both in seen and unseen languages during\ntuning. In general, we observe that models tuned on multilingual mixtures\nexhibit comparable or superior performance in several languages compared to\nmonolingually tuned models, despite training on 10x fewer examples in those\nlanguages. Finally, we find that increasing the number of languages in the\ninstruction tuning set from 1 to only 2, 3, or 4 increases cross-lingual\ngeneralization. Our results suggest that building massively multilingual\ninstruction-tuned models can be done with only a very small set of multilingual\ninstruction-responses.\n","authors":["Uri Shaham","Jonathan Herzig","Roee Aharoni","Idan Szpektor","Reut Tsarfaty","Matan Eyal"],"pdf_url":"https://arxiv.org/pdf/2401.01854v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01843v1","updated":"2024-01-03T17:22:48Z","published":"2024-01-03T17:22:48Z","title":"Investigating Semi-Supervised Learning Algorithms in Text Datasets","summary":"  Using large training datasets enhances the generalization capabilities of\nneural networks. Semi-supervised learning (SSL) is useful when there are few\nlabeled data and a lot of unlabeled data. SSL methods that use data\naugmentation are most successful for image datasets. In contrast, texts do not\nhave consistent augmentation methods as images. Consequently, methods that use\naugmentation are not as effective in text data as they are in image data. In\nthis study, we compared SSL algorithms that do not require augmentation; these\nare self-training, co-training, tri-training, and tri-training with\ndisagreement. In the experiments, we used 4 different text datasets for\ndifferent tasks. We examined the algorithms from a variety of perspectives by\nasking experiment questions and suggested several improvements. Among the\nalgorithms, tri-training with disagreement showed the closest performance to\nthe Oracle; however, performance gap shows that new semi-supervised algorithms\nor improvements in existing methods are needed.\n","authors":["Himmet Toprak Kesgin","Mehmet Fatih Amasyali"],"pdf_url":"https://arxiv.org/pdf/2401.01843v1.pdf","comment":"Innovations in Intelligent Systems and Applications Conference (ASYU)"},{"id":"http://arxiv.org/abs/2401.01313v2","updated":"2024-01-03T17:13:00Z","published":"2024-01-02T17:56:30Z","title":"A Comprehensive Survey of Hallucination Mitigation Techniques in Large\n  Language Models","summary":"  As Large Language Models (LLMs) continue to advance in their ability to write\nhuman-like text, a key challenge remains around their tendency to hallucinate\ngenerating content that appears factual but is ungrounded. This issue of\nhallucination is arguably the biggest hindrance to safely deploying these\npowerful LLMs into real-world production systems that impact people's lives.\nThe journey toward widespread adoption of LLMs in practical settings heavily\nrelies on addressing and mitigating hallucinations. Unlike traditional AI\nsystems focused on limited tasks, LLMs have been exposed to vast amounts of\nonline text data during training. While this allows them to display impressive\nlanguage fluency, it also means they are capable of extrapolating information\nfrom the biases in training data, misinterpreting ambiguous prompts, or\nmodifying the information to align superficially with the input. This becomes\nhugely alarming when we rely on language generation capabilities for sensitive\napplications, such as summarizing medical records, financial analysis reports,\netc. This paper presents a comprehensive survey of over 32 techniques developed\nto mitigate hallucination in LLMs. Notable among these are Retrieval Augmented\nGeneration (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023),\nCoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we\nintroduce a detailed taxonomy categorizing these methods based on various\nparameters, such as dataset utilization, common tasks, feedback mechanisms, and\nretriever types. This classification helps distinguish the diverse approaches\nspecifically designed to tackle hallucination issues in LLMs. Additionally, we\nanalyze the challenges and limitations inherent in these techniques, providing\na solid foundation for future research in addressing hallucinations and related\nphenomena within the realm of LLMs.\n","authors":["S. M Towhidul Islam Tonmoy","S M Mehedi Zaman","Vinija Jain","Anku Rani","Vipula Rawte","Aman Chadha","Amitava Das"],"pdf_url":"https://arxiv.org/pdf/2401.01313v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10997v3","updated":"2024-01-03T17:04:40Z","published":"2023-12-18T07:47:33Z","title":"Retrieval-Augmented Generation for Large Language Models: A Survey","summary":"  Large Language Models (LLMs) demonstrate significant capabilities but face\nchallenges such as hallucination, outdated knowledge, and non-transparent,\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\nemerged as a promising solution by incorporating knowledge from external\ndatabases. This enhances the accuracy and credibility of the models,\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\nupdates and integration of domain-specific information. RAG synergistically\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\nexternal databases. This comprehensive review paper offers a detailed\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\ntripartite foundation of RAG frameworks, which includes the retrieval , the\ngeneration and the augmentation techniques. The paper highlights the\nstate-of-the-art technologies embedded in each of these critical components,\nproviding a profound understanding of the advancements in RAG systems.\nFurthermore, this paper introduces the metrics and benchmarks for assessing RAG\nmodels, along with the most up-to-date evaluation framework. In conclusion, the\npaper delineates prospective avenues for research, including the identification\nof challenges, the expansion of multi-modalities, and the progression of the\nRAG infrastructure and its ecosystem.\n","authors":["Yunfan Gao","Yun Xiong","Xinyu Gao","Kangxiang Jia","Jinliu Pan","Yuxi Bi","Yi Dai","Jiawei Sun","Qianyu Guo","Meng Wang","Haofen Wang"],"pdf_url":"https://arxiv.org/pdf/2312.10997v3.pdf","comment":"Ongoing Work"},{"id":"http://arxiv.org/abs/2401.01830v1","updated":"2024-01-03T16:47:13Z","published":"2024-01-03T16:47:13Z","title":"Iterative Mask Filling: An Effective Text Augmentation Method Using\n  Masked Language Modeling","summary":"  Data augmentation is an effective technique for improving the performance of\nmachine learning models. However, it has not been explored as extensively in\nnatural language processing (NLP) as it has in computer vision. In this paper,\nwe propose a novel text augmentation method that leverages the Fill-Mask\nfeature of the transformer-based BERT model. Our method involves iteratively\nmasking words in a sentence and replacing them with language model predictions.\nWe have tested our proposed method on various NLP tasks and found it to be\neffective in many cases. Our results are presented along with a comparison to\nexisting augmentation methods. Experimental results show that our proposed\nmethod significantly improves performance, especially on topic classification\ndatasets.\n","authors":["Himmet Toprak Kesgin","Mehmet Fatih Amasyali"],"pdf_url":"https://arxiv.org/pdf/2401.01830v1.pdf","comment":"Published in International Conference on Advanced Engineering,\n  Technology and Applications (ICAETA 2023). The final version is available\n  online at https://link.springer.com/chapter/10.1007/978-3-031-50920-9_35"},{"id":"http://arxiv.org/abs/2401.01825v1","updated":"2024-01-03T16:42:13Z","published":"2024-01-03T16:42:13Z","title":"Physio: An LLM-Based Physiotherapy Advisor","summary":"  The capabilities of the most recent language models have increased the\ninterest in integrating them into real-world applications. However, the fact\nthat these models generate plausible, yet incorrect text poses a constraint\nwhen considering their use in several domains. Healthcare is a prime example of\na domain where text-generative trustworthiness is a hard requirement to\nsafeguard patient well-being. In this paper, we present Physio, a chat-based\napplication for physical rehabilitation. Physio is capable of making an initial\ndiagnosis while citing reliable health sources to support the information\nprovided. Furthermore, drawing upon external knowledge databases, Physio can\nrecommend rehabilitation exercises and over-the-counter medication for symptom\nrelief. By combining these features, Physio can leverage the power of\ngenerative models for language processing while also conditioning its response\non dependable and verifiable sources. A live demo of Physio is available at\nhttps://physio.inesctec.pt.\n","authors":["Rúben Almeida","Hugo Sousa","Luís F. Cunha","Nuno Guimarães","Ricardo Campos","Alípio Jorge"],"pdf_url":"https://arxiv.org/pdf/2401.01825v1.pdf","comment":"Demo, ECIR 2024, 3rd Sword AI challenge 2023"},{"id":"http://arxiv.org/abs/2401.01780v1","updated":"2024-01-03T15:12:42Z","published":"2024-01-03T15:12:42Z","title":"Navigating Uncertainty: Optimizing API Dependency for Hallucination\n  Reduction in Closed-Book Question Answering","summary":"  While Large Language Models (LLM) are able to accumulate and restore\nknowledge, they are still prone to hallucination. Especially when faced with\nfactual questions, LLM cannot only rely on knowledge stored in parameters to\nguarantee truthful and correct answers. Augmenting these models with the\nability to search on external information sources, such as the web, is a\npromising approach to ground knowledge to retrieve information. However,\nsearching in a large collection of documents introduces additional\ncomputational/time costs. An optimal behavior would be to query external\nresources only when the LLM is not confident about answers. In this paper, we\npropose a new LLM able to self-estimate if it is able to answer directly or\nneeds to request an external tool. We investigate a supervised approach by\nintroducing a hallucination masking mechanism in which labels are generated\nusing a close book question-answering task. In addition, we propose to leverage\nparameter-efficient fine-tuning techniques to train our model on a small amount\nof data. Our model directly provides answers for $78.2\\%$ of the known queries\nand opts to search for $77.2\\%$ of the unknown ones. This results in the API\nbeing utilized only $62\\%$ of the time.\n","authors":["Pierre Erbacher","Louis Falissar","Vincent Guigue","Laure Soulier"],"pdf_url":"https://arxiv.org/pdf/2401.01780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01761v1","updated":"2024-01-03T14:28:55Z","published":"2024-01-03T14:28:55Z","title":"Cross-target Stance Detection by Exploiting Target Analytical\n  Perspectives","summary":"  Cross-target stance detection (CTSD) is an important task, which infers the\nattitude of the destination target by utilizing annotated data derived from the\nsource target. One important approach in CTSD is to extract domain-invariant\nfeatures to bridge the knowledge gap between multiple targets. However, the\nanalysis of informal and short text structure, and implicit expressions,\ncomplicate the extraction of domain-invariant knowledge. In this paper, we\npropose a Multi-Perspective Prompt-Tuning (MPPT) model for CTSD that uses the\nanalysis perspective as a bridge to transfer knowledge. First, we develop a\ntwo-stage instruct-based chain-of-thought method (TsCoT) to elicit target\nanalysis perspectives and provide natural language explanations (NLEs) from\nmultiple viewpoints by formulating instructions based on large language model\n(LLM). Second, we propose a multi-perspective prompt-tuning framework\n(MultiPLN) to fuse the NLEs into the stance predictor. Extensive experiments\nresults demonstrate the superiority of MPPT against the state-of-the-art\nbaseline methods.\n","authors":["Daijun Ding","Rong Chen","Bowen Zhang","Xu Huang","Li Dong","Xiaowen Zhao","Ge Song","Liwen Jing"],"pdf_url":"https://arxiv.org/pdf/2401.01761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01759v1","updated":"2024-01-03T14:24:02Z","published":"2024-01-03T14:24:02Z","title":"VGA: Vision and Graph Fused Attention Network for Rumor Detection","summary":"  With the development of social media, rumors have been spread broadly on\nsocial media platforms, causing great harm to society. Beside textual\ninformation, many rumors also use manipulated images or conceal textual\ninformation within images to deceive people and avoid being detected, making\nmultimodal rumor detection be a critical problem. The majority of multimodal\nrumor detection methods mainly concentrate on extracting features of source\nclaims and their corresponding images, while ignoring the comments of rumors\nand their propagation structures. These comments and structures imply the\nwisdom of crowds and are proved to be crucial to debunk rumors. Moreover, these\nmethods usually only extract visual features in a basic manner, seldom consider\ntampering or textual information in images. Therefore, in this study, we\npropose a novel Vision and Graph Fused Attention Network (VGA) for rumor\ndetection to utilize propagation structures among posts so as to obtain the\ncrowd opinions and further explore visual tampering features, as well as the\ntextual information hidden in images. We conduct extensive experiments on three\ndatasets, demonstrating that VGA can effectively detect multimodal rumors and\noutperform state-of-the-art methods significantly.\n","authors":["Lin Bai","Caiyan Jia","Ziying Song","Chaoqun Cui"],"pdf_url":"https://arxiv.org/pdf/2401.01759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01283v2","updated":"2024-01-03T14:01:49Z","published":"2024-01-02T16:51:17Z","title":"Quality and Quantity of Machine Translation References for Automated\n  Metrics","summary":"  Automatic machine translation metrics often use human translations to\ndetermine the quality system translations. Common wisdom in the field dictates\nthat the human references should be of very high quality. However, there are no\ncost-benefit analyses that could be used to guide practitioners who plan to\ncollect references for machine translation evaluation. We find that\nhigher-quality references lead to better metric correlations with humans at the\nsegment-level. Having up to 7 references per segment and taking their average\nhelps all metrics. Interestingly, the references from vendors of different\nqualities can be mixed together and improve metric success. Higher quality\nreferences, however, cost more to create and we frame this as an optimization\nproblem: given a specific budget, what references should be collected to\nmaximize metric success. These findings can be used by evaluators of shared\ntasks when references need to be created under a certain budget.\n","authors":["Vilém Zouhar","Ondřej Bojar"],"pdf_url":"https://arxiv.org/pdf/2401.01283v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07913v3","updated":"2024-01-03T13:29:43Z","published":"2023-12-13T06:11:42Z","title":"A Survey of Text Watermarking in the Era of Large Language Models","summary":"  Text watermarking algorithms play a crucial role in the copyright protection\nof textual content, yet their capabilities and application scenarios have been\nlimited historically. The recent developments in large language models (LLMs)\nhave opened new opportunities for the advancement of text watermarking\ntechniques. LLMs not only enhance the capabilities of text watermarking\nalgorithms through their text understanding and generation abilities but also\nnecessitate the use of text watermarking algorithms for their own copyright\nprotection. This paper conducts a comprehensive survey of the current state of\ntext watermarking technology, covering four main aspects: (1) an overview and\ncomparison of different text watermarking techniques; (2) evaluation methods\nfor text watermarking algorithms, including their success rates, impact on text\nquality, robustness, and unforgeability; (3) potential application scenarios\nfor text watermarking technology; (4) current challenges and future directions\nfor development. This survey aims to provide researchers with a thorough\nunderstanding of text watermarking technology, thereby promoting its further\nadvancement.\n","authors":["Aiwei Liu","Leyi Pan","Yijian Lu","Jingjing Li","Xuming Hu","Lijie Wen","Irwin King","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2312.07913v3.pdf","comment":"39 pages, 7 figures"},{"id":"http://arxiv.org/abs/2310.19923v3","updated":"2024-01-03T13:26:41Z","published":"2023-10-30T18:35:30Z","title":"Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long\n  Documents","summary":"  Text embedding models have emerged as powerful tools for transforming\nsentences into fixed-sized feature vectors that encapsulate semantic\ninformation. While these models are essential for tasks like information\nretrieval, semantic clustering, and text re-ranking, most existing open-source\nmodels, especially those built on architectures like BERT, struggle to\nrepresent lengthy documents and often resort to truncation. One common approach\nto mitigate this challenge involves splitting documents into smaller paragraphs\nfor embedding. However, this strategy results in a much larger set of vectors,\nconsequently leading to increased memory consumption and computationally\nintensive vector searches with elevated latency.\n  To address these challenges, we introduce Jina Embeddings 2, an open-source\ntext embedding model capable of accommodating up to 8192 tokens. This model is\ndesigned to transcend the conventional 512-token limit and adeptly process long\ndocuments. Jina Embeddings 2 not only achieves state-of-the-art performance on\na range of embedding-related tasks in the MTEB benchmark but also matches the\nperformance of OpenAI's proprietary ada-002 model. Additionally, our\nexperiments indicate that an extended context can enhance performance in tasks\nsuch as NarrativeQA.\n","authors":["Michael Günther","Jackmin Ong","Isabelle Mohr","Alaeddine Abdessalem","Tanguy Abel","Mohammad Kalim Akram","Susana Guzman","Georgios Mastrapas","Saba Sturua","Bo Wang","Maximilian Werk","Nan Wang","Han Xiao"],"pdf_url":"https://arxiv.org/pdf/2310.19923v3.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2401.01711v1","updated":"2024-01-03T12:28:33Z","published":"2024-01-03T12:28:33Z","title":"Evaluating Large Language Models in Semantic Parsing for Conversational\n  Question Answering over Knowledge Graphs","summary":"  Conversational question answering systems often rely on semantic parsing to\nenable interactive information retrieval, which involves the generation of\nstructured database queries from a natural language input. For\ninformation-seeking conversations about facts stored within a knowledge graph,\ndialogue utterances are transformed into graph queries in a process that is\ncalled knowledge-based conversational question answering. This paper evaluates\nthe performance of large language models that have not been explicitly\npre-trained on this task. Through a series of experiments on an extensive\nbenchmark dataset, we compare models of varying sizes with different prompting\ntechniques and identify common issue types in the generated output. Our results\ndemonstrate that large language models are capable of generating graph queries\nfrom dialogues, with significant improvements achievable through few-shot\nprompting and fine-tuning techniques, especially for smaller models that\nexhibit lower zero-shot performance.\n","authors":["Phillip Schneider","Manuel Klettner","Kristiina Jokinen","Elena Simperl","Florian Matthes"],"pdf_url":"https://arxiv.org/pdf/2401.01711v1.pdf","comment":"Accepted to ICAART 2024"},{"id":"http://arxiv.org/abs/2312.06281v2","updated":"2024-01-03T12:20:35Z","published":"2023-12-11T10:35:32Z","title":"EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models","summary":"  We introduce EQ-Bench, a novel benchmark designed to evaluate aspects of\nemotional intelligence in Large Language Models (LLMs). We assess the ability\nof LLMs to understand complex emotions and social interactions by asking them\nto predict the intensity of emotional states of characters in a dialogue. The\nbenchmark is able to discriminate effectively between a wide range of models.\nWe find that EQ-Bench correlates strongly with comprehensive multi-domain\nbenchmarks like MMLU (Hendrycks et al., 2020) (r=0.97), indicating that we may\nbe capturing similar aspects of broad intelligence. Our benchmark produces\nhighly repeatable results using a set of 60 English-language questions. We also\nprovide open-source code for an automated benchmarking pipeline at\nhttps://github.com/EQ-bench/EQ-Bench and a leaderboard at https://eqbench.com\n","authors":["Samuel J. Paech"],"pdf_url":"https://arxiv.org/pdf/2312.06281v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01699v1","updated":"2024-01-03T12:06:02Z","published":"2024-01-03T12:06:02Z","title":"WordArt Designer API: User-Driven Artistic Typography Synthesis with\n  Large Language Models on ModelScope","summary":"  This paper introduces the WordArt Designer API, a novel framework for\nuser-driven artistic typography synthesis utilizing Large Language Models\n(LLMs) on ModelScope. We address the challenge of simplifying artistic\ntypography for non-professionals by offering a dynamic, adaptive, and\ncomputationally efficient alternative to traditional rigid templates. Our\napproach leverages the power of LLMs to understand and interpret user input,\nfacilitating a more intuitive design process. We demonstrate through various\ncase studies how users can articulate their aesthetic preferences and\nfunctional requirements, which the system then translates into unique and\ncreative typographic designs. Our evaluations indicate significant improvements\nin user satisfaction, design flexibility, and creative expression over existing\nsystems. The WordArt Designer API not only democratizes the art of typography\nbut also opens up new possibilities for personalized digital communication and\ndesign.\n","authors":["Jun-Yan He","Zhi-Qi Cheng","Chenyang Li","Jingdong Sun","Wangmeng Xiang","Yusen Hu","Xianhui Lin","Xiaoyang Kang","Zengke Jin","Bin Luo","Yifeng Geng","Xuansong Xie","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2401.01699v1.pdf","comment":"Spotlight Paper from the Workshop on Machine Learning for Creativity\n  and Design at the 37th Conference on Neural Information Processing Systems\n  (NeurIPS 2023). 5 pages, 5 figures"},{"id":"http://arxiv.org/abs/2401.01698v1","updated":"2024-01-03T12:05:38Z","published":"2024-01-03T12:05:38Z","title":"Patterns of Persistence and Diffusibility across World's Languages","summary":"  Language similarities can be caused by genetic relatedness, areal contact,\nuniversality, or chance. Colexification, i.e.~a type of similarity where a\nsingle lexical form is used to convey multiple meanings, is underexplored. In\nour work, we shed light on the linguistic causes of cross-lingual similarity in\ncolexification and phonology, by exploring genealogical stability (persistence)\nand contact-induced change (diffusibility). We construct large-scale graphs\nincorporating semantic, genealogical, phonological and geographical data for\n1,966 languages. We then show the potential of this resource, by investigating\nseveral established hypotheses from previous work in linguistics, while\nproposing new ones. Our results strongly support a previously established\nhypothesis in the linguistic literature, while offering contradicting evidence\nto another. Our large scale resource opens for further research across\ndisciplines, e.g.~in multilingual NLP and comparative linguistics.\n","authors":["Yiyi Chen","Johannes Bjerva"],"pdf_url":"https://arxiv.org/pdf/2401.01698v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2310.06452v2","updated":"2024-01-03T11:58:42Z","published":"2023-10-10T09:25:44Z","title":"Understanding the Effects of RLHF on LLM Generalisation and Diversity","summary":"  Large language models (LLMs) fine-tuned with reinforcement learning from\nhuman feedback (RLHF) have been used in some of the most widely deployed AI\nmodels to date, such as OpenAI's ChatGPT or Anthropic's Claude. % , or Meta's\nLLaMA-2. While there has been significant work developing these methods, our\nunderstanding of the benefits and downsides of each stage in RLHF is still\nlimited. To fill this gap, we present an extensive analysis of how each stage\nof the process (i.e.~supervised fine-tuning (SFT), reward modelling, and RLHF)\naffects two key properties: out-of-distribution (OOD) generalisation and output\ndiversity. OOD generalisation is crucial given the wide range of real-world\nscenarios in which these models are being used, while output diversity refers\nto the model's ability to generate varied outputs and is important for a\nvariety of use cases. We perform our analysis across two base models on both\nsummarisation and instruction following tasks, the latter being highly relevant\nfor current LLM use cases. We find that RLHF generalises better than SFT to new\ninputs, particularly as the distribution shift between train and test becomes\nlarger. However, RLHF significantly reduces output diversity compared to SFT\nacross a variety of measures, implying a tradeoff in current LLM fine-tuning\nmethods between generalisation and diversity. Our results provide guidance on\nwhich fine-tuning method should be used depending on the application, and show\nthat more research is needed to improve the tradeoff between generalisation and\ndiversity.\n","authors":["Robert Kirk","Ishita Mediratta","Christoforos Nalmpantis","Jelena Luketina","Eric Hambro","Edward Grefenstette","Roberta Raileanu"],"pdf_url":"https://arxiv.org/pdf/2310.06452v2.pdf","comment":"Code available here: https://github.com/facebookresearch/rlfh-gen-div"},{"id":"http://arxiv.org/abs/2401.01692v1","updated":"2024-01-03T11:54:30Z","published":"2024-01-03T11:54:30Z","title":"Predicting challenge moments from students' discourse: A comparison of\n  GPT-4 to two traditional natural language processing approaches","summary":"  Effective collaboration requires groups to strategically regulate themselves\nto overcome challenges. Research has shown that groups may fail to regulate due\nto differences in members' perceptions of challenges which may benefit from\nexternal support. In this study, we investigated the potential of leveraging\nthree distinct natural language processing models: an expert knowledge\nrule-based model, a supervised machine learning (ML) model and a Large Language\nmodel (LLM), in challenge detection and challenge dimension identification\n(cognitive, metacognitive, emotional and technical/other challenges) from\nstudent discourse, was investigated. The results show that the supervised ML\nand the LLM approaches performed considerably well in both tasks, in contrast\nto the rule-based approach, whose efficacy heavily relies on the engineered\nfeatures by experts. The paper provides an extensive discussion of the three\napproaches' performance for automated detection and support of students'\nchallenge moments in collaborative learning activities. It argues that,\nalthough LLMs provide many advantages, they are unlikely to be the panacea to\nissues of the detection and feedback provision of socially shared regulation of\nlearning due to their lack of reliability, as well as issues of validity\nevaluation, privacy and confabulation. We conclude the paper with a discussion\non additional considerations, including model transparency to explore feasible\nand meaningful analytical feedback for students and educators using LLMs.\n","authors":["Wannapon Suraworachet","Jennifer Seon","Mutlu Cukurova"],"pdf_url":"https://arxiv.org/pdf/2401.01692v1.pdf","comment":"13 pages, 1 figure"},{"id":"http://arxiv.org/abs/2401.01078v2","updated":"2024-01-03T11:54:14Z","published":"2024-01-02T07:46:34Z","title":"Vietnamese Poem Generation & The Prospect Of Cross-Language Poem-To-Poem\n  Translation","summary":"  Poetry generation has been a challenging task in the field of Natural\nLanguage Processing, as it requires the model to understand the nuances of\nlanguage, sentiment, and style. In this paper, we propose using Large Language\nModels to generate Vietnamese poems from natural language prompts, thereby\nfacilitating an intuitive process with enhanced content control. Our most\nefficacious model, the GPT-3 Babbage variant, achieves a custom evaluation\nscore of 0.8, specifically tailored to the \"luc bat\" genre of Vietnamese\npoetry. Furthermore, we also explore the idea of paraphrasing poems into normal\ntext prompts and yield a relatively high score of 0.718 in the \"luc bat\" genre.\nThis experiment presents the potential for cross-Language poem-to-poem\ntranslation with translated poems as the inputs while concurrently maintaining\ncomplete control over the generated content.\n","authors":["Triet Minh Huynh","Quan Le Bao"],"pdf_url":"https://arxiv.org/pdf/2401.01078v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01667v1","updated":"2024-01-03T11:06:01Z","published":"2024-01-03T11:06:01Z","title":"MLPs Compass: What is learned when MLPs are combined with PLMs?","summary":"  While Transformer-based pre-trained language models and their variants\nexhibit strong semantic representation capabilities, the question of\ncomprehending the information gain derived from the additional components of\nPLMs remains an open question in this field. Motivated by recent efforts that\nprove Multilayer-Perceptrons (MLPs) modules achieving robust structural capture\ncapabilities, even outperforming Graph Neural Networks (GNNs), this paper aims\nto quantify whether simple MLPs can further enhance the already potent ability\nof PLMs to capture linguistic information. Specifically, we design a simple yet\neffective probing framework containing MLPs components based on BERT structure\nand conduct extensive experiments encompassing 10 probing tasks spanning three\ndistinct linguistic levels. The experimental results demonstrate that MLPs can\nindeed enhance the comprehension of linguistic structure by PLMs. Our research\nprovides interpretable and valuable insights into crafting variations of PLMs\nutilizing MLPs for tasks that emphasize diverse linguistic structures.\n","authors":["Li Zhou","Wenyu Chen","Yong Cao","Dingyi Zeng","Wanlong Liu","Hong Qu"],"pdf_url":"https://arxiv.org/pdf/2401.01667v1.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2401.01637v1","updated":"2024-01-03T09:27:01Z","published":"2024-01-03T09:27:01Z","title":"Social Media Ready Caption Generation for Brands","summary":"  Social media advertisements are key for brand marketing, aiming to attract\nconsumers with captivating captions and pictures or logos. While previous\nresearch has focused on generating captions for general images, incorporating\nbrand personalities into social media captioning remains unexplored. Brand\npersonalities are shown to be affecting consumers' behaviours and social\ninteractions and thus are proven to be a key aspect of marketing strategies.\nCurrent open-source multimodal LLMs are not directly suited for this task.\nHence, we propose a pipeline solution to assist brands in creating engaging\nsocial media captions that align with the image and the brand personalities.\nOur architecture is based on two parts: a the first part contains an image\ncaptioning model that takes in an image that the brand wants to post online and\ngives a plain English caption; b the second part takes in the generated caption\nalong with the target brand personality and outputs a catchy\npersonality-aligned social media caption. Along with brand personality, our\nsystem also gives users the flexibility to provide hashtags, Instagram handles,\nURLs, and named entities they want the caption to contain, making the captions\nmore semantically related to the social media handles. Comparative evaluations\nagainst various baselines demonstrate the effectiveness of our approach, both\nqualitatively and quantitatively.\n","authors":["Himanshu Maheshwari","Koustava Goswami","Apoorv Saxena","Balaji Vasan Srinivasan"],"pdf_url":"https://arxiv.org/pdf/2401.01637v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01623v1","updated":"2024-01-03T08:49:12Z","published":"2024-01-03T08:49:12Z","title":"Can AI Be as Creative as Humans?","summary":"  Creativity serves as a cornerstone for societal progress and innovation, but\nits assessment remains a complex and often subjective endeavor. With the rise\nof advanced generative AI models capable of tasks once reserved for human\ncreativity, the study of AI's creative potential becomes imperative for its\nresponsible development and application. This paper addresses the complexities\nin defining and evaluating creativity by introducing a new concept called\nRelative Creativity. Instead of trying to define creativity universally, we\nshift the focus to whether AI can match the creative abilities of a\nhypothetical human. This perspective draws inspiration from the Turing Test,\nexpanding upon it to address the challenges and subjectivities inherent in\nevaluating creativity. This methodological shift facilitates a statistically\nquantifiable evaluation of AI's creativity, which we term Statistical\nCreativity. This approach allows for direct comparisons of AI's creative\nabilities with those of specific human groups. Building on this foundation, we\ndiscuss the application of statistical creativity in contemporary\nprompt-conditioned autoregressive models. In addition to defining and analyzing\na measure of creativity, we introduce an actionable training guideline,\neffectively bridging the gap between theoretical quantification of creativity\nand practical model training. Through these multifaceted contributions, the\npaper establishes a cohesive, continuously evolving, and transformative\nframework for assessing and fostering statistical creativity in AI models.\n","authors":["Haonan Wang","James Zou","Michael Mozer","Linjun Zhang","Anirudh Goyal","Alex Lamb","Zhun Deng","Michael Qizhe Xie","Hannah Brown","Kenji Kawaguchi"],"pdf_url":"https://arxiv.org/pdf/2401.01623v1.pdf","comment":"The paper presents \"Relative Creativity,\" comparing AI creativity to\n  human creativity, inspired by the Turing Test. It introduces \"Statistical\n  Creativity\" for measurable assessment and provides AI training guidelines to\n  foster AI's creative capabilities"},{"id":"http://arxiv.org/abs/2401.01620v1","updated":"2024-01-03T08:41:27Z","published":"2024-01-03T08:41:27Z","title":"Large Language Model Capabilities in Perioperative Risk Prediction and\n  Prognostication","summary":"  We investigate whether general-domain large language models such as GPT-4\nTurbo can perform risk stratification and predict post-operative outcome\nmeasures using a description of the procedure and a patient's clinical notes\nderived from the electronic health record. We examine predictive performance on\n8 different tasks: prediction of ASA Physical Status Classification, hospital\nadmission, ICU admission, unplanned admission, hospital mortality, PACU Phase 1\nduration, hospital duration, and ICU duration. Few-shot and chain-of-thought\nprompting improves predictive performance for several of the tasks. We achieve\nF1 scores of 0.50 for ASA Physical Status Classification, 0.81 for ICU\nadmission, and 0.86 for hospital mortality. Performance on duration prediction\ntasks were universally poor across all prompt strategies. Current generation\nlarge language models can assist clinicians in perioperative risk\nstratification on classification tasks and produce high-quality natural\nlanguage summaries and explanations.\n","authors":["Philip Chung","Christine T Fong","Andrew M Walters","Nima Aghaeepour","Meliha Yetisgen","Vikas N O'Reilly-Shah"],"pdf_url":"https://arxiv.org/pdf/2401.01620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01614v1","updated":"2024-01-03T08:33:09Z","published":"2024-01-03T08:33:09Z","title":"GPT-4V(ision) is a Generalist Web Agent, if Grounded","summary":"  The recent development on large multimodal models (LMMs), especially\nGPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries\nof multimodal models beyond traditional tasks like image captioning and visual\nquestion answering. In this work, we explore the potential of LMMs like GPT-4V\nas a generalist web agent that can follow natural language instructions to\ncomplete tasks on any given website. We propose SEEACT, a generalist web agent\nthat harnesses the power of LMMs for integrated visual understanding and acting\non the web. We evaluate on the recent MIND2WEB benchmark. In addition to\nstandard offline evaluation on cached websites, we enable a new online\nevaluation setting by developing a tool that allows running web agents on live\nwebsites. We show that GPT-4V presents a great potential for web agents - it\ncan successfully complete 50% of the tasks on live websites if we manually\nground its textual plans into actions on the websites. This substantially\noutperforms text-only LLMs like GPT-4 or smaller models (FLAN-T5 and BLIP-2)\nspecifically fine-tuned for web agents. However, grounding still remains a\nmajor challenge. Existing LMM grounding strategies like set-of-mark prompting\nturns out not effective for web agents, and the best grounding strategy we\ndevelop in this paper leverages both the HTML text and visuals. Yet, there is\nstill a substantial gap with oracle grounding, leaving ample room for further\nimprovement.\n","authors":["Boyuan Zheng","Boyu Gou","Jihyung Kil","Huan Sun","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2401.01614v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01262v2","updated":"2024-01-03T08:17:53Z","published":"2024-01-02T16:09:36Z","title":"Fairness Certification for Natural Language Processing and Large\n  Language Models","summary":"  Natural Language Processing (NLP) plays an important role in our daily lives,\nparticularly due to the enormous progress of Large Language Models (LLM).\nHowever, NLP has many fairness-critical use cases, e.g., as an expert system in\nrecruitment or as an LLM-based tutor in education. Since NLP is based on human\nlanguage, potentially harmful biases can diffuse into NLP systems and produce\nunfair results, discriminate against minorities or generate legal issues.\nHence, it is important to develop a fairness certification for NLP approaches.\nWe follow a qualitative research approach towards a fairness certification for\nNLP. In particular, we have reviewed a large body of literature on algorithmic\nfairness, and we have conducted semi-structured expert interviews with a wide\nrange of experts from that area. We have systematically devised six fairness\ncriteria for NLP, which can be further refined into 18 sub-categories. Our\ncriteria offer a foundation for operationalizing and testing processes to\ncertify fairness, both from the perspective of the auditor and the audited\norganization.\n","authors":["Vincent Freiberger","Erik Buchmann"],"pdf_url":"https://arxiv.org/pdf/2401.01262v2.pdf","comment":"In depth discussion of our results can be found in the Appendix B"},{"id":"http://arxiv.org/abs/2401.01600v1","updated":"2024-01-03T08:06:26Z","published":"2024-01-03T08:06:26Z","title":"PLLaMa: An Open-source Large Language Model for Plant Science","summary":"  Large Language Models (LLMs) have exhibited remarkable capabilities in\nunderstanding and interacting with natural language across various sectors.\nHowever, their effectiveness is limited in specialized areas requiring high\naccuracy, such as plant science, due to a lack of specific expertise in these\nfields. This paper introduces PLLaMa, an open-source language model that\nevolved from LLaMa-2. It's enhanced with a comprehensive database, comprising\nmore than 1.5 million scholarly articles in plant science. This development\nsignificantly enriches PLLaMa with extensive knowledge and proficiency in plant\nand agricultural sciences. Our initial tests, involving specific datasets\nrelated to plants and agriculture, show that PLLaMa substantially improves its\nunderstanding of plant science-related topics. Moreover, we have formed an\ninternational panel of professionals, including plant scientists, agricultural\nengineers, and plant breeders. This team plays a crucial role in verifying the\naccuracy of PLLaMa's responses to various academic inquiries, ensuring its\neffective and reliable application in the field. To support further research\nand development, we have made the model's checkpoints and source codes\naccessible to the scientific community. These resources are available for\ndownload at \\url{https://github.com/Xianjun-Yang/PLLaMa}.\n","authors":["Xianjun Yang","Junfeng Gao","Wenxin Xue","Erik Alexandersson"],"pdf_url":"https://arxiv.org/pdf/2401.01600v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2401.01596v1","updated":"2024-01-03T07:58:25Z","published":"2024-01-03T07:58:25Z","title":"MedSumm: A Multimodal Approach to Summarizing Code-Mixed Hindi-English\n  Clinical Queries","summary":"  In the healthcare domain, summarizing medical questions posed by patients is\ncritical for improving doctor-patient interactions and medical decision-making.\nAlthough medical data has grown in complexity and quantity, the current body of\nresearch in this domain has primarily concentrated on text-based methods,\noverlooking the integration of visual cues. Also prior works in the area of\nmedical question summarisation have been limited to the English language. This\nwork introduces the task of multimodal medical question summarization for\ncodemixed input in a low-resource setting. To address this gap, we introduce\nthe Multimodal Medical Codemixed Question Summarization MMCQS dataset, which\ncombines Hindi-English codemixed medical queries with visual aids. This\nintegration enriches the representation of a patient's medical condition,\nproviding a more comprehensive perspective. We also propose a framework named\nMedSumm that leverages the power of LLMs and VLMs for this task. By utilizing\nour MMCQS dataset, we demonstrate the value of integrating visual information\nfrom images to improve the creation of medically detailed summaries. This\nmultimodal strategy not only improves healthcare decision-making but also\npromotes a deeper comprehension of patient queries, paving the way for future\nexploration in personalized and responsive medical care. Our dataset, code, and\npre-trained models will be made publicly available.\n","authors":["Akash Ghosh","Arkadeep Acharya","Prince Jha","Aniket Gaudgaul","Rajdeep Majumdar","Sriparna Saha","Aman Chadha","Raghav Jain","Setu Sinha","Shivani Agarwal"],"pdf_url":"https://arxiv.org/pdf/2401.01596v1.pdf","comment":"ECIR 2024"},{"id":"http://arxiv.org/abs/2401.01572v1","updated":"2024-01-03T06:56:56Z","published":"2024-01-03T06:56:56Z","title":"Hallucinations in Neural Automatic Speech Recognition: Identifying\n  Errors and Hallucinatory Models","summary":"  Hallucinations are a type of output error produced by deep neural networks.\nWhile this has been studied in natural language processing, they have not been\nresearched previously in automatic speech recognition. Here, we define\nhallucinations in ASR as transcriptions generated by a model that are\nsemantically unrelated to the source utterance, yet still fluent and coherent.\nThe similarity of hallucinations to probable natural language outputs of the\nmodel creates a danger of deception and impacts the credibility of the system.\nWe show that commonly used metrics, such as word error rates, cannot\ndifferentiate between hallucinatory and non-hallucinatory models. To address\nthis, we propose a perturbation-based method for assessing the susceptibility\nof an automatic speech recognition (ASR) model to hallucination at test time,\nwhich does not require access to the training dataset. We demonstrate that this\nmethod helps to distinguish between hallucinatory and non-hallucinatory models\nthat have similar baseline word error rates. We further explore the\nrelationship between the types of ASR errors and the types of dataset noise to\ndetermine what types of noise are most likely to create hallucinatory outputs.\nWe devise a framework for identifying hallucinations by analysing their\nsemantic connection with the ground truth and their fluency. Finally, we\ndiscover how to induce hallucinations with a random noise injection to the\nutterance.\n","authors":["Rita Frieske","Bertram E. Shi"],"pdf_url":"https://arxiv.org/pdf/2401.01572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04444v3","updated":"2024-01-03T06:38:36Z","published":"2023-10-02T22:35:40Z","title":"What's the Magic Word? A Control Theory of LLM Prompting","summary":"  Prompt engineering is crucial for deploying LLMs but is poorly understood\nmathematically. We formalize LLM systems as a class of discrete stochastic\ndynamical systems to explore prompt engineering through the lens of control\ntheory. We investigate the reachable set of output token sequences $R_y(\\mathbf\nx_0)$ for which there exists a control input sequence $\\mathbf u$ for each\n$\\mathbf y \\in R_y(\\mathbf x_0)$ that steers the LLM to output $\\mathbf y$ from\ninitial state sequence $\\mathbf x_0$. We offer analytic analysis on the\nlimitations on the controllability of self-attention in terms of reachable set,\nwhere we prove an upper bound on the reachable set of outputs $R_y(\\mathbf\nx_0)$ as a function of the singular values of the parameter matrices. We\npresent complementary empirical analysis on the controllability of a panel of\nLLMs, including Falcon-7b, Llama-7b, and Falcon-40b. Our results demonstrate a\nlower bound on the reachable set of outputs $R_y(\\mathbf x_0)$ w.r.t. initial\nstate sequences $\\mathbf x_0$ sampled from the Wikitext dataset. We find that\nthe correct next Wikitext token following sequence $\\mathbf x_0$ is reachable\nover 97% of the time with prompts of $k\\leq 10$ tokens. We also establish that\nthe top 75 most likely next tokens, as estimated by the LLM itself, are\nreachable at least 85% of the time with prompts of $k\\leq 10$ tokens.\nIntriguingly, short prompt sequences can dramatically alter the likelihood of\nspecific outputs, even making the least likely tokens become the most likely\nones. This control-centric analysis of LLMs demonstrates the significant and\npoorly understood role of input sequences in steering output probabilities,\noffering a foundational perspective for enhancing language model system\ncapabilities.\n","authors":["Aman Bhargava","Cameron Witkowski","Manav Shah","Matt Thomson"],"pdf_url":"https://arxiv.org/pdf/2310.04444v3.pdf","comment":"23 pages, 8 figures. Under review for ICLR 2024"},{"id":"http://arxiv.org/abs/2306.17408v3","updated":"2024-01-03T05:00:00Z","published":"2023-06-30T05:50:26Z","title":"LMBot: Distilling Graph Knowledge into Language Model for Graph-less\n  Deployment in Twitter Bot Detection","summary":"  As malicious actors employ increasingly advanced and widespread bots to\ndisseminate misinformation and manipulate public opinion, the detection of\nTwitter bots has become a crucial task. Though graph-based Twitter bot\ndetection methods achieve state-of-the-art performance, we find that their\ninference depends on the neighbor users multi-hop away from the targets, and\nfetching neighbors is time-consuming and may introduce bias. At the same time,\nwe find that after finetuning on Twitter bot detection, pretrained language\nmodels achieve competitive performance and do not require a graph structure\nduring deployment. Inspired by this finding, we propose a novel bot detection\nframework LMBot that distills the knowledge of graph neural networks (GNNs)\ninto language models (LMs) for graph-less deployment in Twitter bot detection\nto combat the challenge of data dependency. Moreover, LMBot is compatible with\ngraph-based and graph-less datasets. Specifically, we first represent each user\nas a textual sequence and feed them into the LM for domain adaptation. For\ngraph-based datasets, the output of LMs provides input features for the GNN,\nenabling it to optimize for bot detection and distill knowledge back to the LM\nin an iterative, mutually enhancing process. Armed with the LM, we can perform\ngraph-less inference, which resolves the graph data dependency and sampling\nbias issues. For datasets without graph structure, we simply replace the GNN\nwith an MLP, which has also shown strong performance. Our experiments\ndemonstrate that LMBot achieves state-of-the-art performance on four Twitter\nbot detection benchmarks. Extensive studies also show that LMBot is more\nrobust, versatile, and efficient compared to graph-based Twitter bot detection\nmethods.\n","authors":["Zijian Cai","Zhaoxuan Tan","Zhenyu Lei","Zifeng Zhu","Hongrui Wang","Qinghua Zheng","Minnan Luo"],"pdf_url":"https://arxiv.org/pdf/2306.17408v3.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2305.11348v2","updated":"2024-01-03T04:00:15Z","published":"2023-05-18T23:47:00Z","title":"In the Name of Fairness: Assessing the Bias in Clinical Record\n  De-identification","summary":"  Data sharing is crucial for open science and reproducible research, but the\nlegal sharing of clinical data requires the removal of protected health\ninformation from electronic health records. This process, known as\nde-identification, is often achieved through the use of machine learning\nalgorithms by many commercial and open-source systems. While these systems have\nshown compelling results on average, the variation in their performance across\ndifferent demographic groups has not been thoroughly examined. In this work, we\ninvestigate the bias of de-identification systems on names in clinical notes\nvia a large-scale empirical analysis. To achieve this, we create 16 name sets\nthat vary along four demographic dimensions: gender, race, name popularity, and\nthe decade of popularity. We insert these names into 100 manually curated\nclinical templates and evaluate the performance of nine public and private\nde-identification methods. Our findings reveal that there are statistically\nsignificant performance gaps along a majority of the demographic dimensions in\nmost methods. We further illustrate that de-identification quality is affected\nby polysemy in names, gender context, and clinical note characteristics. To\nmitigate the identified gaps, we propose a simple and method-agnostic solution\nby fine-tuning de-identification methods with clinical context and diverse\nnames. Overall, it is imperative to address the bias in existing methods\nimmediately so that downstream stakeholders can build high-quality systems to\nserve all demographic parties fairly.\n","authors":["Yuxin Xiao","Shulammite Lim","Tom Joseph Pollard","Marzyeh Ghassemi"],"pdf_url":"https://arxiv.org/pdf/2305.11348v2.pdf","comment":"Accepted by FAccT 2023; updated appendix with the de-identification\n  performance of GPT-4"},{"id":"http://arxiv.org/abs/2401.01523v1","updated":"2024-01-03T03:28:55Z","published":"2024-01-03T03:28:55Z","title":"GOAT-Bench: Safety Insights to Large Multimodal Models through\n  Meme-Based Social Abuse","summary":"  The exponential growth of social media has profoundly transformed how\ninformation is created, disseminated, and absorbed, exceeding any precedent in\nthe digital age. Regrettably, this explosion has also spawned a significant\nincrease in the online abuse of memes. Evaluating the negative impact of memes\nis notably challenging, owing to their often subtle and implicit meanings,\nwhich are not directly conveyed through the overt text and imagery. In light of\nthis, large multimodal models (LMMs) have emerged as a focal point of interest\ndue to their remarkable capabilities in handling diverse multimodal tasks. In\nresponse to this development, our paper aims to thoroughly examine the capacity\nof various LMMs (e.g. GPT-4V) to discern and respond to the nuanced aspects of\nsocial abuse manifested in memes. We introduce the comprehensive meme\nbenchmark, GOAT-Bench, comprising over 6K varied memes encapsulating themes\nsuch as implicit hate speech, sexism, and cyberbullying, etc. Utilizing\nGOAT-Bench, we delve into the ability of LMMs to accurately assess hatefulness,\nmisogyny, offensiveness, sarcasm, and harmful content. Our extensive\nexperiments across a range of LMMs reveal that current models still exhibit a\ndeficiency in safety awareness, showing insensitivity to various forms of\nimplicit abuse. We posit that this shortfall represents a critical impediment\nto the realization of safe artificial intelligence. The GOAT-Bench and\naccompanying resources are publicly accessible at https://goatlmm.github.io/,\ncontributing to ongoing research in this vital field.\n","authors":["Hongzhan Lin","Ziyang Luo","Bo Wang","Ruichao Yang","Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2401.01523v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2401.01076v2","updated":"2024-01-03T02:13:29Z","published":"2024-01-02T07:40:12Z","title":"DialCLIP: Empowering CLIP as Multi-Modal Dialog Retriever","summary":"  Recently, substantial advancements in pre-trained vision-language models have\ngreatly enhanced the capabilities of multi-modal dialog systems. These models\nhave demonstrated significant improvements by fine-tuning on downstream tasks.\nHowever, the existing pre-trained models primarily focus on effectively\ncapturing the alignment between vision and language modalities, often ignoring\nthe intricate nature of dialog context. In this paper, we propose a\nparameter-efficient prompt-tuning method named DialCLIP for multi-modal dialog\nretrieval. Specifically, our approach introduces a multi-modal context prompt\ngenerator to learn context features which are subsequently distilled into\nprompts within the pre-trained vision-language model CLIP. Besides, we\nintroduce domain prompt to mitigate the disc repancy from the downstream dialog\ndata. To facilitate various types of retrieval, we also design multiple experts\nto learn mappings from CLIP outputs to multi-modal representation space, with\neach expert being responsible to one specific retrieval type. Extensive\nexperiments show that DialCLIP achieves state-of-the-art performance on two\nwidely recognized benchmark datasets (i.e., PhotoChat and MMDialog) by tuning a\nmere 0.04% of the total parameters. These results highlight the efficacy and\nefficiency of our proposed approach, underscoring its potential to advance the\nfield of multi-modal dialog retrieval.\n","authors":["Zhichao Yin","Binyuan Hui","Min Yang","Fei Huang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2401.01076v2.pdf","comment":"ICASSP 2024"},{"id":"http://arxiv.org/abs/2401.01498v1","updated":"2024-01-03T02:03:36Z","published":"2024-01-03T02:03:36Z","title":"Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic\n  Token Prediction","summary":"  We propose a novel text-to-speech (TTS) framework centered around a neural\ntransducer. Our approach divides the whole TTS pipeline into semantic-level\nsequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling\nstages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings.\nFor a robust and efficient alignment modeling, we employ a neural transducer\nnamed token transducer for the semantic token prediction, benefiting from its\nhard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR)\nspeech generator efficiently synthesizes waveforms from these semantic tokens.\nAdditionally, a reference speech controls temporal dynamics and acoustic\nconditions at each stage. This decoupled framework reduces the training\ncomplexity of TTS while allowing each stage to focus on semantic and acoustic\nmodeling. Our experimental results on zero-shot adaptive TTS demonstrate that\nour model surpasses the baseline in terms of speech quality and speaker\nsimilarity, both objectively and subjectively. We also delve into the inference\nspeed and prosody control capabilities of our approach, highlighting the\npotential of neural transducers in TTS frameworks.\n","authors":["Minchan Kim","Myeonghun Jeong","Byoung Jin Choi","Semin Kim","Joun Yeop Lee","Nam Soo Kim"],"pdf_url":"https://arxiv.org/pdf/2401.01498v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2401.01495v1","updated":"2024-01-03T01:58:31Z","published":"2024-01-03T01:58:31Z","title":"A Two-Stage Multimodal Emotion Recognition Model Based on Graph\n  Contrastive Learning","summary":"  In terms of human-computer interaction, it is becoming more and more\nimportant to correctly understand the user's emotional state in a conversation,\nso the task of multimodal emotion recognition (MER) started to receive more\nattention. However, existing emotion classification methods usually perform\nclassification only once. Sentences are likely to be misclassified in a single\nround of classification. Previous work usually ignores the similarities and\ndifferences between different morphological features in the fusion process. To\naddress the above issues, we propose a two-stage emotion recognition model\nbased on graph contrastive learning (TS-GCL). First, we encode the original\ndataset with different preprocessing modalities. Second, a graph contrastive\nlearning (GCL) strategy is introduced for these three modal data with other\nstructures to learn similarities and differences within and between modalities.\nFinally, we use MLP twice to achieve the final emotion classification. This\nstaged classification method can help the model to better focus on different\nlevels of emotional information, thereby improving the performance of the\nmodel. Extensive experiments show that TS-GCL has superior performance on\nIEMOCAP and MELD datasets compared with previous methods.\n","authors":["Wei Ai","FuChen Zhang","Tao Meng","YunTao Shou","HongEn Shao","Keqin Li"],"pdf_url":"https://arxiv.org/pdf/2401.01495v1.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2401.01487v1","updated":"2024-01-03T01:21:30Z","published":"2024-01-03T01:21:30Z","title":"Natural Language Processing and Multimodal Stock Price Prediction","summary":"  In the realm of financial decision-making, predicting stock prices is\npivotal. Artificial intelligence techniques such as long short-term memory\nnetworks (LSTMs), support-vector machines (SVMs), and natural language\nprocessing (NLP) models are commonly employed to predict said prices. This\npaper utilizes stock percentage change as training data, in contrast to the\ntraditional use of raw currency values, with a focus on analyzing publicly\nreleased news articles. The choice of percentage change aims to provide models\nwith context regarding the significance of price fluctuations and overall price\nchange impact on a given stock. The study employs specialized BERT natural\nlanguage processing models to predict stock price trends, with a particular\nemphasis on various data modalities. The results showcase the capabilities of\nsuch strategies with a small natural language processing model to accurately\npredict overall stock trends, and highlight the effectiveness of certain data\nfeatures and sector-specific data.\n","authors":["Kevin Taylor","Jerry Ng"],"pdf_url":"https://arxiv.org/pdf/2401.01487v1.pdf","comment":"13 pages, 13 figures"},{"id":"http://arxiv.org/abs/2306.02052v2","updated":"2024-01-03T00:56:20Z","published":"2023-06-03T08:50:13Z","title":"Conflicts, Villains, Resolutions: Towards models of Narrative Media\n  Framing","summary":"  Despite increasing interest in the automatic detection of media frames in\nNLP, the problem is typically simplified as single-label classification and\nadopts a topic-like view on frames, evading modelling the broader\ndocument-level narrative. In this work, we revisit a widely used\nconceptualization of framing from the communication sciences which explicitly\ncaptures elements of narratives, including conflict and its resolution, and\nintegrate it with the narrative framing of key entities in the story as heroes,\nvictims or villains. We adapt an effective annotation paradigm that breaks a\ncomplex annotation task into a series of simpler binary questions, and present\nan annotated data set of English news articles, and a case study on the framing\nof climate change in articles from news outlets across the political spectrum.\nFinally, we explore automatic multi-label prediction of our frames with\nsupervised and semi-supervised approaches, and present a novel retrieval-based\nmethod which is both effective and transparent in its predictions. We conclude\nwith a discussion of opportunities and challenges for future work on\ndocument-level models of narrative framing.\n","authors":["Lea Frermann","Jiatong Li","Shima Khanehzar","Gosia Mikolajczak"],"pdf_url":"https://arxiv.org/pdf/2306.02052v2.pdf","comment":"Published in ACL 2023"},{"id":"http://arxiv.org/abs/2310.12072v2","updated":"2024-01-03T00:32:43Z","published":"2023-10-18T16:07:01Z","title":"SPEED: Speculative Pipelined Execution for Efficient Decoding","summary":"  Generative Large Language Models (LLMs) based on the Transformer architecture\nhave recently emerged as a dominant foundation model for a wide range of\nNatural Language Processing tasks. Nevertheless, their application in real-time\nscenarios has been highly restricted due to the significant inference latency\nassociated with these models. This is particularly pronounced due to the\nautoregressive nature of generative LLM inference, where tokens are generated\nsequentially since each token depends on all previous output tokens. It is\ntherefore challenging to achieve any token-level parallelism, making inference\nextremely memory-bound. In this work, we propose SPEED, which improves\ninference efficiency by speculatively executing multiple future tokens in\nparallel with the current token using predicted values based on early-layer\nhidden states. For Transformer decoders that employ parameter sharing, the\nmemory operations for the tokens executing in parallel can be amortized, which\nallows us to accelerate generative LLM inference. We demonstrate the efficiency\nof our method in terms of latency reduction relative to model accuracy and\ndemonstrate how speculation allows for training deeper decoders with parameter\nsharing with minimal runtime overhead.\n","authors":["Coleman Hooper","Sehoon Kim","Hiva Mohammadzadeh","Hasan Genc","Kurt Keutzer","Amir Gholami","Sophia Shao"],"pdf_url":"https://arxiv.org/pdf/2310.12072v2.pdf","comment":"NeurIPS Workshop on Efficient Natural Language and Speech Processing\n  (2023)"},{"id":"http://arxiv.org/abs/2401.01472v1","updated":"2024-01-03T00:13:52Z","published":"2024-01-03T00:13:52Z","title":"A First Look at Information Highlighting in Stack Overflow Answers","summary":"  Context: Navigating the knowledge of Stack Overflow (SO) remains challenging.\nTo make the posts vivid to users, SO allows users to write and edit posts with\nMarkdown or HTML so that users can leverage various formatting styles (e.g.,\nbold, italic, and code) to highlight the important information. Nonetheless,\nthere have been limited studies on the highlighted information. Objective: We\ncarried out the first large-scale exploratory study on the information\nhighlighted in SO answers in our recent study. To extend our previous study, we\ndevelop approaches to automatically recommend highlighted content with\nformatting styles using neural network architectures initially designed for the\nNamed Entity Recognition task. Method: In this paper, we studied 31,169,429\nanswers of Stack Overflow. For training recommendation models, we choose CNN\nand BERT models for each type of formatting (i.e., Bold, Italic, Code, and\nHeading) using the information highlighting dataset we collected from SO\nanswers. Results: Our models based on CNN architecture achieve precision\nranging from 0.71 to 0.82. The trained model for automatic code content\nhighlighting achieves a recall of 0.73 and an F1 score of 0.71, outperforming\nthe trained models for other formatting styles. The BERT models have even lower\nrecalls and F1 scores than the CNN models. Our analysis of failure cases\nindicates that the majority of the failure cases are missing identification\n(i.e., the model misses the content that is supposed to be highlighted) due to\nthe models tend to learn the frequently highlighted words while struggling to\nlearn less frequent words. Conclusion: Our findings suggest that it is possible\nto develop recommendation models for highlighting information for answers with\ndifferent formatting styles on Stack Overflow.\n","authors":["Shahla Shaan Ahmed","Shaowei Wang","Yuan Tian"," Tse-Hsun"," Chen","Haoxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.01472v1.pdf","comment":"This work is submitted to Information and Software Technology Journal"},{"id":"http://arxiv.org/abs/2401.01469v1","updated":"2024-01-03T00:09:34Z","published":"2024-01-03T00:09:34Z","title":"Question-Answering Based Summarization of Electronic Health Records\n  using Retrieval Augmented Generation","summary":"  Summarization of electronic health records (EHRs) can substantially minimize\n'screen time' for both patients as well as medical personnel. In recent years\nsummarization of EHRs have employed machine learning pipelines using state of\nthe art neural models. However, these models have produced less than adequate\nresults that are attributed to the difficulty of obtaining sufficient annotated\ndata for training. Moreover, the requirement to consider the entire content of\nan EHR in summarization has resulted in poor performance due to the fact that\nattention mechanisms in modern large language models (LLMs) adds a quadratic\ncomplexity in terms of the size of the input. We propose here a method that\nmitigates these shortcomings by combining semantic search, retrieval augmented\ngeneration (RAG) and question-answering using the latest LLMs. In our approach\nsummarization is the extraction of answers to specific questions that are\ndeemed important by subject-matter experts (SMEs). Our approach is quite\nefficient; requires minimal to no training; does not suffer from the\n'hallucination' problem of LLMs; and it ensures diversity, since the summary\nwill not have repeated content but diverse answers to specific questions.\n","authors":["Walid Saba","Suzanne Wendelken","James. Shanahan"],"pdf_url":"https://arxiv.org/pdf/2401.01469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01989v1","updated":"2024-01-03T21:38:40Z","published":"2024-01-03T21:38:40Z","title":"Revisiting Zero-Shot Abstractive Summarization in the Era of Large\n  Language Models from the Perspective of Position Bias","summary":"  We characterize and study zero-shot abstractive summarization in Large\nLanguage Models (LLMs) by measuring position bias, which we propose as a\ngeneral formulation of the more restrictive lead bias phenomenon studied\npreviously in the literature. Position bias captures the tendency of a model\nunfairly prioritizing information from certain parts of the input text over\nothers, leading to undesirable behavior. Through numerous experiments on four\ndiverse real-world datasets, we study position bias in multiple LLM models such\nas GPT 3.5-Turbo, Llama-2, and Dolly-v2, as well as state-of-the-art pretrained\nencoder-decoder abstractive summarization models such as Pegasus and BART. Our\nfindings lead to novel insights and discussion on performance and position bias\nof models for zero-shot summarization tasks.\n","authors":["Anshuman Chhabra","Hadi Askari","Prasant Mohapatra"],"pdf_url":"https://arxiv.org/pdf/2401.01989v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01967v1","updated":"2024-01-03T20:26:15Z","published":"2024-01-03T20:26:15Z","title":"A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO\n  and Toxicity","summary":"  While alignment algorithms are now commonly used to tune pre-trained language\nmodels towards a user's preferences, we lack explanations for the underlying\nmechanisms in which models become ``aligned'', thus making it difficult to\nexplain phenomena like jailbreaks. In this work we study a popular algorithm,\ndirect preference optimization (DPO), and the mechanisms by which it reduces\ntoxicity. Namely, we first study how toxicity is represented and elicited in a\npre-trained language model, GPT2-medium. We then apply DPO with a carefully\ncrafted pairwise dataset to reduce toxicity. We examine how the resulting model\naverts toxic outputs, and find that capabilities learned from pre-training are\nnot removed, but rather bypassed. We use this insight to demonstrate a simple\nmethod to un-align the model, reverting it back to its toxic behavior.\n","authors":["Andrew Lee","Xiaoyan Bai","Itamar Pres","Martin Wattenberg","Jonathan K. Kummerfeld","Rada Mihalcea"],"pdf_url":"https://arxiv.org/pdf/2401.01967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.02468v4","updated":"2024-01-03T19:33:57Z","published":"2023-03-04T17:59:43Z","title":"Lon-ea at SemEval-2023 Task 11: A Comparison of Activation Functions for\n  Soft and Hard Label Prediction","summary":"  We study the influence of different activation functions in the output layer\nof deep neural network models for soft and hard label prediction in the\nlearning with disagreement task. In this task, the goal is to quantify the\namount of disagreement via predicting soft labels. To predict the soft labels,\nwe use BERT-based preprocessors and encoders and vary the activation function\nused in the output layer, while keeping other parameters constant. The soft\nlabels are then used for the hard label prediction. The activation functions\nconsidered are sigmoid as well as a step-function that is added to the model\npost-training and a sinusoidal activation function, which is introduced for the\nfirst time in this paper.\n","authors":["Peyman Hosseini","Mehran Hosseini","Sana Sabah Al-Azzawi","Marcus Liwicki","Ignacio Castro","Matthew Purver"],"pdf_url":"https://arxiv.org/pdf/2303.02468v4.pdf","comment":"Accepted in ACL 2023 SemEval Workshop as selected task paper"},{"id":"http://arxiv.org/abs/2401.01952v1","updated":"2024-01-03T19:31:58Z","published":"2024-01-03T19:31:58Z","title":"Instruct-Imagen: Image Generation with Multi-modal Instruction","summary":"  This paper presents instruct-imagen, a model that tackles heterogeneous image\ngeneration tasks and generalizes across unseen tasks. We introduce *multi-modal\ninstruction* for image generation, a task representation articulating a range\nof generation intents with precision. It uses natural language to amalgamate\ndisparate modalities (e.g., text, edge, style, subject, etc.), such that\nabundant generation intents can be standardized in a uniform format.\n  We then build instruct-imagen by fine-tuning a pre-trained text-to-image\ndiffusion model with a two-stage framework. First, we adapt the model using the\nretrieval-augmented training, to enhance model's capabilities to ground its\ngeneration on external multimodal context. Subsequently, we fine-tune the\nadapted model on diverse image generation tasks that requires vision-language\nunderstanding (e.g., subject-driven generation, etc.), each paired with a\nmulti-modal instruction encapsulating the task's essence. Human evaluation on\nvarious image generation datasets reveals that instruct-imagen matches or\nsurpasses prior task-specific models in-domain and demonstrates promising\ngeneralization to unseen and more complex tasks.\n","authors":["Hexiang Hu","Kelvin C. K. Chan","Yu-Chuan Su","Wenhu Chen","Yandong Li","Kihyuk Sohn","Yang Zhao","Xue Ben","Boqing Gong","William Cohen","Ming-Wei Chang","Xuhui Jia"],"pdf_url":"https://arxiv.org/pdf/2401.01952v1.pdf","comment":"20 pages, 18 figures"},{"id":"http://arxiv.org/abs/2401.01943v1","updated":"2024-01-03T19:03:32Z","published":"2024-01-03T19:03:32Z","title":"Generalist embedding models are better at short-context clinical\n  semantic search than specialized embedding models","summary":"  The increasing use of tools and solutions based on Large Language Models\n(LLMs) for various tasks in the medical domain has become a prominent trend.\nTheir use in this highly critical and sensitive domain has thus raised\nimportant questions about their robustness, especially in response to\nvariations in input, and the reliability of the generated outputs. This study\naddresses these questions by constructing a textual dataset based on the\nICD-10-CM code descriptions, widely used in US hospitals and containing many\nclinical terms, and their easily reproducible rephrasing. We then benchmarked\nexisting embedding models, either generalist or specialized in the clinical\ndomain, in a semantic search task where the goal was to correctly match the\nrephrased text to the original description. Our results showed that generalist\nmodels performed better than clinical models, suggesting that existing clinical\nspecialized models are more sensitive to small changes in input that confuse\nthem. The highlighted problem of specialized models may be due to the fact that\nthey have not been trained on sufficient data, and in particular on datasets\nthat are not diverse enough to have a reliable global language understanding,\nwhich is still necessary for accurate handling of medical documents.\n","authors":["Jean-Baptiste Excoffier","Tom Roehr","Alexei Figueroa","Michalis Papaaioannou","Keno Bressem","Matthieu Ortala"],"pdf_url":"https://arxiv.org/pdf/2401.01943v1.pdf","comment":"11 pages, 1 figure, 5 tables"},{"id":"http://arxiv.org/abs/2401.01916v1","updated":"2024-01-03T04:47:02Z","published":"2024-01-03T04:47:02Z","title":"AstroLLaMA-Chat: Scaling AstroLLaMA with Conversational and Diverse\n  Datasets","summary":"  We explore the potential of enhancing LLM performance in astronomy-focused\nquestion-answering through targeted, continual pre-training. By employing a\ncompact 7B-parameter LLaMA-2 model and focusing exclusively on a curated set of\nastronomy corpus -- comprising abstracts, introductions, and conclusions -- we\nachieve notable improvements in specialized topic comprehension. While general\nLLMs like GPT-4 outperform in broader question-answering scenarios due to\nsuperior reasoning capabilities, our findings suggest that continual\npre-training with limited resources can still enhance model performance on\nspecialized topics. Additionally, we present an extension of AstroLLaMA: the\nfine-tuning of the 7B LLaMA model on a domain-specific conversational dataset,\nculminating in the release of the chat-enabled AstroLLaMA for community use.\nComprehensive quantitative benchmarking is currently in progress and will be\ndetailed in an upcoming full paper. The model, AstroLLaMA-Chat, is now\navailable at https://huggingface.co/universeTBD, providing the first\nopen-source conversational AI tool tailored for the astronomy community.\n","authors":["Ernest Perkowski","Rui Pan","Tuan Dung Nguyen","Yuan-Sen Ting","Sandor Kruk","Tong Zhang","Charlie O'Neill","Maja Jablonska","Michael J. Smith","Kevin Schawinski","Kartheik Iyer","Ioana Ciucă for UniverseTBD"],"pdf_url":"https://arxiv.org/pdf/2401.01916v1.pdf","comment":"4 pages, 1 figure, model is available at\n  https://huggingface.co/universeTBD, submitted to RNAAS"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2301.02310v3","updated":"2024-01-03T18:59:57Z","published":"2023-01-05T21:48:33Z","title":"PressureVision++: Estimating Fingertip Pressure from Diverse RGB Images","summary":"  Touch plays a fundamental role in manipulation for humans; however, machine\nperception of contact and pressure typically requires invasive sensors. Recent\nresearch has shown that deep models can estimate hand pressure based on a\nsingle RGB image. However, evaluations have been limited to controlled settings\nsince collecting diverse data with ground-truth pressure measurements is\ndifficult. We present a novel approach that enables diverse data to be captured\nwith only an RGB camera and a cooperative participant. Our key insight is that\npeople can be prompted to apply pressure in a certain way, and this prompt can\nserve as a weak label to supervise models to perform well under varied\nconditions. We collect a novel dataset with 51 participants making fingertip\ncontact with diverse objects. Our network, PressureVision++, outperforms human\nannotators and prior work. We also demonstrate an application of\nPressureVision++ to mixed reality where pressure estimation allows everyday\nsurfaces to be used as arbitrary touch-sensitive interfaces. Code, data, and\nmodels are available online.\n","authors":["Patrick Grady","Jeremy A. Collins","Chengcheng Tang","Christopher D. Twigg","Kunal Aneja","James Hays","Charles C. Kemp"],"pdf_url":"https://arxiv.org/pdf/2301.02310v3.pdf","comment":"WACV 2024"},{"id":"http://arxiv.org/abs/2401.01887v1","updated":"2024-01-03T18:57:27Z","published":"2024-01-03T18:57:27Z","title":"LEAP-VO: Long-term Effective Any Point Tracking for Visual Odometry","summary":"  Visual odometry estimates the motion of a moving camera based on visual\ninput. Existing methods, mostly focusing on two-view point tracking, often\nignore the rich temporal context in the image sequence, thereby overlooking the\nglobal motion patterns and providing no assessment of the full trajectory\nreliability. These shortcomings hinder performance in scenarios with occlusion,\ndynamic objects, and low-texture areas. To address these challenges, we present\nthe Long-term Effective Any Point Tracking (LEAP) module. LEAP innovatively\ncombines visual, inter-track, and temporal cues with mindfully selected anchors\nfor dynamic track estimation. Moreover, LEAP's temporal probabilistic\nformulation integrates distribution updates into a learnable iterative\nrefinement module to reason about point-wise uncertainty. Based on these\ntraits, we develop LEAP-VO, a robust visual odometry system adept at handling\nocclusions and dynamic scenes. Our mindful integration showcases a novel\npractice by employing long-term point tracking as the front-end. Extensive\nexperiments demonstrate that the proposed pipeline significantly outperforms\nexisting baselines across various visual odometry benchmarks.\n","authors":["Weirong Chen","Le Chen","Rui Wang","Marc Pollefeys"],"pdf_url":"https://arxiv.org/pdf/2401.01887v1.pdf","comment":"15 pages, 11 figures"},{"id":"http://arxiv.org/abs/2401.01885v1","updated":"2024-01-03T18:55:16Z","published":"2024-01-03T18:55:16Z","title":"From Audio to Photoreal Embodiment: Synthesizing Humans in Conversations","summary":"  We present a framework for generating full-bodied photorealistic avatars that\ngesture according to the conversational dynamics of a dyadic interaction. Given\nspeech audio, we output multiple possibilities of gestural motion for an\nindividual, including face, body, and hands. The key behind our method is in\ncombining the benefits of sample diversity from vector quantization with the\nhigh-frequency details obtained through diffusion to generate more dynamic,\nexpressive motion. We visualize the generated motion using highly\nphotorealistic avatars that can express crucial nuances in gestures (e.g.\nsneers and smirks). To facilitate this line of research, we introduce a\nfirst-of-its-kind multi-view conversational dataset that allows for\nphotorealistic reconstruction. Experiments show our model generates appropriate\nand diverse gestures, outperforming both diffusion- and VQ-only methods.\nFurthermore, our perceptual evaluation highlights the importance of\nphotorealism (vs. meshes) in accurately assessing subtle motion details in\nconversational gestures. Code and dataset available online.\n","authors":["Evonne Ng","Javier Romero","Timur Bagautdinov","Shaojie Bai","Trevor Darrell","Angjoo Kanazawa","Alexander Richard"],"pdf_url":"https://arxiv.org/pdf/2401.01885v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.17033v3","updated":"2024-01-03T18:41:04Z","published":"2023-05-26T15:40:11Z","title":"The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics\n  (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs)","summary":"  Pediatric tumors of the central nervous system are the most common cause of\ncancer-related death in children. The five-year survival rate for high-grade\ngliomas in children is less than 20\\%. Due to their rarity, the diagnosis of\nthese entities is often delayed, their treatment is mainly based on historic\ntreatment concepts, and clinical trials require multi-institutional\ncollaborations. The MICCAI Brain Tumor Segmentation (BraTS) Challenge is a\nlandmark community benchmark event with a successful history of 12 years of\nresource creation for the segmentation and analysis of adult glioma. Here we\npresent the CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023 challenge, which\nrepresents the first BraTS challenge focused on pediatric brain tumors with\ndata acquired across multiple international consortia dedicated to pediatric\nneuro-oncology and clinical trials. The BraTS-PEDs 2023 challenge focuses on\nbenchmarking the development of volumentric segmentation algorithms for\npediatric brain glioma through standardized quantitative performance evaluation\nmetrics utilized across the BraTS 2023 cluster of challenges. Models gaining\nknowledge from the BraTS-PEDs multi-parametric structural MRI (mpMRI) training\ndata will be evaluated on separate validation and unseen test mpMRI dataof\nhigh-grade pediatric glioma. The CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023\nchallenge brings together clinicians and AI/imaging scientists to lead to\nfaster development of automated segmentation techniques that could benefit\nclinical trials, and ultimately the care of children with brain tumors.\n","authors":["Anahita Fathi Kazerooni","Nastaran Khalili","Xinyang Liu","Debanjan Haldar","Zhifan Jiang","Syed Muhammed Anwar","Jake Albrecht","Maruf Adewole","Udunna Anazodo","Hannah Anderson","Sina Bagheri","Ujjwal Baid","Timothy Bergquist","Austin J. Borja","Evan Calabrese","Verena Chung","Gian-Marco Conte","Farouk Dako","James Eddy","Ivan Ezhov","Ariana Familiar","Keyvan Farahani","Shuvanjan Haldar","Juan Eugenio Iglesias","Anastasia Janas","Elaine Johansen","Blaise V Jones","Florian Kofler","Dominic LaBella","Hollie Anne Lai","Koen Van Leemput","Hongwei Bran Li","Nazanin Maleki","Aaron S McAllister","Zeke Meier","Bjoern Menze","Ahmed W Moawad","Khanak K Nandolia","Julija Pavaine","Marie Piraud","Tina Poussaint","Sanjay P Prabhu","Zachary Reitman","Andres Rodriguez","Jeffrey D Rudie","Ibraheem Salman Shaikh","Lubdha M. Shah","Nakul Sheth","Russel Taki Shinohara","Wenxin Tu","Karthik Viswanathan","Chunhao Wang","Jeffrey B Ware","Benedikt Wiestler","Walter Wiggins","Anna Zapaishchykova","Mariam Aboian","Miriam Bornhorst","Peter de Blank","Michelle Deutsch","Maryam Fouladi","Lindsey Hoffman","Benjamin Kann","Margot Lazow","Leonie Mikael","Ali Nabavizadeh","Roger Packer","Adam Resnick","Brian Rood","Arastoo Vossough","Spyridon Bakas","Marius George Linguraru"],"pdf_url":"https://arxiv.org/pdf/2305.17033v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01868v1","updated":"2024-01-03T18:23:30Z","published":"2024-01-03T18:23:30Z","title":"Step length measurement in the wild using FMCW radar","summary":"  With an aging population, numerous assistive and monitoring technologies are\nunder development to enable older adults to age in place. To facilitate aging\nin place predicting risk factors such as falls, and hospitalization and\nproviding early interventions are important. Much of the work on ambient\nmonitoring for risk prediction has centered on gait speed analysis, utilizing\nprivacy-preserving sensors like radar. Despite compelling evidence that\nmonitoring step length, in addition to gait speed, is crucial for predicting\nrisk, radar-based methods have not explored step length measurement in the\nhome. Furthermore, laboratory experiments on step length measurement using\nradars are limited to proof of concept studies with few healthy subjects. To\naddress this gap, a radar-based step length measurement system for the home is\nproposed based on detection and tracking using radar point cloud, followed by\nDoppler speed profiling of the torso to obtain step lengths in the home. The\nproposed method was evaluated in a clinical environment, involving 35 frail\nolder adults, to establish its validity. Additionally, the method was assessed\nin people's homes, with 21 frail older adults who had participated in the\nclinical assessment. The proposed radar-based step length measurement method\nwas compared to the gold standard Zeno Walkway Gait Analysis System, revealing\na 4.5cm/8.3% error in a clinical setting. Furthermore, it exhibited excellent\nreliability (ICC(2,k)=0.91, 95% CI 0.82 to 0.96) in uncontrolled home settings.\nThe method also proved accurate in uncontrolled home settings, as indicated by\na strong agreement (ICC(3,k)=0.81 (95% CI 0.53 to 0.92)) between home\nmeasurements and in-clinic assessments.\n","authors":["Parthipan Siva","Alexander Wong","Patricia Hewston","George Ioannidis","Dr. Jonathan Adachi","Dr. Alexander Rabinovich","Andrea Lee","Alexandra Papaioannou"],"pdf_url":"https://arxiv.org/pdf/2401.01868v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01862v1","updated":"2024-01-03T18:09:33Z","published":"2024-01-03T18:09:33Z","title":"A Vision Check-up for Language Models","summary":"  What does learning to model relationships between strings teach large\nlanguage models (LLMs) about the visual world? We systematically evaluate LLMs'\nabilities to generate and recognize an assortment of visual concepts of\nincreasing complexity and then demonstrate how a preliminary visual\nrepresentation learning system can be trained using models of text. As language\nmodels lack the ability to consume or output visual information as pixels, we\nuse code to represent images in our study. Although LLM-generated images do not\nlook like natural images, results on image generation and the ability of models\nto correct these generated images indicate that precise modeling of strings can\nteach language models about numerous aspects of the visual world. Furthermore,\nexperiments on self-supervised visual representation learning, utilizing images\ngenerated with text models, highlight the potential to train vision models\ncapable of making semantic assessments of natural images using just LLMs.\n","authors":["Pratyusha Sharma","Tamar Rott Shaham","Manel Baradad","Stephanie Fu","Adrian Rodriguez-Munoz","Shivam Duggal","Phillip Isola","Antonio Torralba"],"pdf_url":"https://arxiv.org/pdf/2401.01862v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01858v1","updated":"2024-01-03T18:06:28Z","published":"2024-01-03T18:06:28Z","title":"Synthetic dataset of ID and Travel Document","summary":"  This paper presents a new synthetic dataset of ID and travel documents,\ncalled SIDTD. The SIDTD dataset is created to help training and evaluating\nforged ID documents detection systems. Such a dataset has become a necessity as\nID documents contain personal information and a public dataset of real\ndocuments can not be released. Moreover, forged documents are scarce, compared\nto legit ones, and the way they are generated varies from one fraudster to\nanother resulting in a class of high intra-variability. In this paper we\ntrained state-of-the-art models on this dataset and we compare them to the\nperformance achieved in larger, but private, datasets. The creation of this\ndataset will help to document image analysis community to progress in the task\nof ID document verification.\n","authors":["Carlos Boned","Maxime Talarmain","Nabil Ghanmi","Guillaume Chiron","Sanket Biswas","Ahmad Montaser Awal","Oriol Ramos Terrades"],"pdf_url":"https://arxiv.org/pdf/2401.01858v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01839v1","updated":"2024-01-03T17:11:27Z","published":"2024-01-03T17:11:27Z","title":"Frequency Domain Modality-invariant Feature Learning for\n  Visible-infrared Person Re-Identification","summary":"  Visible-infrared person re-identification (VI-ReID) is challenging due to the\nsignificant cross-modality discrepancies between visible and infrared images.\nWhile existing methods have focused on designing complex network architectures\nor using metric learning constraints to learn modality-invariant features, they\noften overlook which specific component of the image causes the modality\ndiscrepancy problem. In this paper, we first reveal that the difference in the\namplitude component of visible and infrared images is the primary factor that\ncauses the modality discrepancy and further propose a novel Frequency Domain\nmodality-invariant feature learning framework (FDMNet) to reduce modality\ndiscrepancy from the frequency domain perspective. Our framework introduces two\nnovel modules, namely the Instance-Adaptive Amplitude Filter (IAF) module and\nthe Phrase-Preserving Normalization (PPNorm) module, to enhance the\nmodality-invariant amplitude component and suppress the modality-specific\ncomponent at both the image- and feature-levels. Extensive experimental results\non two standard benchmarks, SYSU-MM01 and RegDB, demonstrate the superior\nperformance of our FDMNet against state-of-the-art methods.\n","authors":["Yulin Li","Tianzhu Zhang","Yongdong Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.01839v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01827v1","updated":"2024-01-03T16:43:47Z","published":"2024-01-03T16:43:47Z","title":"Moonshot: Towards Controllable Video Generation and Editing with\n  Multimodal Conditions","summary":"  Most existing video diffusion models (VDMs) are limited to mere text\nconditions. Thereby, they are usually lacking in control over visual appearance\nand geometry structure of the generated videos. This work presents Moonshot, a\nnew video generation model that conditions simultaneously on multimodal inputs\nof image and text. The model builts upon a core module, called multimodal video\nblock (MVB), which consists of conventional spatialtemporal layers for\nrepresenting video features, and a decoupled cross-attention layer to address\nimage and text inputs for appearance conditioning. In addition, we carefully\ndesign the model architecture such that it can optionally integrate with\npre-trained image ControlNet modules for geometry visual conditions, without\nneeding of extra training overhead as opposed to prior methods. Experiments\nshow that with versatile multimodal conditioning mechanisms, Moonshot\ndemonstrates significant improvement on visual quality and temporal consistency\ncompared to existing models. In addition, the model can be easily repurposed\nfor a variety of generative applications, such as personalized video\ngeneration, image animation and video editing, unveiling its potential to serve\nas a fundamental architecture for controllable video generation. Models will be\nmade public on https://github.com/salesforce/LAVIS.\n","authors":["David Junhao Zhang","Dongxu Li","Hung Le","Mike Zheng Shou","Caiming Xiong","Doyen Sahoo"],"pdf_url":"https://arxiv.org/pdf/2401.01827v1.pdf","comment":"project page: https://showlab.github.io/Moonshot/"},{"id":"http://arxiv.org/abs/2312.15927v2","updated":"2024-01-03T16:43:33Z","published":"2023-12-26T07:45:32Z","title":"M3D: Dataset Condensation by Minimizing Maximum Mean Discrepancy","summary":"  Training state-of-the-art (SOTA) deep models often requires extensive data,\nresulting in substantial training and storage costs. To address these\nchallenges, dataset condensation has been developed to learn a small synthetic\nset that preserves essential information from the original large-scale dataset.\nNowadays, optimization-oriented methods have been the primary method in the\nfield of dataset condensation for achieving SOTA results. However, the bi-level\noptimization process hinders the practical application of such methods to\nrealistic and larger datasets. To enhance condensation efficiency, previous\nworks proposed Distribution-Matching (DM) as an alternative, which\nsignificantly reduces the condensation cost. Nonetheless, current DM-based\nmethods have yielded less comparable results to optimization-oriented methods\ndue to their focus on aligning only the first moment of the distributions. In\nthis paper, we present a novel DM-based method named M3D for dataset\ncondensation by Minimizing the Maximum Mean Discrepancy between feature\nrepresentations of the synthetic and real images. By embedding their\ndistributions in a reproducing kernel Hilbert space, we align all orders of\nmoments of the distributions of real and synthetic images, resulting in a more\ngeneralized condensed set. Notably, our method even surpasses the SOTA\noptimization-oriented method IDC on the high-resolution ImageNet dataset.\nExtensive analysis is conducted to verify the effectiveness of the proposed\nmethod.\n","authors":["Hansong Zhang","Shikun Li","Pengju Wang","Dan Zeng","Shiming Ge"],"pdf_url":"https://arxiv.org/pdf/2312.15927v2.pdf","comment":"This work has been accepted in AAAI-24"},{"id":"http://arxiv.org/abs/2401.01822v1","updated":"2024-01-03T16:38:56Z","published":"2024-01-03T16:38:56Z","title":"HawkRover: An Autonomous mmWave Vehicular Communication Testbed with\n  Multi-sensor Fusion and Deep Learning","summary":"  Connected and automated vehicles (CAVs) have become a transformative\ntechnology that can change our daily life. Currently, millimeter-wave (mmWave)\nbands are identified as the promising CAV connectivity solution. While it can\nprovide high data rate, their realization faces many challenges such as high\nattenuation during mmWave signal propagation and mobility management. Existing\nsolution has to initiate pilot signal to measure channel information, then\napply signal processing to calculate the best narrow beam towards the receiver\nend to guarantee sufficient signal power. This process takes significant\noverhead and time, hence not suitable for vehicles. In this study, we propose\nan autonomous and low-cost testbed to collect extensive co-located mmWave\nsignal and other sensors data such as LiDAR (Light Detection and Ranging),\ncameras, ultrasonic, etc, traditionally for ``automated'', to facilitate mmWave\nvehicular communications. Intuitively, these sensors can build a 3D map around\nthe vehicle and signal propagation path can be estimated, eliminating iterative\nthe process via pilot signals. This multimodal data fusion, together with AI,\nis expected to bring significant advances in ``connected'' research.\n","authors":["Ethan Zhu","Haijian Sun"],"pdf_url":"https://arxiv.org/pdf/2401.01822v1.pdf","comment":"submitted to IEEE conferences for future publications"},{"id":"http://arxiv.org/abs/2401.01823v1","updated":"2024-01-03T16:38:56Z","published":"2024-01-03T16:38:56Z","title":"Detours for Navigating Instructional Videos","summary":"  We introduce the video detours problem for navigating instructional videos.\nGiven a source video and a natural language query asking to alter the how-to\nvideo's current path of execution in a certain way, the goal is to find a\nrelated ''detour video'' that satisfies the requested alteration. To address\nthis challenge, we propose VidDetours, a novel video-language approach that\nlearns to retrieve the targeted temporal segments from a large repository of\nhow-to's using video-and-text conditioned queries. Furthermore, we devise a\nlanguage-based pipeline that exploits how-to video narration text to create\nweakly supervised training data. We demonstrate our idea applied to the domain\nof how-to cooking videos, where a user can detour from their current recipe to\nfind steps with alternate ingredients, tools, and techniques. Validating on a\nground truth annotated dataset of 16K samples, we show our model's significant\nimprovements over best available methods for video retrieval and question\nanswering, with recall rates exceeding the state of the art by 35%.\n","authors":["Kumar Ashutosh","Zihui Xue","Tushar Nagarajan","Kristen Grauman"],"pdf_url":"https://arxiv.org/pdf/2401.01823v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2401.01808v1","updated":"2024-01-03T16:10:07Z","published":"2024-01-03T16:10:07Z","title":"aMUSEd: An Open MUSE Reproduction","summary":"  We present aMUSEd, an open-source, lightweight masked image model (MIM) for\ntext-to-image generation based on MUSE. With 10 percent of MUSE's parameters,\naMUSEd is focused on fast image generation. We believe MIM is under-explored\ncompared to latent diffusion, the prevailing approach for text-to-image\ngeneration. Compared to latent diffusion, MIM requires fewer inference steps\nand is more interpretable. Additionally, MIM can be fine-tuned to learn\nadditional styles with only a single image. We hope to encourage further\nexploration of MIM by demonstrating its effectiveness on large-scale\ntext-to-image generation and releasing reproducible training code. We also\nrelease checkpoints for two models which directly produce images at 256x256 and\n512x512 resolutions.\n","authors":["Suraj Patil","William Berman","Robin Rombach","Patrick von Platen"],"pdf_url":"https://arxiv.org/pdf/2401.01808v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.14650v3","updated":"2024-01-03T15:18:44Z","published":"2022-07-29T12:50:32Z","title":"SYNTA: A novel approach for deep learning-based image analysis in muscle\n  histopathology using photo-realistic synthetic data","summary":"  Artificial intelligence (AI), machine learning, and deep learning (DL)\nmethods are becoming increasingly important in the field of biomedical image\nanalysis. However, to exploit the full potential of such methods, a\nrepresentative number of experimentally acquired images containing a\nsignificant number of manually annotated objects is needed as training data.\nHere we introduce SYNTA (synthetic data) as a novel approach for the generation\nof synthetic, photo-realistic, and highly complex biomedical images as training\ndata for DL systems. We show the versatility of our approach in the context of\nmuscle fiber and connective tissue analysis in histological sections. We\ndemonstrate that it is possible to perform robust and expert-level segmentation\ntasks on previously unseen real-world data, without the need for manual\nannotations using synthetic training data alone. Being a fully parametric\ntechnique, our approach poses an interpretable and controllable alternative to\nGenerative Adversarial Networks (GANs) and has the potential to significantly\naccelerate quantitative image analysis in a variety of biomedical applications\nin microscopy and beyond.\n","authors":["Leonid Mill","Oliver Aust","Jochen A. Ackermann","Philipp Burger","Monica Pascual","Katrin Palumbo-Zerr","Gerhard Krönke","Stefan Uderhardt","Georg Schett","Christoph S. Clemen","Rolf Schröder","Christian Holtzhausen","Samir Jabari","Andreas Maier","Anika Grüneboom"],"pdf_url":"https://arxiv.org/pdf/2207.14650v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01219v2","updated":"2024-01-03T15:00:34Z","published":"2024-01-02T14:18:11Z","title":"Distribution Matching for Multi-Task Learning of Classification Tasks: a\n  Large-Scale Study on Faces & Beyond","summary":"  Multi-Task Learning (MTL) is a framework, where multiple related tasks are\nlearned jointly and benefit from a shared representation space, or parameter\ntransfer. To provide sufficient learning support, modern MTL uses annotated\ndata with full, or sufficiently large overlap across tasks, i.e., each input\nsample is annotated for all, or most of the tasks. However, collecting such\nannotations is prohibitive in many real applications, and cannot benefit from\ndatasets available for individual tasks. In this work, we challenge this setup\nand show that MTL can be successful with classification tasks with little, or\nnon-overlapping annotations, or when there is big discrepancy in the size of\nlabeled data per task. We explore task-relatedness for co-annotation and\nco-training, and propose a novel approach, where knowledge exchange is enabled\nbetween the tasks via distribution matching. To demonstrate the general\napplicability of our method, we conducted diverse case studies in the domains\nof affective computing, face recognition, species recognition, and shopping\nitem classification using nine datasets. Our large-scale study of affective\ntasks for basic expression recognition and facial action unit detection\nillustrates that our approach is network agnostic and brings large performance\nimprovements compared to the state-of-the-art in both tasks and across all\nstudied databases. In all case studies, we show that co-training via\ntask-relatedness is advantageous and prevents negative transfer (which occurs\nwhen MT model's performance is worse than that of at least one single-task\nmodel).\n","authors":["Dimitrios Kollias","Viktoriia Sharmanska","Stefanos Zafeiriou"],"pdf_url":"https://arxiv.org/pdf/2401.01219v2.pdf","comment":"accepted at AAAI 2024. arXiv admin note: text overlap with\n  arXiv:2105.03790"},{"id":"http://arxiv.org/abs/2312.16476v2","updated":"2024-01-03T14:40:49Z","published":"2023-12-27T08:50:01Z","title":"SVGDreamer: Text Guided SVG Generation with Diffusion Model","summary":"  Recently, text-guided scalable vector graphics (SVGs) synthesis has shown\npromise in domains such as iconography and sketch. However, existing\ntext-to-SVG generation methods lack editability and struggle with visual\nquality and result diversity. To address these limitations, we propose a novel\ntext-guided vector graphics synthesis method called SVGDreamer. SVGDreamer\nincorporates a semantic-driven image vectorization (SIVE) process that enables\nthe decomposition of synthesis into foreground objects and background, thereby\nenhancing editability. Specifically, the SIVE process introduce attention-based\nprimitive control and an attention-mask loss function for effective control and\nmanipulation of individual elements. Additionally, we propose a Vectorized\nParticle-based Score Distillation (VPSD) approach to tackle the challenges of\ncolor over-saturation, vector primitives over-smoothing, and limited result\ndiversity in existing text-to-SVG generation methods. Furthermore, on the basis\nof VPSD, we introduce Reward Feedback Learning (ReFL) to accelerate VPSD\nconvergence and improve aesthetic appeal. Extensive experiments have been\nconducted to validate the effectiveness of SVGDreamer, demonstrating its\nsuperiority over baseline methods in terms of editability, visual quality, and\ndiversity. The code and demo of SVGDreamer can be found at\n\\href{https://ximinng.github.io/SVGDreamer-project/}{https://ximinng.github.io/SVGDreamer-project/}.\n","authors":["Ximing Xing","Haitao Zhou","Chuang Wang","Jing Zhang","Dong Xu","Qian Yu"],"pdf_url":"https://arxiv.org/pdf/2312.16476v2.pdf","comment":"19 pages, 15 figures, project link:\n  https://ximinng.github.io/SVGDreamer-project/"},{"id":"http://arxiv.org/abs/2307.14823v2","updated":"2024-01-03T14:38:02Z","published":"2023-07-27T13:00:21Z","title":"Fading memory as inductive bias in residual recurrent networks","summary":"  Residual connections have been proposed as an architecture-based inductive\nbias to mitigate the problem of exploding and vanishing gradients and increased\ntask performance in both feed-forward and recurrent networks (RNNs) when\ntrained with the backpropagation algorithm. Yet, little is known about how\nresidual connections in RNNs influence their dynamics and fading memory\nproperties. Here, we introduce weakly coupled residual recurrent networks\n(WCRNNs) in which residual connections result in well-defined Lyapunov\nexponents and allow for studying properties of fading memory. We investigate\nhow the residual connections of WCRNNs influence their performance, network\ndynamics, and memory properties on a set of benchmark tasks. We show that\nseveral distinct forms of residual connections yield effective inductive biases\nthat result in increased network expressivity. In particular, those are\nresidual connections that (i) result in network dynamics at the proximity of\nthe edge of chaos, (ii) allow networks to capitalize on characteristic spectral\nproperties of the data, and (iii) result in heterogeneous memory properties. In\naddition, we demonstrate how our results can be extended to non-linear\nresiduals and introduce a weakly coupled residual initialization scheme that\ncan be used for Elman RNNs.\n","authors":["Igor Dubinin","Felix Effenberger"],"pdf_url":"https://arxiv.org/pdf/2307.14823v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.07665v2","updated":"2024-01-03T14:36:11Z","published":"2023-08-15T09:27:57Z","title":"Inversion-by-Inversion: Exemplar-based Sketch-to-Photo Synthesis via\n  Stochastic Differential Equations without Training","summary":"  Exemplar-based sketch-to-photo synthesis allows users to generate\nphoto-realistic images based on sketches. Recently, diffusion-based methods\nhave achieved impressive performance on image generation tasks, enabling\nhighly-flexible control through text-driven generation or energy functions.\nHowever, generating photo-realistic images with color and texture from sketch\nimages remains challenging for diffusion models. Sketches typically consist of\nonly a few strokes, with most regions left blank, making it difficult for\ndiffusion-based methods to produce photo-realistic images. In this work, we\npropose a two-stage method named ``Inversion-by-Inversion\" for exemplar-based\nsketch-to-photo synthesis. This approach includes shape-enhancing inversion and\nfull-control inversion. During the shape-enhancing inversion process, an\nuncolored photo is generated with the guidance of a shape-energy function. This\nstep is essential to ensure control over the shape of the generated photo. In\nthe full-control inversion process, we propose an appearance-energy function to\ncontrol the color and texture of the final generated photo.Importantly, our\nInversion-by-Inversion pipeline is training-free and can accept different types\nof exemplars for color and texture control. We conducted extensive experiments\nto evaluate our proposed method, and the results demonstrate its effectiveness.\nThe code and project can be found at\nhttps://ximinng.github.io/inversion-by-inversion-project/.\n","authors":["Ximing Xing","Chuang Wang","Haitao Zhou","Zhihao Hu","Chongxuan Li","Dong Xu","Qian Yu"],"pdf_url":"https://arxiv.org/pdf/2308.07665v2.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2203.01577v4","updated":"2024-01-03T14:31:13Z","published":"2022-03-03T09:02:52Z","title":"HOI4D: A 4D Egocentric Dataset for Category-Level Human-Object\n  Interaction","summary":"  We present HOI4D, a large-scale 4D egocentric dataset with rich annotations,\nto catalyze the research of category-level human-object interaction. HOI4D\nconsists of 2.4M RGB-D egocentric video frames over 4000 sequences collected by\n4 participants interacting with 800 different object instances from 16\ncategories over 610 different indoor rooms. Frame-wise annotations for panoptic\nsegmentation, motion segmentation, 3D hand pose, category-level object pose and\nhand action have also been provided, together with reconstructed object meshes\nand scene point clouds. With HOI4D, we establish three benchmarking tasks to\npromote category-level HOI from 4D visual signals including semantic\nsegmentation of 4D dynamic point cloud sequences, category-level object pose\ntracking, and egocentric action segmentation with diverse interaction targets.\nIn-depth analysis shows HOI4D poses great challenges to existing methods and\nproduces great research opportunities.\n","authors":["Yunze Liu","Yun Liu","Che Jiang","Kangbo Lyu","Weikang Wan","Hao Shen","Boqiang Liang","Zhoujie Fu","He Wang","Li Yi"],"pdf_url":"https://arxiv.org/pdf/2203.01577v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01759v1","updated":"2024-01-03T14:24:02Z","published":"2024-01-03T14:24:02Z","title":"VGA: Vision and Graph Fused Attention Network for Rumor Detection","summary":"  With the development of social media, rumors have been spread broadly on\nsocial media platforms, causing great harm to society. Beside textual\ninformation, many rumors also use manipulated images or conceal textual\ninformation within images to deceive people and avoid being detected, making\nmultimodal rumor detection be a critical problem. The majority of multimodal\nrumor detection methods mainly concentrate on extracting features of source\nclaims and their corresponding images, while ignoring the comments of rumors\nand their propagation structures. These comments and structures imply the\nwisdom of crowds and are proved to be crucial to debunk rumors. Moreover, these\nmethods usually only extract visual features in a basic manner, seldom consider\ntampering or textual information in images. Therefore, in this study, we\npropose a novel Vision and Graph Fused Attention Network (VGA) for rumor\ndetection to utilize propagation structures among posts so as to obtain the\ncrowd opinions and further explore visual tampering features, as well as the\ntextual information hidden in images. We conduct extensive experiments on three\ndatasets, demonstrating that VGA can effectively detect multimodal rumors and\noutperform state-of-the-art methods significantly.\n","authors":["Lin Bai","Caiyan Jia","Ziying Song","Chaoqun Cui"],"pdf_url":"https://arxiv.org/pdf/2401.01759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01752v1","updated":"2024-01-03T14:08:39Z","published":"2024-01-03T14:08:39Z","title":"FullLoRA-AT: Efficiently Boosting the Robustness of Pretrained Vision\n  Transformers","summary":"  In recent years, the Vision Transformer (ViT) model has gradually become\nmainstream in various computer vision tasks, and the robustness of the model\nhas received increasing attention. However, existing large models tend to\nprioritize performance during training, potentially neglecting the robustness,\nwhich may lead to serious security concerns. In this paper, we establish a new\nchallenge: exploring how to use a small number of additional parameters for\nadversarial finetuning to quickly and effectively enhance the adversarial\nrobustness of a standardly trained model. To address this challenge, we develop\nthe novel LNLoRA module, incorporating a learnable layer normalization before\nthe conventional LoRA module, which helps mitigate magnitude differences in\nparameters between the adversarial and standard training paradigms.\n  Furthermore, we propose the FullLoRA-AT framework by integrating the\nlearnable LNLoRA modules into all key components of ViT-based models while\nkeeping the pretrained model frozen, which can significantly improve the model\nrobustness via adversarial finetuning in a parameter-efficient manner.\n  Extensive experiments on CIFAR-10, CIFAR-100, and Imagenette demonstrate the\nsuperiority of our proposed FullLoRA-AT framework. It achieves comparable\nrobustness with full finetuning while only requiring about 5% of the learnable\nparameters. This also effectively addresses concerns regarding extra model\nstorage space and enormous training time caused by adversarial finetuning.\n","authors":["Zheng Yuan","Jie Zhang","Shiguang Shan"],"pdf_url":"https://arxiv.org/pdf/2401.01752v1.pdf","comment":"10 pages, 2 figures, 6 tables"},{"id":"http://arxiv.org/abs/2312.16250v2","updated":"2024-01-03T13:59:14Z","published":"2023-12-25T17:20:57Z","title":"A Comprehensive Study of Object Tracking in Low-Light Environments","summary":"  Accurate object tracking in low-light environments is crucial, particularly\nin surveillance and ethology applications. However, achieving this is\nsignificantly challenging due to the poor quality of captured sequences.\nFactors such as noise, color imbalance, and low contrast contribute to these\nchallenges. This paper presents a comprehensive study examining the impact of\nthese distortions on automatic object trackers. Additionally, we propose a\nsolution to enhance tracking performance by integrating denoising and low-light\nenhancement methods into the transformer-based object tracking system.\nExperimental results show that the proposed tracker, trained with low-light\nsynthetic datasets, outperforms both the vanilla MixFormer and Siam R-CNN.\n","authors":["Anqi Yi","Nantheera Anantrasirichai"],"pdf_url":"https://arxiv.org/pdf/2312.16250v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01750v1","updated":"2024-01-03T13:58:35Z","published":"2024-01-03T13:58:35Z","title":"Towards Robust Semantic Segmentation against Patch-based Attack via\n  Attention Refinement","summary":"  The attention mechanism has been proven effective on various visual tasks in\nrecent years. In the semantic segmentation task, the attention mechanism is\napplied in various methods, including the case of both Convolution Neural\nNetworks (CNN) and Vision Transformer (ViT) as backbones. However, we observe\nthat the attention mechanism is vulnerable to patch-based adversarial attacks.\nThrough the analysis of the effective receptive field, we attribute it to the\nfact that the wide receptive field brought by global attention may lead to the\nspread of the adversarial patch. To address this issue, in this paper, we\npropose a Robust Attention Mechanism (RAM) to improve the robustness of the\nsemantic segmentation model, which can notably relieve the vulnerability\nagainst patch-based attacks. Compared to the vallina attention mechanism, RAM\nintroduces two novel modules called Max Attention Suppression and Random\nAttention Dropout, both of which aim to refine the attention matrix and limit\nthe influence of a single adversarial patch on the semantic segmentation\nresults of other positions. Extensive experiments demonstrate the effectiveness\nof our RAM to improve the robustness of semantic segmentation models against\nvarious patch-based attack methods under different attack settings.\n","authors":["Zheng Yuan","Jie Zhang","Yude Wang","Shiguang Shan","Xilin Chen"],"pdf_url":"https://arxiv.org/pdf/2401.01750v1.pdf","comment":"30 pages, 3 figures, 12 tables"},{"id":"http://arxiv.org/abs/2401.01749v1","updated":"2024-01-03T13:57:09Z","published":"2024-01-03T13:57:09Z","title":"Few-shot Image Generation via Information Transfer from the Built\n  Geodesic Surface","summary":"  Images generated by most of generative models trained with limited data often\nexhibit deficiencies in either fidelity, diversity, or both. One effective\nsolution to address the limitation is few-shot generative model adaption.\nHowever, the type of approaches typically rely on a large-scale pre-trained\nmodel, serving as a source domain, to facilitate information transfer to the\ntarget domain. In this paper, we propose a method called Information Transfer\nfrom the Built Geodesic Surface (ITBGS), which contains two module: Feature\nAugmentation on Geodesic Surface (FAGS); Interpolation and Regularization\n(I\\&R). With the FAGS module, a pseudo-source domain is created by projecting\nimage features from the training dataset into the Pre-Shape Space, subsequently\ngenerating new features on the Geodesic surface. Thus, no pre-trained models is\nneeded for the adaption process during the training of generative models with\nFAGS. I\\&R module are introduced for supervising the interpolated images and\nregularizing their relative distances, respectively, to further enhance the\nquality of generated images. Through qualitative and quantitative experiments,\nwe demonstrate that the proposed method consistently achieves optimal or\ncomparable results across a diverse range of semantically distinct datasets,\neven in extremely few-shot scenarios.\n","authors":["Yuexing Han","Liheng Ruan","Bing Wang"],"pdf_url":"https://arxiv.org/pdf/2401.01749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01736v1","updated":"2024-01-03T13:19:14Z","published":"2024-01-03T13:19:14Z","title":"Few-shot Adaptation of Multi-modal Foundation Models: A Survey","summary":"  Multi-modal (vision-language) models, such as CLIP, are replacing traditional\nsupervised pre-training models (e.g., ImageNet-based pre-training) as the new\ngeneration of visual foundation models. These models with robust and aligned\nsemantic representations learned from billions of internet image-text pairs and\ncan be applied to various downstream tasks in a zero-shot manner. However, in\nsome fine-grained domains like medical imaging and remote sensing, the\nperformance of multi-modal foundation models often leaves much to be desired.\nConsequently, many researchers have begun to explore few-shot adaptation\nmethods for these models, gradually deriving three main technical approaches:\n1) prompt-based methods, 2) adapter-based methods, and 3) external\nknowledge-based methods. Nevertheless, this rapidly developing field has\nproduced numerous results without a comprehensive survey to systematically\norganize the research progress. Therefore, in this survey, we introduce and\nanalyze the research advancements in few-shot adaptation methods for\nmulti-modal models, summarizing commonly used datasets and experimental setups,\nand comparing the results of different methods. In addition, due to the lack of\nreliable theoretical support for existing methods, we derive the few-shot\nadaptation generalization error bound for multi-modal models. The theorem\nreveals that the generalization error of multi-modal foundation models is\nconstrained by three factors: domain gap, model capacity, and sample size.\nBased on this, we propose three possible solutions from the following aspects:\n1) adaptive domain generalization, 2) adaptive model selection, and 3) adaptive\nknowledge utilization.\n","authors":["Fan Liu","Tianshu Zhang","Wenwen Dai","Wenwen Cai Xiaocong Zhou","Delong Chen"],"pdf_url":"https://arxiv.org/pdf/2401.01736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01734v1","updated":"2024-01-03T13:16:38Z","published":"2024-01-03T13:16:38Z","title":"Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data","summary":"  Assistive robots should be able to wash, fold or iron clothes. However, due\nto the variety, deformability and self-occlusions of clothes, creating\ngeneral-purpose robot systems for cloth manipulation is challenging. Synthetic\ndata is a promising direction to improve generalization, though its usability\nis often limited by the sim-to-real gap. To advance the use of synthetic data\nfor cloth manipulation and to enable tasks such as robotic folding, we present\na synthetic data pipeline to train keypoint detectors for almost flattened\ncloth items. To test its performance, we have also collected a real-world\ndataset. We train detectors for both T-shirts, towels and shorts and obtain an\naverage precision of 64.3%. Fine-tuning on real-world data improves performance\nto 74.2%. Additional insight is provided by discussing various failure modes of\nthe keypoint detectors and by comparing different approaches to obtain cloth\nmeshes and materials. We also quantify the remaining sim-to-real gap and argue\nthat further improvements to the fidelity of cloth assets will be required to\nfurther reduce this gap. The code, dataset and trained models are available\nonline.\n","authors":["Thomas Lips","Victor-Louis De Gusseme","Francis wyffels"],"pdf_url":"https://arxiv.org/pdf/2401.01734v1.pdf","comment":"submitted to journal on 20/12"},{"id":"http://arxiv.org/abs/2401.01730v1","updated":"2024-01-03T13:07:14Z","published":"2024-01-03T13:07:14Z","title":"STAF: 3D Human Mesh Recovery from Video with Spatio-Temporal Alignment\n  Fusion","summary":"  The recovery of 3D human mesh from monocular images has significantly been\ndeveloped in recent years. However, existing models usually ignore spatial and\ntemporal information, which might lead to mesh and image misalignment and\ntemporal discontinuity. For this reason, we propose a novel Spatio-Temporal\nAlignment Fusion (STAF) model. As a video-based model, it leverages coherence\nclues from human motion by an attention-based Temporal Coherence Fusion Module\n(TCFM). As for spatial mesh-alignment evidence, we extract fine-grained local\ninformation through predicted mesh projection on the feature maps. Based on the\nspatial features, we further introduce a multi-stage adjacent Spatial Alignment\nFusion Module (SAFM) to enhance the feature representation of the target frame.\nIn addition to the above, we propose an Average Pooling Module (APM) to allow\nthe model to focus on the entire input sequence rather than just the target\nframe. This method can remarkably improve the smoothness of recovery results\nfrom video. Extensive experiments on 3DPW, MPII3D, and H36M demonstrate the\nsuperiority of STAF. We achieve a state-of-the-art trade-off between precision\nand smoothness. Our code and more video results are on the project page\nhttps://yw0208.github.io/staf/\n","authors":["Wei Yao","Hongwen Zhang","Yunlian Sun","Jinhui Tang"],"pdf_url":"https://arxiv.org/pdf/2401.01730v1.pdf","comment":"Project Page: https://yw0208.github.io/staf/"},{"id":"http://arxiv.org/abs/2401.01724v1","updated":"2024-01-03T13:03:44Z","published":"2024-01-03T13:03:44Z","title":"Lightweight Adaptive Feature De-drifting for Compressed Image\n  Classification","summary":"  JPEG is a widely used compression scheme to efficiently reduce the volume of\ntransmitted images. The artifacts appear among blocks due to the information\nloss, which not only affects the quality of images but also harms the\nsubsequent high-level tasks in terms of feature drifting. High-level vision\nmodels trained on high-quality images will suffer performance degradation when\ndealing with compressed images, especially on mobile devices. Numerous\nlearning-based JPEG artifact removal methods have been proposed to handle\nvisual artifacts. However, it is not an ideal choice to use these JPEG artifact\nremoval methods as a pre-processing for compressed image classification for the\nfollowing reasons: 1. These methods are designed for human vision rather than\nhigh-level vision models; 2. These methods are not efficient enough to serve as\npre-processing on resource-constrained devices. To address these issues, this\npaper proposes a novel lightweight AFD module to boost the performance of\npre-trained image classification models when facing compressed images. First, a\nFDE-Net is devised to generate the spatial-wise FDM in the DCT domain. Next,\nthe estimated FDM is transmitted to the FE-Net to generate the mapping\nrelationship between degraded features and corresponding high-quality features.\nA simple but effective RepConv block equipped with structural\nre-parameterization is utilized in FE-Net, which enriches feature\nrepresentation in the training phase while maintaining efficiency in the\ndeployment phase. After training on limited compressed images, the AFD-Module\ncan serve as a \"plug-and-play\" model for pre-trained classification models to\nimprove their performance on compressed images. Experiments demonstrate that\nour proposed AFD module can comprehensively improve the accuracy of the\npre-trained classification models and significantly outperform the existing\nmethods.\n","authors":["Long Peng","Yang Cao","Yuejin Sun","Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2401.01724v1.pdf","comment":"Accepted by IEEE Transactions on Multimedia 2024"},{"id":"http://arxiv.org/abs/2401.01720v1","updated":"2024-01-03T12:54:31Z","published":"2024-01-03T12:54:31Z","title":"Local Adaptive Clustering Based Image Matching for Automatic Visual\n  Identification","summary":"  Monitoring cameras are extensively utilized in industrial production to\nmonitor equipment running. With advancements in computer vision, device\nrecognition using image features is viable. This paper presents a\nvision-assisted identification system that implements real-time automatic\nequipment labeling through image matching in surveillance videos. The system\ndeploys the ORB algorithm to extract image features and the GMS algorithm to\nremove incorrect matching points. According to the principles of clustering and\ntemplate locality, a method known as Local Adaptive Clustering (LAC) has been\nestablished to enhance label positioning. This method segments matching\ntemplates using the cluster center, which improves the efficiency and stability\nof labels. The experimental results demonstrate that LAC effectively curtails\nthe label drift.\n","authors":["Zhizhen Wang"],"pdf_url":"https://arxiv.org/pdf/2401.01720v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01717v1","updated":"2024-01-03T12:47:02Z","published":"2024-01-03T12:47:02Z","title":"Fact-checking based fake news detection: a review","summary":"  This paper reviews and summarizes the research results on fact-based fake\nnews from the perspectives of tasks and problems, algorithm strategies, and\ndatasets. First, the paper systematically explains the task definition and core\nproblems of fact-based fake news detection. Second, the paper summarizes the\nexisting detection methods based on the algorithm principles. Third, the paper\nanalyzes the classic and newly proposed datasets in the field, and summarizes\nthe experimental results on each dataset. Finally, the paper summarizes the\nadvantages and disadvantages of existing methods, proposes several challenges\nthat methods in this field may face, and looks forward to the next stage of\nresearch. It is hoped that this paper will provide reference for subsequent\nwork in the field.\n","authors":["Yuzhou Yang","Yangming Zhou","Qichao Ying","Zhenxing Qian","Dan Zeng","Liang Liu"],"pdf_url":"https://arxiv.org/pdf/2401.01717v1.pdf","comment":"Invited short review paper (in Chinese)"},{"id":"http://arxiv.org/abs/2212.08123v3","updated":"2024-01-03T12:22:46Z","published":"2022-12-15T20:23:09Z","title":"Bayesian posterior approximation with stochastic ensembles","summary":"  We introduce ensembles of stochastic neural networks to approximate the\nBayesian posterior, combining stochastic methods such as dropout with deep\nensembles. The stochastic ensembles are formulated as families of distributions\nand trained to approximate the Bayesian posterior with variational inference.\nWe implement stochastic ensembles based on Monte Carlo dropout, DropConnect and\na novel non-parametric version of dropout and evaluate them on a toy problem\nand CIFAR image classification. For both tasks, we test the quality of the\nposteriors directly against Hamiltonian Monte Carlo simulations. Our results\nshow that stochastic ensembles provide more accurate posterior estimates than\nother popular baselines for Bayesian inference.\n","authors":["Oleksandr Balabanov","Bernhard Mehlig","Hampus Linander"],"pdf_url":"https://arxiv.org/pdf/2212.08123v3.pdf","comment":"19 pages, CVPR 2023"},{"id":"http://arxiv.org/abs/2311.11467v2","updated":"2024-01-03T12:09:50Z","published":"2023-08-02T13:37:08Z","title":"SkateboardAI: The Coolest Video Action Recognition for Skateboarding","summary":"  Impressed by the coolest skateboarding sports program from 2021 Tokyo Olympic\nGames, we are the first to curate the original real-world video datasets\n\"SkateboardAI\" in the wild, even self-design and implement diverse uni-modal\nand multi-modal video action recognition approaches to recognize different\ntricks accurately. For uni-modal methods, we separately apply (1) CNN and LSTM;\n(2) CNN and BiLSTM; (3) CNN and BiLSTM with effective attention mechanisms; (4)\nTransformer-based action recognition pipeline. Transferred to the multi-modal\nconditions, we investigated the two-stream Inflated-3D architecture on\n\"SkateboardAI\" datasets to compare its performance with uni-modal cases. In\nsum, our objective is developing an excellent AI sport referee for the coolest\nskateboarding competitions.\n","authors":["Hanxiao Chen"],"pdf_url":"https://arxiv.org/pdf/2311.11467v2.pdf","comment":"The original first-author work has been accepted and presented by\n  CVPR 2022 WiCV Workshop (This is the long-version paper)"},{"id":"http://arxiv.org/abs/2401.01699v1","updated":"2024-01-03T12:06:02Z","published":"2024-01-03T12:06:02Z","title":"WordArt Designer API: User-Driven Artistic Typography Synthesis with\n  Large Language Models on ModelScope","summary":"  This paper introduces the WordArt Designer API, a novel framework for\nuser-driven artistic typography synthesis utilizing Large Language Models\n(LLMs) on ModelScope. We address the challenge of simplifying artistic\ntypography for non-professionals by offering a dynamic, adaptive, and\ncomputationally efficient alternative to traditional rigid templates. Our\napproach leverages the power of LLMs to understand and interpret user input,\nfacilitating a more intuitive design process. We demonstrate through various\ncase studies how users can articulate their aesthetic preferences and\nfunctional requirements, which the system then translates into unique and\ncreative typographic designs. Our evaluations indicate significant improvements\nin user satisfaction, design flexibility, and creative expression over existing\nsystems. The WordArt Designer API not only democratizes the art of typography\nbut also opens up new possibilities for personalized digital communication and\ndesign.\n","authors":["Jun-Yan He","Zhi-Qi Cheng","Chenyang Li","Jingdong Sun","Wangmeng Xiang","Yusen Hu","Xianhui Lin","Xiaoyang Kang","Zengke Jin","Bin Luo","Yifeng Geng","Xuansong Xie","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2401.01699v1.pdf","comment":"Spotlight Paper from the Workshop on Machine Learning for Creativity\n  and Design at the 37th Conference on Neural Information Processing Systems\n  (NeurIPS 2023). 5 pages, 5 figures"},{"id":"http://arxiv.org/abs/2401.01693v1","updated":"2024-01-03T11:54:48Z","published":"2024-01-03T11:54:48Z","title":"AID-DTI: Accelerating High-fidelity Diffusion Tensor Imaging with\n  Detail-Preserving Model-based Deep Learning","summary":"  Deep learning has shown great potential in accelerating diffusion tensor\nimaging (DTI). Nevertheless, existing methods tend to suffer from Rician noise\nand detail loss in reconstructing the DTI-derived parametric maps especially\nwhen sparsely sampled q-space data are used. This paper proposes a novel\nmethod, AID-DTI (Accelerating hIgh fiDelity Diffusion Tensor Imaging), to\nfacilitate fast and accurate DTI with only six measurements. AID-DTI is\nequipped with a newly designed Singular Value Decomposition (SVD)-based\nregularizer, which can effectively capture fine details while suppressing noise\nduring network training. Experimental results on Human Connectome Project (HCP)\ndata consistently demonstrate that the proposed method estimates DTI parameter\nmaps with fine-grained details and outperforms three state-of-the-art methods\nboth quantitatively and qualitatively.\n","authors":["Wenxin Fan","Jian Cheng","Cheng Li","Xinrui Ma","Jing Yang","Juan Zou","Ruoyou Wu","Qiegen Liu","Shanshan Wang"],"pdf_url":"https://arxiv.org/pdf/2401.01693v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.17190v2","updated":"2024-01-03T11:44:30Z","published":"2023-10-26T07:05:38Z","title":"Lookup Table meets Local Laplacian Filter: Pyramid Reconstruction\n  Network for Tone Mapping","summary":"  Tone mapping aims to convert high dynamic range (HDR) images to low dynamic\nrange (LDR) representations, a critical task in the camera imaging pipeline. In\nrecent years, 3-Dimensional LookUp Table (3D LUT) based methods have gained\nattention due to their ability to strike a favorable balance between\nenhancement performance and computational efficiency. However, these methods\noften fail to deliver satisfactory results in local areas since the look-up\ntable is a global operator for tone mapping, which works based on pixel values\nand fails to incorporate crucial local information. To this end, this paper\naims to address this issue by exploring a novel strategy that integrates global\nand local operators by utilizing closed-form Laplacian pyramid decomposition\nand reconstruction. Specifically, we employ image-adaptive 3D LUTs to\nmanipulate the tone in the low-frequency image by leveraging the specific\ncharacteristics of the frequency information. Furthermore, we utilize local\nLaplacian filters to refine the edge details in the high-frequency components\nin an adaptive manner. Local Laplacian filters are widely used to preserve edge\ndetails in photographs, but their conventional usage involves manual tuning and\nfixed implementation within camera imaging pipelines or photo editing tools. We\npropose to learn parameter value maps progressively for local Laplacian filters\nfrom annotated data using a lightweight network. Our model achieves\nsimultaneous global tone manipulation and local edge detail preservation in an\nend-to-end manner. Extensive experimental results on two benchmark datasets\ndemonstrate that the proposed method performs favorably against\nstate-of-the-art methods.\n","authors":["Feng Zhang","Ming Tian","Zhiqiang Li","Bin Xu","Qingbo Lu","Changxin Gao","Nong Sang"],"pdf_url":"https://arxiv.org/pdf/2310.17190v2.pdf","comment":"12 pages, 6 figures, accepted by NeurlPS 2023"},{"id":"http://arxiv.org/abs/2401.01686v1","updated":"2024-01-03T11:44:09Z","published":"2024-01-03T11:44:09Z","title":"ODTrack: Online Dense Temporal Token Learning for Visual Tracking","summary":"  Online contextual reasoning and association across consecutive video frames\nare critical to perceive instances in visual tracking. However, most current\ntop-performing trackers persistently lean on sparse temporal relationships\nbetween reference and search frames via an offline mode. Consequently, they can\nonly interact independently within each image-pair and establish limited\ntemporal correlations. To alleviate the above problem, we propose a simple,\nflexible and effective video-level tracking pipeline, named \\textbf{ODTrack},\nwhich densely associates the contextual relationships of video frames in an\nonline token propagation manner. ODTrack receives video frames of arbitrary\nlength to capture the spatio-temporal trajectory relationships of an instance,\nand compresses the discrimination features (localization information) of a\ntarget into a token sequence to achieve frame-to-frame association. This new\nsolution brings the following benefits: 1) the purified token sequences can\nserve as prompts for the inference in the next video frame, whereby past\ninformation is leveraged to guide future inference; 2) the complex online\nupdate strategies are effectively avoided by the iterative propagation of token\nsequences, and thus we can achieve more efficient model representation and\ncomputation. ODTrack achieves a new \\textit{SOTA} performance on seven\nbenchmarks, while running at real-time speed. Code and models are available at\n\\url{https://github.com/GXNU-ZhongLab/ODTrack}.\n","authors":["Yaozong Zheng","Bineng Zhong","Qihua Liang","Zhiyi Mo","Shengping Zhang","Xianxian Li"],"pdf_url":"https://arxiv.org/pdf/2401.01686v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01685v1","updated":"2024-01-03T11:41:57Z","published":"2024-01-03T11:41:57Z","title":"Modality Exchange Network for Retinogeniculate Visual Pathway\n  Segmentation","summary":"  Accurate segmentation of the retinogeniculate visual pathway (RGVP) aids in\nthe diagnosis and treatment of visual disorders by identifying disruptions or\nabnormalities within the pathway. However, the complex anatomical structure and\nconnectivity of RGVP make it challenging to achieve accurate segmentation. In\nthis study, we propose a novel Modality Exchange Network (ME-Net) that\neffectively utilizes multi-modal magnetic resonance (MR) imaging information to\nenhance RGVP segmentation. Our ME-Net has two main contributions. Firstly, we\nintroduce an effective multi-modal soft-exchange technique. Specifically, we\ndesign a channel and spatially mixed attention module to exchange modality\ninformation between T1-weighted and fractional anisotropy MR images. Secondly,\nwe propose a cross-fusion module that further enhances the fusion of\ninformation between the two modalities. Experimental results demonstrate that\nour method outperforms existing state-of-the-art approaches in terms of RGVP\nsegmentation performance.\n","authors":["Hua Han","Cheng Li","Lei Xie","Yuanjing Feng","Alou Diakite","Shanshan Wang"],"pdf_url":"https://arxiv.org/pdf/2401.01685v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01676v1","updated":"2024-01-03T11:25:11Z","published":"2024-01-03T11:25:11Z","title":"Performance Evaluation of GPS Trajectory Rasterization Methods","summary":"  The availability of the Global Positioning System (GPS) trajectory data is\nincreasing along with the availability of different GPS receivers and with the\nincreasing use of various mobility services. GPS trajectory is an important\ndata source which is used in traffic density detection, transport mode\ndetection, mapping data inferences with the use of different methods such as\nimage processing and machine learning methods. While the data size increases,\nefficient representation of this type of data is becoming difficult to be used\nin these methods. A common approach is the representation of GPS trajectory\ninformation such as average speed, bearing, etc. in raster image form and\napplying analysis methods. In this study, we evaluate GPS trajectory data\nrasterization using the spatial join functions of QGIS, PostGIS+QGIS, and our\niterative spatial structured grid aggregation implementation coded in the\nPython programming language. Our implementation is also parallelizable, and\nthis parallelization is also included as the fourth method. According to the\nresults of experiment carried out with an example GPS trajectory dataset, QGIS\nmethod and PostGIS+QGIS method showed relatively low performance with respect\nto our method using the metric of total processing time. PostGIS+QGIS method\nachieved the best results for spatial join though its total performance\ndecreased quickly while test area size increases. On the other hand, both of\nour methods' performances decrease directly proportional to GPS point. And our\nmethods' performance can be increased proportional to the increase with the\nnumber of processor cores and/or with multiple computing clusters.\n","authors":["Necip Enes Gengec","Ergin Tari"],"pdf_url":"https://arxiv.org/pdf/2401.01676v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01674v1","updated":"2024-01-03T11:16:38Z","published":"2024-01-03T11:16:38Z","title":"Transformer RGBT Tracking with Spatio-Temporal Multimodal Tokens","summary":"  Many RGBT tracking researches primarily focus on modal fusion design, while\noverlooking the effective handling of target appearance changes. While some\napproaches have introduced historical frames or fuse and replace initial\ntemplates to incorporate temporal information, they have the risk of disrupting\nthe original target appearance and accumulating errors over time. To alleviate\nthese limitations, we propose a novel Transformer RGBT tracking approach, which\nmixes spatio-temporal multimodal tokens from the static multimodal templates\nand multimodal search regions in Transformer to handle target appearance\nchanges, for robust RGBT tracking. We introduce independent dynamic template\ntokens to interact with the search region, embedding temporal information to\naddress appearance changes, while also retaining the involvement of the initial\nstatic template tokens in the joint feature extraction process to ensure the\npreservation of the original reliable target appearance information that\nprevent deviations from the target appearance caused by traditional temporal\nupdates. We also use attention mechanisms to enhance the target features of\nmultimodal template tokens by incorporating supplementary modal cues, and make\nthe multimodal search region tokens interact with multimodal dynamic template\ntokens via attention mechanisms, which facilitates the conveyance of\nmultimodal-enhanced target change information. Our module is inserted into the\ntransformer backbone network and inherits joint feature extraction,\nsearch-template matching, and cross-modal interaction. Extensive experiments on\nthree RGBT benchmark datasets show that the proposed approach maintains\ncompetitive performance compared to other state-of-the-art tracking algorithms\nwhile running at 39.1 FPS.\n","authors":["Dengdi Sun","Yajie Pan","Andong Lu","Chenglong Li","Bin Luo"],"pdf_url":"https://arxiv.org/pdf/2401.01674v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.05308v2","updated":"2024-01-03T11:12:33Z","published":"2023-03-09T14:58:01Z","title":"SpyroPose: SE(3) Pyramids for Object Pose Distribution Estimation","summary":"  Object pose estimation is a core computer vision problem and often an\nessential component in robotics. Pose estimation is usually approached by\nseeking the single best estimate of an object's pose, but this approach is\nill-suited for tasks involving visual ambiguity. In such cases it is desirable\nto estimate the uncertainty as a pose distribution to allow downstream tasks to\nmake informed decisions. Pose distributions can have arbitrary complexity which\nmotivates estimating unparameterized distributions, however, until now they\nhave only been used for orientation estimation on SO(3) due to the difficulty\nin training on and normalizing over SE(3). We propose a novel method for pose\ndistribution estimation on SE(3). We use a hierarchical grid, a pyramid, which\nenables efficient importance sampling during training and sparse evaluation of\nthe pyramid at inference, allowing real time 6D pose distribution estimation.\nOur method outperforms state-of-the-art methods on SO(3), and to the best of\nour knowledge, we provide the first quantitative results on pose distribution\nestimation on SE(3). Code will be available at spyropose.github.io\n","authors":["Rasmus Laurvig Haugaard","Frederik Hagelskjær","Thorbjørn Mosekjær Iversen"],"pdf_url":"https://arxiv.org/pdf/2303.05308v2.pdf","comment":"ICCVW 2023 (R6D)"},{"id":"http://arxiv.org/abs/2401.01662v1","updated":"2024-01-03T10:47:20Z","published":"2024-01-03T10:47:20Z","title":"Simultaneous q-Space Sampling Optimization and Reconstruction for Fast\n  and High-fidelity Diffusion Magnetic Resonance Imaging","summary":"  Diffusion Magnetic Resonance Imaging (dMRI) plays a crucial role in the\nnoninvasive investigation of tissue microstructural properties and structural\nconnectivity in the \\textit{in vivo} human brain. However, to effectively\ncapture the intricate characteristics of water diffusion at various directions\nand scales, it is important to employ comprehensive q-space sampling.\nUnfortunately, this requirement leads to long scan times, limiting the clinical\napplicability of dMRI. To address this challenge, we propose SSOR, a\nSimultaneous q-Space sampling Optimization and Reconstruction framework. We\njointly optimize a subset of q-space samples using a continuous representation\nof spherical harmonic functions and a reconstruction network. Additionally, we\nintegrate the unique properties of diffusion magnetic resonance imaging (dMRI)\nin both the q-space and image domains by applying $l1$-norm and total-variation\nregularization. The experiments conducted on HCP data demonstrate that SSOR has\npromising strengths both quantitatively and qualitatively and exhibits\nrobustness to noise.\n","authors":["Jing Yang","Jian Cheng","Cheng Li","Wenxin Fan","Juan Zou","Ruoyou Wu","Shanshan Wang"],"pdf_url":"https://arxiv.org/pdf/2401.01662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01659v1","updated":"2024-01-03T10:35:35Z","published":"2024-01-03T10:35:35Z","title":"DiffYOLO: Object Detection for Anti-Noise via YOLO and Diffusion Models","summary":"  Object detection models represented by YOLO series have been widely used and\nhave achieved great results on the high quality datasets, but not all the\nworking conditions are ideal. To settle down the problem of locating targets on\nlow quality datasets, the existing methods either train a new object detection\nnetwork, or need a large collection of low-quality datasets to train. However,\nwe propose a framework in this paper and apply it on the YOLO models called\nDiffYOLO. Specifically, we extract feature maps from the denoising diffusion\nprobabilistic models to enhance the well-trained models, which allows us\nfine-tune YOLO on high-quality datasets and test on low-quality datasets. The\nresults proved this framework can not only prove the performance on noisy\ndatasets, but also prove the detection results on high-quality test datasets.\nWe will supplement more experiments later (with various datasets and network\narchitectures).\n","authors":["Yichen Liu","Huajian Zhang","Daqing Gao"],"pdf_url":"https://arxiv.org/pdf/2401.01659v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00110v2","updated":"2024-01-03T10:12:30Z","published":"2023-12-30T01:24:25Z","title":"Diffusion Model with Perceptual Loss","summary":"  Diffusion models trained with mean squared error loss tend to generate\nunrealistic samples. Current state-of-the-art models rely on classifier-free\nguidance to improve sample quality, yet its surprising effectiveness is not\nfully understood. In this paper, We show that the effectiveness of\nclassifier-free guidance partly originates from it being a form of implicit\nperceptual guidance. As a result, we can directly incorporate perceptual loss\nin diffusion training to improve sample quality. Since the score matching\nobjective used in diffusion training strongly resembles the denoising\nautoencoder objective used in unsupervised training of perceptual networks, the\ndiffusion model itself is a perceptual network and can be used to generate\nmeaningful perceptual loss. We propose a novel self-perceptual objective that\nresults in diffusion models capable of generating more realistic samples. For\nconditional generation, our method only improves sample quality without\nentanglement with the conditional input and therefore does not sacrifice sample\ndiversity. Our method can also improve sample quality for unconditional\ngeneration, which was not possible with classifier-free guidance before.\n","authors":["Shanchuan Lin","Xiao Yang"],"pdf_url":"https://arxiv.org/pdf/2401.00110v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01651v1","updated":"2024-01-03T10:08:40Z","published":"2024-01-03T10:08:40Z","title":"AIGCBench: Comprehensive Evaluation of Image-to-Video Content Generated\n  by AI","summary":"  The burgeoning field of Artificial Intelligence Generated Content (AIGC) is\nwitnessing rapid advancements, particularly in video generation. This paper\nintroduces AIGCBench, a pioneering comprehensive and scalable benchmark\ndesigned to evaluate a variety of video generation tasks, with a primary focus\non Image-to-Video (I2V) generation. AIGCBench tackles the limitations of\nexisting benchmarks, which suffer from a lack of diverse datasets, by including\na varied and open-domain image-text dataset that evaluates different\nstate-of-the-art algorithms under equivalent conditions. We employ a novel text\ncombiner and GPT-4 to create rich text prompts, which are then used to generate\nimages via advanced Text-to-Image models. To establish a unified evaluation\nframework for video generation tasks, our benchmark includes 11 metrics\nspanning four dimensions to assess algorithm performance. These dimensions are\ncontrol-video alignment, motion effects, temporal consistency, and video\nquality. These metrics are both reference video-dependent and video-free,\nensuring a comprehensive evaluation strategy. The evaluation standard proposed\ncorrelates well with human judgment, providing insights into the strengths and\nweaknesses of current I2V algorithms. The findings from our extensive\nexperiments aim to stimulate further research and development in the I2V field.\nAIGCBench represents a significant step toward creating standardized benchmarks\nfor the broader AIGC landscape, proposing an adaptable and equitable framework\nfor future assessments of video generation tasks.\n","authors":["Fanda Fan","Chunjie Luo","Jianfeng Zhan","Wanling Gao"],"pdf_url":"https://arxiv.org/pdf/2401.01651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01650v1","updated":"2024-01-03T10:07:11Z","published":"2024-01-03T10:07:11Z","title":"De-Confusing Pseudo-Labels in Source-Free Domain Adaptation","summary":"  Source-free domain adaptation (SFDA) aims to transfer knowledge learned from\na source domain to an unlabeled target domain, where the source data is\nunavailable during adaptation. Existing approaches for SFDA focus on\nself-training usually including well-established entropy minimization and\npseudo-labeling techniques. Recent work suggested a co-learning strategy to\nimprove the quality of the generated target pseudo-labels using robust\npretrained networks such as Swin-B. However, since the generated pseudo-labels\ndepend on the source model, they may be noisy due to domain shift. In this\npaper, we view SFDA from the perspective of label noise learning and learn to\nde-confuse the pseudo-labels. More specifically, we learn a noise transition\nmatrix of the pseudo-labels to capture the label corruption of each class and\nlearn the underlying true label distribution. Estimating the noise transition\nmatrix enables a better true class-posterior estimation results with better\nprediction accuracy. We demonstrate the effectiveness of our approach applied\nwith several SFDA methods: SHOT, SHOT++, and AaD. We obtain state-of-the-art\nresults on three domain adaptation datasets: VisDA, DomainNet, and OfficeHome.\n","authors":["Idit Diamant","Idan Achituve","Arnon Netzer"],"pdf_url":"https://arxiv.org/pdf/2401.01650v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2212.03795"},{"id":"http://arxiv.org/abs/2401.01647v1","updated":"2024-01-03T09:46:43Z","published":"2024-01-03T09:46:43Z","title":"SIGNeRF: Scene Integrated Generation for Neural Radiance Fields","summary":"  Advances in image diffusion models have recently led to notable improvements\nin the generation of high-quality images. In combination with Neural Radiance\nFields (NeRFs), they enabled new opportunities in 3D generation. However, most\ngenerative 3D approaches are object-centric and applying them to editing\nexisting photorealistic scenes is not trivial. We propose SIGNeRF, a novel\napproach for fast and controllable NeRF scene editing and scene-integrated\nobject generation. A new generative update strategy ensures 3D consistency\nacross the edited images, without requiring iterative optimization. We find\nthat depth-conditioned diffusion models inherently possess the capability to\ngenerate 3D consistent views by requesting a grid of images instead of single\nviews. Based on these insights, we introduce a multi-view reference sheet of\nmodified images. Our method updates an image collection consistently based on\nthe reference sheet and refines the original NeRF with the newly generated\nimage set in one go. By exploiting the depth conditioning mechanism of the\nimage diffusion model, we gain fine control over the spatial location of the\nedit and enforce shape guidance by a selected region or an external mesh.\n","authors":["Jan-Niklas Dihlmann","Andreas Engelhardt","Hendrik Lensch"],"pdf_url":"https://arxiv.org/pdf/2401.01647v1.pdf","comment":"Project Page: https://signerf.jdihlmann.com"},{"id":"http://arxiv.org/abs/2312.13763v2","updated":"2024-01-03T09:40:56Z","published":"2023-12-21T11:41:02Z","title":"Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed\n  Diffusion Models","summary":"  Text-guided diffusion models have revolutionized image and video generation\nand have also been successfully used for optimization-based 3D object\nsynthesis. Here, we instead focus on the underexplored text-to-4D setting and\nsynthesize dynamic, animated 3D objects using score distillation methods with\nan additional temporal dimension. Compared to previous work, we pursue a novel\ncompositional generation-based approach, and combine text-to-image,\ntext-to-video, and 3D-aware multiview diffusion models to provide feedback\nduring 4D object optimization, thereby simultaneously enforcing temporal\nconsistency, high-quality visual appearance and realistic geometry. Our method,\ncalled Align Your Gaussians (AYG), leverages dynamic 3D Gaussian Splatting with\ndeformation fields as 4D representation. Crucial to AYG is a novel method to\nregularize the distribution of the moving 3D Gaussians and thereby stabilize\nthe optimization and induce motion. We also propose a motion amplification\nmechanism as well as a new autoregressive synthesis scheme to generate and\ncombine multiple 4D sequences for longer generation. These techniques allow us\nto synthesize vivid dynamic scenes, outperform previous work qualitatively and\nquantitatively and achieve state-of-the-art text-to-4D performance. Due to the\nGaussian 4D representation, different 4D animations can be seamlessly combined,\nas we demonstrate. AYG opens up promising avenues for animation, simulation and\ndigital content creation as well as synthetic data generation.\n","authors":["Huan Ling","Seung Wook Kim","Antonio Torralba","Sanja Fidler","Karsten Kreis"],"pdf_url":"https://arxiv.org/pdf/2312.13763v2.pdf","comment":"Project page:\n  https://research.nvidia.com/labs/toronto-ai/AlignYourGaussians/"},{"id":"http://arxiv.org/abs/2401.01646v1","updated":"2024-01-03T09:39:36Z","published":"2024-01-03T09:39:36Z","title":"Prototypical Information Bottlenecking and Disentangling for Multimodal\n  Cancer Survival Prediction","summary":"  Multimodal learning significantly benefits cancer survival prediction,\nespecially the integration of pathological images and genomic data. Despite\nadvantages of multimodal learning for cancer survival prediction, massive\nredundancy in multimodal data prevents it from extracting discriminative and\ncompact information: (1) An extensive amount of intra-modal task-unrelated\ninformation blurs discriminability, especially for gigapixel whole slide images\n(WSIs) with many patches in pathology and thousands of pathways in genomic\ndata, leading to an ``intra-modal redundancy\" issue. (2) Duplicated information\namong modalities dominates the representation of multimodal data, which makes\nmodality-specific information prone to being ignored, resulting in an\n``inter-modal redundancy\" issue. To address these, we propose a new framework,\nPrototypical Information Bottlenecking and Disentangling (PIBD), consisting of\nPrototypical Information Bottleneck (PIB) module for intra-modal redundancy and\nPrototypical Information Disentanglement (PID) module for inter-modal\nredundancy. Specifically, a variant of information bottleneck, PIB, is proposed\nto model prototypes approximating a bunch of instances for different risk\nlevels, which can be used for selection of discriminative instances within\nmodality. PID module decouples entangled multimodal data into compact distinct\ncomponents: modality-common and modality-specific knowledge, under the guidance\nof the joint prototypical distribution. Extensive experiments on five cancer\nbenchmark datasets demonstrated our superiority over other methods.\n","authors":["Yilan Zhang","Yingxue Xu","Jianqi Chen","Fengying Xie","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2401.01646v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01643v1","updated":"2024-01-03T09:37:33Z","published":"2024-01-03T09:37:33Z","title":"S3Net: Innovating Stereo Matching and Semantic Segmentation with a\n  Single-Branch Semantic Stereo Network in Satellite Epipolar Imagery","summary":"  Stereo matching and semantic segmentation are significant tasks in binocular\nsatellite 3D reconstruction. However, previous studies primarily view these as\nindependent parallel tasks, lacking an integrated multitask learning framework.\nThis work introduces a solution, the Single-branch Semantic Stereo Network\n(S3Net), which innovatively combines semantic segmentation and stereo matching\nusing Self-Fuse and Mutual-Fuse modules. Unlike preceding methods that utilize\nsemantic or disparity information independently, our method dentifies and\nleverages the intrinsic link between these two tasks, leading to a more\naccurate understanding of semantic information and disparity estimation.\nComparative testing on the US3D dataset proves the effectiveness of our S3Net.\nOur model improves the mIoU in semantic segmentation from 61.38 to 67.39, and\nreduces the D1-Error and average endpoint error (EPE) in disparity estimation\nfrom 10.051 to 9.579 and 1.439 to 1.403 respectively, surpassing existing\ncompetitive methods. Our codes are available at:https://github.com/CVEO/S3Net.\n","authors":["Qingyuan Yang","Guanzhou Chen","Xiaoliang Tan","Tong Wang","Jiaqi Wang","Xiaodong Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.01643v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01642v1","updated":"2024-01-03T09:37:03Z","published":"2024-01-03T09:37:03Z","title":"BLADE: Box-Level Supervised Amodal Segmentation through Directed\n  Expansion","summary":"  Perceiving the complete shape of occluded objects is essential for human and\nmachine intelligence. While the amodal segmentation task is to predict the\ncomplete mask of partially occluded objects, it is time-consuming and\nlabor-intensive to annotate the pixel-level ground truth amodal masks.\nBox-level supervised amodal segmentation addresses this challenge by relying\nsolely on ground truth bounding boxes and instance classes as supervision,\nthereby alleviating the need for exhaustive pixel-level annotations.\nNevertheless, current box-level methodologies encounter limitations in\ngenerating low-resolution masks and imprecise boundaries, failing to meet the\ndemands of practical real-world applications. We present a novel solution to\ntackle this problem by introducing a directed expansion approach from visible\nmasks to corresponding amodal masks. Our approach involves a hybrid end-to-end\nnetwork based on the overlapping region - the area where different instances\nintersect. Diverse segmentation strategies are applied for overlapping regions\nand non-overlapping regions according to distinct characteristics. To guide the\nexpansion of visible masks, we introduce an elaborately-designed connectivity\nloss for overlapping regions, which leverages correlations with visible masks\nand facilitates accurate amodal segmentation. Experiments are conducted on\nseveral challenging datasets and the results show that our proposed method can\noutperform existing state-of-the-art methods with large margins.\n","authors":["Zhaochen Liu","Zhixuan Li","Tingting Jiang"],"pdf_url":"https://arxiv.org/pdf/2401.01642v1.pdf","comment":"Accepted to AAAI 2024;"},{"id":"http://arxiv.org/abs/2312.01711v3","updated":"2024-01-03T09:35:21Z","published":"2023-12-04T07:53:59Z","title":"Regressor-Segmenter Mutual Prompt Learning for Crowd Counting","summary":"  Crowd counting has achieved significant progress by training regressors to\npredict instance positions. In heavily crowded scenarios, however, regressors\nare challenged by uncontrollable annotation variance, which causes density map\nbias and context information inaccuracy. In this study, we propose mutual\nprompt learning (mPrompt), which leverages a regressor and a segmenter as\nguidance for each other, solving bias and inaccuracy caused by annotation\nvariance while distinguishing foreground from background. In specific, mPrompt\nleverages point annotations to tune the segmenter and predict pseudo head masks\nin a way of point prompt learning. It then uses the predicted segmentation\nmasks, which serve as spatial constraint, to rectify biased point annotations\nas context prompt learning. mPrompt defines a way of mutual information\nmaximization from prompt learning, mitigating the impact of annotation variance\nwhile improving model accuracy. Experiments show that mPrompt significantly\nreduces the Mean Average Error (MAE), demonstrating the potential to be general\nframework for down-stream vision tasks.\n","authors":["Mingyue Guo","Li Yuan","Zhaoyi Yan","Binghui Chen","Yaowei Wang","Qixiang Ye"],"pdf_url":"https://arxiv.org/pdf/2312.01711v3.pdf","comment":"mPrompt defines a way of mutual information maximization from prompt\n  learning"},{"id":"http://arxiv.org/abs/2304.01186v2","updated":"2024-01-03T09:10:12Z","published":"2023-04-03T17:55:14Z","title":"Follow Your Pose: Pose-Guided Text-to-Video Generation using Pose-Free\n  Videos","summary":"  Generating text-editable and pose-controllable character videos have an\nimperious demand in creating various digital human. Nevertheless, this task has\nbeen restricted by the absence of a comprehensive dataset featuring paired\nvideo-pose captions and the generative prior models for videos. In this work,\nwe design a novel two-stage training scheme that can utilize easily obtained\ndatasets (i.e.,image pose pair and pose-free video) and the pre-trained\ntext-to-image (T2I) model to obtain the pose-controllable character videos.\nSpecifically, in the first stage, only the keypoint-image pairs are used only\nfor a controllable text-to-image generation. We learn a zero-initialized\nconvolutional encoder to encode the pose information. In the second stage, we\nfinetune the motion of the above network via a pose-free video dataset by\nadding the learnable temporal self-attention and reformed cross-frame\nself-attention blocks. Powered by our new designs, our method successfully\ngenerates continuously pose-controllable character videos while keeps the\nediting and concept composition ability of the pre-trained T2I model. The code\nand models will be made publicly available.\n","authors":["Yue Ma","Yingqing He","Xiaodong Cun","Xintao Wang","Siran Chen","Ying Shan","Xiu Li","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2304.01186v2.pdf","comment":"Project page: https://follow-your-pose.github.io/; Github repository:\n  https://github.com/mayuelala/FollowYourPose"},{"id":"http://arxiv.org/abs/2401.01624v1","updated":"2024-01-03T08:49:29Z","published":"2024-01-03T08:49:29Z","title":"Context-Aware Interaction Network for RGB-T Semantic Segmentation","summary":"  RGB-T semantic segmentation is a key technique for autonomous driving scenes\nunderstanding. For the existing RGB-T semantic segmentation methods, however,\nthe effective exploration of the complementary relationship between different\nmodalities is not implemented in the information interaction between multiple\nlevels. To address such an issue, the Context-Aware Interaction Network\n(CAINet) is proposed for RGB-T semantic segmentation, which constructs\ninteraction space to exploit auxiliary tasks and global context for explicitly\nguided learning. Specifically, we propose a Context-Aware Complementary\nReasoning (CACR) module aimed at establishing the complementary relationship\nbetween multimodal features with the long-term context in both spatial and\nchannel dimensions. Further, considering the importance of global contextual\nand detailed information, we propose the Global Context Modeling (GCM) module\nand Detail Aggregation (DA) module, and we introduce specific auxiliary\nsupervision to explicitly guide the context interaction and refine the\nsegmentation map. Extensive experiments on two benchmark datasets of MFNet and\nPST900 demonstrate that the proposed CAINet achieves state-of-the-art\nperformance. The code is available at https://github.com/YingLv1106/CAINet.\n","authors":["Ying Lv","Zhi Liu","Gongyang Li"],"pdf_url":"https://arxiv.org/pdf/2401.01624v1.pdf","comment":"13 pages, 7 figures, Accepted by IEEE Transactions on Multimedia 2024"},{"id":"http://arxiv.org/abs/2401.01614v1","updated":"2024-01-03T08:33:09Z","published":"2024-01-03T08:33:09Z","title":"GPT-4V(ision) is a Generalist Web Agent, if Grounded","summary":"  The recent development on large multimodal models (LMMs), especially\nGPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries\nof multimodal models beyond traditional tasks like image captioning and visual\nquestion answering. In this work, we explore the potential of LMMs like GPT-4V\nas a generalist web agent that can follow natural language instructions to\ncomplete tasks on any given website. We propose SEEACT, a generalist web agent\nthat harnesses the power of LMMs for integrated visual understanding and acting\non the web. We evaluate on the recent MIND2WEB benchmark. In addition to\nstandard offline evaluation on cached websites, we enable a new online\nevaluation setting by developing a tool that allows running web agents on live\nwebsites. We show that GPT-4V presents a great potential for web agents - it\ncan successfully complete 50% of the tasks on live websites if we manually\nground its textual plans into actions on the websites. This substantially\noutperforms text-only LLMs like GPT-4 or smaller models (FLAN-T5 and BLIP-2)\nspecifically fine-tuned for web agents. However, grounding still remains a\nmajor challenge. Existing LMM grounding strategies like set-of-mark prompting\nturns out not effective for web agents, and the best grounding strategy we\ndevelop in this paper leverages both the HTML text and visuals. Yet, there is\nstill a substantial gap with oracle grounding, leaving ample room for further\nimprovement.\n","authors":["Boyuan Zheng","Boyu Gou","Jihyung Kil","Huan Sun","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2401.01614v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12867v2","updated":"2024-01-03T08:29:03Z","published":"2023-09-22T13:43:22Z","title":"Accurate and Fast Compressed Video Captioning","summary":"  Existing video captioning approaches typically require to first sample video\nframes from a decoded video and then conduct a subsequent process (e.g.,\nfeature extraction and/or captioning model learning). In this pipeline, manual\nframe sampling may ignore key information in videos and thus degrade\nperformance. Additionally, redundant information in the sampled frames may\nresult in low efficiency in the inference of video captioning. Addressing this,\nwe study video captioning from a different perspective in compressed domain,\nwhich brings multi-fold advantages over the existing pipeline: 1) Compared to\nraw images from the decoded video, the compressed video, consisting of\nI-frames, motion vectors and residuals, is highly distinguishable, which allows\nus to leverage the entire video for learning without manual sampling through a\nspecialized model design; 2) The captioning model is more efficient in\ninference as smaller and less redundant information is processed. We propose a\nsimple yet effective end-to-end transformer in the compressed domain for video\ncaptioning that enables learning from the compressed video for captioning. We\nshow that even with a simple design, our method can achieve state-of-the-art\nperformance on different benchmarks while running almost 2x faster than\nexisting approaches. Code is available at https://github.com/acherstyx/CoCap.\n","authors":["Yaojie Shen","Xin Gu","Kai Xu","Heng Fan","Longyin Wen","Libo Zhang"],"pdf_url":"https://arxiv.org/pdf/2309.12867v2.pdf","comment":"ICCV 2023"},{"id":"http://arxiv.org/abs/2312.06892v2","updated":"2024-01-03T08:21:03Z","published":"2023-12-11T23:30:37Z","title":"VitalLens: Take A Vital Selfie","summary":"  This report introduces VitalLens, an app that estimates vital signs such as\nheart rate and respiration rate from selfie video in real time. VitalLens uses\na computer vision model trained on a diverse dataset of video and physiological\nsensor data. We benchmark performance on several diverse datasets, including\nVV-Medium, which consists of 289 unique participants. VitalLens outperforms\nseveral existing methods including POS and MTTS-CAN on all datasets while\nmaintaining a fast inference speed. On VV-Medium, VitalLens achieves mean\nabsolute errors of 0.71 bpm for heart rate estimation, and 0.76 bpm for\nrespiratory rate estimation.\n","authors":["Philipp V. Rouast"],"pdf_url":"https://arxiv.org/pdf/2312.06892v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2401.01598v1","updated":"2024-01-03T07:59:17Z","published":"2024-01-03T07:59:17Z","title":"Learning Prompt with Distribution-Based Feature Replay for Few-Shot\n  Class-Incremental Learning","summary":"  Few-shot Class-Incremental Learning (FSCIL) aims to continuously learn new\nclasses based on very limited training data without forgetting the old ones\nencountered. Existing studies solely relied on pure visual networks, while in\nthis paper we solved FSCIL by leveraging the Vision-Language model (e.g., CLIP)\nand propose a simple yet effective framework, named Learning Prompt with\nDistribution-based Feature Replay (LP-DiF). We observe that simply using CLIP\nfor zero-shot evaluation can substantially outperform the most influential\nmethods. Then, prompt tuning technique is involved to further improve its\nadaptation ability, allowing the model to continually capture specific\nknowledge from each session. To prevent the learnable prompt from forgetting\nold knowledge in the new session, we propose a pseudo-feature replay approach.\nSpecifically, we preserve the old knowledge of each class by maintaining a\nfeature-level Gaussian distribution with a diagonal covariance matrix, which is\nestimated by the image features of training images and synthesized features\ngenerated from a VAE. When progressing to a new session, pseudo-features are\nsampled from old-class distributions combined with training images of the\ncurrent session to optimize the prompt, thus enabling the model to learn new\nknowledge while retaining old knowledge. Experiments on three prevalent\nbenchmarks, i.e., CIFAR100, mini-ImageNet, CUB-200, and two more challenging\nbenchmarks, i.e., SUN-397 and CUB-200$^*$ proposed in this paper showcase the\nsuperiority of LP-DiF, achieving new state-of-the-art (SOTA) in FSCIL. Code is\npublicly available at https://github.com/1170300714/LP-DiF.\n","authors":["Zitong Huang","Ze Chen","Zhixing Chen","Erjin Zhou","Xinxing Xu","Rick Siow Mong Goh","Yong Liu","Chunmei Feng","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2401.01598v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01591v1","updated":"2024-01-03T07:54:13Z","published":"2024-01-03T07:54:13Z","title":"MLIP: Medical Language-Image Pre-training with Masked Local\n  Representation Learning","summary":"  Existing contrastive language-image pre-training aims to learn a joint\nrepresentation by matching abundant image-text pairs. However, the number of\nimage-text pairs in medical datasets is usually orders of magnitude smaller\nthan that in natural datasets. Besides, medical image-text pairs often involve\nnumerous complex fine-grained correspondences. This paper aims to enhance the\ndata efficiency by introducing multiple-to-multiple local relationship modeling\nto capture denser supervisions. More specifically, we propose a Medical\nLanguage-Image Pre-training (MLIP) framework, which exploits the limited\nimage-text medical data more efficiently through patch-sentence matching.\nFurthermore, we introduce a masked contrastive learning strategy with semantic\nintegrity estimation to reduce redundancy in images while preserving the\nunderlying semantics. Our evaluation results show that MLIP outperforms\nprevious work in zero/few-shot classification and few-shot segmentation tasks\nby a large margin.\n","authors":["Jiarun Liu","Hong-Yu Zhou","Cheng Li","Weijian Huang","Hao Yang","Yong Liang","Shanshan Wang"],"pdf_url":"https://arxiv.org/pdf/2401.01591v1.pdf","comment":"5 pages, 3 figures"},{"id":"http://arxiv.org/abs/2310.05195v2","updated":"2024-01-03T07:40:15Z","published":"2023-10-08T15:04:50Z","title":"GMMFormer: Gaussian-Mixture-Model Based Transformer for Efficient\n  Partially Relevant Video Retrieval","summary":"  Given a text query, partially relevant video retrieval (PRVR) seeks to find\nuntrimmed videos containing pertinent moments in a database. For PRVR, clip\nmodeling is essential to capture the partial relationship between texts and\nvideos. Current PRVR methods adopt scanning-based clip construction to achieve\nexplicit clip modeling, which is information-redundant and requires a large\nstorage overhead. To solve the efficiency problem of PRVR methods, this paper\nproposes GMMFormer, a Gaussian-Mixture-Model based Transformer which models\nclip representations implicitly. During frame interactions, we incorporate\nGaussian-Mixture-Model constraints to focus each frame on its adjacent frames\ninstead of the whole video. Then generated representations will contain\nmulti-scale clip information, achieving implicit clip modeling. In addition,\nPRVR methods ignore semantic differences between text queries relevant to the\nsame video, leading to a sparse embedding space. We propose a query diverse\nloss to distinguish these text queries, making the embedding space more\nintensive and contain more semantic information. Extensive experiments on three\nlarge-scale video datasets (i.e., TVR, ActivityNet Captions, and Charades-STA)\ndemonstrate the superiority and efficiency of GMMFormer. Code is available at\n\\url{https://github.com/huangmozhi9527/GMMFormer}.\n","authors":["Yuting Wang","Jinpeng Wang","Bin Chen","Ziyun Zeng","Shu-Tao Xia"],"pdf_url":"https://arxiv.org/pdf/2310.05195v2.pdf","comment":"Accepted by AAAI 2024. Code is released at\n  https://github.com/huangmozhi9527/GMMFormer"},{"id":"http://arxiv.org/abs/2401.01587v1","updated":"2024-01-03T07:39:58Z","published":"2024-01-03T07:39:58Z","title":"Real-Time Human Fall Detection using a Lightweight Pose Estimation\n  Technique","summary":"  The elderly population is increasing rapidly around the world. There are no\nenough caretakers for them. Use of AI-based in-home medical care systems is\ngaining momentum due to this. Human fall detection is one of the most important\ntasks of medical care system for the aged people. Human fall is a common\nproblem among elderly people. Detection of a fall and providing medical help as\nearly as possible is very important to reduce any further complexity. The\nchances of death and other medical complications can be reduced by detecting\nand providing medical help as early as possible after the fall. There are many\nstate-of-the-art fall detection techniques available these days, but the\nmajority of them need very high computing power. In this paper, we proposed a\nlightweight and fast human fall detection system using pose estimation. We used\n`Movenet' for human joins key-points extraction. Our proposed method can work\nin real-time on any low-computing device with any basic camera. All computation\ncan be processed locally, so there is no problem of privacy of the subject. We\nused two datasets `GMDCSA' and `URFD' for the experiment. We got the\nsensitivity value of 0.9375 and 0.9167 for the dataset `GMDCSA' and `URFD'\nrespectively. The source code and the dataset GMDCSA of our work are available\nonline to access.\n","authors":["Ekram Alam","Abu Sufian","Paramartha Dutta","Marco Leo"],"pdf_url":"https://arxiv.org/pdf/2401.01587v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01583v1","updated":"2024-01-03T07:22:54Z","published":"2024-01-03T07:22:54Z","title":"Enhancing the medical foundation model with multi-scale and\n  cross-modality feature learning","summary":"  The development of multi-modal medical foundation models has attracted\nsignificant attention in the field of medicine and healthcare due to their\npromising prospects in various clinical applications. One area of focus in this\nresearch direction is the extractions of features at different scales. While\nprevious studies have explored feature learning at individual scales,\ninvestigation on integrating the diverse scales and modalities of information\nis lacking, which may hinder the potential for mutual reinforcement among these\nfeatures. This paper aims to bridge this gap by proposing a method that\neffectively exploits multi-scale and cross-modality information to enhance the\nperformance of medical foundation models. The proposed method simultaneously\nexploit features at the local, instance, modality and global aspects,\nfacilitating comprehensive representation learning within the models. We\nevaluate the effectiveness of the proposed method on six open-source datasets\nacross different clinical tasks, demonstrating its ability to enhance the\nperformance of medical foundation models.\n","authors":["Weijian Huang","Cheng Li","Hong-Yu Zhou","Jiarun Liu","Hao Yang","Yong Liang","Shanshan Wang"],"pdf_url":"https://arxiv.org/pdf/2401.01583v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.17240v2","updated":"2024-01-03T07:08:12Z","published":"2023-12-28T18:58:33Z","title":"An Improved Baseline for Reasoning Segmentation with Large Language\n  Model","summary":"  While LISA effectively bridges the gap between segmentation and large\nlanguage models to enable reasoning segmentation, it poses certain limitations:\nunable to distinguish different instances of the target region, and constrained\nby the pre-defined textual response formats. In this work, we introduce LISA++,\nan update to the existing LISA model, focusing on improving core\nfunctionalities while keeping the base architecture intact. The main\nenhancements in LISA++ include: \\textbf{1) Enhanced Segmentation}: The instance\nsegmentation ability has been added, providing a more detailed scene analysis\nalong with the existing multi-region semantic segmentation. \\textbf{2) More\nNatural Conversation}: Improved capability for multi-turn dialogue, with the\nability to incorporate segmentation results directly into text responses, i.e.,\nSegmentation in Dialogue (SiD). These improvements are achieved by curating the\nexisting samples of generic segmentation datasets, aimed specifically at\nenhancing the segmentation and conversational skills without structural change\nand additional data sources. Comparative analysis with the original LISA model\nshows significant advancements in these areas, positioning LISA++ as a notable\nupgrade in visual understanding and interaction. LISA++'s adaptability and\nimproved features highlight the versatility of the mask-as-embedding paradigm\nproposed by LISA, and the potential as a foundational model for diverse\napplications.\n","authors":["Senqiao Yang","Tianyuan Qu","Xin Lai","Zhuotao Tian","Bohao Peng","Shu Liu","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2312.17240v2.pdf","comment":"Tech report. The LaTex compilation crash was fixed"},{"id":"http://arxiv.org/abs/2401.01578v1","updated":"2024-01-03T07:05:49Z","published":"2024-01-03T07:05:49Z","title":"Context-Guided Spatio-Temporal Video Grounding","summary":"  Spatio-temporal video grounding (or STVG) task aims at locating a\nspatio-temporal tube for a specific instance given a text query. Despite\nadvancements, current methods easily suffer the distractors or heavy object\nappearance variations in videos due to insufficient object information from the\ntext, leading to degradation. Addressing this, we propose a novel framework,\ncontext-guided STVG (CG-STVG), which mines discriminative instance context for\nobject in videos and applies it as a supplementary guidance for target\nlocalization. The key of CG-STVG lies in two specially designed modules,\nincluding instance context generation (ICG), which focuses on discovering\nvisual context information (in both appearance and motion) of the instance, and\ninstance context refinement (ICR), which aims to improve the instance context\nfrom ICG by eliminating irrelevant or even harmful information from the\ncontext. During grounding, ICG, together with ICR, are deployed at each\ndecoding stage of a Transformer architecture for instance context learning.\nParticularly, instance context learned from one decoding stage is fed to the\nnext stage, and leveraged as a guidance containing rich and discriminative\nobject feature to enhance the target-awareness in decoding feature, which\nconversely benefits generating better new instance context for improving\nlocalization finally. Compared to existing methods, CG-STVG enjoys object\ninformation in text query and guidance from mined instance visual context for\nmore accurate target localization. In our experiments on three benchmarks,\nincluding HCSTVG-v1/-v2 and VidSTG, CG-STVG sets new state-of-the-arts in\nm_tIoU and m_vIoU on all of them, showing its efficacy. The code will be\nreleased at https://github.com/HengLan/CGSTVG.\n","authors":["Xin Gu","Heng Fan","Yan Huang","Tiejian Luo","Libo Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.01578v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01577v1","updated":"2024-01-03T07:02:35Z","published":"2024-01-03T07:02:35Z","title":"Test-Time Personalization with Meta Prompt for Gaze Estimation","summary":"  Despite the recent remarkable achievement in gaze estimation, efficient and\naccurate personalization of gaze estimation without labels is a practical\nproblem but rarely touched on in the literature. To achieve efficient\npersonalization, we take inspiration from the recent advances in Natural\nLanguage Processing (NLP) by updating a negligible number of parameters,\n\"prompts\", at the test time. Specifically, the prompt is additionally attached\nwithout perturbing original network and can contain less than 1% of a\nResNet-18's parameters. Our experiments show high efficiency of the prompt\ntuning approach. The proposed one can be 10 times faster in terms of adaptation\nspeed than the methods compared. However, it is non-trivial to update the\nprompt for personalized gaze estimation without labels. At the test time, it is\nessential to ensure that the minimizing of particular unsupervised loss leads\nto the goals of minimizing gaze estimation error. To address this difficulty,\nwe propose to meta-learn the prompt to ensure that its updates align with the\ngoal. Our experiments show that the meta-learned prompt can be effectively\nadapted even with a simple symmetry loss. In addition, we experiment on four\ncross-dataset validations to show the remarkable advantages of the proposed\nmethod.\n","authors":["Huan Liu","Julia Qi","Zhenhao Li","Mohammad Hassanpour","Yang Wang","Konstantinos Plataniotis","Yuanhao Yu"],"pdf_url":"https://arxiv.org/pdf/2401.01577v1.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2401.01575v1","updated":"2024-01-03T07:00:32Z","published":"2024-01-03T07:00:32Z","title":"Enhancing Generalization of Invisible Facial Privacy Cloak via Gradient\n  Accumulation","summary":"  The blooming of social media and face recognition (FR) systems has increased\npeople's concern about privacy and security. A new type of adversarial privacy\ncloak (class-universal) can be applied to all the images of regular users, to\nprevent malicious FR systems from acquiring their identity information. In this\nwork, we discover the optimization dilemma in the existing methods -- the local\noptima problem in large-batch optimization and the gradient information\nelimination problem in small-batch optimization. To solve these problems, we\npropose Gradient Accumulation (GA) to aggregate multiple small-batch gradients\ninto a one-step iterative gradient to enhance the gradient stability and reduce\nthe usage of quantization operations. Experiments show that our proposed method\nachieves high performance on the Privacy-Commons dataset against black-box face\nrecognition models.\n","authors":["Xuannan Liu","Yaoyao Zhong","Weihong Deng","Hongzhi Shi","Xingchen Cui","Yunfeng Yin","Dongchao Wen"],"pdf_url":"https://arxiv.org/pdf/2401.01575v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01574v1","updated":"2024-01-03T06:58:52Z","published":"2024-01-03T06:58:52Z","title":"A Transformer-Based Adaptive Semantic Aggregation Method for UAV Visual\n  Geo-Localization","summary":"  This paper addresses the task of Unmanned Aerial Vehicles (UAV) visual\ngeo-localization, which aims to match images of the same geographic target\ntaken by different platforms, i.e., UAVs and satellites. In general, the key to\nachieving accurate UAV-satellite image matching lies in extracting visual\nfeatures that are robust against viewpoint changes, scale variations, and\nrotations. Current works have shown that part matching is crucial for UAV\nvisual geo-localization since part-level representations can capture image\ndetails and help to understand the semantic information of scenes. However, the\nimportance of preserving semantic characteristics in part-level representations\nis not well discussed. In this paper, we introduce a transformer-based adaptive\nsemantic aggregation method that regards parts as the most representative\nsemantics in an image. Correlations of image patches to different parts are\nlearned in terms of the transformer's feature map. Then our method decomposes\npart-level features into an adaptive sum of all patch features. By doing this,\nthe learned parts are encouraged to focus on patches with typical semantics.\nExtensive experiments on the University-1652 dataset have shown the superiority\nof our method over the current works.\n","authors":["Shishen Li","Cuiwei Liu","Huaijun Qiu","Zhaokui Li"],"pdf_url":"https://arxiv.org/pdf/2401.01574v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2401.01573v1","updated":"2024-01-03T06:58:09Z","published":"2024-01-03T06:58:09Z","title":"View Distribution Alignment with Progressive Adversarial Learning for\n  UAV Visual Geo-Localization","summary":"  Unmanned Aerial Vehicle (UAV) visual geo-localization aims to match images of\nthe same geographic target captured from different views, i.e., the UAV view\nand the satellite view. It is very challenging due to the large appearance\ndifferences in UAV-satellite image pairs. Previous works map images captured by\nUAVs and satellites to a shared feature space and employ a classification\nframework to learn location-dependent features while neglecting the overall\ndistribution shift between the UAV view and the satellite view. In this paper,\nwe address these limitations by introducing distribution alignment of the two\nviews to shorten their distance in a common space. Specifically, we propose an\nend-to-end network, called PVDA (Progressive View Distribution Alignment).\nDuring training, feature encoder, location classifier, and view discriminator\nare jointly optimized by a novel progressive adversarial learning strategy.\nCompetition between feature encoder and view discriminator prompts both of them\nto be stronger. It turns out that the adversarial learning is progressively\nemphasized until UAV-view images are indistinguishable from satellite-view\nimages. As a result, the proposed PVDA becomes powerful in learning\nlocation-dependent yet view-invariant features with good scalability towards\nunseen images of new locations. Compared to the state-of-the-art methods, the\nproposed PVDA requires less inference time but has achieved superior\nperformance on the University-1652 dataset.\n","authors":["Cuiwei Liu","Jiahao Liu","Huaijun Qiu","Zhaokui Li","Xiangbin Shi"],"pdf_url":"https://arxiv.org/pdf/2401.01573v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2305.18891v2","updated":"2024-01-03T06:55:36Z","published":"2023-05-30T09:47:29Z","title":"EmotionGesture: Audio-Driven Diverse Emotional Co-Speech 3D Gesture\n  Generation","summary":"  Generating vivid and diverse 3D co-speech gestures is crucial for various\napplications in animating virtual avatars. While most existing methods can\ngenerate gestures from audio directly, they usually overlook that emotion is\none of the key factors of authentic co-speech gesture generation. In this work,\nwe propose EmotionGesture, a novel framework for synthesizing vivid and diverse\nemotional co-speech 3D gestures from audio. Considering emotion is often\nentangled with the rhythmic beat in speech audio, we first develop an\nEmotion-Beat Mining module (EBM) to extract the emotion and audio beat features\nas well as model their correlation via a transcript-based visual-rhythm\nalignment. Then, we propose an initial pose based Spatial-Temporal Prompter\n(STP) to generate future gestures from the given initial poses. STP effectively\nmodels the spatial-temporal correlations between the initial poses and the\nfuture gestures, thus producing the spatial-temporal coherent pose prompt. Once\nwe obtain pose prompts, emotion, and audio beat features, we will generate 3D\nco-speech gestures through a transformer architecture. However, considering the\nposes of existing datasets often contain jittering effects, this would lead to\ngenerating unstable gestures. To address this issue, we propose an effective\nobjective function, dubbed Motion-Smooth Loss. Specifically, we model motion\noffset to compensate for jittering ground-truth by forcing gestures to be\nsmooth. Last, we present an emotion-conditioned VAE to sample emotion features,\nenabling us to generate diverse emotional results. Extensive experiments\ndemonstrate that our framework outperforms the state-of-the-art, achieving\nvivid and diverse emotional co-speech 3D gestures. Our code and dataset will be\nreleased at the project page:\nhttps://xingqunqi-lab.github.io/Emotion-Gesture-Web/\n","authors":["Xingqun Qi","Chen Liu","Lincheng Li","Jie Hou","Haoran Xin","Xin Yu"],"pdf_url":"https://arxiv.org/pdf/2305.18891v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2401.01569v1","updated":"2024-01-03T06:55:06Z","published":"2024-01-03T06:55:06Z","title":"AttentionLut: Attention Fusion-based Canonical Polyadic LUT for\n  Real-time Image Enhancement","summary":"  Recently, many algorithms have employed image-adaptive lookup tables (LUTs)\nto achieve real-time image enhancement. Nonetheless, a prevailing trend among\nexisting methods has been the employment of linear combinations of basic LUTs\nto formulate image-adaptive LUTs, which limits the generalization ability of\nthese methods. To address this limitation, we propose a novel framework named\nAttentionLut for real-time image enhancement, which utilizes the attention\nmechanism to generate image-adaptive LUTs. Our proposed framework consists of\nthree lightweight modules. We begin by employing the global image context\nfeature module to extract image-adaptive features. Subsequently, the attention\nfusion module integrates the image feature with the priori attention feature\nobtained during training to generate image-adaptive canonical polyadic tensors.\nFinally, the canonical polyadic reconstruction module is deployed to\nreconstruct image-adaptive residual 3DLUT, which is subsequently utilized for\nenhancing input images. Experiments on the benchmark MIT-Adobe FiveK dataset\ndemonstrate that the proposed method achieves better enhancement performance\nquantitatively and qualitatively than the state-of-the-art methods.\n","authors":["Kang Fu","Yicong Peng","Zicheng Zhang","Qihang Xu","Xiaohong Liu","Jia Wang","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2401.01569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.14484v3","updated":"2024-01-03T06:34:30Z","published":"2023-04-27T19:52:47Z","title":"OriCon3D: Effective 3D Object Detection using Orientation and Confidence","summary":"  In this paper, we propose an advanced methodology for the detection of 3D\nobjects and precise estimation of their spatial positions from a single image.\nUnlike conventional frameworks that rely solely on center-point and dimension\npredictions, our research leverages a deep convolutional neural network-based\n3D object weighted orientation regression paradigm. These estimates are then\nseamlessly integrated with geometric constraints obtained from a 2D bounding\nbox, resulting in derivation of a comprehensive 3D bounding box. Our novel\nnetwork design encompasses two key outputs. The first output involves the\nestimation of 3D object orientation through the utilization of a\ndiscrete-continuous loss function. Simultaneously, the second output predicts\nobjectivity-based confidence scores with minimal variance. Additionally, we\nalso introduce enhancements to our methodology through the incorporation of\nlightweight residual feature extractors. By combining the derived estimates\nwith the geometric constraints inherent in the 2D bounding box, our approach\nsignificantly improves the accuracy of 3D object pose determination, surpassing\nbaseline methodologies. Our method is rigorously evaluated on the KITTI 3D\nobject detection benchmark, demonstrating superior performance.\n","authors":["Dhyey Manish Rajani","Surya Pratap Singh","Rahul Kashyap Swayampakula"],"pdf_url":"https://arxiv.org/pdf/2304.14484v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01558v1","updated":"2024-01-03T06:18:30Z","published":"2024-01-03T06:18:30Z","title":"One-Step Late Fusion Multi-view Clustering with Compressed Subspace","summary":"  Late fusion multi-view clustering (LFMVC) has become a rapidly growing class\nof methods in the multi-view clustering (MVC) field, owing to its excellent\ncomputational speed and clustering performance. One bottleneck faced by\nexisting late fusion methods is that they are usually aligned to the average\nkernel function, which makes the clustering performance highly dependent on the\nquality of datasets. Another problem is that they require subsequent k-means\nclustering after obtaining the consensus partition matrix to get the final\ndiscrete labels, and the resulting separation of the label learning and cluster\nstructure optimization processes limits the integrity of these models. To\naddress the above issues, we propose an integrated framework named One-Step\nLate Fusion Multi-view Clustering with Compressed Subspace (OS-LFMVC-CS).\nSpecifically, we use the consensus subspace to align the partition matrix while\noptimizing the partition fusion, and utilize the fused partition matrix to\nguide the learning of discrete labels. A six-step iterative optimization\napproach with verified convergence is proposed. Sufficient experiments on\nmultiple datasets validate the effectiveness and efficiency of our proposed\nmethod.\n","authors":["Qiyuan Ou","Pei Zhang","Sihang Zhou","En Zhu"],"pdf_url":"https://arxiv.org/pdf/2401.01558v1.pdf","comment":"Accepted by ICASSP2024"},{"id":"http://arxiv.org/abs/2401.01553v1","updated":"2024-01-03T05:59:48Z","published":"2024-01-03T05:59:48Z","title":"Multi-modal Learning with Missing Modality in Predicting Axillary Lymph\n  Node Metastasis","summary":"  Multi-modal Learning has attracted widespread attention in medical image\nanalysis. Using multi-modal data, whole slide images (WSIs) and clinical\ninformation, can improve the performance of deep learning models in the\ndiagnosis of axillary lymph node metastasis. However, clinical information is\nnot easy to collect in clinical practice due to privacy concerns, limited\nresources, lack of interoperability, etc. Although patient selection can ensure\nthe training set to have multi-modal data for model development, missing\nmodality of clinical information can appear during test. This normally leads to\nperformance degradation, which limits the use of multi-modal models in the\nclinic. To alleviate this problem, we propose a bidirectional distillation\nframework consisting of a multi-modal branch and a single-modal branch. The\nsingle-modal branch acquires the complete multi-modal knowledge from the\nmulti-modal branch, while the multi-modal learns the robust features of WSI\nfrom the single-modal. We conduct experiments on a public dataset of Lymph Node\nMetastasis in Early Breast Cancer to validate the method. Our approach not only\nachieves state-of-the-art performance with an AUC of 0.861 on the test set\nwithout missing data, but also yields an AUC of 0.842 when the rate of missing\nmodality is 80\\%. This shows the effectiveness of the approach in dealing with\nmulti-modal data and missing modality. Such a model has the potential to\nimprove treatment decision-making for early breast cancer patients who have\naxillary lymph node metastatic status.\n","authors":["Shichuan Zhang","Sunyi Zheng","Zhongyi Shui","Honglin Li","Lin Yang"],"pdf_url":"https://arxiv.org/pdf/2401.01553v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01552v1","updated":"2024-01-03T05:57:39Z","published":"2024-01-03T05:57:39Z","title":"CRA-PCN: Point Cloud Completion with Intra- and Inter-level\n  Cross-Resolution Transformers","summary":"  Point cloud completion is an indispensable task for recovering complete point\nclouds due to incompleteness caused by occlusion, limited sensor resolution,\netc. The family of coarse-to-fine generation architectures has recently\nexhibited great success in point cloud completion and gradually became\nmainstream. In this work, we unveil one of the key ingredients behind these\nmethods: meticulously devised feature extraction operations with explicit\ncross-resolution aggregation. We present Cross-Resolution Transformer that\nefficiently performs cross-resolution aggregation with local attention\nmechanisms. With the help of our recursive designs, the proposed operation can\ncapture more scales of features than common aggregation operations, which is\nbeneficial for capturing fine geometric characteristics. While prior\nmethodologies have ventured into various manifestations of inter-level\ncross-resolution aggregation, the effectiveness of intra-level one and their\ncombination has not been analyzed. With unified designs, Cross-Resolution\nTransformer can perform intra- or inter-level cross-resolution aggregation by\nswitching inputs. We integrate two forms of Cross-Resolution Transformers into\none up-sampling block for point generation, and following the coarse-to-fine\nmanner, we construct CRA-PCN to incrementally predict complete shapes with\nstacked up-sampling blocks. Extensive experiments demonstrate that our method\noutperforms state-of-the-art methods by a large margin on several widely used\nbenchmarks. Codes are available at https://github.com/EasyRy/CRA-PCN.\n","authors":["Yi Rong","Haoran Zhou","Lixin Yuan","Cheng Mei","Jiahao Wang","Tong Lu"],"pdf_url":"https://arxiv.org/pdf/2401.01552v1.pdf","comment":"Accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2401.01548v1","updated":"2024-01-03T05:51:25Z","published":"2024-01-03T05:51:25Z","title":"Boosting of Implicit Neural Representation-based Image Denoiser","summary":"  Implicit Neural Representation (INR) has emerged as an effective method for\nunsupervised image denoising. However, INR models are typically\noverparameterized; consequently, these models are prone to overfitting during\nlearning, resulting in suboptimal results, even noisy ones. To tackle this\nproblem, we propose a general recipe for regularizing INR models in image\ndenoising. In detail, we propose to iteratively substitute the supervision\nsignal with the mean value derived from both the prediction and supervision\nsignal during the learning process. We theoretically prove that such a simple\niterative substitute can gradually enhance the signal-to-noise ratio of the\nsupervision signal, thereby benefiting INR models during the learning process.\nOur experimental results demonstrate that INR models can be effectively\nregularized by the proposed approach, relieving overfitting and boosting image\ndenoising performance.\n","authors":["Zipei Yan","Zhengji Liu","Jizhou Li"],"pdf_url":"https://arxiv.org/pdf/2401.01548v1.pdf","comment":"Accepted by ICASSP 2024, code: https://github.com/TIDS-Lab/ITS"},{"id":"http://arxiv.org/abs/2401.01545v1","updated":"2024-01-03T05:42:17Z","published":"2024-01-03T05:42:17Z","title":"DDN-SLAM: Real-time Dense Dynamic Neural Implicit SLAM with Joint\n  Semantic Encoding","summary":"  We propose DDN-SLAM, a real-time dense neural implicit semantic SLAM system\ndesigned for dynamic scenes. While existing neural implicit SLAM systems\nperform well in static scenes, they often encounter challenges in real-world\nenvironments with dynamic interferences, leading to ineffective tracking and\nmapping. DDN-SLAM utilizes the priors provided by the deep semantic system,\ncombined with conditional probability fields, for segmentation.By constructing\ndepth-guided static masks and employing joint multi-resolution hashing\nencoding, we ensure fast hole filling and high-quality mapping while mitigating\nthe effects of dynamic information interference. To enhance tracking\nrobustness, we utilize sparse feature points validated with optical flow and\nkeyframes, enabling loop closure detection and global bundle optimization.\nFurthermore, DDN-SLAM supports monocular, stereo, and RGB-D inputs, operating\nrobustly at a frequency of 20-30Hz. Extensive experiments on 6 virtual/real\ndatasets demonstrate that our method outperforms state-of-the-art approaches in\nboth dynamic and static scenes.\n","authors":["Mingrui Li","Jiaming He","Guangan Jiang","Hongyu Wang"],"pdf_url":"https://arxiv.org/pdf/2401.01545v1.pdf","comment":"11pages, 4figures"},{"id":"http://arxiv.org/abs/2401.01544v1","updated":"2024-01-03T05:33:14Z","published":"2024-01-03T05:33:14Z","title":"Collaborative Perception for Connected and Autonomous Driving:\n  Challenges, Possible Solutions and Opportunities","summary":"  Autonomous driving has attracted significant attention from both academia and\nindustries, which is expected to offer a safer and more efficient driving\nsystem. However, current autonomous driving systems are mostly based on a\nsingle vehicle, which has significant limitations which still poses threats to\ndriving safety. Collaborative perception with connected and autonomous vehicles\n(CAVs) shows a promising solution to overcoming these limitations. In this\narticle, we first identify the challenges of collaborative perception, such as\ndata sharing asynchrony, data volume, and pose errors. Then, we discuss the\npossible solutions to address these challenges with various technologies, where\nthe research opportunities are also elaborated. Furthermore, we propose a\nscheme to deal with communication efficiency and latency problems, which is a\nchannel-aware collaborative perception framework to dynamically adjust the\ncommunication graph and minimize latency, thereby improving perception\nperformance while increasing communication efficiency. Finally, we conduct\nexperiments to demonstrate the effectiveness of our proposed scheme.\n","authors":["Senkang Hu","Zhengru Fang","Yiqin Deng","Xianhao Chen","Yuguang Fang"],"pdf_url":"https://arxiv.org/pdf/2401.01544v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.03359v5","updated":"2024-01-03T05:30:54Z","published":"2022-04-07T10:57:12Z","title":"ECCV Caption: Correcting False Negatives by Collecting\n  Machine-and-Human-verified Image-Caption Associations for MS-COCO","summary":"  Image-Text matching (ITM) is a common task for evaluating the quality of\nVision and Language (VL) models. However, existing ITM benchmarks have a\nsignificant limitation. They have many missing correspondences, originating\nfrom the data construction process itself. For example, a caption is only\nmatched with one image although the caption can be matched with other similar\nimages and vice versa. To correct the massive false negatives, we construct the\nExtended COCO Validation (ECCV) Caption dataset by supplying the missing\nassociations with machine and human annotators. We employ five state-of-the-art\nITM models with diverse properties for our annotation process. Our dataset\nprovides x3.6 positive image-to-caption associations and x8.5 caption-to-image\nassociations compared to the original MS-COCO. We also propose to use an\ninformative ranking-based metric mAP@R, rather than the popular Recall@K (R@K).\nWe re-evaluate the existing 25 VL models on existing and proposed benchmarks.\nOur findings are that the existing benchmarks, such as COCO 1K R@K, COCO 5K\nR@K, CxC R@1 are highly correlated with each other, while the rankings change\nwhen we shift to the ECCV mAP@R. Lastly, we delve into the effect of the bias\nintroduced by the choice of machine annotator. Source code and dataset are\navailable at https://github.com/naver-ai/eccv-caption\n","authors":["Sanghyuk Chun","Wonjae Kim","Song Park","Minsuk Chang","Seong Joon Oh"],"pdf_url":"https://arxiv.org/pdf/2204.03359v5.pdf","comment":"Published in ECCV 2022; 32 pages (2.3MB); Code and dataset:\n  https://github.com/naver-ai/eccv-caption; v5 fixes errors in Table 4: the\n  COCO 1K R@1 numbers were incorrect. All other tables and figures are correct.\n  v5 also adds RSUM scores in Tab 4 and 5: RSUM has a high correlation with\n  COCO 1K recalls; v4 fixes errors in v3 -- see the v4 comment for details"},{"id":"http://arxiv.org/abs/2401.01543v1","updated":"2024-01-03T05:26:57Z","published":"2024-01-03T05:26:57Z","title":"Retraining-free Model Quantization via One-Shot Weight-Coupling Learning","summary":"  Quantization is of significance for compressing the over-parameterized deep\nneural models and deploying them on resource-limited devices. Fixed-precision\nquantization suffers from performance drop due to the limited numerical\nrepresentation ability. Conversely, mixed-precision quantization (MPQ) is\nadvocated to compress the model effectively by allocating heterogeneous\nbit-width for layers. MPQ is typically organized into a searching-retraining\ntwo-stage process. Previous works only focus on determining the optimal\nbit-width configuration in the first stage efficiently, while ignoring the\nconsiderable time costs in the second stage. However, retraining always\nconsumes hundreds of GPU-hours on the cutting-edge GPUs, thus hindering\ndeployment efficiency significantly. In this paper, we devise a one-shot\ntraining-searching paradigm for mixed-precision model compression.\nSpecifically, in the first stage, all potential bit-width configurations are\ncoupled and thus optimized simultaneously within a set of shared weights.\nHowever, our observations reveal a previously unseen and severe bit-width\ninterference phenomenon among highly coupled weights during optimization,\nleading to considerable performance degradation under a high compression ratio.\nTo tackle this problem, we first design a bit-width scheduler to dynamically\nfreeze the most turbulent bit-width of layers during training, to ensure the\nrest bit-widths converged properly. Then, taking inspiration from information\ntheory, we present an information distortion mitigation technique to align the\nbehaviour of the bad-performing bit-widths to the well-performing ones.\n","authors":["Chen Tang","Yuan Meng","Jiacheng Jiang","Shuzhao Xie","Rongwei Lu","Xinzhu Ma","Zhi Wang","Wenwu Zhu"],"pdf_url":"https://arxiv.org/pdf/2401.01543v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00789v2","updated":"2024-01-03T05:08:23Z","published":"2024-01-01T15:31:06Z","title":"Retrieval-Augmented Egocentric Video Captioning","summary":"  Understanding human actions from videos of first-person view poses\nsignificant challenges. Most prior approaches explore representation learning\non egocentric videos only, while overlooking the potential benefit of\nexploiting existing large-scale third-person videos. In this paper, (1) we\ndevelop EgoInstructor, a retrieval-augmented multimodal captioning model that\nautomatically retrieves semantically relevant third-person instructional videos\nto enhance the video captioning of egocentric videos. (2) For training the\ncross-view retrieval module, we devise an automatic pipeline to discover\nego-exo video pairs from distinct large-scale egocentric and exocentric\ndatasets. (3) We train the cross-view retrieval module with a novel EgoExoNCE\nloss that pulls egocentric and exocentric video features closer by aligning\nthem to shared text features that describe similar actions. (4) Through\nextensive experiments, our cross-view retrieval module demonstrates superior\nperformance across seven benchmarks. Regarding egocentric video captioning,\nEgoInstructor exhibits significant improvements by leveraging third-person\nvideos as references.\n","authors":["Jilan Xu","Yifei Huang","Junlin Hou","Guo Chen","Yuejie Zhang","Rui Feng","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2401.00789v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01539v1","updated":"2024-01-03T04:35:58Z","published":"2024-01-03T04:35:58Z","title":"DDPM based X-ray Image Synthesizer","summary":"  Access to high-quality datasets in the medical industry limits machine\nlearning model performance. To address this issue, we propose a Denoising\nDiffusion Probabilistic Model (DDPM) combined with a UNet architecture for\nX-ray image synthesis. Focused on pneumonia medical condition, our methodology\nemploys over 3000 pneumonia X-ray images obtained from Kaggle for training.\nResults demonstrate the effectiveness of our approach, as the model\nsuccessfully generated realistic images with low Mean Squared Error (MSE). The\nsynthesized images showed distinct differences from non-pneumonia images,\nhighlighting the model's ability to capture key features of positive cases.\nBeyond pneumonia, the applications of this synthesizer extend to various\nmedical conditions, provided an ample dataset is available. The capability to\nproduce high-quality images can potentially enhance machine learning models'\nperformance, aiding in more accurate and efficient medical diagnoses. This\ninnovative DDPM-based X-ray photo synthesizer presents a promising avenue for\naddressing the scarcity of positive medical image datasets, paving the way for\nimproved medical image analysis and diagnosis in the healthcare industry.\n","authors":["Praveen Mahaulpatha","Thulana Abeywardane","Tomson George"],"pdf_url":"https://arxiv.org/pdf/2401.01539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14705v2","updated":"2024-01-03T04:14:07Z","published":"2023-12-22T14:06:03Z","title":"SCUNet++: Swin-UNet and CNN Bottleneck Hybrid Architecture with\n  Multi-Fusion Dense Skip Connection for Pulmonary Embolism CT Image\n  Segmentation","summary":"  Pulmonary embolism (PE) is a prevalent lung disease that can lead to right\nventricular hypertrophy and failure in severe cases, ranking second in severity\nonly to myocardial infarction and sudden death. Pulmonary artery CT angiography\n(CTPA) is a widely used diagnostic method for PE. However, PE detection\npresents challenges in clinical practice due to limitations in imaging\ntechnology. CTPA can produce noises similar to PE, making confirmation of its\npresence time-consuming and prone to overdiagnosis. Nevertheless, the\ntraditional segmentation method of PE can not fully consider the hierarchical\nstructure of features, local and global spatial features of PE CT images. In\nthis paper, we propose an automatic PE segmentation method called SCUNet++\n(Swin Conv UNet++). This method incorporates multiple fusion dense skip\nconnections between the encoder and decoder, utilizing the Swin Transformer as\nthe encoder. And fuses features of different scales in the decoder subnetwork\nto compensate for spatial information loss caused by the inevitable\ndownsampling in Swin-UNet or other state-of-the-art methods, effectively\nsolving the above problem. We provide a theoretical analysis of this method in\ndetail and validate it on publicly available PE CT image datasets FUMPE and\nCAD-PE. The experimental results indicate that our proposed method achieved a\nDice similarity coefficient (DSC) of 83.47% and a Hausdorff distance 95th\npercentile (HD95) of 3.83 on the FUMPE dataset, as well as a DSC of 83.42% and\nan HD95 of 5.10 on the CAD-PE dataset. These findings demonstrate that our\nmethod exhibits strong performance in PE segmentation tasks, potentially\nenhancing the accuracy of automatic segmentation of PE and providing a powerful\ndiagnostic tool for clinical physicians. Our source code and new FUMPE dataset\nare available at https://github.com/JustlfC03/SCUNet-plusplus.\n","authors":["Yifei Chen","Binfeng Zou","Zhaoxin Guo","Yiyu Huang","Yifan Huang","Feiwei Qin","Qinhai Li","Changmiao Wang"],"pdf_url":"https://arxiv.org/pdf/2312.14705v2.pdf","comment":"10 pages, 7 figures, accept WACV2024"},{"id":"http://arxiv.org/abs/2401.01529v1","updated":"2024-01-03T03:51:16Z","published":"2024-01-03T03:51:16Z","title":"Glance and Focus: Memory Prompting for Multi-Event Video Question\n  Answering","summary":"  Video Question Answering (VideoQA) has emerged as a vital tool to evaluate\nagents' ability to understand human daily behaviors. Despite the recent success\nof large vision language models in many multi-modal tasks, complex situation\nreasoning over videos involving multiple human-object interaction events still\nremains challenging. In contrast, humans can easily tackle it by using a series\nof episode memories as anchors to quickly locate question-related key moments\nfor reasoning. To mimic this effective reasoning strategy, we propose the\nGlance-Focus model. One simple way is to apply an action detection model to\npredict a set of actions as key memories. However, these actions within a\nclosed set vocabulary are hard to generalize to various video domains. Instead\nof that, we train an Encoder-Decoder to generate a set of dynamic event\nmemories at the glancing stage. Apart from using supervised bipartite matching\nto obtain the event memories, we further design an unsupervised memory\ngeneration method to get rid of dependence on event annotations. Next, at the\nfocusing stage, these event memories act as a bridge to establish the\ncorrelation between the questions with high-level event concepts and low-level\nlengthy video content. Given the question, the model first focuses on the\ngenerated key event memory, then focuses on the most relevant moment for\nreasoning through our designed multi-level cross-attention mechanism. We\nconduct extensive experiments on four Multi-Event VideoQA benchmarks including\nSTAR, EgoTaskQA, AGQA, and NExT-QA. Our proposed model achieves\nstate-of-the-art results, surpassing current large models in various\nchallenging reasoning tasks. The code and models are available at\nhttps://github.com/ByZ0e/Glance-Focus.\n","authors":["Ziyi Bai","Ruiping Wang","Xilin Chen"],"pdf_url":"https://arxiv.org/pdf/2401.01529v1.pdf","comment":"Accepted in NeurIPS 2023"},{"id":"http://arxiv.org/abs/2401.01524v1","updated":"2024-01-03T03:33:48Z","published":"2024-01-03T03:33:48Z","title":"Multimodal self-supervised learning for lesion localization","summary":"  Multimodal deep learning utilizing imaging and diagnostic reports has made\nimpressive progress in the field of medical imaging diagnostics, demonstrating\na particularly strong capability for auxiliary diagnosis in cases where\nsufficient annotation information is lacking. Nonetheless, localizing diseases\naccurately without detailed positional annotations remains a challenge.\nAlthough existing methods have attempted to utilize local information to\nachieve fine-grained semantic alignment, their capability in extracting the\nfine-grained semantics of the comprehensive contextual within reports is\nlimited. To solve this problem, we introduce a new method that takes full\nsentences from textual reports as the basic units for local semantic alignment.\nOur approach combines chest X-ray images with their corresponding textual\nreports, performing contrastive learning at both global and local levels. The\nleading results obtained by our method on multiple datasets confirm its\nefficacy in the task of lesion localization.\n","authors":["Hao Yang","Hong-Yu Zhou","Cheng Li","Weijian Huang","Jiarun Liu","Yong Liang","Shanshan Wang"],"pdf_url":"https://arxiv.org/pdf/2401.01524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01522v1","updated":"2024-01-03T03:14:55Z","published":"2024-01-03T03:14:55Z","title":"LORE++: Logical Location Regression Network for Table Structure\n  Recognition with Pre-training","summary":"  Table structure recognition (TSR) aims at extracting tables in images into\nmachine-understandable formats. Recent methods solve this problem by predicting\nthe adjacency relations of detected cell boxes or learning to directly generate\nthe corresponding markup sequences from the table images. However, existing\napproaches either count on additional heuristic rules to recover the table\nstructures, or face challenges in capturing long-range dependencies within\ntables, resulting in increased complexity. In this paper, we propose an\nalternative paradigm. We model TSR as a logical location regression problem and\npropose a new TSR framework called LORE, standing for LOgical location\nREgression network, which for the first time regresses logical location as well\nas spatial location of table cells in a unified network. Our proposed LORE is\nconceptually simpler, easier to train, and more accurate than other paradigms\nof TSR. Moreover, inspired by the persuasive success of pre-trained models on a\nnumber of computer vision and natural language processing tasks, we propose two\npre-training tasks to enrich the spatial and logical representations at the\nfeature level of LORE, resulting in an upgraded version called LORE++. The\nincorporation of pre-training in LORE++ has proven to enjoy significant\nadvantages, leading to a substantial enhancement in terms of accuracy,\ngeneralization, and few-shot capability compared to its predecessor.\nExperiments on standard benchmarks against methods of previous paradigms\ndemonstrate the superiority of LORE++, which highlights the potential and\npromising prospect of the logical location regression paradigm for TSR.\n","authors":["Rujiao Long","Hangdi Xing","Zhibo Yang","Qi Zheng","Zhi Yu","Cong Yao","Fei Huang"],"pdf_url":"https://arxiv.org/pdf/2401.01522v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2303.03730"},{"id":"http://arxiv.org/abs/2401.01520v1","updated":"2024-01-03T03:08:32Z","published":"2024-01-03T03:08:32Z","title":"S$^{2}$-DMs:Skip-Step Diffusion Models","summary":"  Diffusion models have emerged as powerful generative tools, rivaling GANs in\nsample quality and mirroring the likelihood scores of autoregressive models. A\nsubset of these models, exemplified by DDIMs, exhibit an inherent asymmetry:\nthey are trained over $T$ steps but only sample from a subset of $T$ during\ngeneration. This selective sampling approach, though optimized for speed,\ninadvertently misses out on vital information from the unsampled steps, leading\nto potential compromises in sample quality. To address this issue, we present\nthe S$^{2}$-DMs, which is a new training method by using an innovative\n$L_{skip}$, meticulously designed to reintegrate the information omitted during\nthe selective sampling phase. The benefits of this approach are manifold: it\nnotably enhances sample quality, is exceptionally simple to implement, requires\nminimal code modifications, and is flexible enough to be compatible with\nvarious sampling algorithms. On the CIFAR10 dataset, models trained using our\nalgorithm showed an improvement of 3.27% to 14.06% over models trained with\ntraditional methods across various sampling algorithms (DDIMs, PNDMs, DEIS) and\ndifferent numbers of sampling steps (10, 20, ..., 1000). On the CELEBA dataset,\nthe improvement ranged from 8.97% to 27.08%. Access to the code and additional\nresources is provided in the github.\n","authors":["Yixuan Wang","Shuangyin Li"],"pdf_url":"https://arxiv.org/pdf/2401.01520v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2401.00695v2","updated":"2024-01-03T02:33:49Z","published":"2024-01-01T08:19:21Z","title":"Credible Teacher for Semi-Supervised Object Detection in Open Scene","summary":"  Semi-Supervised Object Detection (SSOD) has achieved resounding success by\nleveraging unlabeled data to improve detection performance. However, in Open\nScene Semi-Supervised Object Detection (O-SSOD), unlabeled data may contains\nunknown objects not observed in the labeled data, which will increase\nuncertainty in the model's predictions for known objects. It is detrimental to\nthe current methods that mainly rely on self-training, as more uncertainty\nleads to the lower localization and classification precision of pseudo labels.\nTo this end, we propose Credible Teacher, an end-to-end framework. Credible\nTeacher adopts an interactive teaching mechanism using flexible labels to\nprevent uncertain pseudo labels from misleading the model and gradually reduces\nits uncertainty through the guidance of other credible pseudo labels. Empirical\nresults have demonstrated our method effectively restrains the adverse effect\ncaused by O-SSOD and significantly outperforms existing counterparts.\n","authors":["Jingyu Zhuang","Kuo Wang","Liang Lin","Guanbin Li"],"pdf_url":"https://arxiv.org/pdf/2401.00695v2.pdf","comment":"Accpet by ICASSP 2024"},{"id":"http://arxiv.org/abs/2401.01510v1","updated":"2024-01-03T02:29:34Z","published":"2024-01-03T02:29:34Z","title":"Answering from Sure to Uncertain: Uncertainty-Aware Curriculum Learning\n  for Video Question Answering","summary":"  While significant advancements have been made in video question answering\n(VideoQA), the potential benefits of enhancing model generalization through\ntailored difficulty scheduling have been largely overlooked in existing\nresearch. This paper seeks to bridge that gap by incorporating VideoQA into a\ncurriculum learning (CL) framework that progressively trains models from\nsimpler to more complex data. Recognizing that conventional self-paced CL\nmethods rely on training loss for difficulty measurement, which might not\naccurately reflect the intricacies of video-question pairs, we introduce the\nconcept of uncertainty-aware CL. Here, uncertainty serves as the guiding\nprinciple for dynamically adjusting the difficulty. Furthermore, we address the\nchallenge posed by uncertainty by presenting a probabilistic modeling approach\nfor VideoQA. Specifically, we conceptualize VideoQA as a stochastic computation\ngraph, where the hidden representations are treated as stochastic variables.\nThis yields two distinct types of uncertainty: one related to the inherent\nuncertainty in the data and another pertaining to the model's confidence. In\npractice, we seamlessly integrate the VideoQA model into our framework and\nconduct comprehensive experiments. The findings affirm that our approach not\nonly achieves enhanced performance but also effectively quantifies uncertainty\nin the context of VideoQA.\n","authors":["Haopeng Li","Qiuhong Ke","Mingming Gong","Tom Drummond"],"pdf_url":"https://arxiv.org/pdf/2401.01510v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01505v1","updated":"2024-01-03T02:22:34Z","published":"2024-01-03T02:22:34Z","title":"Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex\n  and Professional Sports","summary":"  Reasoning over sports videos for question answering is an important task with\nnumerous applications, such as player training and information retrieval.\nHowever, this task has not been explored due to the lack of relevant datasets\nand the challenging nature it presents. Most datasets for video question\nanswering (VideoQA) focus mainly on general and coarse-grained understanding of\ndaily-life videos, which is not applicable to sports scenarios requiring\nprofessional action understanding and fine-grained motion analysis. In this\npaper, we introduce the first dataset, named Sports-QA, specifically designed\nfor the sports VideoQA task. The Sports-QA dataset includes various types of\nquestions, such as descriptions, chronologies, causalities, and counterfactual\nconditions, covering multiple sports. Furthermore, to address the\ncharacteristics of the sports VideoQA task, we propose a new Auto-Focus\nTransformer (AFT) capable of automatically focusing on particular scales of\ntemporal information for question answering. We conduct extensive experiments\non Sports-QA, including baseline studies and the evaluation of different\nmethods. The results demonstrate that our AFT achieves state-of-the-art\nperformance.\n","authors":["Haopeng Li","Andong Deng","Qiuhong Ke","Jun Liu","Hossein Rahmani","Yulan Guo","Bernt Schiele","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2401.01505v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01107v2","updated":"2024-01-03T02:13:31Z","published":"2024-01-02T08:57:09Z","title":"CityPulse: Fine-Grained Assessment of Urban Change with Street View Time\n  Series","summary":"  Urban transformations have profound societal impact on both individuals and\ncommunities at large. Accurately assessing these shifts is essential for\nunderstanding their underlying causes and ensuring sustainable urban planning.\nTraditional measurements often encounter constraints in spatial and temporal\ngranularity, failing to capture real-time physical changes. While street view\nimagery, capturing the heartbeat of urban spaces from a pedestrian point of\nview, can add as a high-definition, up-to-date, and on-the-ground visual proxy\nof urban change. We curate the largest street view time series dataset to date,\nand propose an end-to-end change detection model to effectively capture\nphysical alterations in the built environment at scale. We demonstrate the\neffectiveness of our proposed method by benchmark comparisons with previous\nliterature and implementing it at the city-wide level. Our approach has the\npotential to supplement existing dataset and serve as a fine-grained and\naccurate assessment of urban change.\n","authors":["Tianyuan Huang","Zejia Wu","Jiajun Wu","Jackelyn Hwang","Ram Rajagopal"],"pdf_url":"https://arxiv.org/pdf/2401.01107v2.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2309.11715v3","updated":"2024-01-03T02:01:09Z","published":"2023-09-21T01:35:13Z","title":"Deshadow-Anything: When Segment Anything Model Meets Zero-shot shadow\n  removal","summary":"  Segment Anything (SAM), an advanced universal image segmentation model\ntrained on an expansive visual dataset, has set a new benchmark in image\nsegmentation and computer vision. However, it faced challenges when it came to\ndistinguishing between shadows and their backgrounds. To address this, we\ndeveloped Deshadow-Anything, considering the generalization of large-scale\ndatasets, and we performed Fine-tuning on large-scale datasets to achieve image\nshadow removal. The diffusion model can diffuse along the edges and textures of\nan image, helping to remove shadows while preserving the details of the image.\nFurthermore, we design Multi-Self-Attention Guidance (MSAG) and adaptive input\nperturbation (DDPM-AIP) to accelerate the iterative training speed of\ndiffusion. Experiments on shadow removal tasks demonstrate that these methods\ncan effectively improve image restoration performance.\n","authors":["Xiao Feng Zhang","Tian Yi Song","Jia Wei Yao"],"pdf_url":"https://arxiv.org/pdf/2309.11715v3.pdf","comment":"it needs revised"},{"id":"http://arxiv.org/abs/2401.01496v1","updated":"2024-01-03T02:01:09Z","published":"2024-01-03T02:01:09Z","title":"From Pixel to Slide image: Polarization Modality-based Pathological\n  Diagnosis Using Representation Learning","summary":"  Thyroid cancer is the most common endocrine malignancy, and accurately\ndistinguishing between benign and malignant thyroid tumors is crucial for\ndeveloping effective treatment plans in clinical practice. Pathologically,\nthyroid tumors pose diagnostic challenges due to improper specimen sampling. In\nthis study, we have designed a three-stage model using representation learning\nto integrate pixel-level and slice-level annotations for distinguishing thyroid\ntumors. This structure includes a pathology structure recognition method to\npredict structures related to thyroid tumors, an encoder-decoder network to\nextract pixel-level annotation information by learning the feature\nrepresentations of image blocks, and an attention-based learning mechanism for\nthe final classification task. This mechanism learns the importance of\ndifferent image blocks in a pathological region, globally considering the\ninformation from each block. In the third stage, all information from the image\nblocks in a region is aggregated using attention mechanisms, followed by\nclassification to determine the category of the region. Experimental results\ndemonstrate that our proposed method can predict microscopic structures more\naccurately. After color-coding, the method achieves results on unstained\npathology slides that approximate the quality of Hematoxylin and eosin\nstaining, reducing the need for stained pathology slides. Furthermore, by\nleveraging the concept of indirect measurement and extracting polarized\nfeatures from structures correlated with lesions, the proposed method can also\nclassify samples where membrane structures cannot be obtained through sampling,\nproviding a potential objective and highly accurate indirect diagnostic\ntechnique for thyroid tumors.\n","authors":["Jia Dong","Yao Yao","Yang Dong","Hui Ma"],"pdf_url":"https://arxiv.org/pdf/2401.01496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00436v2","updated":"2024-01-03T01:42:22Z","published":"2023-12-31T09:24:28Z","title":"Diff-PCR: Diffusion-Based Correspondence Searching in Doubly Stochastic\n  Matrix Space for Point Cloud Registration","summary":"  Efficiently finding optimal correspondences between point clouds is crucial\nfor solving both rigid and non-rigid point cloud registration problems.\nExisting methods often rely on geometric or semantic feature embedding to\nestablish correspondences and estimate transformations or flow fields.\nRecently, state-of-the-art methods have employed RAFT-like iterative updates to\nrefine the solution. However, these methods have certain limitations. Firstly,\ntheir iterative refinement design lacks transparency, and their iterative\nupdates follow a fixed path during the refinement process, which can lead to\nsuboptimal results. Secondly, these methods overlook the importance of refining\nor optimizing correspondences (or matching matrices) as a precursor to solving\ntransformations or flow fields. They typically compute candidate\ncorrespondences based on distances in the point feature space. However, they\nonly project the candidate matching matrix into some matrix space once with\nSinkhorn or dual softmax operations to obtain final correspondences. This\none-shot projected matching matrix may be far from the globally optimal one,\nand these approaches do not consider the distribution of the target matching\nmatrix. In this paper, we propose a novel approach that exploits the Denoising\nDiffusion Model to predict a searching gradient for the optimal matching matrix\nwithin the Doubly Stochastic Matrix Space. During the reverse denoising\nprocess, our method iteratively searches for better solutions along this\ndenoising gradient, which points towards the maximum likelihood direction of\nthe target matching matrix. Our method offers flexibility by allowing the\nsearch to start from any initial matching matrix provided by the online\nbackbone or white noise. Experimental evaluations on the 3DMatch/3DLoMatch and\n4DMatch/4DLoMatch datasets demonstrate the effectiveness of our newly designed\nframework.\n","authors":["Qianliang Wu","Haobo Jiang","Yaqing Ding","Lei Luo","Jin Xie","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2401.00436v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.00731v2","updated":"2024-01-03T01:26:10Z","published":"2022-11-30T02:47:18Z","title":"FuRPE: Learning Full-body Reconstruction from Part Experts","summary":"  In the field of full-body reconstruction, the scarcity of annotated data\noften impedes the efficacy of prevailing methods. To address this issue, we\nintroduce FuRPE, a novel framework that employs part-experts and an ingenious\npseudo ground-truth selection scheme to derive high-quality pseudo labels.\nThese labels, central to our approach, equip our network with the capability to\nefficiently learn from the available data. Integral to FuRPE is a unique\nexponential moving average training strategy and expert-derived feature\ndistillation strategy. These novel elements of FuRPE not only serve to further\nrefine the model but also to reduce potential biases that may arise from\ninaccuracies in pseudo labels, thereby optimizing the network's training\nprocess and enhancing the robustness of the model. We apply FuRPE to train both\ntwo-stage and fully convolutional single-stage full-body reconstruction\nnetworks. Our exhaustive experiments on numerous benchmark datasets illustrate\na substantial performance boost over existing methods, underscoring FuRPE's\npotential to reshape the state-of-the-art in full-body reconstruction.\n","authors":["Zhaoxin Fan","Yuqing Pan","Hao Xu","Zhenbo Song","Zhicheng Wang","Kejian Wu","Hongyan Liu","Jun He"],"pdf_url":"https://arxiv.org/pdf/2212.00731v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01597v3","updated":"2024-01-03T01:15:50Z","published":"2023-12-04T03:18:46Z","title":"SCLIP: Rethinking Self-Attention for Dense Vision-Language Inference","summary":"  Recent advances in contrastive language-image pretraining (CLIP) have\ndemonstrated strong capabilities in zero-shot classification by aligning visual\nrepresentations with target text embeddings in an image level. However, in\ndense prediction tasks, CLIP often struggles to localize visual features within\nan image and fails to give accurate pixel-level predictions, which prevents it\nfrom functioning as a generalized visual foundation model. In this work, we aim\nto enhance CLIP's potential for semantic segmentation with minimal\nmodifications to its pretrained models. By rethinking self-attention, we\nsurprisingly find that CLIP can adapt to dense prediction tasks by simply\nintroducing a novel Correlative Self-Attention (CSA) mechanism. Specifically,\nwe replace the traditional self-attention block of CLIP vision encoder's last\nlayer by our CSA module and reuse its pretrained projection matrices of query,\nkey, and value, leading to a training-free adaptation approach for CLIP's\nzero-shot semantic segmentation. Extensive experiments show the advantage of\nCSA: we obtain a 38.2% average zero-shot mIoU across eight semantic\nsegmentation benchmarks highlighted in this paper, significantly outperforming\nthe existing SoTA's 33.9% and the vanilla CLIP's 14.1%.\n","authors":["Feng Wang","Jieru Mei","Alan Yuille"],"pdf_url":"https://arxiv.org/pdf/2312.01597v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01482v1","updated":"2024-01-03T01:11:16Z","published":"2024-01-03T01:11:16Z","title":"Incorporating Geo-Diverse Knowledge into Prompting for Increased\n  Geographical Robustness in Object Recognition","summary":"  Existing object recognition models have been shown to lack robustness in\ndiverse geographical scenarios due to significant domain shifts in design and\ncontext. Class representations need to be adapted to more accurately reflect an\nobject concept under these shifts. In the absence of training data from target\ngeographies, we hypothesize that geography-specific descriptive knowledge of\nobject categories can be leveraged to enhance robustness. For this purpose, we\nexplore the feasibility of probing a large-language model for\ngeography-specific object knowledge, and we investigate integrating knowledge\nin zero-shot and learnable soft prompting with the CLIP vision-language model.\nIn particular, we propose a geography knowledge regularization method to ensure\nthat soft prompts trained on a source set of geographies generalize to an\nunseen target set of geographies. Our gains on DollarStreet when generalizing\nfrom a model trained only on data from Europe are as large as +2.8 on countries\nfrom Africa, and +4.6 on the hardest classes. We further show competitive\nperformance vs. few-shot target training, and provide insights into how\ndescriptive knowledge captures geographical differences.\n","authors":["Kyle Buettner","Sina Malakouti","Xiang Lorraine Li","Adriana Kovashka"],"pdf_url":"https://arxiv.org/pdf/2401.01482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.17097v3","updated":"2024-01-03T01:03:58Z","published":"2023-10-26T01:40:28Z","title":"Navigating Data Heterogeneity in Federated Learning A Semi-Supervised\n  Federated Object Detection","summary":"  Federated Learning (FL) has emerged as a potent framework for training models\nacross distributed data sources while maintaining data privacy. Nevertheless,\nit faces challenges with limited high-quality labels and non-IID client data,\nparticularly in applications like autonomous driving. To address these hurdles,\nwe navigate the uncharted waters of Semi-Supervised Federated Object Detection\n(SSFOD). We present a pioneering SSFOD framework, designed for scenarios where\nlabeled data reside only at the server while clients possess unlabeled data.\nNotably, our method represents the inaugural implementation of SSFOD for\nclients with 0% labeled non-IID data, a stark contrast to previous studies that\nmaintain some subset of labels at each client. We propose FedSTO, a two-stage\nstrategy encompassing Selective Training followed by Orthogonally enhanced\nfull-parameter training, to effectively address data shift (e.g. weather\nconditions) between server and clients. Our contributions include selectively\nrefining the backbone of the detector to avert overfitting, orthogonality\nregularization to boost representation divergence, and local EMA-driven pseudo\nlabel assignment to yield high-quality pseudo labels. Extensive validation on\nprominent autonomous driving datasets (BDD100K, Cityscapes, and SODA10M)\nattests to the efficacy of our approach, demonstrating state-of-the-art\nresults. Remarkably, FedSTO, using just 20-30% of labels, performs nearly as\nwell as fully-supervised centralized training methods.\n","authors":["Taehyeon Kim","Eric Lin","Junu Lee","Christian Lau","Vaikkunth Mugunthan"],"pdf_url":"https://arxiv.org/pdf/2310.17097v3.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2401.01470v1","updated":"2024-01-03T00:10:33Z","published":"2024-01-03T00:10:33Z","title":"Token Propagation Controller for Efficient Vision Transformer","summary":"  Vision transformers (ViTs) have achieved promising results on a variety of\nComputer Vision tasks, however their quadratic complexity in the number of\ninput tokens has limited their application specially in resource-constrained\nsettings. Previous approaches that employ gradual token reduction to address\nthis challenge assume that token redundancy in one layer implies redundancy in\nall the following layers. We empirically demonstrate that this assumption is\noften not correct, i.e., tokens that are redundant in one layer can be useful\nin later layers. We employ this key insight to propose a novel token\npropagation controller (TPC) that incorporates two different\ntoken-distributions, i.e., pause probability and restart probability to control\nthe reduction and reuse of tokens respectively, which results in more efficient\ntoken utilization. To improve the estimates of token distributions, we propose\na smoothing mechanism that acts as a regularizer and helps remove noisy\noutliers. Furthermore, to improve the training-stability of our proposed TPC,\nwe introduce a model stabilizer that is able to implicitly encode local image\nstructures and minimize accuracy fluctuations during model training. We present\nextensive experimental results on the ImageNet-1K dataset using DeiT, LV-ViT\nand Swin models to demonstrate the effectiveness of our proposed method. For\nexample, compared to baseline models, our proposed method improves the\ninference speed of the DeiT-S by 250% while increasing the classification\naccuracy by 1.0%.\n","authors":["Wentao Zhu"],"pdf_url":"https://arxiv.org/pdf/2401.01470v1.pdf","comment":"Accepted by WACV 2024"},{"id":"http://arxiv.org/abs/2401.01990v1","updated":"2024-01-03T21:39:06Z","published":"2024-01-03T21:39:06Z","title":"GPS-SSL: Guided Positive Sampling to Inject Prior Into Self-Supervised\n  Learning","summary":"  We propose Guided Positive Sampling Self-Supervised Learning (GPS-SSL), a\ngeneral method to inject a priori knowledge into Self-Supervised Learning (SSL)\npositive samples selection. Current SSL methods leverage Data-Augmentations\n(DA) for generating positive samples and incorporate prior knowledge - an\nincorrect, or too weak DA will drastically reduce the quality of the learned\nrepresentation. GPS-SSL proposes instead to design a metric space where\nEuclidean distances become a meaningful proxy for semantic relationship. In\nthat space, it is now possible to generate positive samples from nearest\nneighbor sampling. Any prior knowledge can now be embedded into that metric\nspace independently from the employed DA. From its simplicity, GPS-SSL is\napplicable to any SSL method, e.g. SimCLR or BYOL. A key benefit of GPS-SSL is\nin reducing the pressure in tailoring strong DAs. For example GPS-SSL reaches\n85.58% on Cifar10 with weak DA while the baseline only reaches 37.51%. We\ntherefore move a step forward towards the goal of making SSL less reliant on\nDA. We also show that even when using strong DAs, GPS-SSL outperforms the\nbaselines on under-studied domains. We evaluate GPS-SSL along with multiple\nbaseline SSL methods on numerous downstream datasets from different domains\nwhen the models use strong or minimal data augmentations. We hope that GPS-SSL\nwill open new avenues in studying how to inject a priori knowledge into SSL in\na principled manner.\n","authors":["Aarash Feizi","Randall Balestriero","Adriana Romero-Soriano","Reihaneh Rabbany"],"pdf_url":"https://arxiv.org/pdf/2401.01990v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01984v1","updated":"2024-01-03T21:24:44Z","published":"2024-01-03T21:24:44Z","title":"AUPIMO: Redefining Visual Anomaly Detection Benchmarks with High Speed\n  and Low Tolerance","summary":"  Recent advances in visual anomaly detection research have seen AUROC and\nAUPRO scores on public benchmark datasets such as MVTec and VisA converge\ntowards perfect recall, giving the impression that these benchmarks are\nnear-solved. However, high AUROC and AUPRO scores do not always reflect\nqualitative performance, which limits the validity of these metrics in\nreal-world applications. We argue that the artificial ceiling imposed by the\nlack of an adequate evaluation metric restrains progression of the field, and\nit is crucial that we revisit the evaluation metrics used to rate our\nalgorithms. In response, we introduce Per-IMage Overlap (PIMO), a novel metric\nthat addresses the shortcomings of AUROC and AUPRO. PIMO retains the\nrecall-based nature of the existing metrics but introduces two distinctions:\nthe assignment of curves (and respective area under the curve) is per-image,\nand its X-axis relies solely on normal images. Measuring recall per image\nsimplifies instance score indexing and is more robust to noisy annotations. As\nwe show, it also accelerates computation and enables the usage of statistical\ntests to compare models. By imposing low tolerance for false positives on\nnormal images, PIMO provides an enhanced model validation procedure and\nhighlights performance variations across datasets. Our experiments demonstrate\nthat PIMO offers practical advantages and nuanced performance insights that\nredefine anomaly detection benchmarks -- notably challenging the perception\nthat MVTec AD and VisA datasets have been solved by contemporary models.\nAvailable on GitHub: https://github.com/jpcbertoldo/aupimo.\n","authors":["Joao P. C. Bertoldo","Dick Ameln","Ashwin Vaidya","Samet Akçay"],"pdf_url":"https://arxiv.org/pdf/2401.01984v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01974v1","updated":"2024-01-03T20:48:47Z","published":"2024-01-03T20:48:47Z","title":"Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as\n  Programmers","summary":"  Visual reasoning is dominated by end-to-end neural networks scaled to\nbillions of model parameters and training examples. However, even the largest\nmodels struggle with compositional reasoning, generalization, fine-grained\nspatial and temporal reasoning, and counting. Visual reasoning with large\nlanguage models (LLMs) as controllers can, in principle, address these\nlimitations by decomposing the task and solving subtasks by orchestrating a set\nof (visual) tools. Recently, these models achieved great performance on tasks\nsuch as compositional visual question answering, visual grounding, and video\ntemporal reasoning. Nevertheless, in their current form, these models heavily\nrely on human engineering of in-context examples in the prompt, which are often\ndataset- and task-specific and require significant labor by highly skilled\nprogrammers. In this work, we present a framework that mitigates these issues\nby introducing spatially and temporally abstract routines and by leveraging a\nsmall number of labeled examples to automatically generate in-context examples,\nthereby avoiding human-created in-context examples. On a number of visual\nreasoning tasks, we show that our framework leads to consistent gains in\nperformance, makes LLMs as controllers setup more robust, and removes the need\nfor human engineering of in-context examples.\n","authors":["Aleksandar Stanić","Sergi Caelles","Michael Tschannen"],"pdf_url":"https://arxiv.org/pdf/2401.01974v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01970v1","updated":"2024-01-03T20:39:02Z","published":"2024-01-03T20:39:02Z","title":"FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D\n  Scene Understanding","summary":"  Precisely perceiving the geometric and semantic properties of real-world 3D\nobjects is crucial for the continued evolution of augmented reality and robotic\napplications. To this end, we present \\algfull{} (\\algname{}), which\nincorporates vision-language embeddings of foundation models into 3D Gaussian\nSplatting (GS). The key contribution of this work is an efficient method to\nreconstruct and represent 3D vision-language models. This is achieved by\ndistilling feature maps generated from image-based foundation models into those\nrendered from our 3D model. To ensure high-quality rendering and fast training,\nwe introduce a novel scene representation by integrating strengths from both GS\nand multi-resolution hash encodings (MHE). Our effective training procedure\nalso introduces a pixel alignment loss that makes the rendered feature distance\nof same semantic entities close, following the pixel-level semantic boundaries.\nOur results demonstrate remarkable multi-view semantic consistency,\nfacilitating diverse downstream tasks, beating state-of-the-art methods by\n$\\mathbf{10.2}$ percent on open-vocabulary language-based object detection,\ndespite that we are $\\mathbf{851\\times}$ faster for inference. This research\nexplores the intersection of vision, language, and 3D scene representation,\npaving the way for enhanced scene understanding in uncontrolled real-world\nenvironments. We plan to release the code upon paper acceptance.\n","authors":["Xingxing Zuo","Pouya Samangouei","Yunwen Zhou","Yan Di","Mingyang Li"],"pdf_url":"https://arxiv.org/pdf/2401.01970v1.pdf","comment":"19 pages, Project page coming soon"},{"id":"http://arxiv.org/abs/2310.20065v2","updated":"2024-01-03T19:57:42Z","published":"2023-10-30T22:29:50Z","title":"LinFlo-Net: A two-stage deep learning method to generate simulation\n  ready meshes of the heart","summary":"  We present a deep learning model to automatically generate computer models of\nthe human heart from patient imaging data with an emphasis on its capability to\ngenerate thin-walled cardiac structures. Our method works by deforming a\ntemplate mesh to fit the cardiac structures to the given image. Compared with\nprior deep learning methods that adopted this approach, our framework is\ndesigned to minimize mesh self-penetration, which typically arises when\ndeforming surface meshes separated by small distances. We achieve this by using\na two-stage diffeomorphic deformation process along with a novel loss function\nderived from the kinematics of motion that penalizes surface contact and\ninterpenetration. Our model demonstrates comparable accuracy with\nstate-of-the-art methods while additionally producing meshes free of\nself-intersections. The resultant meshes are readily usable in physics based\nsimulation, minimizing the need for post-processing and cleanup.\n","authors":["Arjun Narayanan","Fanwei Kong","Shawn Shadden"],"pdf_url":"https://arxiv.org/pdf/2310.20065v2.pdf","comment":"Accepted manuscript in the Journal of Biomechanical Engineering"},{"id":"http://arxiv.org/abs/2401.01952v1","updated":"2024-01-03T19:31:58Z","published":"2024-01-03T19:31:58Z","title":"Instruct-Imagen: Image Generation with Multi-modal Instruction","summary":"  This paper presents instruct-imagen, a model that tackles heterogeneous image\ngeneration tasks and generalizes across unseen tasks. We introduce *multi-modal\ninstruction* for image generation, a task representation articulating a range\nof generation intents with precision. It uses natural language to amalgamate\ndisparate modalities (e.g., text, edge, style, subject, etc.), such that\nabundant generation intents can be standardized in a uniform format.\n  We then build instruct-imagen by fine-tuning a pre-trained text-to-image\ndiffusion model with a two-stage framework. First, we adapt the model using the\nretrieval-augmented training, to enhance model's capabilities to ground its\ngeneration on external multimodal context. Subsequently, we fine-tune the\nadapted model on diverse image generation tasks that requires vision-language\nunderstanding (e.g., subject-driven generation, etc.), each paired with a\nmulti-modal instruction encapsulating the task's essence. Human evaluation on\nvarious image generation datasets reveals that instruct-imagen matches or\nsurpasses prior task-specific models in-domain and demonstrates promising\ngeneralization to unseen and more complex tasks.\n","authors":["Hexiang Hu","Kelvin C. K. Chan","Yu-Chuan Su","Wenhu Chen","Yandong Li","Kihyuk Sohn","Yang Zhao","Xue Ben","Boqing Gong","William Cohen","Ming-Wei Chang","Xuhui Jia"],"pdf_url":"https://arxiv.org/pdf/2401.01952v1.pdf","comment":"20 pages, 18 figures"},{"id":"http://arxiv.org/abs/2401.01951v1","updated":"2024-01-03T19:27:20Z","published":"2024-01-03T19:27:20Z","title":"Can We Generate Realistic Hands Only Using Convolution?","summary":"  The enduring inability of image generative models to recreate intricate\ngeometric features, such as those present in human hands and fingers has been\nan ongoing problem in image generation for nearly a decade. While strides have\nbeen made by increasing model sizes and diversifying training datasets, this\nissue remains prevalent across all models, from denoising diffusion models to\nGenerative Adversarial Networks (GAN), pointing to a fundamental shortcoming in\nthe underlying architectures. In this paper, we demonstrate how this problem\ncan be mitigated by augmenting convolution layers geometric capabilities\nthrough providing them with a single input channel incorporating the relative\n$n$-dimensional Cartesian coordinate system. We show that this drastically\nimproves quality of hand and face images generated by GANs and Variational\nAutoEncoders (VAE).\n","authors":["Mehran Hosseini","Peyman Hosseini"],"pdf_url":"https://arxiv.org/pdf/2401.01951v1.pdf","comment":"Contains 17 pages, 14 figures, and 6 tables"},{"id":"http://arxiv.org/abs/2401.01922v1","updated":"2024-01-03T15:09:25Z","published":"2024-01-03T15:09:25Z","title":"Unsupervised Object-Centric Learning from Multiple Unspecified\n  Viewpoints","summary":"  Visual scenes are extremely diverse, not only because there are infinite\npossible combinations of objects and backgrounds but also because the\nobservations of the same scene may vary greatly with the change of viewpoints.\nWhen observing a multi-object visual scene from multiple viewpoints, humans can\nperceive the scene compositionally from each viewpoint while achieving the\nso-called ``object constancy'' across different viewpoints, even though the\nexact viewpoints are untold. This ability is essential for humans to identify\nthe same object while moving and to learn from vision efficiently. It is\nintriguing to design models that have a similar ability. In this paper, we\nconsider a novel problem of learning compositional scene representations from\nmultiple unspecified (i.e., unknown and unrelated) viewpoints without using any\nsupervision and propose a deep generative model which separates latent\nrepresentations into a viewpoint-independent part and a viewpoint-dependent\npart to solve this problem. During the inference, latent representations are\nrandomly initialized and iteratively updated by integrating the information in\ndifferent viewpoints with neural networks. Experiments on several specifically\ndesigned synthetic datasets have shown that the proposed method can effectively\nlearn from multiple unspecified viewpoints.\n","authors":["Jinyang Yuan","Tonglin Chen","Zhimeng Shen","Bin Li","Xiangyang Xue"],"pdf_url":"https://arxiv.org/pdf/2401.01922v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2112.03568"},{"id":"http://arxiv.org/abs/2309.15216v2","updated":"2024-01-03T12:52:42Z","published":"2023-09-26T19:21:09Z","title":"Diabetic Retinopathy Using Gaussian Filter","summary":"  The retina is an essential component of the visual system, and maintaining\neyesight depends on the timely and correct detection of disorders. This\nresearch specifically addresses the early-stage detection and severity\nclassification of diabetic retinopathy (DR), a serious public health hazard. We\ncompare the results of different deep learning models such as InceptionV3,\nDenseNet121 and other CNN based models by using different image filters, such\nas Gaussian, grayscale and Gabor. These models could detect subtle pathological\nalterations and use that information to estimate the risk of retinal illnesses.\nThe objective is to improve the diagnostic processes for diabetic retinopathy,\nthe primary cause of diabetes-related blindness, by utilizing deep learning\nmodels. A comparative analysis between Greyscale, Gaussian and Gabor filters\nhas been provided after applying these filters on the retinal images. The\nGaussian filter resulted to be the most promising filter giving the best\naccuracies for all the models. The best performing model was InceptionV3 which\ngave an accuracy of 96% on Gaussian images, therefore Gaussian filter emerged\nas our most promising filter.\n","authors":["Roshan Vasu Muddaluru","Sharvaani Ravikumar Thoguluva","Shruti Prabha","Tanuja Konda Reddy","Dr. Suja P"],"pdf_url":"https://arxiv.org/pdf/2309.15216v2.pdf","comment":"6 pages, 5 figures, conference, 2 tables"}],"Robotics":[{"id":"http://arxiv.org/abs/2401.01881v1","updated":"2024-01-03T18:42:22Z","published":"2024-01-03T18:42:22Z","title":"Robust Control Barrier Functions using Uncertainty Estimation with\n  Application to Mobile Robots","summary":"  Model uncertainty poses a significant challenge to the implementation of\nsafety-critical control systems. With this as motivation, this paper proposes a\nsafe control design approach that guarantees the robustness of nonlinear\nfeedback systems in the presence of matched or unmatched unmodelled system\ndynamics and external disturbances. Our approach couples control barrier\nfunctions (CBFs) with a new uncertainty/disturbance estimator to ensure robust\nsafety against input and state-dependent model uncertainties. We prove upper\nbounds on the estimator's error and estimated outputs. We use an uncertainty\nestimator-based composite feedback control law to adaptively improve robust\ncontrol performance under hard safety constraints by compensating for the\nmatched uncertainty. Then, we robustify existing CBF constraints with this\nuncertainty estimate and the estimation error bounds to ensure robust safety\nvia a quadratic program (CBF-QP). We also extend our method to higher-order\nCBFs (HOCBFs) to achieve safety under unmatched uncertainty, which causes\nrelative degree differences with respect to control input and disturbance. We\nassume the relative degree difference is at most one, resulting in a\nsecond-order cone (SOC) condition. The proposed robust HOCBFs method is\ndemonstrated in a simulation of an uncertain elastic actuator control problem.\nFinally, the efficacy of our method is experimentally demonstrated on a tracked\nrobot with slope-induced matched and unmatched perturbations.\n","authors":["Ersin Das","Joel W. Burdick"],"pdf_url":"https://arxiv.org/pdf/2401.01881v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01817v1","updated":"2024-01-03T16:20:11Z","published":"2024-01-03T16:20:11Z","title":"Many-Objective-Optimized Semi-Automated Robotic Disassembly Sequences","summary":"  This study tasckles the problem of many-objective sequence optimization for\nsemi-automated robotic disassembly operations. To this end, we employ a\nmany-objective genetic algorithm (MaOGA) algorithm inspired by the\nNon-dominated Sorting Genetic Algorithm (NSGA)-III, along with\nrobotic-disassembly-oriented constraints and objective functions derived from\ngeometrical and robot simulations using 3-dimensional (3D) geometrical\ninformation stored in a 3D Computer-Aided Design (CAD) model of the target\nproduct. The MaOGA begins by generating a set of initial chromosomes based on a\ncontact and connection graph (CCG), rather than random chromosomes, to avoid\nfalling into a local minimum and yield repeatable convergence. The optimization\nimposes constraints on feasibility and stability as well as objective functions\nregarding difficulty, efficiency, prioritization, and allocability to generate\na sequence that satisfies many preferred conditions under mandatory\nrequirements for semi-automated robotic disassembly. The NSGA-III-inspired\nMaOGA also utilizes non-dominated sorting and niching with reference lines to\nfurther encourage steady and stable exploration and uniformly lower the overall\nevaluation values. Our sequence generation experiments for a complex product\n(36 parts) demonstrated that the proposed method can consistently produce\nfeasible and stable sequences with a 100% success rate, bringing the multiple\npreferred conditions closer to the optimal solution required for semi-automated\nrobotic disassembly operations.\n","authors":["Takuya Kiyokawa","Kensuke Harada","Weiwei Wan","Tomoki Ishikura","Naoya Miyaji","Genichiro Matsuda"],"pdf_url":"https://arxiv.org/pdf/2401.01817v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.11349v5","updated":"2024-01-03T16:01:52Z","published":"2023-07-21T04:50:59Z","title":"EV-Planner: Energy-Efficient Robot Navigation via Event-Based\n  Physics-Guided Neuromorphic Planner","summary":"  Vision-based object tracking is an essential precursor to performing\nautonomous aerial navigation in order to avoid obstacles. Biologically inspired\nneuromorphic event cameras are emerging as a powerful alternative to\nframe-based cameras, due to their ability to asynchronously detect varying\nintensities (even in poor lighting conditions), high dynamic range, and\nrobustness to motion blur. Spiking neural networks (SNNs) have gained traction\nfor processing events asynchronously in an energy-efficient manner. On the\nother hand, physics-based artificial intelligence (AI) has gained prominence\nrecently, as they enable embedding system knowledge via physical modeling\ninside traditional analog neural networks (ANNs). In this letter, we present an\nevent-based physics-guided neuromorphic planner (EV-Planner) to perform\nobstacle avoidance using neuromorphic event cameras and physics-based AI. We\nconsider the task of autonomous drone navigation where the mission is to detect\nmoving gates and fly through them while avoiding a collision. We use event\ncameras to perform object detection using a shallow spiking neural network in\nan unsupervised fashion. Utilizing the physical equations of the brushless DC\nmotors present in the drone rotors, we train a lightweight energy-aware\nphysics-guided neural network (PgNN) with depth inputs. This predicts the\noptimal flight time responsible for generating near-minimum energy paths. We\nspawn the drone in the Gazebo simulator and implement a sensor-fused\nvision-to-planning neuro-symbolic framework using Robot Operating System (ROS).\nSimulation results for safe collision-free flight trajectories are presented\nwith performance analysis, ablation study and potential future research\ndirections\n","authors":["Sourav Sanyal","Rohan Kumar Manna","Kaushik Roy"],"pdf_url":"https://arxiv.org/pdf/2307.11349v5.pdf","comment":"accepted for publication at IEEE Robotics and Automation Letters"},{"id":"http://arxiv.org/abs/2312.13863v2","updated":"2024-01-03T15:52:24Z","published":"2023-12-21T14:01:51Z","title":"Manipulating Trajectory Prediction with Backdoors","summary":"  Autonomous vehicles ought to predict the surrounding agents' trajectories to\nallow safe maneuvers in uncertain and complex traffic situations. As companies\nincreasingly apply trajectory prediction in the real world, security becomes a\nrelevant concern. In this paper, we focus on backdoors - a security threat\nacknowledged in other fields but so far overlooked for trajectory prediction.\nTo this end, we describe and investigate four triggers that could affect\ntrajectory prediction. We then show that these triggers (for example, a braking\nvehicle), when correlated with a desired output (for example, a curve) during\ntraining, cause the desired output of a state-of-the-art trajectory prediction\nmodel. In other words, the model has good benign performance but is vulnerable\nto backdoors. This is the case even if the trigger maneuver is performed by a\nnon-casual agent behind the target vehicle. As a side-effect, our analysis\nreveals interesting limitations within trajectory prediction models. Finally,\nwe evaluate a range of defenses against backdoors. While some, like simple\noffroad checks, do not enable detection for all triggers, clustering is a\npromising candidate to support manual inspection to find backdoors.\n","authors":["Kaouther Messaoud","Kathrin Grosse","Mickael Chen","Matthieu Cord","Patrick Pérez","Alexandre Alahi"],"pdf_url":"https://arxiv.org/pdf/2312.13863v2.pdf","comment":"9 pages, 7 figures"},{"id":"http://arxiv.org/abs/2401.01756v1","updated":"2024-01-03T14:19:04Z","published":"2024-01-03T14:19:04Z","title":"Fuzzy Logic Controller Design for Mobile Robot Outdoor Navigation","summary":"  Many researchers around the world are researching to get control solutions\nthat enhance robots' ability to navigate in dynamic environments autonomously.\nHowever, until these days robots have limited capability and many navigation\ntasks on Earth and other planets have been difficult so far. This paperwork\npresents the development of a control system for a differential drive-wheeled\nmobile robot that autonomously controls its position, heading, and speed based\non destination information given and surrounding data gathered through mounted\nproximity and GPS sensors. The intelligence of this control system is\nimplemented by using a fuzzy logic algorithm which is a very powerful tool to\nhandle un-modeled systems like the dynamically changing environment dealt with\nin this research. The fuzzy controller is used to address the problems\nassociated with navigation in an obstacle-strewn environment. Such issues\ninclude position estimation, path planning, and obstacle avoidance. In this\nstudy modeling, design, and simulation of the system have been done. The\nsimulation result shows that the developed mobile robot travels successfully\nfrom any location to the destination location without colliding with obstacles.\n","authors":["A. Wondosen","Dereje Shiferaw"],"pdf_url":"https://arxiv.org/pdf/2401.01756v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2401.01739v1","updated":"2024-01-03T13:36:51Z","published":"2024-01-03T13:36:51Z","title":"A Soft Continuum Robot with Self-Controllable Variable Curvature","summary":"  This paper introduces a new type of soft continuum robot, called SCoReS,\nwhich is capable of self-controlling continuously its curvature at the segment\nlevel; in contrast to previous designs which either require external forces or\nmachine elements, or whose variable curvature capabilities are discrete --\ndepending on the number of locking mechanisms and segments. The ability to have\na variable curvature, whose control is continuous and independent from external\nfactors, makes a soft continuum robot more adaptive in constrained\nenvironments, similar to what is observed in nature in the elephant's trunk or\nostrich's neck for instance which exhibit multiple curvatures. To this end, our\nsoft continuum robot enables reconfigurable variable curvatures utilizing a\nvariable stiffness growing spine based on micro-particle granular jamming for\nthe first time. We detail the design of the proposed robot, presenting its\nmodeling through beam theory and FEA simulation -- which is validated through\nexperiments. The robot's versatile bending profiles are then explored in\nexperiments and an application to grasp fruits at different configurations is\ndemonstrated.\n","authors":["Xinran Wang","Qiujie Lu","Dongmyoung Lee","Zhongxue Gan","Nicolas Rojas"],"pdf_url":"https://arxiv.org/pdf/2401.01739v1.pdf","comment":"accpeted for IEEE Robotics and Automation letters in January 2024"},{"id":"http://arxiv.org/abs/2303.05308v2","updated":"2024-01-03T11:12:33Z","published":"2023-03-09T14:58:01Z","title":"SpyroPose: SE(3) Pyramids for Object Pose Distribution Estimation","summary":"  Object pose estimation is a core computer vision problem and often an\nessential component in robotics. Pose estimation is usually approached by\nseeking the single best estimate of an object's pose, but this approach is\nill-suited for tasks involving visual ambiguity. In such cases it is desirable\nto estimate the uncertainty as a pose distribution to allow downstream tasks to\nmake informed decisions. Pose distributions can have arbitrary complexity which\nmotivates estimating unparameterized distributions, however, until now they\nhave only been used for orientation estimation on SO(3) due to the difficulty\nin training on and normalizing over SE(3). We propose a novel method for pose\ndistribution estimation on SE(3). We use a hierarchical grid, a pyramid, which\nenables efficient importance sampling during training and sparse evaluation of\nthe pyramid at inference, allowing real time 6D pose distribution estimation.\nOur method outperforms state-of-the-art methods on SO(3), and to the best of\nour knowledge, we provide the first quantitative results on pose distribution\nestimation on SE(3). Code will be available at spyropose.github.io\n","authors":["Rasmus Laurvig Haugaard","Frederik Hagelskjær","Thorbjørn Mosekjær Iversen"],"pdf_url":"https://arxiv.org/pdf/2303.05308v2.pdf","comment":"ICCVW 2023 (R6D)"},{"id":"http://arxiv.org/abs/2304.02359v2","updated":"2024-01-03T10:55:49Z","published":"2023-04-05T10:49:41Z","title":"Efficient Optimization-based Cable Force Allocation for Geometric\n  Control of a Multirotor Team Transporting a Payload","summary":"  We consider transporting a heavy payload that is attached to multiple\nmultirotors. The current state-of-the-art controllers either do not avoid\ninter-robot collision at all, leading to crashes when tasked with carrying\npayloads that are small in size compared to the cable lengths, or use\ncomputational demanding nonlinear optimization. We propose an efficient\noptimization-based cable force allocation for a geometric payload transport\ncontroller to effectively avoid such collisions, while retaining the stability\nproperties of the geometric controller. Our approach introduces a cascade of\ncarefully designed quadratic programs that can be solved efficiently on highly\nconstrained embedded flight controllers.\n  We show that our approach exceeds the state-of-the-art controllers in terms\nof scalability by at least an order of magnitude for up to 10 robots. We\ndemonstrate our method on challenging scenarios with up to three small\nmultirotors with various payloads and cable lengths, where our controller runs\nin realtime directly on a microcontroller on the robots.\n","authors":["Khaled Wahba","Wolfgang Hönig"],"pdf_url":"https://arxiv.org/pdf/2304.02359v2.pdf","comment":"Accepted at IEEE RA-L, 2024"},{"id":"http://arxiv.org/abs/2401.01657v1","updated":"2024-01-03T10:31:12Z","published":"2024-01-03T10:31:12Z","title":"Distributed Pose-graph Optimization with Multi-level Partitioning for\n  Collaborative SLAM","summary":"  The back-end module of Distributed Collaborative Simultaneous Localization\nand Mapping (DCSLAM) requires solving a nonlinear Pose Graph Optimization (PGO)\nunder a distributed setting, also known as SE(d)-synchronization. Most existing\ndistributed graph optimization algorithms employ a simple sequential\npartitioning scheme, which may result in unbalanced subgraph dimensions due to\nthe different geographic locations of each robot, and hence imposes extra\ncommunication load. Moreover, the performance of current Riemannian\noptimization algorithms can be further accelerated. In this letter, we propose\na novel distributed pose graph optimization algorithm combining multi-level\npartitioning with an accelerated Riemannian optimization method. Firstly, we\nemploy the multi-level graph partitioning algorithm to preprocess the naive\npose graph to formulate a balanced optimization problem. In addition, inspired\nby the accelerated coordinate descent method, we devise an Improved Riemannian\nBlock Coordinate Descent (IRBCD) algorithm and the critical point obtained is\nglobally optimal. Finally, we evaluate the effects of four common graph\npartitioning approaches on the correlation of the inter-subgraphs, and discover\nthat the Highest scheme has the best partitioning performance. Also, we\nimplement simulations to quantitatively demonstrate that our proposed algorithm\noutperforms the state-of-the-art distributed pose graph optimization protocols.\n","authors":["Cunhao Li","Peng Yi","Guanghui Guo","Yiguang Hong"],"pdf_url":"https://arxiv.org/pdf/2401.01657v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01644v1","updated":"2024-01-03T09:38:08Z","published":"2024-01-03T09:38:08Z","title":"Motion Control of Interactive Robotic Arms Based on Mixed Reality\n  Development","summary":"  Mixed Reality (MR) is constantly evolving to inspire new patterns of robot\nmanipulation for more advanced Human- Robot Interaction under the 4th\nIndustrial Revolution Paradigm. Consider that Mixed Reality aims to connect\nphysical and digital worlds to provide special immersive experiences, it is\nnecessary to establish the information exchange platform and robot control\nsystems within the developed MR scenarios. In this work, we mainly present\nmultiple effective motion control methods applied on different interactive\nrobotic arms (e.g., UR5, UR5e, myCobot) for the Unity-based development of MR\napplications, including GUI control panel, text input control panel,\nend-effector object dynamic tracking and ROS-Unity digital-twin connection.\n","authors":["Hanxiao Chen"],"pdf_url":"https://arxiv.org/pdf/2401.01644v1.pdf","comment":"The full paper has been accepted by CompAuto 2023 with an online Oral\n  Presentation. (http://www.icca.net/, The 3rd International Conference on\n  Computers and Automation, December 7-9, 2023, Paris France)"},{"id":"http://arxiv.org/abs/2305.18942v2","updated":"2024-01-03T07:12:33Z","published":"2023-05-30T11:16:27Z","title":"Scaling Planning for Automated Driving using Simplistic Synthetic Data","summary":"  We challenge the perceived consensus that the application of deep learning to\nsolve the automated driving planning task necessarily requires huge amounts of\nreal-world data or highly realistic simulation. Focusing on a roundabout\nscenario, we show that this requirement can be relaxed in favour of targeted,\nsimplistic simulated data. A benefit is that such data can be easily generated\nfor critical scenarios that are typically underrepresented in realistic\ndatasets. By applying vanilla behavioural cloning almost exclusively to\nlightweight simulated data, we achieve reliable and comfortable driving in a\nreal-world test vehicle. We leverage an incremental development approach that\nincludes regular in-vehicle testing to identify sim-to-real gaps, targeted data\naugmentation, and training scenario variations. In addition to a detailed\ndescription of the methodology, we share our lessons learned, touching upon\nscenario generation, simulation features, and evaluation metrics.\n","authors":["Martin Stoll","Markus Mazzola","Maxim Dolgov","Jürgen Mathes","Nicolas Möser"],"pdf_url":"https://arxiv.org/pdf/2305.18942v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01545v1","updated":"2024-01-03T05:42:17Z","published":"2024-01-03T05:42:17Z","title":"DDN-SLAM: Real-time Dense Dynamic Neural Implicit SLAM with Joint\n  Semantic Encoding","summary":"  We propose DDN-SLAM, a real-time dense neural implicit semantic SLAM system\ndesigned for dynamic scenes. While existing neural implicit SLAM systems\nperform well in static scenes, they often encounter challenges in real-world\nenvironments with dynamic interferences, leading to ineffective tracking and\nmapping. DDN-SLAM utilizes the priors provided by the deep semantic system,\ncombined with conditional probability fields, for segmentation.By constructing\ndepth-guided static masks and employing joint multi-resolution hashing\nencoding, we ensure fast hole filling and high-quality mapping while mitigating\nthe effects of dynamic information interference. To enhance tracking\nrobustness, we utilize sparse feature points validated with optical flow and\nkeyframes, enabling loop closure detection and global bundle optimization.\nFurthermore, DDN-SLAM supports monocular, stereo, and RGB-D inputs, operating\nrobustly at a frequency of 20-30Hz. Extensive experiments on 6 virtual/real\ndatasets demonstrate that our method outperforms state-of-the-art approaches in\nboth dynamic and static scenes.\n","authors":["Mingrui Li","Jiaming He","Guangan Jiang","Hongyu Wang"],"pdf_url":"https://arxiv.org/pdf/2401.01545v1.pdf","comment":"11pages, 4figures"},{"id":"http://arxiv.org/abs/2312.16839v2","updated":"2024-01-03T03:44:45Z","published":"2023-12-28T05:57:51Z","title":"Similar but Different: A Survey of Ground Segmentation and\n  Traversability Estimation for Terrestrial Robots","summary":"  With the increasing demand for mobile robots and autonomous vehicles, several\napproaches for long-term robot navigation have been proposed. Among these\ntechniques, ground segmentation and traversability estimation play important\nroles in perception and path planning, respectively. Even though these two\ntechniques appear similar, their objectives are different. Ground segmentation\ndivides data into ground and non-ground elements; thus, it is used as a\npreprocessing stage to extract objects of interest by rejecting ground points.\nIn contrast, traversability estimation identifies and comprehends areas in\nwhich robots can move safely. Nevertheless, some researchers use these terms\nwithout clear distinction, leading to misunderstanding the two concepts.\nTherefore, in this study, we survey related literature and clearly distinguish\nground and traversable regions considering four aspects: a) maneuverability of\nrobot platforms, b) position of a robot in the surroundings, c) subset relation\nof negative obstacles, and d) subset relation of deformable objects.\n","authors":["Hyungtae Lim","Minho Oh","Seungjae Lee","Seunguk Ahn","Hyun Myung"],"pdf_url":"https://arxiv.org/pdf/2312.16839v2.pdf","comment":"10 pages, 8 figures"},{"id":"http://arxiv.org/abs/2401.01502v1","updated":"2024-01-03T02:15:32Z","published":"2024-01-03T02:15:32Z","title":"Pontryagin Neural Operator for Solving Parametric General-Sum\n  Differential Games","summary":"  The values of two-player general-sum differential games are viscosity\nsolutions to Hamilton-Jacobi-Isaacs (HJI) equations. Value and policy\napproximations for such games suffer from the curse of dimensionality (CoD).\nAlleviating CoD through physics-informed neural networks (PINN) encounters\nconvergence issues when value discontinuity is present due to state\nconstraints. On top of these challenges, it is often necessary to learn\ngeneralizable values and policies across a parametric space of games, e.g., for\ngame parameter inference when information is incomplete. To address these\nchallenges, we propose in this paper a Pontryagin-mode neural operator that\noutperforms existing state-of-the-art (SOTA) on safety performance across games\nwith parametric state constraints. Our key contribution is the introduction of\na costate loss defined on the discrepancy between forward and backward costate\nrollouts, which are computationally cheap. We show that the discontinuity of\ncostate dynamics (in the presence of state constraints) effectively enables the\nlearning of discontinuous values, without requiring manually supervised data as\nsuggested by the current SOTA. More importantly, we show that the close\nrelationship between costates and policies makes the former critical in\nlearning feedback control policies with generalizable safety performance.\n","authors":["Lei Zhang","Mukesh Ghimire","Zhe Xu","Wenlong Zhang","Yi Ren"],"pdf_url":"https://arxiv.org/pdf/2401.01502v1.pdf","comment":"Submitted to L4DC 2024"},{"id":"http://arxiv.org/abs/2401.01501v1","updated":"2024-01-03T02:14:41Z","published":"2024-01-03T02:14:41Z","title":"Evaluation of automated driving system safety metrics with logged\n  vehicle trajectory data","summary":"  Real-time safety metrics are important for the automated driving system (ADS)\nto assess the risk of driving situations and to assist the decision-making.\nAlthough a number of real-time safety metrics have been proposed in the\nliterature, systematic performance evaluation of these safety metrics has been\nlacking. As different behavioral assumptions are adopted in different safety\nmetrics, it is difficult to compare the safety metrics and evaluate their\nperformance. To overcome this challenge, in this study, we propose an\nevaluation framework utilizing logged vehicle trajectory data, in that vehicle\ntrajectories for both subject vehicle (SV) and background vehicles (BVs) are\nobtained and the prediction errors caused by behavioral assumptions can be\neliminated. Specifically, we examine whether the SV is in a collision\nunavoidable situation at each moment, given all near-future trajectories of\nBVs. In this way, we level the ground for a fair comparison of different safety\nmetrics, as a good safety metric should always alarm in advance to the\ncollision unavoidable moment. When trajectory data from a large number of trips\nare available, we can systematically evaluate and compare different metrics'\nstatistical performance. In the case study, three representative real-time\nsafety metrics, including the time-to-collision (TTC), the PEGASUS Criticality\nMetric (PCM), and the Model Predictive Instantaneous Safety Metric (MPrISM),\nare evaluated using a large-scale simulated trajectory dataset. The proposed\nevaluation framework is important for researchers, practitioners, and\nregulators to characterize different metrics, and to select appropriate metrics\nfor different applications. Moreover, by conducting failure analysis on moments\nwhen a safety metric failed, we can identify its potential weaknesses which are\nvaluable for its potential refinements and improvements.\n","authors":["Xintao Yan","Shuo Feng","David J. LeBlanc","Carol Flannagan","Henry X. Liu"],"pdf_url":"https://arxiv.org/pdf/2401.01501v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01483v1","updated":"2024-01-03T01:15:55Z","published":"2024-01-03T01:15:55Z","title":"To Lead or to Follow? Adaptive Robot Task Planning in Human-Robot\n  Collaboration","summary":"  Adaptive task planning is fundamental to ensuring effective and seamless\nhuman-robot collaboration. This paper introduces a robot task planning\nframework that takes into account both human leading/following preferences and\nperformance, specifically focusing on task allocation and scheduling in\ncollaborative settings. We present a proactive task allocation approach with\nthree primary objectives: enhancing team performance, incorporating human\npreferences, and upholding a positive human perception of the robot and the\ncollaborative experience. Through a user study, involving an autonomous mobile\nmanipulator robot working alongside participants in a collaborative scenario,\nwe confirm that the task planning framework successfully attains all three\nintended goals, thereby contributing to the advancement of adaptive task\nplanning in human-robot collaboration. This paper mainly focuses on the first\ntwo objectives, and we discuss the third objective, participants' perception of\nthe robot, tasks, and collaboration in a companion paper.\n","authors":["Ali Noormohammadi-Asl","Stephen L. Smith","Kerstin Dautenhahn"],"pdf_url":"https://arxiv.org/pdf/2401.01483v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01481v1","updated":"2024-01-03T01:09:56Z","published":"2024-01-03T01:09:56Z","title":"Optimizing UAV-UGV Coalition Operations: A Hybrid Clustering and\n  Multi-Agent Reinforcement Learning Approach for Path Planning in Obstructed\n  Environment","summary":"  One of the most critical applications undertaken by coalitions of Unmanned\nAerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) is reaching\npredefined targets by following the most time-efficient routes while avoiding\ncollisions. Unfortunately, UAVs are hampered by limited battery life, and UGVs\nface challenges in reachability due to obstacles and elevation variations.\nExisting literature primarily focuses on one-to-one coalitions, which\nconstrains the efficiency of reaching targets. In this work, we introduce a\nnovel approach for a UAV-UGV coalition with a variable number of vehicles,\nemploying a modified mean-shift clustering algorithm to segment targets into\nmultiple zones. Each vehicle utilizes Multi-agent Deep Deterministic Policy\nGradient (MADDPG) and Multi-agent Proximal Policy Optimization (MAPPO), two\nadvanced reinforcement learning algorithms, to form an effective coalition for\nnavigating obstructed environments without collisions. This approach of\nassigning targets to various circular zones, based on density and range,\nsignificantly reduces the time required to reach these targets. Moreover,\nintroducing variability in the number of UAVs and UGVs in a coalition enhances\ntask efficiency by enabling simultaneous multi-target engagement. The results\nof our experimental evaluation demonstrate that our proposed method\nsubstantially surpasses current state-of-the-art techniques, nearly doubling\nefficiency in terms of target navigation time and task completion rate.\n","authors":["Shamyo Brotee","Farhan Kabir","Md. Abdur Razzaque","Palash Roy","Md. Mamun-Or-Rashid","Md. Rafiul Hassan","Mohammad Mehedi Hassan"],"pdf_url":"https://arxiv.org/pdf/2401.01481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01474v1","updated":"2024-01-03T00:22:41Z","published":"2024-01-03T00:22:41Z","title":"Demonstrating Mobile Manipulation in the Wild: A Metrics-Driven Approach","summary":"  We present our general-purpose mobile manipulation system consisting of a\ncustom robot platform and key algorithms spanning perception and planning. To\nextensively test the system in the wild and benchmark its performance, we\nchoose a grocery shopping scenario in an actual, unmodified grocery store. We\nderive key performance metrics from detailed robot log data collected during\nsix week-long field tests, spread across 18 months. These objective metrics,\ngained from complex yet repeatable tests, drive the direction of our research\nefforts and let us continuously improve our system's performance. We find that\nthorough end-to-end system-level testing of a complex mobile manipulation\nsystem can serve as a reality-check for state-of-the-art methods in robotics.\nThis effectively grounds robotics research efforts in real world needs and\nchallenges, which we deem highly useful for the advancement of the field. To\nthis end, we share our key insights and takeaways to inspire and accelerate\nsimilar system-level research projects.\n","authors":["Max Bajracharya","James Borders","Richard Cheng","Dan Helmick","Lukas Kaul","Dan Kruse","John Leichty","Jeremy Ma","Carolyn Matl","Frank Michel","Chavdar Papazov","Josh Petersen","Krishna Shankar","Mark Tjersland"],"pdf_url":"https://arxiv.org/pdf/2401.01474v1.pdf","comment":"Presented at RSS 2023 [Best Demo Paper Award]"},{"id":"http://arxiv.org/abs/2312.06365v2","updated":"2024-01-03T00:18:10Z","published":"2023-12-11T13:20:58Z","title":"A Simulation-based Approach to Kinematics Analysis of a Quadruped Robot\n  and Prototype Leg Testing","summary":"  Kinematics analysis is a crucial part of multiple joint-enabled robots. A\nmulti-joint enabled robot requires extensive mathematical calculations to be\ndone so the end effector's position can be determined with respect to the other\nconnective joints involved and their respective frames in a specific coordinate\nsystem. For a locomotive quadruped robot, it is essential to determine two\ntypes of kinematics for the robot's leg position on the coordinate. For the\npart of forward kinematics, it measures the position, and joint angles can be\ncalculated using inverse kinematics. Mathematical derivation of the joint\nangles is derived here, and Python-based simulation has been done to verify and\nsimulate the robot's locomotion. This approach has been tested beneficial over\nother methods as Python-based code is used which makes it easier to do serial\ncommunication and therefore it could be deployed in a micro-controller unit to\ninteract with a prototype leg.\n","authors":["Abid Shahriar"],"pdf_url":"https://arxiv.org/pdf/2312.06365v2.pdf","comment":"19 pages, 10 Figures. In this updated version, several typographical\n  errors have been corrected for improved clarity. The introduction has been\n  expanded to provide a more comprehensive overview of the related work.\n  Additionally, more figures have been included to better illustrate the\n  concepts and results. Also, a better title has been given"},{"id":"http://arxiv.org/abs/2401.01993v1","updated":"2024-01-03T22:05:48Z","published":"2024-01-03T22:05:48Z","title":"On Time-Indexing as Inductive Bias in Deep RL for Sequential\n  Manipulation Tasks","summary":"  While solving complex manipulation tasks, manipulation policies often need to\nlearn a set of diverse skills to accomplish these tasks. The set of skills is\noften quite multimodal - each one may have a quite distinct distribution of\nactions and states. Standard deep policy-learning algorithms often model\npolicies as deep neural networks with a single output head (deterministic or\nstochastic). This structure requires the network to learn to switch between\nmodes internally, which can lead to lower sample efficiency and poor\nperformance. In this paper we explore a simple structure which is conducive to\nskill learning required for so many of the manipulation tasks. Specifically, we\npropose a policy architecture that sequentially executes different action heads\nfor fixed durations, enabling the learning of primitive skills such as reaching\nand grasping. Our empirical evaluation on the Metaworld tasks reveals that this\nsimple structure outperforms standard policy learning methods, highlighting its\npotential for improved skill acquisition.\n","authors":["M. Nomaan Qureshi","Ben Eisner","David Held"],"pdf_url":"https://arxiv.org/pdf/2401.01993v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.20669v2","updated":"2024-01-03T19:37:59Z","published":"2023-10-31T17:32:07Z","title":"Modeling multi-legged robot locomotion with slipping and its\n  experimental validation","summary":"  Multi-legged robots with six or more legs are not in common use, despite\ndesigns with superior stability, maneuverability, and a low number of actuators\nbeing available for over 20 years. This may be in part due to the difficulty in\nmodeling multi-legged motion with slipping and producing reliable predictions\nof body velocity. Here we present a detailed measurement of the foot contact\nforces in a hexapedal robot with multiple sliding contacts, and provide an\nalgorithm for predicting these contact forces and the body velocity. The\nalgorithm relies on the recently published observation that even while\nslipping, multi-legged robots are principally kinematic, and employ a friction\nlaw ansatz that allows us to compute the shape-change to body-velocity\nconnection and the foot contact forces. This results in the ability to simulate\nmotion plans for a large number of potentially slipping legs. In homogeneous\nenvironments, this can run in (parallel) logarithmic time of the planning\nhorizon\n","authors":["Ziyou Wu","Dan Zhao","Shai Revzen"],"pdf_url":"https://arxiv.org/pdf/2310.20669v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2401.01759v1","updated":"2024-01-03T14:24:02Z","published":"2024-01-03T14:24:02Z","title":"VGA: Vision and Graph Fused Attention Network for Rumor Detection","summary":"  With the development of social media, rumors have been spread broadly on\nsocial media platforms, causing great harm to society. Beside textual\ninformation, many rumors also use manipulated images or conceal textual\ninformation within images to deceive people and avoid being detected, making\nmultimodal rumor detection be a critical problem. The majority of multimodal\nrumor detection methods mainly concentrate on extracting features of source\nclaims and their corresponding images, while ignoring the comments of rumors\nand their propagation structures. These comments and structures imply the\nwisdom of crowds and are proved to be crucial to debunk rumors. Moreover, these\nmethods usually only extract visual features in a basic manner, seldom consider\ntampering or textual information in images. Therefore, in this study, we\npropose a novel Vision and Graph Fused Attention Network (VGA) for rumor\ndetection to utilize propagation structures among posts so as to obtain the\ncrowd opinions and further explore visual tampering features, as well as the\ntextual information hidden in images. We conduct extensive experiments on three\ndatasets, demonstrating that VGA can effectively detect multimodal rumors and\noutperform state-of-the-art methods significantly.\n","authors":["Lin Bai","Caiyan Jia","Ziying Song","Chaoqun Cui"],"pdf_url":"https://arxiv.org/pdf/2401.01759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01699v1","updated":"2024-01-03T12:06:02Z","published":"2024-01-03T12:06:02Z","title":"WordArt Designer API: User-Driven Artistic Typography Synthesis with\n  Large Language Models on ModelScope","summary":"  This paper introduces the WordArt Designer API, a novel framework for\nuser-driven artistic typography synthesis utilizing Large Language Models\n(LLMs) on ModelScope. We address the challenge of simplifying artistic\ntypography for non-professionals by offering a dynamic, adaptive, and\ncomputationally efficient alternative to traditional rigid templates. Our\napproach leverages the power of LLMs to understand and interpret user input,\nfacilitating a more intuitive design process. We demonstrate through various\ncase studies how users can articulate their aesthetic preferences and\nfunctional requirements, which the system then translates into unique and\ncreative typographic designs. Our evaluations indicate significant improvements\nin user satisfaction, design flexibility, and creative expression over existing\nsystems. The WordArt Designer API not only democratizes the art of typography\nbut also opens up new possibilities for personalized digital communication and\ndesign.\n","authors":["Jun-Yan He","Zhi-Qi Cheng","Chenyang Li","Jingdong Sun","Wangmeng Xiang","Yusen Hu","Xianhui Lin","Xiaoyang Kang","Zengke Jin","Bin Luo","Yifeng Geng","Xuansong Xie","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2401.01699v1.pdf","comment":"Spotlight Paper from the Workshop on Machine Learning for Creativity\n  and Design at the 37th Conference on Neural Information Processing Systems\n  (NeurIPS 2023). 5 pages, 5 figures"},{"id":"http://arxiv.org/abs/2310.05195v2","updated":"2024-01-03T07:40:15Z","published":"2023-10-08T15:04:50Z","title":"GMMFormer: Gaussian-Mixture-Model Based Transformer for Efficient\n  Partially Relevant Video Retrieval","summary":"  Given a text query, partially relevant video retrieval (PRVR) seeks to find\nuntrimmed videos containing pertinent moments in a database. For PRVR, clip\nmodeling is essential to capture the partial relationship between texts and\nvideos. Current PRVR methods adopt scanning-based clip construction to achieve\nexplicit clip modeling, which is information-redundant and requires a large\nstorage overhead. To solve the efficiency problem of PRVR methods, this paper\nproposes GMMFormer, a Gaussian-Mixture-Model based Transformer which models\nclip representations implicitly. During frame interactions, we incorporate\nGaussian-Mixture-Model constraints to focus each frame on its adjacent frames\ninstead of the whole video. Then generated representations will contain\nmulti-scale clip information, achieving implicit clip modeling. In addition,\nPRVR methods ignore semantic differences between text queries relevant to the\nsame video, leading to a sparse embedding space. We propose a query diverse\nloss to distinguish these text queries, making the embedding space more\nintensive and contain more semantic information. Extensive experiments on three\nlarge-scale video datasets (i.e., TVR, ActivityNet Captions, and Charades-STA)\ndemonstrate the superiority and efficiency of GMMFormer. Code is available at\n\\url{https://github.com/huangmozhi9527/GMMFormer}.\n","authors":["Yuting Wang","Jinpeng Wang","Bin Chen","Ziyun Zeng","Shu-Tao Xia"],"pdf_url":"https://arxiv.org/pdf/2310.05195v2.pdf","comment":"Accepted by AAAI 2024. Code is released at\n  https://github.com/huangmozhi9527/GMMFormer"},{"id":"http://arxiv.org/abs/2305.18891v2","updated":"2024-01-03T06:55:36Z","published":"2023-05-30T09:47:29Z","title":"EmotionGesture: Audio-Driven Diverse Emotional Co-Speech 3D Gesture\n  Generation","summary":"  Generating vivid and diverse 3D co-speech gestures is crucial for various\napplications in animating virtual avatars. While most existing methods can\ngenerate gestures from audio directly, they usually overlook that emotion is\none of the key factors of authentic co-speech gesture generation. In this work,\nwe propose EmotionGesture, a novel framework for synthesizing vivid and diverse\nemotional co-speech 3D gestures from audio. Considering emotion is often\nentangled with the rhythmic beat in speech audio, we first develop an\nEmotion-Beat Mining module (EBM) to extract the emotion and audio beat features\nas well as model their correlation via a transcript-based visual-rhythm\nalignment. Then, we propose an initial pose based Spatial-Temporal Prompter\n(STP) to generate future gestures from the given initial poses. STP effectively\nmodels the spatial-temporal correlations between the initial poses and the\nfuture gestures, thus producing the spatial-temporal coherent pose prompt. Once\nwe obtain pose prompts, emotion, and audio beat features, we will generate 3D\nco-speech gestures through a transformer architecture. However, considering the\nposes of existing datasets often contain jittering effects, this would lead to\ngenerating unstable gestures. To address this issue, we propose an effective\nobjective function, dubbed Motion-Smooth Loss. Specifically, we model motion\noffset to compensate for jittering ground-truth by forcing gestures to be\nsmooth. Last, we present an emotion-conditioned VAE to sample emotion features,\nenabling us to generate diverse emotional results. Extensive experiments\ndemonstrate that our framework outperforms the state-of-the-art, achieving\nvivid and diverse emotional co-speech 3D gestures. Our code and dataset will be\nreleased at the project page:\nhttps://xingqunqi-lab.github.io/Emotion-Gesture-Web/\n","authors":["Xingqun Qi","Chen Liu","Lincheng Li","Jie Hou","Haoran Xin","Xin Yu"],"pdf_url":"https://arxiv.org/pdf/2305.18891v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2401.01470v1","updated":"2024-01-03T00:10:33Z","published":"2024-01-03T00:10:33Z","title":"Token Propagation Controller for Efficient Vision Transformer","summary":"  Vision transformers (ViTs) have achieved promising results on a variety of\nComputer Vision tasks, however their quadratic complexity in the number of\ninput tokens has limited their application specially in resource-constrained\nsettings. Previous approaches that employ gradual token reduction to address\nthis challenge assume that token redundancy in one layer implies redundancy in\nall the following layers. We empirically demonstrate that this assumption is\noften not correct, i.e., tokens that are redundant in one layer can be useful\nin later layers. We employ this key insight to propose a novel token\npropagation controller (TPC) that incorporates two different\ntoken-distributions, i.e., pause probability and restart probability to control\nthe reduction and reuse of tokens respectively, which results in more efficient\ntoken utilization. To improve the estimates of token distributions, we propose\na smoothing mechanism that acts as a regularizer and helps remove noisy\noutliers. Furthermore, to improve the training-stability of our proposed TPC,\nwe introduce a model stabilizer that is able to implicitly encode local image\nstructures and minimize accuracy fluctuations during model training. We present\nextensive experimental results on the ImageNet-1K dataset using DeiT, LV-ViT\nand Swin models to demonstrate the effectiveness of our proposed method. For\nexample, compared to baseline models, our proposed method improves the\ninference speed of the DeiT-S by 250% while increasing the classification\naccuracy by 1.0%.\n","authors":["Wentao Zhu"],"pdf_url":"https://arxiv.org/pdf/2401.01470v1.pdf","comment":"Accepted by WACV 2024"},{"id":"http://arxiv.org/abs/2401.01955v1","updated":"2024-01-03T19:39:23Z","published":"2024-01-03T19:39:23Z","title":"MULTI-CASE: A Transformer-based Ethics-aware Multimodal Investigative\n  Intelligence Framework","summary":"  AI-driven models are increasingly deployed in operational analytics\nsolutions, for instance, in investigative journalism or the intelligence\ncommunity. Current approaches face two primary challenges: ethical and privacy\nconcerns, as well as difficulties in efficiently combining heterogeneous data\nsources for multimodal analytics. To tackle the challenge of multimodal\nanalytics, we present MULTI-CASE, a holistic visual analytics framework\ntailored towards ethics-aware and multimodal intelligence exploration, designed\nin collaboration with domain experts. It leverages an equal joint agency\nbetween human and AI to explore and assess heterogeneous information spaces,\nchecking and balancing automation through Visual Analytics. MULTI-CASE operates\non a fully-integrated data model and features type-specific analysis with\nmultiple linked components, including a combined search, annotated text view,\nand graph-based analysis. Parts of the underlying entity detection are based on\na RoBERTa-based language model, which we tailored towards user requirements\nthrough fine-tuning. An overarching knowledge exploration graph combines all\ninformation streams, provides in-situ explanations, transparent source\nattribution, and facilitates effective exploration. To assess our approach, we\nconducted a comprehensive set of evaluations: We benchmarked the underlying\nlanguage model on relevant NER tasks, achieving state-of-the-art performance.\nThe demonstrator was assessed according to intelligence capability assessments,\nwhile the methodology was evaluated according to ethics design guidelines. As a\ncase study, we present our framework in an investigative journalism setting,\nsupporting war crime investigations. Finally, we conduct a formative user\nevaluation with domain experts in law enforcement. Our evaluations confirm that\nour framework facilitates human agency and steering in security-sensitive\napplications.\n","authors":["Maximilian T. Fischer","Yannick Metz","Lucas Joos","Matthias Miller","Daniel A. Keim"],"pdf_url":"https://arxiv.org/pdf/2401.01955v1.pdf","comment":"6 pages, 3 figures, 1 table"}]},"2024-01-02T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2309.08163v2","updated":"2024-01-02T23:00:41Z","published":"2023-09-15T05:19:39Z","title":"Self-Assessment Tests are Unreliable Measures of LLM Personality","summary":"  As large language models (LLM) evolve in their capabilities, various recent\nstudies have tried to quantify their behavior using psychological tools created\nto study human behavior. One such example is the measurement of \"personality\"\nof LLMs using self-assessment personality tests developed to measure human\npersonality. Yet almost none of these works verify the applicability of these\ntests on LLMs. In this paper, we analyze the reliability of LLM personality\nscores obtained from self-assessment personality tests using two simple\nexperiments. We first introduce the property of prompt sensitivity, where three\nsemantically equivalent prompts representing three intuitive ways of\nadministering self-assessment tests on LLMs are used to measure the personality\nof the same LLM. We find that all three prompts lead to very different\npersonality scores, a difference that is statistically significant for all\ntraits in a large majority of scenarios. We then introduce the property of\noption-order symmetry for personality measurement of LLMs. Since most of the\nself-assessment tests exist in the form of multiple choice question (MCQ)\nquestions, we argue that the scores should also be robust to not just the\nprompt template but also the order in which the options are presented. This\ntest unsurprisingly reveals that the self-assessment test scores are not robust\nto the order of the options. These simple tests, done on ChatGPT and three\nLlama2 models of different sizes, show that self-assessment personality tests\ncreated for humans are unreliable measures of personality in LLMs.\n","authors":["Akshat Gupta","Xiaoyang Song","Gopala Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2309.08163v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.19555v3","updated":"2024-01-02T22:30:00Z","published":"2023-05-31T04:50:29Z","title":"Large Language Models Are Not Strong Abstract Reasoners","summary":"  Large Language Models have shown tremendous performance on a large variety of\nnatural language processing tasks, ranging from text comprehension to common\nsense reasoning. However, the mechanisms responsible for this success remain\nopaque, and it is unclear whether LLMs can achieve human-like cognitive\ncapabilities or whether these models are still fundamentally circumscribed.\nAbstract reasoning is a fundamental task for cognition, consisting of finding\nand applying a general pattern from few data. Evaluating deep neural\narchitectures on this task could give insight into their potential limitations\nregarding reasoning and their broad generalisation abilities, yet this is\ncurrently an under-explored area. In this paper, we introduce a new benchmark\nfor evaluating language models beyond memorization on abstract reasoning tasks.\nWe perform extensive evaluations of state-of-the-art LLMs, showing that they\ncurrently achieve very limited performance in contrast with other natural\nlanguage tasks, even when applying techniques that have been shown to improve\nperformance on other NLP tasks. We argue that guiding LLM generation to follow\ncausal paths could help improve the generalisation and reasoning abilities of\nLLMs.\n","authors":["Gaël Gendron","Qiming Bao","Michael Witbrock","Gillian Dobbie"],"pdf_url":"https://arxiv.org/pdf/2305.19555v3.pdf","comment":"50 pages, 14 pages for the main paper and 36 pages for the\n  supplement, 35 figures, 17 tables. V3: performed additional experiments"},{"id":"http://arxiv.org/abs/2307.05134v2","updated":"2024-01-02T21:18:48Z","published":"2023-07-11T09:23:05Z","title":"TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation","summary":"  The progress in the generation of synthetic images has made it crucial to\nassess their quality. While several metrics have been proposed to assess the\nrendering of images, it is crucial for Text-to-Image (T2I) models, which\ngenerate images based on a prompt, to consider additional aspects such as to\nwhich extent the generated image matches the important content of the prompt.\nMoreover, although the generated images usually result from a random starting\npoint, the influence of this one is generally not considered. In this article,\nwe propose a new metric based on prompt templates to study the alignment\nbetween the content specified in the prompt and the corresponding generated\nimages. It allows us to better characterize the alignment in terms of the type\nof the specified objects, their number, and their color. We conducted a study\non several recent T2I models about various aspects. An additional interesting\nresult we obtained with our approach is that image quality can vary drastically\ndepending on the noise used as a seed for the images. We also quantify the\ninfluence of the number of concepts in the prompt, their order as well as their\n(color) attributes. Finally, our method allows us to identify some seeds that\nproduce better images than others, opening novel directions of research on this\nunderstudied topic.\n","authors":["Paul Grimal","Hervé Le Borgne","Olivier Ferret","Julien Tourille"],"pdf_url":"https://arxiv.org/pdf/2307.05134v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01419v1","updated":"2024-01-02T20:05:56Z","published":"2024-01-02T20:05:56Z","title":"To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine\n  Translation vs Human Translation","summary":"  We conduct a large-scale fine-grained comparative analysis of machine\ntranslations (MT) against human translations (HT) through the lens of\nmorphosyntactic divergence. Across three language pairs and two types of\ndivergence defined as the structural difference between the source and the\ntarget, MT is consistently more conservative than HT, with less morphosyntactic\ndiversity, more convergent patterns, and more one-to-one alignments. Through\nanalysis on different decoding algorithms, we attribute this discrepancy to the\nuse of beam search that biases MT towards more convergent patterns. This bias\nis most amplified when the convergent pattern appears around 50% of the time in\ntraining data. Lastly, we show that for a majority of morphosyntactic\ndivergences, their presence in HT is correlated with decreased MT performance,\npresenting a greater challenge for MT systems.\n","authors":["Jiaming Luo","Colin Cherry","George Foster"],"pdf_url":"https://arxiv.org/pdf/2401.01419v1.pdf","comment":"TACL, pre-MIT Press publication version"},{"id":"http://arxiv.org/abs/2401.01405v1","updated":"2024-01-02T19:00:17Z","published":"2024-01-02T19:00:17Z","title":"Quantifying the Uniqueness of Donald Trump in Presidential Discourse","summary":"  Does Donald Trump speak differently from other presidents? If so, in what\nways? Are these differences confined to any single medium of communication? To\ninvestigate these questions, this paper introduces a novel metric of uniqueness\nbased on large language models, develops a new lexicon for divisive speech, and\npresents a framework for comparing the lexical features of political opponents.\nApplying these tools to a variety of corpora of presidential speeches, we find\nconsiderable evidence that Trump's speech patterns diverge from those of all\nmajor party nominees for the presidency in recent history. Some notable\nfindings include Trump's employment of particularly divisive and antagonistic\nlanguage targeting of his political opponents and his patterns of repetition\nfor emphasis. Furthermore, Trump is significantly more distinctive than his\nfellow Republicans, whose uniqueness values are comparably closer to those of\nthe Democrats. These differences hold across a variety of measurement\nstrategies, arise on both the campaign trail and in official presidential\naddresses, and do not appear to be an artifact of secular time trends.\n","authors":["Karen Zhou","Alexander A. Meitus","Milo Chase","Grace Wang","Anne Mykland","William Howell","Chenhao Tan"],"pdf_url":"https://arxiv.org/pdf/2401.01405v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01335v1","updated":"2024-01-02T18:53:13Z","published":"2024-01-02T18:53:13Z","title":"Self-Play Fine-Tuning Converts Weak Language Models to Strong Language\n  Models","summary":"  Harnessing the power of human-annotated data through Supervised Fine-Tuning\n(SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we\ndelve into the prospect of growing a strong LLM out of a weak one without the\nneed for acquiring additional human-annotated data. We propose a new\nfine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a\nsupervised fine-tuned model. At the heart of SPIN lies a self-play mechanism,\nwhere the LLM refines its capability by playing against instances of itself.\nMore specifically, the LLM generates its own training data from its previous\niterations, refining its policy by discerning these self-generated responses\nfrom those obtained from human-annotated data. Our method progressively\nelevates the LLM from a nascent model to a formidable one, unlocking the full\npotential of human-annotated demonstration data for SFT. Theoretically, we\nprove that the global optimum to the training objective function of our method\nis achieved only when the LLM policy aligns with the target data distribution.\nEmpirically, we evaluate our method on several benchmark datasets including the\nHuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench. Our\nresults show that SPIN can significantly improve the LLM's performance across a\nvariety of benchmarks and even outperform models trained through direct\npreference optimization (DPO) supplemented with extra GPT-4 preference data.\nThis sheds light on the promise of self-play, enabling the achievement of\nhuman-level performance in LLMs without the need for expert opponents.\n","authors":["Zixiang Chen","Yihe Deng","Huizhuo Yuan","Kaixuan Ji","Quanquan Gu"],"pdf_url":"https://arxiv.org/pdf/2401.01335v1.pdf","comment":"28 pages, 6 figures, 6 tables"},{"id":"http://arxiv.org/abs/2401.01330v1","updated":"2024-01-02T18:40:03Z","published":"2024-01-02T18:40:03Z","title":"TREC iKAT 2023: The Interactive Knowledge Assistance Track Overview","summary":"  Conversational Information Seeking stands as a pivotal research area with\nsignificant contributions from previous works. The TREC Interactive Knowledge\nAssistance Track (iKAT) builds on the foundational work of the TREC\nConversational Assistance Track (CAsT). However, iKAT distinctively emphasizes\nthe creation and research of conversational search agents that adapt responses\nbased on user's prior interactions and present context. The challenge lies in\nenabling Conversational Search Agents (CSA) to incorporate this personalized\ncontext to efficiency and effectively guide users through the relevant\ninformation to them. iKAT also emphasizes decisional search tasks, where users\nsift through data and information to weigh up options in order to reach a\nconclusion or perform an action. These tasks, prevalent in everyday\ninformation-seeking decisions -- be it related to travel, health, or shopping\n-- often revolve around a subset of high-level information operators where\nqueries or questions about the information space include: finding options,\ncomparing options, identifying the pros and cons of options, etc. Given the\ndifferent personas and their information need (expressed through the sequence\nof questions), diverse conversation trajectories will arise -- because the\nanswers to these similar queries will be very different. In this paper, we\nreport on the first year of TREC iKAT, describing the task, topics, data\ncollection, and evaluation framework. We further review the submissions and\nsummarize the findings.\n","authors":["Mohammad Aliannejadi","Zahra Abbasiantaeb","Shubham Chatterjee","Jeffery Dalton","Leif Azzopardi"],"pdf_url":"https://arxiv.org/pdf/2401.01330v1.pdf","comment":"TREC 2023 Overview Paper"},{"id":"http://arxiv.org/abs/2401.01326v1","updated":"2024-01-02T18:32:14Z","published":"2024-01-02T18:32:14Z","title":"An Autoregressive Text-to-Graph Framework for Joint Entity and Relation\n  Extraction","summary":"  In this paper, we propose a novel method for joint entity and relation\nextraction from unstructured text by framing it as a conditional sequence\ngeneration problem. In contrast to conventional generative information\nextraction models that are left-to-right token-level generators, our approach\nis \\textit{span-based}. It generates a linearized graph where nodes represent\ntext spans and edges represent relation triplets. Our method employs a\ntransformer encoder-decoder architecture with pointing mechanism on a dynamic\nvocabulary of spans and relation types. Our model can capture the structural\ncharacteristics and boundaries of entities and relations through span\nrepresentations while simultaneously grounding the generated output in the\noriginal text thanks to the pointing mechanism. Evaluation on benchmark\ndatasets validates the effectiveness of our approach, demonstrating competitive\nresults. Code is available at https://github.com/urchade/ATG.\n","authors":["Zaratiana Urchade","Nadi Tomeh","Pierre Holat","Thierry Charnois"],"pdf_url":"https://arxiv.org/pdf/2401.01326v1.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2401.01325v1","updated":"2024-01-02T18:30:51Z","published":"2024-01-02T18:30:51Z","title":"LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning","summary":"  This work elicits LLMs' inherent ability to handle long contexts without\nfine-tuning. The limited length of the training sequence during training may\nlimit the application of Large Language Models (LLMs) on long input sequences\nfor inference. In this work, we argue that existing LLMs themselves have\ninherent capabilities for handling long contexts. Based on this argument, we\nsuggest extending LLMs' context window by themselves to fully utilize the\ninherent ability.We propose Self-Extend to stimulate LLMs' long context\nhandling potential. The basic idea is to construct bi-level attention\ninformation: the group level and the neighbor level. The two levels are\ncomputed by the original model's self-attention, which means the proposed does\nnot require any training. With only four lines of code modification, the\nproposed method can effortlessly extend existing LLMs' context window without\nany fine-tuning. We conduct comprehensive experiments and the results show that\nthe proposed method can effectively extend existing LLMs' context window's\nlength.\n","authors":["Hongye Jin","Xiaotian Han","Jingfeng Yang","Zhimeng Jiang","Zirui Liu","Chia-Yuan Chang","Huiyuan Chen","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2401.01325v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01301v1","updated":"2024-01-02T17:28:06Z","published":"2024-01-02T17:28:06Z","title":"Large Legal Fictions: Profiling Legal Hallucinations in Large Language\n  Models","summary":"  Large language models (LLMs) have the potential to transform the practice of\nlaw, but this potential is threatened by the presence of legal hallucinations\n-- responses from these models that are not consistent with legal facts. We\ninvestigate the extent of these hallucinations using an original suite of legal\nqueries, comparing LLMs' responses to structured legal metadata and examining\ntheir consistency. Our work makes four key contributions: (1) We develop a\ntypology of legal hallucinations, providing a conceptual framework for future\nresearch in this area. (2) We find that legal hallucinations are alarmingly\nprevalent, occurring between 69% of the time with ChatGPT 3.5 and 88% with\nLlama 2, when these models are asked specific, verifiable questions about\nrandom federal court cases. (3) We illustrate that LLMs often fail to correct a\nuser's incorrect legal assumptions in a contra-factual question setup. (4) We\nprovide evidence that LLMs cannot always predict, or do not always know, when\nthey are producing legal hallucinations. Taken together, these findings caution\nagainst the rapid and unsupervised integration of popular LLMs into legal\ntasks. Even experienced lawyers must remain wary of legal hallucinations, and\nthe risks are highest for those who stand to benefit from LLMs the most -- pro\nse litigants or those without access to traditional legal resources.\n","authors":["Matthew Dahl","Varun Magesh","Mirac Suzgun","Daniel E. Ho"],"pdf_url":"https://arxiv.org/pdf/2401.01301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01286v1","updated":"2024-01-02T16:54:58Z","published":"2024-01-02T16:54:58Z","title":"A Comprehensive Study of Knowledge Editing for Large Language Models","summary":"  Large Language Models (LLMs) have shown extraordinary capabilities in\nunderstanding and generating text that closely mirrors human communication.\nHowever, a primary limitation lies in the significant computational demands\nduring training, arising from their extensive parameterization. This challenge\nis further intensified by the dynamic nature of the world, necessitating\nfrequent updates to LLMs to correct outdated information or integrate new\nknowledge, thereby ensuring their continued relevance. Note that many\napplications demand continual model adjustments post-training to address\ndeficiencies or undesirable behaviors. There is an increasing interest in\nefficient, lightweight methods for on-the-fly model modifications. To this end,\nrecent years have seen a burgeoning in the techniques of knowledge editing for\nLLMs, which aim to efficiently modify LLMs' behaviors within specific domains\nwhile preserving overall performance across various inputs. In this paper, we\nfirst define the knowledge editing problem and then provide a comprehensive\nreview of cutting-edge approaches. Drawing inspiration from educational and\ncognitive research theories, we propose a unified categorization criterion that\nclassifies knowledge editing methods into three groups: resorting to external\nknowledge, merging knowledge into the model, and editing intrinsic knowledge.\nFurthermore, we introduce a new benchmark, KnowEdit, for a comprehensive\nempirical evaluation of representative knowledge editing approaches.\nAdditionally, we provide an in-depth analysis of knowledge location, which can\nprovide a deeper understanding of the knowledge structures inherent within\nLLMs. Finally, we discuss several potential applications of knowledge editing,\noutlining its broad and impactful implications.\n","authors":["Ningyu Zhang","Yunzhi Yao","Bozhong Tian","Peng Wang","Shumin Deng","Mengru Wang","Zekun Xi","Shengyu Mao","Jintian Zhang","Yuansheng Ni","Siyuan Cheng","Ziwen Xu","Xin Xu","Jia-Chen Gu","Yong Jiang","Pengjun Xie","Fei Huang","Lei Liang","Zhiqiang Zhang","Xiaowei Zhu","Jun Zhou","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2401.01286v1.pdf","comment":"Ongoing work; 50 pages, 265 citations; benchmark is available at\n  https://huggingface.co/datasets/zjunlp/KnowEdit; code is available at\n  https://github.com/zjunlp/EasyEdit; paper list is available at\n  https://github.com/zjunlp/KnowledgeEditingPapers"},{"id":"http://arxiv.org/abs/2401.01275v1","updated":"2024-01-02T16:20:40Z","published":"2024-01-02T16:20:40Z","title":"CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent\n  Evaluation","summary":"  Recently, the advent of large language models (LLMs) has revolutionized\ngenerative agents. Among them, Role-Playing Conversational Agents (RPCAs)\nattract considerable attention due to their ability to emotionally engage\nusers. However, the absence of a comprehensive benchmark impedes progress in\nthis field. To bridge this gap, we introduce CharacterEval, a Chinese benchmark\nfor comprehensive RPCA assessment, complemented by a tailored high-quality\ndataset. The dataset comprises 1,785 multi-turn role-playing dialogues,\nencompassing 23,020 examples and featuring 77 characters derived from Chinese\nnovels and scripts. It was carefully constructed, beginning with initial\ndialogue extraction via GPT-4, followed by rigorous human-led quality control,\nand enhanced with in-depth character profiles sourced from Baidu Baike.\nCharacterEval employs a multifaceted evaluation approach, encompassing thirteen\ntargeted metrics on four dimensions. Comprehensive experiments on CharacterEval\ndemonstrate that Chinese LLMs exhibit more promising capabilities than GPT-4 in\nChinese role-playing conversation. Source code, data source and reward model\nwill be publicly accessible at https://github.com/morecry/CharacterEval.\n","authors":["Quan Tu","Shilong Fan","Zihang Tian","Rui Yan"],"pdf_url":"https://arxiv.org/pdf/2401.01275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01256v1","updated":"2024-01-02T15:56:48Z","published":"2024-01-02T15:56:48Z","title":"VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM","summary":"  The recent innovations and breakthroughs in diffusion models have\nsignificantly expanded the possibilities of generating high-quality videos for\nthe given prompts. Most existing works tackle the single-scene scenario with\nonly one video event occurring in a single background. Extending to generate\nmulti-scene videos nevertheless is not trivial and necessitates to nicely\nmanage the logic in between while preserving the consistent visual appearance\nof key content across video scenes. In this paper, we propose a novel\nframework, namely VideoDrafter, for content-consistent multi-scene video\ngeneration. Technically, VideoDrafter leverages Large Language Models (LLM) to\nconvert the input prompt into comprehensive multi-scene script that benefits\nfrom the logical knowledge learnt by LLM. The script for each scene includes a\nprompt describing the event, the foreground/background entities, as well as\ncamera movement. VideoDrafter identifies the common entities throughout the\nscript and asks LLM to detail each entity. The resultant entity description is\nthen fed into a text-to-image model to generate a reference image for each\nentity. Finally, VideoDrafter outputs a multi-scene video by generating each\nscene video via a diffusion process that takes the reference images, the\ndescriptive prompt of the event and camera movement into account. The diffusion\nmodel incorporates the reference images as the condition and alignment to\nstrengthen the content consistency of multi-scene videos. Extensive experiments\ndemonstrate that VideoDrafter outperforms the SOTA video generation models in\nterms of visual quality, content consistency, and user preference.\n","authors":["Fuchen Long","Zhaofan Qiu","Ting Yao","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2401.01256v1.pdf","comment":"Project website: https://videodrafter.github.io"},{"id":"http://arxiv.org/abs/2308.06035v3","updated":"2024-01-02T15:33:20Z","published":"2023-08-11T09:30:07Z","title":"Multimodality and Attention Increase Alignment in Natural Language\n  Prediction Between Humans and Computational Models","summary":"  The potential of multimodal generative artificial intelligence (mAI) to\nreplicate human grounded language understanding, including the pragmatic,\ncontext-rich aspects of communication, remains to be clarified. Humans are\nknown to use salient multimodal features, such as visual cues, to facilitate\nthe processing of upcoming words. Correspondingly, multimodal computational\nmodels can integrate visual and linguistic data using a visual attention\nmechanism to assign next-word probabilities. To test whether these processes\nalign, we tasked both human participants (N = 200) as well as several\nstate-of-the-art computational models with evaluating the predictability of\nforthcoming words after viewing short audio-only or audio-visual clips with\nspeech. During the task, the model's attention weights were recorded and human\nattention was indexed via eye tracking. Results show that predictability\nestimates from humans aligned more closely with scores generated from\nmultimodal models vs. their unimodal counterparts. Furthermore, including an\nattention mechanism doubled alignment with human judgments when visual and\nlinguistic context facilitated predictions. In these cases, the model's\nattention patches and human eye tracking significantly overlapped. Our results\nindicate that improved modeling of naturalistic language processing in mAI does\nnot merely depend on training diet but can be driven by multimodality in\ncombination with attention-based architectures. Humans and computational models\nalike can leverage the predictive constraints of multimodal information by\nattending to relevant features in the input.\n","authors":["Viktor Kewenig","Andrew Lampinen","Samuel A. Nastase","Christopher Edwards","Quitterie Lacome DEstalenx","Akilles Rechardt","Jeremy I Skipper","Gabriella Vigliocco"],"pdf_url":"https://arxiv.org/pdf/2308.06035v3.pdf","comment":"20 pages, 4 figures, submitted to Nature Human Behaviour"},{"id":"http://arxiv.org/abs/2305.07490v5","updated":"2024-01-02T15:29:53Z","published":"2023-05-12T14:04:30Z","title":"ArtGPT-4: Towards Artistic-understanding Large Vision-Language Models\n  with Enhanced Adapter","summary":"  In recent years, advancements in large language models have been remarkable,\nwith models such as ChatGPT demonstrating exceptional proficiency in diverse\nlinguistic tasks. The pre-training of large models with billions of parameters,\nposes a formidable challenge, primarily due to the scarcity of datasets of a\ncommensurate scale for effective training. Nevertheless, innovative strategies\nhave emerged, including methods to fine-tune these pre-trained models using\nfewer parameters set, as evidenced by models like MiniGPT-4 and LLaVA. Despite\ntheir potential in various domains, these models remain limited in their\nunderstanding of artistic imagery. They have yet to fully grasp the intricate\nnuances of art images or to provide an objective articulation of the emotions\nthey evoke, in a manner akin to human perception. This work introduces\nArtGPT-4, a pioneering large vision-language model tailored to address the\ndeficiencies of contemporary models in artistic comprehension. ArtGPT-4\nunderwent training on image-text pairs utilizing a Tesla A100 device in a mere\n2 hours, with a dataset comprising approximately 0.52M entries. Impressively,\nthe model can render images with an artistic-understanding and convey the\nemotions they inspire, mirroring human interpretation. Additionally, this work\npresents a unique dataset designed to evaluate the efficacy of vision-language\nmodels. In subsequent evaluations, ArtGPT-4 not only achieved state-of-the-art\nperformance on the ArtEmis and ArtEmis-v2.0 datasets but also exceeded the\nestablished benchmarks introduced in This study, lagging behind professional\nartists' descriptions by a negligible 0.15 points on a 6-point scale. The code\nand the pre-trained model are accessible in\nhttps://huggingface.co/Tyrannosaurus/ArtGPT-4.\n","authors":["Zhengqing Yuan","Xinyi Wang","Kun Wang","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2305.07490v5.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2311.15218v4","updated":"2024-01-02T15:13:44Z","published":"2023-11-26T07:19:10Z","title":"Real-Time Online Stock Forecasting Utilizing Integrated Quantitative and\n  Qualitative Analysis","summary":"  The application of Machine learning to finance has become a familiar\napproach, even more so in stock market forecasting. The stock market is highly\nvolatile, and huge amounts of data are generated every minute globally. The\nextraction of effective intelligence from this data is of critical importance.\nHowever, a collaboration of numerical stock data with qualitative text data can\nbe a challenging task. In this work, we accomplish this by providing an\nunprecedented, publicly available dataset with technical and fundamental data\nand sentiment that we gathered from news archives, TV news captions, radio\ntranscripts, tweets, daily financial newspapers, etc. The text data entries\nused for sentiment extraction total more than 1.4 Million. The dataset consists\nof daily entries from January 2018 to December 2022 for eight companies\nrepresenting diverse industrial sectors and the Dow Jones Industrial Average\n(DJIA) as a whole. Holistic Fundamental and Technical data is provided training\nready for Model learning and deployment. Most importantly, the data generated\ncould be used for incremental online learning with real-time data points\nretrieved daily since no stagnant data was utilized. All the data was retired\nfrom APIs or self-designed robust information retrieval technologies with\nextremely low latency and zero monetary cost. These adaptable technologies\nfacilitate data extraction for any stock. Moreover, the utilization of\nSpearman's rank correlation over real-time data, linking stock returns with\nsentiment analysis has produced noteworthy results for the DJIA and the eight\nother stocks, achieving accuracy levels surpassing 60%. The dataset is made\navailable at https://github.com/batking24/Huge-Stock-Dataset.\n","authors":["Sai Akash Bathini","Dagli Cihan"],"pdf_url":"https://arxiv.org/pdf/2311.15218v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.17296v2","updated":"2024-01-02T14:48:56Z","published":"2023-12-28T16:25:52Z","title":"Structured Packing in LLM Training Improves Long Context Utilization","summary":"  Recent advances in long-context Large Language Models (LCLMs) have generated\nsignificant interest, especially in applications such as querying scientific\nresearch papers. However, their potential is often limited by inadequate\ncontext utilization. We identify the absence of long-range semantic\ndependencies in typical training data as a primary hindrance. To address this,\nwe delve into the benefits of frequently incorporating related documents into\ntraining inputs. Using the inherent directory structure of code data as a\nsource of training examples, we demonstrate improvements in perplexity, even\nfor tasks unrelated to coding. Building on these findings, but with a broader\nfocus, we introduce Structured Packing for Long Context (SPLiCe). SPLiCe is an\ninnovative method for creating training examples by using a retrieval method to\ncollate the most mutually relevant documents into a single training context.\nOur results indicate that \\method{} enhances model performance and can be used\nto train large models to utilize long contexts better. We validate our results\nby training a large $3$B model, showing both perplexity improvements and better\nlong-context performance on downstream tasks.\n","authors":["Konrad Staniszewski","Szymon Tworkowski","Sebastian Jaszczur","Henryk Michalewski","Łukasz Kuciński","Piotr Miłoś"],"pdf_url":"https://arxiv.org/pdf/2312.17296v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.11300v5","updated":"2024-01-02T14:18:02Z","published":"2023-06-20T05:30:59Z","title":"RS5M and GeoRSCLIP: A Large Scale Vision-Language Dataset and A Large\n  Vision-Language Model for Remote Sensing","summary":"  Pre-trained Vision-Language Models (VLMs) utilizing extensive image-text\npaired data have demonstrated unprecedented image-text association\ncapabilities, achieving remarkable results across various downstream tasks. A\ncritical challenge is how to make use of existing large-scale pre-trained VLMs,\nwhich are trained on common objects, to perform the domain-specific transfer\nfor accomplishing domain-related downstream tasks. A critical challenge is how\nto make use of existing large-scale pre-trained VLMs, which are trained on\ncommon objects, to perform the domain-specific transfer for accomplishing\ndomain-related downstream tasks. In this paper, we propose a new framework that\nincludes the Domain pre-trained Vision-Language Model (DVLM), bridging the gap\nbetween the General Vision-Language Model (GVLM) and domain-specific downstream\ntasks. Moreover, we present an image-text paired dataset in the field of remote\nsensing (RS), RS5M, which has 5 million RS images with English descriptions.\nThe dataset is obtained from filtering publicly available image-text paired\ndatasets and captioning label-only RS datasets with pre-trained VLM. These\nconstitute the first large-scale RS image-text paired dataset. Additionally, we\nfine-tuned the CLIP model and tried several Parameter-Efficient Fine-Tuning\nmethods on RS5M to implement the DVLM. Experimental results show that our\nproposed dataset is highly effective for various tasks, and our model GeoRSCLIP\nimproves upon the baseline or previous state-of-the-art model by $3\\%\\sim20\\%$\nin Zero-shot Classification (ZSC), $3\\%\\sim6\\%$ in Remote Sensing Cross-Modal\nText-Image Retrieval (RSCTIR) and $4\\%\\sim5\\%$ in Semantic Localization (SeLo)\ntasks. Dataset and models have been released in:\n\\url{https://github.com/om-ai-lab/RS5M}.\n","authors":["Zilun Zhang","Tiancheng Zhao","Yulong Guo","Jianwei Yin"],"pdf_url":"https://arxiv.org/pdf/2306.11300v5.pdf","comment":"RS5M dataset v5"},{"id":"http://arxiv.org/abs/2401.01218v1","updated":"2024-01-02T14:12:41Z","published":"2024-01-02T14:12:41Z","title":"Zero-Shot Position Debiasing for Large Language Models","summary":"  Fine-tuning has been demonstrated to be an effective method to improve the\ndomain performance of large language models (LLMs). However, LLMs might fit the\ndataset bias and shortcuts for prediction, leading to poor generation\nperformance. Experimental result shows that LLMs are prone to exhibit position\nbias, i.e., leveraging information positioned at the beginning or end, or\nspecific positional cues within the input. Existing works on mitigating\nposition bias require external bias knowledge or annotated non-biased samples,\nwhich is unpractical in reality. In this work, we propose a zero-shot position\ndebiasing (ZOE) framework to mitigate position bias for LLMs. ZOE leverages\nunsupervised responses from pre-trained LLMs for debiasing, thus without any\nexternal knowledge or datasets. To improve the quality of unsupervised\nresponses, we propose a master-slave alignment (MSA) module to prune these\nresponses. Experiments on eight datasets and five tasks show that ZOE\nconsistently outperforms existing methods in mitigating four types of position\nbiases. Besides, ZOE achieves this by sacrificing only a small performance on\nbiased samples, which is simple and effective.\n","authors":["Zhongkun Liu","Zheng Chen","Mengqi Zhang","Zhaochun Ren","Zhumin Chen","Pengjie Ren"],"pdf_url":"https://arxiv.org/pdf/2401.01218v1.pdf","comment":"16 pages, 22 figures"},{"id":"http://arxiv.org/abs/2401.01197v1","updated":"2024-01-02T13:01:50Z","published":"2024-01-02T13:01:50Z","title":"Uncertainty Resolution in Misinformation Detection","summary":"  Misinformation poses a variety of risks, such as undermining public trust and\ndistorting factual discourse. Large Language Models (LLMs) like GPT-4 have been\nshown effective in mitigating misinformation, particularly in handling\nstatements where enough context is provided. However, they struggle to assess\nambiguous or context-deficient statements accurately. This work introduces a\nnew method to resolve uncertainty in such statements. We propose a framework to\ncategorize missing information and publish category labels for the LIAR-New\ndataset, which is adaptable to cross-domain content with missing information.\nWe then leverage this framework to generate effective user queries for missing\ncontext. Compared to baselines, our method improves the rate at which generated\nquestions are answerable by the user by 38 percentage points and classification\nperformance by over 10 percentage points macro F1. Thus, this approach may\nprovide a valuable component for future misinformation mitigation pipelines.\n","authors":["Yury Orlovskiy","Camille Thibault","Anne Imouza","Jean-François Godbout","Reihaneh Rabbany","Kellin Pelrine"],"pdf_url":"https://arxiv.org/pdf/2401.01197v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10741v2","updated":"2024-01-02T12:59:20Z","published":"2023-12-17T15:26:16Z","title":"StyleSinger: Style Transfer for Out-of-Domain Singing Voice Synthesis","summary":"  Style transfer for out-of-domain (OOD) singing voice synthesis (SVS) focuses\non generating high-quality singing voices with unseen styles (such as timbre,\nemotion, pronunciation, and articulation skills) derived from reference singing\nvoice samples. However, the endeavor to model the intricate nuances of singing\nvoice styles is an arduous task, as singing voices possess a remarkable degree\nof expressiveness. Moreover, existing SVS methods encounter a decline in the\nquality of synthesized singing voices in OOD scenarios, as they rest upon the\nassumption that the target vocal attributes are discernible during the training\nphase. To overcome these challenges, we propose StyleSinger, the first singing\nvoice synthesis model for zero-shot style transfer of out-of-domain reference\nsinging voice samples. StyleSinger incorporates two critical approaches for\nenhanced effectiveness: 1) the Residual Style Adaptor (RSA) which employs a\nresidual quantization module to capture diverse style characteristics in\nsinging voices, and 2) the Uncertainty Modeling Layer Normalization (UMLN) to\nperturb the style attributes within the content representation during the\ntraining phase and thus improve the model generalization. Our extensive\nevaluations in zero-shot style transfer undeniably establish that StyleSinger\noutperforms baseline models in both audio quality and similarity to the\nreference singing voice samples. Access to singing voice samples can be found\nat https://stylesinger.github.io/.\n","authors":["Yu Zhang","Rongjie Huang","Ruiqi Li","JinZheng He","Yan Xia","Feiyang Chen","Xinyu Duan","Baoxing Huai","Zhou Zhao"],"pdf_url":"https://arxiv.org/pdf/2312.10741v2.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2401.01183v1","updated":"2024-01-02T12:23:49Z","published":"2024-01-02T12:23:49Z","title":"Unifying Structured Data as Graph for Data-to-Text Pre-Training","summary":"  Data-to-text (D2T) generation aims to transform structured data into natural\nlanguage text. Data-to-text pre-training has proved to be powerful in enhancing\nD2T generation and yields impressive performances. However, previous\npre-training methods either oversimplified structured data into a sequence\nwithout considering input structures or designed training objectives tailored\nfor a specific data structure (e.g., table or knowledge graph). In this paper,\nwe unify different types of structured data (i.e., table, key-value data,\nknowledge graph) into the graph format and cast different data-to-text\ngeneration tasks as graph-to-text generation. To effectively exploit the\nstructural information of the input graph, we propose a structure-enhanced\npre-training method for D2T generation by designing a structure-enhanced\nTransformer. Concretely, we devise a position matrix for the Transformer,\nencoding relative positional information of connected nodes in the input graph.\nIn addition, we propose a new attention matrix to incorporate graph structures\ninto the original Transformer by taking the available explicit connectivity\nstructure into account. Extensive experiments on six benchmark datasets show\nthe effectiveness of our model. Our source codes are available at\nhttps://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/unid2t.\n","authors":["Shujie Li","Liang Li","Ruiying Geng","Min Yang","Binhua Li","Guanghu Yuan","Wanwei He","Shao Yuan","Can Ma","Fei Huang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2401.01183v1.pdf","comment":"Accepted for TACL. Pre-MIT Press publication version"},{"id":"http://arxiv.org/abs/2312.12850v3","updated":"2024-01-02T10:01:28Z","published":"2023-12-20T09:01:01Z","title":"A Stochastic Analysis of the Linguistic Provenance of English Place\n  Names","summary":"  In English place name analysis, meanings are often derived from the\nresemblance of roots in place names to topographical features, proper names\nand/or habitation terms in one of the languages that have had an influence on\nEnglish place names. The problem here is that it is sometimes difficult to\ndetermine the base language to use to interpret the roots. The purpose of this\npaper is to stochastically determine the resemblance between 18799 English\nplace names and 84687 place names from Ireland, Scotland, Wales, Denmark,\nNorway, Sweden, France, Germany, the Netherlands and Ancient Rome. Each English\nplace name is ranked according to the extent to which it resembles place names\nfrom the other countries, and this provides a basis for determining the likely\nlanguage to use to interpret the place name. A number of observations can be\nmade using the ranking provided. In particular, it is found that `Harlington'\nis the most archetypically English place name in the English sample, and `Anna'\nis the least. Furthermore, it is found that the place names in the non-English\ndatasets are most similar to Norwegian place names and least similar to Welsh\nplace names.\n","authors":["Michael Dalvean"],"pdf_url":"https://arxiv.org/pdf/2312.12850v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13631v2","updated":"2024-01-02T09:35:33Z","published":"2023-03-21T08:39:56Z","title":"In-depth analysis of music structure as a text network","summary":"  Music, enchanting and poetic, permeates every corner of human civilization.\nAlthough music is not unfamiliar to people, our understanding of its essence\nremains limited, and there is still no universally accepted scientific\ndescription. This is primarily due to music being regarded as a product of both\nreason and emotion, making it difficult to define. In this article, we focus on\nthe fundamental elements of music and construct an evolutionary network from\nthe perspective of music as a natural language, aligning with the statistical\ncharacteristics of texts. Through this approach, we aim to comprehend the\nstructural differences in music across different periods, enabling a more\nscientific exploration of music. Relying on the advantages of structuralism, we\ncan concentrate on the relationships and order between the physical elements of\nmusic, rather than getting entangled in the blurred boundaries of science and\nphilosophy. The scientific framework we present not only conforms to past\nconclusions in music, but also serves as a bridge that connects music to\nnatural language processing and knowledge graphs.\n","authors":["Ping-Rui Tsai","Yen-Ting Chou","Nathan-Christopher Wang","Hui-Ling Chen","Hong-Yue Huang","Zih-Jia Luo","Tzay-Ming Hong"],"pdf_url":"https://arxiv.org/pdf/2303.13631v2.pdf","comment":"7 pages, 8 figures"},{"id":"http://arxiv.org/abs/2401.01108v1","updated":"2024-01-02T08:58:01Z","published":"2024-01-02T08:58:01Z","title":"Unveiling Comparative Sentiments in Vietnamese Product Reviews: A\n  Sequential Classification Framework","summary":"  Comparative opinion mining is a specialized field of sentiment analysis that\naims to identify and extract sentiments expressed comparatively. To address\nthis task, we propose an approach that consists of solving three sequential\nsub-tasks: (i) identifying comparative sentence, i.e., if a sentence has a\ncomparative meaning, (ii) extracting comparative elements, i.e., what are\ncomparison subjects, objects, aspects, predicates, and (iii) classifying\ncomparison types which contribute to a deeper comprehension of user sentiments\nin Vietnamese product reviews. Our method is ranked fifth at the Vietnamese\nLanguage and Speech Processing (VLSP) 2023 challenge on Comparative Opinion\nMining (ComOM) from Vietnamese Product Reviews.\n","authors":["Ha Le","Bao Tran","Phuong Le","Tan Nguyen","Dac Nguyen","Ngoan Pham","Dang Huynh"],"pdf_url":"https://arxiv.org/pdf/2401.01108v1.pdf","comment":"Accepted manuscript at VLSP 2023"},{"id":"http://arxiv.org/abs/2401.01089v1","updated":"2024-01-02T08:14:48Z","published":"2024-01-02T08:14:48Z","title":"Quokka: An Open-source Large Language Model ChatBot for Material Science","summary":"  This paper presents the development of a specialized chatbot for materials\nscience, leveraging the Llama-2 language model, and continuing pre-training on\nthe expansive research articles in the materials science domain from the S2ORC\ndataset. The methodology involves an initial pretraining phase on over one\nmillion domain-specific papers, followed by an instruction-tuning process to\nrefine the chatbot's capabilities. The chatbot is designed to assist\nresearchers, educators, and students by providing instant, context-aware\nresponses to queries in the field of materials science. We make the four\ntrained checkpoints (7B, 13B, with or without chat ability) freely available to\nthe research community at https://github.com/Xianjun-Yang/Quokka.\n","authors":["Xianjun Yang","Stephen D. Wilson","Linda Petzold"],"pdf_url":"https://arxiv.org/pdf/2401.01089v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2310.10477v4","updated":"2024-01-02T07:51:33Z","published":"2023-10-16T14:59:10Z","title":"Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake\n  Analysis","summary":"  The rapid development of large language models (LLMs) has not only provided\nnumerous opportunities but also presented significant challenges. This becomes\nparticularly evident when LLMs inadvertently generate harmful or toxic content,\neither unintentionally or because of intentional inducement. Existing alignment\nmethods usually direct LLMs toward the favorable outcomes by utilizing\nhuman-annotated, flawless instruction-response pairs. Conversely, this study\nproposes a novel alignment technique based on mistake analysis, which\ndeliberately exposes LLMs to erroneous content to learn the reasons for\nmistakes and how to avoid them. In this case, mistakes are repurposed into\nvaluable data for alignment, effectively helping to avoid the production of\nerroneous responses. Without external models or human annotations, our method\nleverages a model's intrinsic ability to discern undesirable mistakes and\nimproves the safety of its generated responses. Experimental results reveal\nthat our method outperforms existing alignment approaches in enhancing model\nsafety while maintaining the overall utility.\n","authors":["Kai Chen","Chunwei Wang","Kuo Yang","Jianhua Han","Lanqing Hong","Fei Mi","Hang Xu","Zhengying Liu","Wenyong Huang","Zhenguo Li","Dit-Yan Yeung","Lifeng Shang","Xin Jiang","Qun Liu"],"pdf_url":"https://arxiv.org/pdf/2310.10477v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.14587v2","updated":"2024-01-02T07:22:04Z","published":"2023-10-23T05:52:09Z","title":"Large Search Model: Redefining Search Stack in the Era of LLMs","summary":"  Modern search engines are built on a stack of different components, including\nquery understanding, retrieval, multi-stage ranking, and question answering,\namong others. These components are often optimized and deployed independently.\nIn this paper, we introduce a novel conceptual framework called large search\nmodel, which redefines the conventional search stack by unifying search tasks\nwith one large language model (LLM). All tasks are formulated as autoregressive\ntext generation problems, allowing for the customization of tasks through the\nuse of natural language prompts. This proposed framework capitalizes on the\nstrong language understanding and reasoning capabilities of LLMs, offering the\npotential to enhance search result quality while simultaneously simplifying the\nexisting cumbersome search stack. To substantiate the feasibility of this\nframework, we present a series of proof-of-concept experiments and discuss the\npotential challenges associated with implementing this approach within\nreal-world search systems.\n","authors":["Liang Wang","Nan Yang","Xiaolong Huang","Linjun Yang","Rangan Majumder","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2310.14587v2.pdf","comment":"SIGIR Forum, Vol. 57 No. 2 - December 2023"},{"id":"http://arxiv.org/abs/2401.01068v1","updated":"2024-01-02T07:00:24Z","published":"2024-01-02T07:00:24Z","title":"Discovering Significant Topics from Legal Decisions with Selective\n  Inference","summary":"  We propose and evaluate an automated pipeline for discovering significant\ntopics from legal decision texts by passing features synthesized with topic\nmodels through penalised regressions and post-selection significance tests. The\nmethod identifies case topics significantly correlated with outcomes,\ntopic-word distributions which can be manually-interpreted to gain insights\nabout significant topics, and case-topic weights which can be used to identify\nrepresentative cases for each topic. We demonstrate the method on a new dataset\nof domain name disputes and a canonical dataset of European Court of Human\nRights violation cases. Topic models based on latent semantic analysis as well\nas language model embeddings are evaluated. We show that topics derived by the\npipeline are consistent with legal doctrines in both areas and can be useful in\nother related legal analysis tasks.\n","authors":["Jerrold Soh"],"pdf_url":"https://arxiv.org/pdf/2401.01068v1.pdf","comment":"This is an accepted manuscript of work forthcoming in PhilTrans A.\n  Please cite the publisher's version only"},{"id":"http://arxiv.org/abs/2401.01055v1","updated":"2024-01-02T06:29:02Z","published":"2024-01-02T06:29:02Z","title":"LLaMA Beyond English: An Empirical Study on Language Capability Transfer","summary":"  In recent times, substantial advancements have been witnessed in large\nlanguage models (LLMs), exemplified by ChatGPT, showcasing remarkable\nproficiency across a range of complex tasks. However, many mainstream LLMs\n(e.g. LLaMA) are pretrained on English-dominant corpus, which limits their\nperformance in other non-English languages. In this paper, we focus on how to\neffectively transfer the capabilities of language generation and following\ninstructions to a non-English language. To answer this question, we conduct an\nextensive empirical investigation based on LLaMA, accumulating over 1440 GPU\nhours. We analyze the impact of key factors such as vocabulary extension,\nfurther pretraining, and instruction tuning on transfer. To accurately assess\nthe model's level of knowledge, we employ four widely used standardized testing\nbenchmarks: C-Eval, MMLU, AGI-Eval, and GAOKAO-Bench. Furthermore, a\ncomprehensive evaluation of the model's response quality is conducted,\nconsidering aspects such as accuracy, fluency, informativeness, logical\ncoherence, and harmlessness, based on LLM-Eval, a benchmarks consisting\ninstruction tasks from 17 diverse categories. Our evaluation results\ndemonstrate that comparable performance to state-of-the-art transfer models can\nbe achieved with less than 1% of the pretraining data, both in terms of\nknowledge alignment and response quality. Furthermore, the experimental\noutcomes across the thirteen low-resource languages also exhibit similar\ntrends. We anticipate that the conclusions revealed by the experiments will aid\nthe community in developing non-English LLMs.\n","authors":["Jun Zhao","Zhihao Zhang","Qi Zhang","Tao Gui","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2401.01055v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01053v1","updated":"2024-01-02T06:24:13Z","published":"2024-01-02T06:24:13Z","title":"Cheetah: Natural Language Generation for 517 African Languages","summary":"  Low-resource African languages pose unique challenges for natural language\nprocessing (NLP) tasks, including natural language generation (NLG). In this\npaper, we develop Cheetah, a massively multilingual NLG language model for\nAfrican languages. Cheetah supports 517 African languages and language\nvarieties, allowing us to address the scarcity of NLG resources and provide a\nsolution to foster linguistic diversity. We demonstrate the effectiveness of\nCheetah through comprehensive evaluations across seven generation downstream\ntasks. In five of the seven tasks, Cheetah significantly outperforms other\nmodels, showcasing its remarkable performance for generating coherent and\ncontextually appropriate text in a wide range of African languages. We\nadditionally conduct a detailed human evaluation to delve deeper into the\nlinguistic capabilities of Cheetah. The introduction of Cheetah has\nfar-reaching benefits for linguistic diversity. By leveraging pretrained models\nand adapting them to specific languages, our approach facilitates the\ndevelopment of practical NLG applications for African communities. The findings\nof this study contribute to advancing NLP research in low-resource settings,\nenabling greater accessibility and inclusion for African languages in a rapidly\nexpanding digital landscape. We will publicly release our models for research.\n","authors":["Ife Adebara","AbdelRahim Elmadany","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2401.01053v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01044v1","updated":"2024-01-02T05:42:14Z","published":"2024-01-02T05:42:14Z","title":"Auffusion: Leveraging the Power of Diffusion and Large Language Models\n  for Text-to-Audio Generation","summary":"  Recent advancements in diffusion models and large language models (LLMs) have\nsignificantly propelled the field of AIGC. Text-to-Audio (TTA), a burgeoning\nAIGC application designed to generate audio from natural language prompts, is\nattracting increasing attention. However, existing TTA studies often struggle\nwith generation quality and text-audio alignment, especially for complex\ntextual inputs. Drawing inspiration from state-of-the-art Text-to-Image (T2I)\ndiffusion models, we introduce Auffusion, a TTA system adapting T2I model\nframeworks to TTA task, by effectively leveraging their inherent generative\nstrengths and precise cross-modal alignment. Our objective and subjective\nevaluations demonstrate that Auffusion surpasses previous TTA approaches using\nlimited data and computational resource. Furthermore, previous studies in T2I\nrecognizes the significant impact of encoder choice on cross-modal alignment,\nlike fine-grained details and object bindings, while similar evaluation is\nlacking in prior TTA works. Through comprehensive ablation studies and\ninnovative cross-attention map visualizations, we provide insightful\nassessments of text-audio alignment in TTA. Our findings reveal Auffusion's\nsuperior capability in generating audios that accurately match textual\ndescriptions, which further demonstrated in several related tasks, such as\naudio style transfer, inpainting and other manipulations. Our implementation\nand demos are available at https://auffusion.github.io.\n","authors":["Jinlong Xue","Yayue Deng","Yingming Gao","Ya Li"],"pdf_url":"https://arxiv.org/pdf/2401.01044v1.pdf","comment":"Demo and implementation at https://auffusion.github.io"},{"id":"http://arxiv.org/abs/2312.04021v3","updated":"2024-01-02T05:10:27Z","published":"2023-12-07T03:37:39Z","title":"A Study on the Calibration of In-context Learning","summary":"  Accurate uncertainty quantification is crucial for the safe deployment of\nlanguage models (LMs), and prior research has demonstrated improvements in the\ncalibration of modern LMs. Our study focuses on in-context learning (ICL), a\nprevalent method for adapting static LMs through tailored prompts, and examines\nthe balance between performance and calibration across a broad spectrum of\nnatural language understanding and reasoning tasks. Through comprehensive\nexperiments, we observe that, with an increasing number of ICL examples, models\ninitially exhibit increased miscalibration before achieving better calibration\nand miscalibration tends to arise in low-shot settings. Moreover, we find that\nmethods aimed at improving usability, such as fine-tuning and chain-of-thought\n(CoT) prompting, can lead to miscalibration and unreliable natural language\nexplanations, suggesting that new methods may be required for scenarios where\nmodels are expected to be reliable.\n","authors":["Hanlin Zhang","Yi-Fan Zhang","Yaodong Yu","Dhruv Madeka","Dean Foster","Eric Xing","Himabindu Lakkaraju","Sham Kakade"],"pdf_url":"https://arxiv.org/pdf/2312.04021v3.pdf","comment":"Spotlight Talk at NeurIPS 2023 Workshop on Failure Modes in the Age\n  of Foundation Models"},{"id":"http://arxiv.org/abs/2310.09520v4","updated":"2024-01-02T00:04:13Z","published":"2023-10-14T07:19:47Z","title":"Reward-Augmented Decoding: Efficient Controlled Text Generation With a\n  Unidirectional Reward Model","summary":"  While large language models have proven effective in a huge range of\ndownstream applications, they often generate text that is problematic or lacks\na desired attribute. In this paper, we introduce Reward-Augmented Decoding\n(RAD), a text generation procedure that uses a small unidirectional reward\nmodel to encourage a language model to generate text that has certain\nproperties. Specifically, RAD uses the reward model to score generations as\nthey are produced and rescales sampling probabilities to favor high-reward\ntokens. By using a unidirectional reward model, RAD can cache activations from\nprior generation steps to decrease computational overhead. Through experiments\non generating non-toxic and sentiment-controlled text, we demonstrate that RAD\nperforms best among methods that change only the generation procedure and\nmatches the performance of state-of-the-art methods that involve re-training\nthe language model. We further validate that RAD is effective on very large\nlanguage models while incurring a minimal computational overhead.\n","authors":["Haikang Deng","Colin Raffel"],"pdf_url":"https://arxiv.org/pdf/2310.09520v4.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2308.15752v2","updated":"2024-01-02T23:39:10Z","published":"2023-08-30T04:29:48Z","title":"Large-scale data extraction from the UNOS organ donor documents","summary":"  The scope of our study is all UNOS data of the USA organ donors since 2008.\nThe data is not analyzable in a large scale in the past because it was captured\nin PDF documents known as \"Attachments\", whereby every donor is represented by\ndozens of PDF documents in heterogenous formats. To make the data analyzable,\none needs to convert the content inside these PDFs to an analyzable data\nformat, such as a standard SQL database. In this paper we will focus on 2022\nUNOS data comprised of $\\approx 400,000$ PDF documents spanning millions of\npages. The totality of UNOS data covers 15 years (2008--20022) and our results\nwill be quickly extended to the entire data. Our method captures a portion of\nthe data in DCD flowsheets, kidney perfusion data, and data captured during\npatient hospital stay (e.g. vital signs, ventilator settings, etc.). The\ncurrent paper assumes that the reader is familiar with the content of the UNOS\ndata. The overview of the types of data and challenges they present is a\nsubject of another paper. Here we focus on demonstrating that the goal of\nbuilding a comprehensive, analyzable database from UNOS documents is an\nattainable task, and we provide an overview of our methodology. The project\nresulted in datasets by far larger than previously available even in this\npreliminary phase.\n","authors":["Marek Rychlik","Bekir Tanriover","Yan Han"],"pdf_url":"https://arxiv.org/pdf/2308.15752v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01461v1","updated":"2024-01-02T23:28:20Z","published":"2024-01-02T23:28:20Z","title":"Efficient Hybrid Zoom using Camera Fusion on Mobile Phones","summary":"  DSLR cameras can achieve multiple zoom levels via shifting lens distances or\nswapping lens types. However, these techniques are not possible on smartphone\ndevices due to space constraints. Most smartphone manufacturers adopt a hybrid\nzoom system: commonly a Wide (W) camera at a low zoom level and a Telephoto (T)\ncamera at a high zoom level. To simulate zoom levels between W and T, these\nsystems crop and digitally upsample images from W, leading to significant\ndetail loss. In this paper, we propose an efficient system for hybrid zoom\nsuper-resolution on mobile devices, which captures a synchronous pair of W and\nT shots and leverages machine learning models to align and transfer details\nfrom T to W. We further develop an adaptive blending method that accounts for\ndepth-of-field mismatches, scene occlusion, flow uncertainty, and alignment\nerrors. To minimize the domain gap, we design a dual-phone camera rig to\ncapture real-world inputs and ground-truths for supervised training. Our method\ngenerates a 12-megapixel image in 500ms on a mobile platform and compares\nfavorably against state-of-the-art methods under extensive evaluation on\nreal-world scenarios.\n","authors":["Xiaotong Wu","Wei-Sheng Lai","YiChang Shih","Charles Herrmann","Michael Krainin","Deqing Sun","Chia-Kai Liang"],"pdf_url":"https://arxiv.org/pdf/2401.01461v1.pdf","comment":"Accepted to SIGGRAPH Asia 2023 (ACM TOG). Project website:\n  https://www.wslai.net/publications/fusion_zoom"},{"id":"http://arxiv.org/abs/2401.01456v1","updated":"2024-01-02T22:46:12Z","published":"2024-01-02T22:46:12Z","title":"ColorizeDiffusion: Adjustable Sketch Colorization with Reference Image\n  and Text","summary":"  Recently, diffusion models have demonstrated their effectiveness in\ngenerating extremely high-quality images and have found wide-ranging\napplications, including automatic sketch colorization. However, most existing\nmodels use text to guide the conditional generation, with fewer attempts\nexploring the potential advantages of using image tokens as conditional inputs\nfor networks. As such, this paper exhaustively investigates image-guided\nmodels, specifically targeting reference-based sketch colorization, which aims\nto colorize sketch images using reference color images. We investigate three\ncritical aspects of reference-based diffusion models: the shortcomings compared\nto text-based counterparts, the training strategies, and the capability in\nzero-shot, sequential text-based manipulation. We introduce two variations of\nan image-guided latent diffusion model using different image tokens from the\npre-trained CLIP image encoder, and we propose corresponding manipulation\nmethods to adjust their results sequentially using weighted text inputs. We\nconduct comprehensive evaluations of our models through qualitative and\nquantitative experiments, as well as a user study.\n","authors":["Dingkun Yan","Liang Yuan","Yuma Nishioka","Issei Fujishiro","Suguru Saito"],"pdf_url":"https://arxiv.org/pdf/2401.01456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01454v1","updated":"2024-01-02T22:35:33Z","published":"2024-01-02T22:35:33Z","title":"A Survey on Autonomous Driving Datasets: Data Statistic, Annotation, and\n  Outlook","summary":"  Autonomous driving has rapidly developed and shown promising performance with\nrecent advances in hardware and deep learning methods. High-quality datasets\nare fundamental for developing reliable autonomous driving algorithms. Previous\ndataset surveys tried to review the datasets but either focused on a limited\nnumber or lacked detailed investigation of the characters of datasets. To this\nend, we present an exhaustive study of over 200 autonomous driving datasets\nfrom multiple perspectives, including sensor modalities, data size, tasks, and\ncontextual conditions. We introduce a novel metric to evaluate the impact of\neach dataset, which can also be a guide for establishing new datasets. We\nfurther analyze the annotation process and quality of datasets. Additionally,\nwe conduct an in-depth analysis of the data distribution of several vital\ndatasets. Finally, we discuss the development trend of the future autonomous\ndriving datasets.\n","authors":["Mingyu Liu","Ekim Yurtsever","Xingcheng Zhou","Jonathan Fossaert","Yuning Cui","Bare Luka Zagar","Alois C. Knoll"],"pdf_url":"https://arxiv.org/pdf/2401.01454v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01448v1","updated":"2024-01-02T22:15:20Z","published":"2024-01-02T22:15:20Z","title":"ProbMCL: Simple Probabilistic Contrastive Learning for Multi-label\n  Visual Classification","summary":"  Multi-label image classification presents a challenging task in many domains,\nincluding computer vision and medical imaging. Recent advancements have\nintroduced graph-based and transformer-based methods to improve performance and\ncapture label dependencies. However, these methods often include complex\nmodules that entail heavy computation and lack interpretability. In this paper,\nwe propose Probabilistic Multi-label Contrastive Learning (ProbMCL), a novel\nframework to address these challenges in multi-label image classification\ntasks. Our simple yet effective approach employs supervised contrastive\nlearning, in which samples that share enough labels with an anchor image based\non a decision threshold are introduced as a positive set. This structure\ncaptures label dependencies by pulling positive pair embeddings together and\npushing away negative samples that fall below the threshold. We enhance\nrepresentation learning by incorporating a mixture density network into\ncontrastive learning and generating Gaussian mixture distributions to explore\nthe epistemic uncertainty of the feature encoder. We validate the effectiveness\nof our framework through experimentation with datasets from the computer vision\nand medical imaging domains. Our method outperforms the existing\nstate-of-the-art methods while achieving a low computational footprint on both\ndatasets. Visualization analyses also demonstrate that ProbMCL-learned\nclassifiers maintain a meaningful semantic topology.\n","authors":["Ahmad Sajedi","Samir Khaki","Yuri A. Lawryshyn","Konstantinos N. Plataniotis"],"pdf_url":"https://arxiv.org/pdf/2401.01448v1.pdf","comment":"This paper has been accepted for the IEEE International Conference on\n  Acoustics, Speech and Signal Processing (ICASSP) 2024"},{"id":"http://arxiv.org/abs/2401.01445v1","updated":"2024-01-02T22:07:44Z","published":"2024-01-02T22:07:44Z","title":"Indoor Obstacle Discovery on Reflective Ground via Monocular Camera","summary":"  Visual obstacle discovery is a key step towards autonomous navigation of\nindoor mobile robots. Successful solutions have many applications in multiple\nscenes. One of the exceptions is the reflective ground. In this case, the\nreflections on the floor resemble the true world, which confuses the obstacle\ndiscovery and leaves navigation unsuccessful. We argue that the key to this\nproblem lies in obtaining discriminative features for reflections and\nobstacles. Note that obstacle and reflection can be separated by the ground\nplane in 3D space. With this observation, we firstly introduce a\npre-calibration based ground detection scheme that uses robot motion to predict\nthe ground plane. Due to the immunity of robot motion to reflection, this\nscheme avoids failed ground detection caused by reflection. Given the detected\nground, we design a ground-pixel parallax to describe the location of a pixel\nrelative to the ground. Based on this, a unified appearance-geometry feature\nrepresentation is proposed to describe objects inside rectangular boxes.\nEventually, based on segmenting by detection framework, an appearance-geometry\nfusion regressor is designed to utilize the proposed feature to discover the\nobstacles. It also prevents our model from concentrating too much on parts of\nobstacles instead of whole obstacles. For evaluation, we introduce a new\ndataset for Obstacle on Reflective Ground (ORG), which comprises 15 scenes with\nvarious ground reflections, a total of more than 200 image sequences and 3400\nRGB images. The pixel-wise annotations of ground and obstacle provide a\ncomparison to our method and other methods. By reducing the misdetection of the\nreflection, the proposed approach outperforms others. The source code and the\ndataset will be available at\nhttps://github.com/XuefengBUPT/IndoorObstacleDiscovery-RG.\n","authors":["Feng Xue","Yicong Chang","Tianxi Wang","Yu Zhou","Anlong Ming"],"pdf_url":"https://arxiv.org/pdf/2401.01445v1.pdf","comment":"International Journal of Computer Vision (IJCV) 2023. Project Page:\n  https://xuefeng-cvr.github.io/IODRG"},{"id":"http://arxiv.org/abs/2303.06876v3","updated":"2024-01-02T21:47:08Z","published":"2023-03-13T05:51:35Z","title":"A Test Statistic Estimation-based Approach for Establishing\n  Self-interpretable CNN-based Binary Classifiers","summary":"  Interpretability is highly desired for deep neural network-based classifiers,\nespecially when addressing high-stake decisions in medical imaging. Commonly\nused post-hoc interpretability methods have the limitation that they can\nproduce plausible but different interpretations of a given model, leading to\nambiguity about which one to choose. To address this problem, a novel\ndecision-theory-inspired approach is investigated to establish a\nself-interpretable model, given a pre-trained deep binary black-box medical\nimage classifier. This approach involves utilizing a self-interpretable\nencoder-decoder model in conjunction with a single-layer fully connected\nnetwork with unity weights. The model is trained to estimate the test statistic\nof the given trained black-box deep binary classifier to maintain a similar\naccuracy. The decoder output image, referred to as an equivalency map, is an\nimage that represents a transformed version of the to-be-classified image that,\nwhen processed by the fixed fully connected layer, produces the same test\nstatistic value as the original classifier. The equivalency map provides a\nvisualization of the transformed image features that directly contribute to the\ntest statistic value and, moreover, permits quantification of their relative\ncontributions. Unlike the traditional post-hoc interpretability methods, the\nproposed method is self-interpretable, quantitative. Detailed quantitative and\nqualitative analyses have been performed with three different medical image\nbinary classification tasks.\n","authors":["Sourya Sengupta","Mark A. Anastasio"],"pdf_url":"https://arxiv.org/pdf/2303.06876v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01439v1","updated":"2024-01-02T21:27:43Z","published":"2024-01-02T21:27:43Z","title":"Off-Road LiDAR Intensity Based Semantic Segmentation","summary":"  LiDAR is used in autonomous driving to provide 3D spatial information and\nenable accurate perception in off-road environments, aiding in obstacle\ndetection, mapping, and path planning. Learning-based LiDAR semantic\nsegmentation utilizes machine learning techniques to automatically classify\nobjects and regions in LiDAR point clouds. Learning-based models struggle in\noff-road environments due to the presence of diverse objects with varying\ncolors, textures, and undefined boundaries, which can lead to difficulties in\naccurately classifying and segmenting objects using traditional geometric-based\nfeatures. In this paper, we address this problem by harnessing the LiDAR\nintensity parameter to enhance object segmentation in off-road environments.\nOur approach was evaluated in the RELLIS-3D data set and yielded promising\nresults as a preliminary analysis with improved mIoU for classes \"puddle\" and\n\"grass\" compared to more complex deep learning-based benchmarks. The\nmethodology was evaluated for compatibility across both Velodyne and Ouster\nLiDAR systems, assuring its cross-platform applicability. This analysis\nadvocates for the incorporation of calibrated intensity as a supplementary\ninput, aiming to enhance the prediction accuracy of learning based semantic\nsegmentation frameworks.\nhttps://github.com/MOONLABIISERB/lidar-intensity-predictor/tree/main\n","authors":["Kasi Viswanath","Peng Jiang","Sujit PB","Srikanth Saripalli"],"pdf_url":"https://arxiv.org/pdf/2401.01439v1.pdf","comment":"Accepted to ISER 2023"},{"id":"http://arxiv.org/abs/2307.05134v2","updated":"2024-01-02T21:18:48Z","published":"2023-07-11T09:23:05Z","title":"TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation","summary":"  The progress in the generation of synthetic images has made it crucial to\nassess their quality. While several metrics have been proposed to assess the\nrendering of images, it is crucial for Text-to-Image (T2I) models, which\ngenerate images based on a prompt, to consider additional aspects such as to\nwhich extent the generated image matches the important content of the prompt.\nMoreover, although the generated images usually result from a random starting\npoint, the influence of this one is generally not considered. In this article,\nwe propose a new metric based on prompt templates to study the alignment\nbetween the content specified in the prompt and the corresponding generated\nimages. It allows us to better characterize the alignment in terms of the type\nof the specified objects, their number, and their color. We conducted a study\non several recent T2I models about various aspects. An additional interesting\nresult we obtained with our approach is that image quality can vary drastically\ndepending on the noise used as a seed for the images. We also quantify the\ninfluence of the number of concepts in the prompt, their order as well as their\n(color) attributes. Finally, our method allows us to identify some seeds that\nproduce better images than others, opening novel directions of research on this\nunderstudied topic.\n","authors":["Paul Grimal","Hervé Le Borgne","Olivier Ferret","Julien Tourille"],"pdf_url":"https://arxiv.org/pdf/2307.05134v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.09073v3","updated":"2024-01-02T20:53:03Z","published":"2023-05-16T00:03:09Z","title":"Consensus and Subjectivity of Skin Tone Annotation for ML Fairness","summary":"  Understanding different human attributes and how they affect model behavior\nmay become a standard need for all model creation and usage, from traditional\ncomputer vision tasks to the newest multimodal generative AI systems. In\ncomputer vision specifically, we have relied on datasets augmented with\nperceived attribute signals (e.g., gender presentation, skin tone, and age) and\nbenchmarks enabled by these datasets. Typically labels for these tasks come\nfrom human annotators. However, annotating attribute signals, especially skin\ntone, is a difficult and subjective task. Perceived skin tone is affected by\ntechnical factors, like lighting conditions, and social factors that shape an\nannotator's lived experience. This paper examines the subjectivity of skin tone\nannotation through a series of annotation experiments using the Monk Skin Tone\n(MST) scale, a small pool of professional photographers, and a much larger pool\nof trained crowdsourced annotators. Along with this study we release the Monk\nSkin Tone Examples (MST-E) dataset, containing 1515 images and 31 videos spread\nacross the full MST scale. MST-E is designed to help train human annotators to\nannotate MST effectively. Our study shows that annotators can reliably annotate\nskin tone in a way that aligns with an expert in the MST scale, even under\nchallenging environmental conditions. We also find evidence that annotators\nfrom different geographic regions rely on different mental models of MST\ncategories resulting in annotations that systematically vary across regions.\nGiven this, we advise practitioners to use a diverse set of annotators and a\nhigher replication count for each image when annotating skin tone for fairness\nresearch.\n","authors":["Candice Schumann","Gbolahan O. Olanubi","Auriel Wright","Ellis Monk Jr.","Courtney Heldreth","Susanna Ricco"],"pdf_url":"https://arxiv.org/pdf/2305.09073v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01425v1","updated":"2024-01-02T20:28:06Z","published":"2024-01-02T20:28:06Z","title":"SwapTransformer: highway overtaking tactical planner model via imitation\n  learning on OSHA dataset","summary":"  This paper investigates the high-level decision-making problem in highway\nscenarios regarding lane changing and over-taking other slower vehicles. In\nparticular, this paper aims to improve the Travel Assist feature for automatic\novertaking and lane changes on highways. About 9 million samples including lane\nimages and other dynamic objects are collected in simulation. This data;\nOvertaking on Simulated HighwAys (OSHA) dataset is released to tackle this\nchallenge. To solve this problem, an architecture called SwapTransformer is\ndesigned and implemented as an imitation learning approach on the OSHA dataset.\nMoreover, auxiliary tasks such as future points and car distance network\npredictions are proposed to aid the model in better understanding the\nsurrounding environment. The performance of the proposed solution is compared\nwith a multi-layer perceptron (MLP) and multi-head self-attention networks as\nbaselines in a simulation environment. We also demonstrate the performance of\nthe model with and without auxiliary tasks. All models are evaluated based on\ndifferent metrics such as time to finish each lap, number of overtakes, and\nspeed difference with speed limit. The evaluation shows that the\nSwapTransformer model outperforms other models in different traffic densities\nin the inference phase.\n","authors":["Alireza Shamsoshoara","Safin B Salih","Pedram Aghazadeh"],"pdf_url":"https://arxiv.org/pdf/2401.01425v1.pdf","comment":"19 pages, 12 Figures, 1 Algorithm, 2 Tables"},{"id":"http://arxiv.org/abs/2312.11195v2","updated":"2024-01-02T19:58:09Z","published":"2023-12-18T13:41:21Z","title":"Cross-Age Contrastive Learning for Age-Invariant Face Recognition","summary":"  Cross-age facial images are typically challenging and expensive to collect,\nmaking noise-free age-oriented datasets relatively small compared to\nwidely-used large-scale facial datasets. Additionally, in real scenarios,\nimages of the same subject at different ages are usually hard or even\nimpossible to obtain. Both of these factors lead to a lack of supervised data,\nwhich limits the versatility of supervised methods for age-invariant face\nrecognition, a critical task in applications such as security and biometrics.\nTo address this issue, we propose a novel semi-supervised learning approach\nnamed Cross-Age Contrastive Learning (CACon). Thanks to the identity-preserving\npower of recent face synthesis models, CACon introduces a new contrastive\nlearning method that leverages an additional synthesized sample from the input\nimage. We also propose a new loss function in association with CACon to perform\ncontrastive learning on a triplet of samples. We demonstrate that our method\nnot only achieves state-of-the-art performance in homogeneous-dataset\nexperiments on several age-invariant face recognition benchmarks but also\noutperforms other methods by a large margin in cross-dataset experiments.\n","authors":["Haoyi Wang","Victor Sanchez","Chang-Tsun Li"],"pdf_url":"https://arxiv.org/pdf/2312.11195v2.pdf","comment":"ICASSP 2024"},{"id":"http://arxiv.org/abs/2401.01339v1","updated":"2024-01-02T18:59:55Z","published":"2024-01-02T18:59:55Z","title":"Street Gaussians for Modeling Dynamic Urban Scenes","summary":"  This paper aims to tackle the problem of modeling dynamic urban street scenes\nfrom monocular videos. Recent methods extend NeRF by incorporating tracked\nvehicle poses to animate vehicles, enabling photo-realistic view synthesis of\ndynamic urban street scenes. However, significant limitations are their slow\ntraining and rendering speed, coupled with the critical need for high precision\nin tracked vehicle poses. We introduce Street Gaussians, a new explicit scene\nrepresentation that tackles all these limitations. Specifically, the dynamic\nurban street is represented as a set of point clouds equipped with semantic\nlogits and 3D Gaussians, each associated with either a foreground vehicle or\nthe background. To model the dynamics of foreground object vehicles, each\nobject point cloud is optimized with optimizable tracked poses, along with a\ndynamic spherical harmonics model for the dynamic appearance. The explicit\nrepresentation allows easy composition of object vehicles and background, which\nin turn allows for scene editing operations and rendering at 133 FPS\n(1066$\\times$1600 resolution) within half an hour of training. The proposed\nmethod is evaluated on multiple challenging benchmarks, including KITTI and\nWaymo Open datasets. Experiments show that the proposed method consistently\noutperforms state-of-the-art methods across all datasets. Furthermore, the\nproposed representation delivers performance on par with that achieved using\nprecise ground-truth poses, despite relying only on poses from an off-the-shelf\ntracker. The code is available at https://zju3dv.github.io/street_gaussians/.\n","authors":["Yunzhi Yan","Haotong Lin","Chenxu Zhou","Weijie Wang","Haiyang Sun","Kun Zhan","Xianpeng Lang","Xiaowei Zhou","Sida Peng"],"pdf_url":"https://arxiv.org/pdf/2401.01339v1.pdf","comment":"Project page: https://zju3dv.github.io/street_gaussians/"},{"id":"http://arxiv.org/abs/2401.01702v1","updated":"2024-01-02T18:59:35Z","published":"2024-01-02T18:59:35Z","title":"Image Sculpting: Precise Object Editing with 3D Geometry Control","summary":"  We present Image Sculpting, a new framework for editing 2D images by\nincorporating tools from 3D geometry and graphics. This approach differs\nmarkedly from existing methods, which are confined to 2D spaces and typically\nrely on textual instructions, leading to ambiguity and limited control. Image\nSculpting converts 2D objects into 3D, enabling direct interaction with their\n3D geometry. Post-editing, these objects are re-rendered into 2D, merging into\nthe original image to produce high-fidelity results through a coarse-to-fine\nenhancement process. The framework supports precise, quantifiable, and\nphysically-plausible editing options such as pose editing, rotation,\ntranslation, 3D composition, carving, and serial addition. It marks an initial\nstep towards combining the creative freedom of generative models with the\nprecision of graphics pipelines.\n","authors":["Jiraphon Yenphraphai","Xichen Pan","Sainan Liu","Daniele Panozzo","Saining Xie"],"pdf_url":"https://arxiv.org/pdf/2401.01702v1.pdf","comment":"Code and project page: https://image-sculpting.github.io"},{"id":"http://arxiv.org/abs/2401.01395v1","updated":"2024-01-02T18:03:57Z","published":"2024-01-02T18:03:57Z","title":"Deep autoregressive modeling for land use land cover","summary":"  Land use / land cover (LULC) modeling is a challenging task due to long-range\ndependencies between geographic features and distinct spatial patterns related\nto topography, ecology, and human development. We identify a close connection\nbetween modeling of spatial patterns of land use and the task of image\ninpainting from computer vision and conduct a study of a modified PixelCNN\narchitecture with approximately 19 million parameters for modeling LULC. In\ncomparison with a benchmark spatial statistical model, we find that the former\nis capable of capturing much richer spatial correlation patterns such as roads\nand water bodies but does not produce a calibrated predictive distribution,\nsuggesting the need for additional tuning. We find evidence of predictive\nunderdispersion with regard to important ecologically-relevant land use\nstatistics such as patch count and adjacency which can be ameliorated to some\nextent by manipulating sampling variability.\n","authors":["Christopher Krapu","Mark Borsuk","Ryan Calder"],"pdf_url":"https://arxiv.org/pdf/2401.01395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01303v1","updated":"2024-01-02T17:30:45Z","published":"2024-01-02T17:30:45Z","title":"Integrating Edges into U-Net Models with Explainable Activation Maps for\n  Brain Tumor Segmentation using MR Images","summary":"  Manual delineation of tumor regions from magnetic resonance (MR) images is\ntime-consuming, requires an expert, and is prone to human error. In recent\nyears, deep learning models have been the go-to approach for the segmentation\nof brain tumors. U-Net and its' variants for semantic segmentation of medical\nimages have achieved good results in the literature. However, U-Net and its'\nvariants tend to over-segment tumor regions and may not accurately segment the\ntumor edges. The edges of the tumor are as important as the tumor regions for\naccurate diagnosis, surgical precision, and treatment planning. In the proposed\nwork, the authors aim to extract edges from the ground truth using a\nderivative-like filter followed by edge reconstruction to obtain an edge ground\ntruth in addition to the brain tumor ground truth. Utilizing both ground\ntruths, the author studies several U-Net and its' variant architectures with\nand without tumor edges ground truth as a target along with the tumor ground\ntruth for brain tumor segmentation. The author used the BraTS2020 benchmark\ndataset to perform the study and the results are tabulated for the dice and\nHausdorff95 metrics. The mean and median metrics are calculated for the whole\ntumor (WT), tumor core (TC), and enhancing tumor (ET) regions. Compared to the\nbaseline U-Net and its variants, the models that learned edges along with the\ntumor regions performed well in core tumor regions in both training and\nvalidation datasets. The improved performance of edge-trained models trained on\nbaseline models like U-Net and V-Net achieved performance similar to baseline\nstate-of-the-art models like Swin U-Net and hybrid MR-U-Net. The edge-target\ntrained models are capable of generating edge maps that can be useful for\ntreatment planning. Additionally, for further explainability of the results,\nthe activation map generated by the hybrid MR-U-Net has been studied.\n","authors":["Subin Sahayam","Umarani Jayaraman"],"pdf_url":"https://arxiv.org/pdf/2401.01303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.17050v2","updated":"2024-01-02T17:24:53Z","published":"2023-12-28T14:51:45Z","title":"KeDuSR: Real-World Dual-Lens Super-Resolution via Kernel-Free Matching","summary":"  Dual-lens super-resolution (SR) is a practical scenario for reference (Ref)\nbased SR by utilizing the telephoto image (Ref) to assist the super-resolution\nof the low-resolution wide-angle image (LR input). Different from general\nRefSR, the Ref in dual-lens SR only covers the overlapped field of view (FoV)\narea. However, current dual-lens SR methods rarely utilize these specific\ncharacteristics and directly perform dense matching between the LR input and\nRef. Due to the resolution gap between LR and Ref, the matching may miss the\nbest-matched candidate and destroy the consistent structures in the overlapped\nFoV area. Different from them, we propose to first align the Ref with the\ncenter region (namely the overlapped FoV area) of the LR input by combining\nglobal warping and local warping to make the aligned Ref be sharp and\nconsistent. Then, we formulate the aligned Ref and LR center as value-key\npairs, and the corner region of the LR is formulated as queries. In this way,\nwe propose a kernel-free matching strategy by matching between the LR-corner\n(query) and LR-center (key) regions, and the corresponding aligned Ref (value)\ncan be warped to the corner region of the target. Our kernel-free matching\nstrategy avoids the resolution gap between LR and Ref, which makes our network\nhave better generalization ability. In addition, we construct a DuSR-Real\ndataset with (LR, Ref, HR) triples, where the LR and HR are well aligned.\nExperiments on three datasets demonstrate that our method outperforms the\nsecond-best method by a large margin. Our code and dataset are available at\nhttps://github.com/ZifanCui/KeDuSR.\n","authors":["Huanjing Yue","Zifan Cui","Kun Li","Jingyu Yang"],"pdf_url":"https://arxiv.org/pdf/2312.17050v2.pdf","comment":"14 pages, 10 figures. Accepted by AAAI-2024"},{"id":"http://arxiv.org/abs/2401.01288v1","updated":"2024-01-02T16:56:13Z","published":"2024-01-02T16:56:13Z","title":"Physics-informed Generalizable Wireless Channel Modeling with\n  Segmentation and Deep Learning: Fundamentals, Methodologies, and Challenges","summary":"  Channel modeling is fundamental in advancing wireless systems and has thus\nattracted considerable research focus. Recent trends have seen a growing\nreliance on data-driven techniques to facilitate the modeling process and yield\naccurate channel predictions. In this work, we first provide a concise overview\nof data-driven channel modeling methods, highlighting their limitations.\nSubsequently, we introduce the concept and advantages of physics-informed\nneural network (PINN)-based modeling and a summary of recent contributions in\nthis area. Our findings demonstrate that PINN-based approaches in channel\nmodeling exhibit promising attributes such as generalizability,\ninterpretability, and robustness. We offer a comprehensive architecture for\nPINN methodology, designed to inform and inspire future model development. A\ncase-study of our recent work on precise indoor channel prediction with\nsemantic segmentation and deep learning is presented. The study concludes by\naddressing the challenges faced and suggesting potential research directions in\nthis field.\n","authors":["Ethan Zhu","Haijian Sun","Mingyue Ji"],"pdf_url":"https://arxiv.org/pdf/2401.01288v1.pdf","comment":"Submitted to IEEE Magazine for potential future publications"},{"id":"http://arxiv.org/abs/2401.01286v1","updated":"2024-01-02T16:54:58Z","published":"2024-01-02T16:54:58Z","title":"A Comprehensive Study of Knowledge Editing for Large Language Models","summary":"  Large Language Models (LLMs) have shown extraordinary capabilities in\nunderstanding and generating text that closely mirrors human communication.\nHowever, a primary limitation lies in the significant computational demands\nduring training, arising from their extensive parameterization. This challenge\nis further intensified by the dynamic nature of the world, necessitating\nfrequent updates to LLMs to correct outdated information or integrate new\nknowledge, thereby ensuring their continued relevance. Note that many\napplications demand continual model adjustments post-training to address\ndeficiencies or undesirable behaviors. There is an increasing interest in\nefficient, lightweight methods for on-the-fly model modifications. To this end,\nrecent years have seen a burgeoning in the techniques of knowledge editing for\nLLMs, which aim to efficiently modify LLMs' behaviors within specific domains\nwhile preserving overall performance across various inputs. In this paper, we\nfirst define the knowledge editing problem and then provide a comprehensive\nreview of cutting-edge approaches. Drawing inspiration from educational and\ncognitive research theories, we propose a unified categorization criterion that\nclassifies knowledge editing methods into three groups: resorting to external\nknowledge, merging knowledge into the model, and editing intrinsic knowledge.\nFurthermore, we introduce a new benchmark, KnowEdit, for a comprehensive\nempirical evaluation of representative knowledge editing approaches.\nAdditionally, we provide an in-depth analysis of knowledge location, which can\nprovide a deeper understanding of the knowledge structures inherent within\nLLMs. Finally, we discuss several potential applications of knowledge editing,\noutlining its broad and impactful implications.\n","authors":["Ningyu Zhang","Yunzhi Yao","Bozhong Tian","Peng Wang","Shumin Deng","Mengru Wang","Zekun Xi","Shengyu Mao","Jintian Zhang","Yuansheng Ni","Siyuan Cheng","Ziwen Xu","Xin Xu","Jia-Chen Gu","Yong Jiang","Pengjun Xie","Fei Huang","Lei Liang","Zhiqiang Zhang","Xiaowei Zhu","Jun Zhou","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2401.01286v1.pdf","comment":"Ongoing work; 50 pages, 265 citations; benchmark is available at\n  https://huggingface.co/datasets/zjunlp/KnowEdit; code is available at\n  https://github.com/zjunlp/EasyEdit; paper list is available at\n  https://github.com/zjunlp/KnowledgeEditingPapers"},{"id":"http://arxiv.org/abs/2401.01272v1","updated":"2024-01-02T16:17:43Z","published":"2024-01-02T16:17:43Z","title":"MOC-RVQ: Multilevel Codebook-assisted Digital Generative Semantic\n  Communication","summary":"  Vector quantization-based image semantic communication systems have\nsuccessfully boosted transmission efficiency, but face a challenge with\nconflicting requirements between codebook design and digital constellation\nmodulation. Traditional codebooks need a wide index range, while modulation\nfavors few discrete states. To address this, we propose a multilevel generative\nsemantic communication system with a two-stage training framework. In the first\nstage, we train a high-quality codebook, using a multi-head octonary codebook\n(MOC) to compress the index range. We also integrate a residual vector\nquantization (RVQ) mechanism for effective multilevel communication. In the\nsecond stage, a noise reduction block (NRB) based on Swin Transformer is\nintroduced, coupled with the multilevel codebook from the first stage, serving\nas a high-quality semantic knowledge base (SKB) for generative feature\nrestoration. Experimental results highlight MOC-RVQ's superior performance over\nmethods like BPG or JPEG, even without channel error correction coding.\n","authors":["Yingbin Zhou","Yaping Sun","Guanying Chen","Xiaodong Xu","Hao Chen","Binhong Huang","Shuguang Cui","Ping Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.01272v1.pdf","comment":"6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2104.02206v8","updated":"2024-01-02T16:12:32Z","published":"2021-04-06T00:53:01Z","title":"Tuned Compositional Feature Replays for Efficient Stream Learning","summary":"  Our brains extract durable, generalizable knowledge from transient\nexperiences of the world. Artificial neural networks come nowhere close to this\nability. When tasked with learning to classify objects by training on\nnon-repeating video frames in temporal order (online stream learning), models\nthat learn well from shuffled datasets catastrophically forget old knowledge\nupon learning new stimuli. We propose a new continual learning algorithm,\nCompositional Replay Using Memory Blocks (CRUMB), which mitigates forgetting by\nreplaying feature maps reconstructed by combining generic parts. CRUMB\nconcatenates trainable and re-usable \"memory block\" vectors to compositionally\nreconstruct feature map tensors in convolutional neural networks. Storing the\nindices of memory blocks used to reconstruct new stimuli enables memories of\nthe stimuli to be replayed during later tasks. This reconstruction mechanism\nalso primes the neural network to minimize catastrophic forgetting by biasing\nit towards attending to information about object shapes more than information\nabout image textures, and stabilizes the network during stream learning by\nproviding a shared feature-level basis for all training examples. These\nproperties allow CRUMB to outperform an otherwise identical algorithm that\nstores and replays raw images, while occupying only 3.6% as much memory. We\nstress-tested CRUMB alongside 13 competing methods on 7 challenging datasets.\nTo address the limited number of existing online stream learning datasets, we\nintroduce 2 new benchmarks by adapting existing datasets for stream learning.\nWith only 3.7-4.1% as much memory and 15-43% as much runtime, CRUMB mitigates\ncatastrophic forgetting more effectively than the state-of-the-art. Our code is\navailable at https://github.com/MorganBDT/crumb.git.\n","authors":["Morgan B. Talbot","Rushikesh Zawar","Rohil Badkundri","Mengmi Zhang","Gabriel Kreiman"],"pdf_url":"https://arxiv.org/pdf/2104.02206v8.pdf","comment":"Copyright 2023 IEEE. The journal version of this article is hosted at\n  https://ieeexplore.ieee.org/document/10373937 and\n  https://klab.tch.harvard.edu/publications/PDFs/gk8019.pdf"},{"id":"http://arxiv.org/abs/2401.01256v1","updated":"2024-01-02T15:56:48Z","published":"2024-01-02T15:56:48Z","title":"VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM","summary":"  The recent innovations and breakthroughs in diffusion models have\nsignificantly expanded the possibilities of generating high-quality videos for\nthe given prompts. Most existing works tackle the single-scene scenario with\nonly one video event occurring in a single background. Extending to generate\nmulti-scene videos nevertheless is not trivial and necessitates to nicely\nmanage the logic in between while preserving the consistent visual appearance\nof key content across video scenes. In this paper, we propose a novel\nframework, namely VideoDrafter, for content-consistent multi-scene video\ngeneration. Technically, VideoDrafter leverages Large Language Models (LLM) to\nconvert the input prompt into comprehensive multi-scene script that benefits\nfrom the logical knowledge learnt by LLM. The script for each scene includes a\nprompt describing the event, the foreground/background entities, as well as\ncamera movement. VideoDrafter identifies the common entities throughout the\nscript and asks LLM to detail each entity. The resultant entity description is\nthen fed into a text-to-image model to generate a reference image for each\nentity. Finally, VideoDrafter outputs a multi-scene video by generating each\nscene video via a diffusion process that takes the reference images, the\ndescriptive prompt of the event and camera movement into account. The diffusion\nmodel incorporates the reference images as the condition and alignment to\nstrengthen the content consistency of multi-scene videos. Extensive experiments\ndemonstrate that VideoDrafter outperforms the SOTA video generation models in\nterms of visual quality, content consistency, and user preference.\n","authors":["Fuchen Long","Zhaofan Qiu","Ting Yao","Tao Mei"],"pdf_url":"https://arxiv.org/pdf/2401.01256v1.pdf","comment":"Project website: https://videodrafter.github.io"},{"id":"http://arxiv.org/abs/2203.01923v6","updated":"2024-01-02T15:38:20Z","published":"2022-03-03T18:56:08Z","title":"Recovering 3D Human Mesh from Monocular Images: A Survey","summary":"  Estimating human pose and shape from monocular images is a long-standing\nproblem in computer vision. Since the release of statistical body models, 3D\nhuman mesh recovery has been drawing broader attention. With the same goal of\nobtaining well-aligned and physically plausible mesh results, two paradigms\nhave been developed to overcome challenges in the 2D-to-3D lifting process: i)\nan optimization-based paradigm, where different data terms and regularization\nterms are exploited as optimization objectives; and ii) a regression-based\nparadigm, where deep learning techniques are embraced to solve the problem in\nan end-to-end fashion. Meanwhile, continuous efforts are devoted to improving\nthe quality of 3D mesh labels for a wide range of datasets. Though remarkable\nprogress has been achieved in the past decade, the task is still challenging\ndue to flexible body motions, diverse appearances, complex environments, and\ninsufficient in-the-wild annotations. To the best of our knowledge, this is the\nfirst survey that focuses on the task of monocular 3D human mesh recovery. We\nstart with the introduction of body models and then elaborate recovery\nframeworks and training objectives by providing in-depth analyses of their\nstrengths and weaknesses. We also summarize datasets, evaluation metrics, and\nbenchmark results. Open issues and future directions are discussed in the end,\nhoping to motivate researchers and facilitate their research in this area. A\nregularly updated project page can be found at\nhttps://github.com/tinatiansjz/hmr-survey.\n","authors":["Yating Tian","Hongwen Zhang","Yebin Liu","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2203.01923v6.pdf","comment":"Published in IEEE TPAMI, Survey on monocular 3D human mesh recovery,\n  Project page: https://github.com/tinatiansjz/hmr-survey"},{"id":"http://arxiv.org/abs/2305.07490v5","updated":"2024-01-02T15:29:53Z","published":"2023-05-12T14:04:30Z","title":"ArtGPT-4: Towards Artistic-understanding Large Vision-Language Models\n  with Enhanced Adapter","summary":"  In recent years, advancements in large language models have been remarkable,\nwith models such as ChatGPT demonstrating exceptional proficiency in diverse\nlinguistic tasks. The pre-training of large models with billions of parameters,\nposes a formidable challenge, primarily due to the scarcity of datasets of a\ncommensurate scale for effective training. Nevertheless, innovative strategies\nhave emerged, including methods to fine-tune these pre-trained models using\nfewer parameters set, as evidenced by models like MiniGPT-4 and LLaVA. Despite\ntheir potential in various domains, these models remain limited in their\nunderstanding of artistic imagery. They have yet to fully grasp the intricate\nnuances of art images or to provide an objective articulation of the emotions\nthey evoke, in a manner akin to human perception. This work introduces\nArtGPT-4, a pioneering large vision-language model tailored to address the\ndeficiencies of contemporary models in artistic comprehension. ArtGPT-4\nunderwent training on image-text pairs utilizing a Tesla A100 device in a mere\n2 hours, with a dataset comprising approximately 0.52M entries. Impressively,\nthe model can render images with an artistic-understanding and convey the\nemotions they inspire, mirroring human interpretation. Additionally, this work\npresents a unique dataset designed to evaluate the efficacy of vision-language\nmodels. In subsequent evaluations, ArtGPT-4 not only achieved state-of-the-art\nperformance on the ArtEmis and ArtEmis-v2.0 datasets but also exceeded the\nestablished benchmarks introduced in This study, lagging behind professional\nartists' descriptions by a negligible 0.15 points on a 6-point scale. The code\nand the pre-trained model are accessible in\nhttps://huggingface.co/Tyrannosaurus/ArtGPT-4.\n","authors":["Zhengqing Yuan","Xinyi Wang","Kun Wang","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2305.07490v5.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2401.01247v1","updated":"2024-01-02T15:23:09Z","published":"2024-01-02T15:23:09Z","title":"Deep Learning-Based Computational Model for Disease Identification in\n  Cocoa Pods (Theobroma cacao L.)","summary":"  The early identification of diseases in cocoa pods is an important task to\nguarantee the production of high-quality cocoa. The use of artificial\nintelligence techniques such as machine learning, computer vision and deep\nlearning are promising solutions to help identify and classify diseases in\ncocoa pods. In this paper we introduce the development and evaluation of a deep\nlearning computational model applied to the identification of diseases in cocoa\npods, focusing on \"monilia\" and \"black pod\" diseases. An exhaustive review of\nstate-of-the-art of computational models was carried out, based on scientific\narticles related to the identification of plant diseases using computer vision\nand deep learning techniques. As a result of the search, EfficientDet-Lite4, an\nefficient and lightweight model for object detection, was selected. A dataset,\nincluding images of both healthy and diseased cocoa pods, has been utilized to\ntrain the model to detect and pinpoint disease manifestations with considerable\naccuracy. Significant enhancements in the model training and evaluation\ndemonstrate the capability of recognizing and classifying diseases through\nimage analysis. Furthermore, the functionalities of the model were integrated\ninto an Android native mobile with an user-friendly interface, allowing to\nyounger or inexperienced farmers a fast and accuracy identification of health\nstatus of cocoa pods\n","authors":["Darlyn Buenaño Vera","Byron Oviedo","Washington Chiriboga Casanova","Cristian Zambrano-Vega"],"pdf_url":"https://arxiv.org/pdf/2401.01247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01244v1","updated":"2024-01-02T15:20:50Z","published":"2024-01-02T15:20:50Z","title":"Temporal Adaptive RGBT Tracking with Modality Prompt","summary":"  RGBT tracking has been widely used in various fields such as robotics,\nsurveillance processing, and autonomous driving. Existing RGBT trackers fully\nexplore the spatial information between the template and the search region and\nlocate the target based on the appearance matching results. However, these RGBT\ntrackers have very limited exploitation of temporal information, either\nignoring temporal information or exploiting it through online sampling and\ntraining. The former struggles to cope with the object state changes, while the\nlatter neglects the correlation between spatial and temporal information. To\nalleviate these limitations, we propose a novel Temporal Adaptive RGBT Tracking\nframework, named as TATrack. TATrack has a spatio-temporal two-stream structure\nand captures temporal information by an online updated template, where the\ntwo-stream structure refers to the multi-modal feature extraction and\ncross-modal interaction for the initial template and the online update template\nrespectively. TATrack contributes to comprehensively exploit spatio-temporal\ninformation and multi-modal information for target localization. In addition,\nwe design a spatio-temporal interaction (STI) mechanism that bridges two\nbranches and enables cross-modal interaction to span longer time scales.\nExtensive experiments on three popular RGBT tracking benchmarks show that our\nmethod achieves state-of-the-art performance, while running at real-time speed.\n","authors":["Hongyu Wang","Xiaotao Liu","Yifan Li","Meng Sun","Dian Yuan","Jing Liu"],"pdf_url":"https://arxiv.org/pdf/2401.01244v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10144v2","updated":"2024-01-02T15:16:36Z","published":"2023-12-15T19:00:07Z","title":"Data-Efficient Multimodal Fusion on a Single GPU","summary":"  The goal of multimodal alignment is to learn a single latent space that is\nshared between multimodal inputs. The most powerful models in this space have\nbeen trained using massive datasets of paired inputs and large-scale\ncomputational resources, making them prohibitively expensive to train in many\npractical scenarios. We surmise that existing unimodal encoders pre-trained on\nlarge amounts of unimodal data should provide an effective bootstrap to create\nmultimodal models from unimodal ones at much lower costs. We therefore propose\nFuseMix, a multimodal augmentation scheme that operates on the latent spaces of\narbitrary pre-trained unimodal encoders. Using FuseMix for multimodal\nalignment, we achieve competitive performance -- and in certain cases\noutperform state-of-the art methods -- in both image-text and audio-text\nretrieval, with orders of magnitude less compute and data: for example, we\noutperform CLIP on the Flickr30K text-to-image retrieval task with $\\sim \\!\n600\\times$ fewer GPU days and $\\sim \\! 80\\times$ fewer image-text pairs.\nAdditionally, we show how our method can be applied to convert pre-trained\ntext-to-image generative models into audio-to-image ones. Code is available at:\nhttps://github.com/layer6ai-labs/fusemix.\n","authors":["Noël Vouitsis","Zhaoyan Liu","Satya Krishna Gorti","Valentin Villecroze","Jesse C. Cresswell","Guangwei Yu","Gabriel Loaiza-Ganem","Maksims Volkovs"],"pdf_url":"https://arxiv.org/pdf/2312.10144v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.06950v8","updated":"2024-01-02T14:49:22Z","published":"2022-09-14T21:53:27Z","title":"Lossy Image Compression with Conditional Diffusion Models","summary":"  This paper outlines an end-to-end optimized lossy image compression framework\nusing diffusion generative models. The approach relies on the transform coding\nparadigm, where an image is mapped into a latent space for entropy coding and,\nfrom there, mapped back to the data space for reconstruction. In contrast to\nVAE-based neural compression, where the (mean) decoder is a deterministic\nneural network, our decoder is a conditional diffusion model. Our approach thus\nintroduces an additional ``content'' latent variable on which the reverse\ndiffusion process is conditioned and uses this variable to store information\nabout the image. The remaining ``texture'' variables characterizing the\ndiffusion process are synthesized at decoding time. We show that the model's\nperformance can be tuned toward perceptual metrics of interest. Our extensive\nexperiments involving multiple datasets and image quality assessment metrics\nshow that our approach yields stronger reported FID scores than the GAN-based\nmodel, while also yielding competitive performance with VAE-based models in\nseveral distortion metrics. Furthermore, training the diffusion with\n$\\mathcal{X}$-parameterization enables high-quality reconstructions in only a\nhandful of decoding steps, greatly affecting the model's practicality. Our code\nis available at: \\url{https://github.com/buggyyang/CDC_compression}\n","authors":["Ruihan Yang","Stephan Mandt"],"pdf_url":"https://arxiv.org/pdf/2209.06950v8.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01227v1","updated":"2024-01-02T14:36:28Z","published":"2024-01-02T14:36:28Z","title":"IdentiFace : A VGG Based Multimodal Facial Biometric System","summary":"  The development of facial biometric systems has contributed greatly to the\ndevelopment of the computer vision field. Nowadays, there's always a need to\ndevelop a multimodal system that combines multiple biometric traits in an\nefficient, meaningful way. In this paper, we introduce \"IdentiFace\" which is a\nmultimodal facial biometric system that combines the core of facial recognition\nwith some of the most important soft biometric traits such as gender, face\nshape, and emotion. We also focused on developing the system using only VGG-16\ninspired architecture with minor changes across different subsystems. This\nunification allows for simpler integration across modalities. It makes it\neasier to interpret the learned features between the tasks which gives a good\nindication about the decision-making process across the facial modalities and\npotential connection. For the recognition problem, we acquired a 99.2% test\naccuracy for five classes with high intra-class variations using data collected\nfrom the FERET database[1]. We achieved 99.4% on our dataset and 95.15% on the\npublic dataset[2] in the gender recognition problem. We were also able to\nachieve a testing accuracy of 88.03% in the face-shape problem using the\ncelebrity face-shape dataset[3]. Finally, we achieved a decent testing accuracy\nof 66.13% in the emotion task which is considered a very acceptable accuracy\ncompared to related work on the FER2013 dataset[4].\n","authors":["Mahmoud Rabea","Hanya Ahmed","Sohaila Mahmoud","Nourhan Sayed"],"pdf_url":"https://arxiv.org/pdf/2401.01227v1.pdf","comment":"12 pages, 22 figures and 9 images"},{"id":"http://arxiv.org/abs/2304.08965v5","updated":"2024-01-02T14:32:16Z","published":"2023-04-18T12:58:21Z","title":"PointDC:Unsupervised Semantic Segmentation of 3D Point Clouds via\n  Cross-modal Distillation and Super-Voxel Clustering","summary":"  Semantic segmentation of point clouds usually requires exhausting efforts of\nhuman annotations, hence it attracts wide attention to the challenging topic of\nlearning from unlabeled or weaker forms of annotations. In this paper, we take\nthe first attempt for fully unsupervised semantic segmentation of point clouds,\nwhich aims to delineate semantically meaningful objects without any form of\nannotations. Previous works of unsupervised pipeline on 2D images fails in this\ntask of point clouds, due to: 1) Clustering Ambiguity caused by limited\nmagnitude of data and imbalanced class distribution; 2) Irregularity Ambiguity\ncaused by the irregular sparsity of point cloud. Therefore, we propose a novel\nframework, PointDC, which is comprised of two steps that handle the\naforementioned problems respectively: Cross-Modal Distillation (CMD) and\nSuper-Voxel Clustering (SVC). In the first stage of CMD, multi-view visual\nfeatures are back-projected to the 3D space and aggregated to a unified point\nfeature to distill the training of the point representation. In the second\nstage of SVC, the point features are aggregated to super-voxels and then fed to\nthe iterative clustering process for excavating semantic classes. PointDC\nyields a significant improvement over the prior state-of-the-art unsupervised\nmethods, on both the ScanNet-v2 (+18.4 mIoU) and S3DIS (+11.5 mIoU) semantic\nsegmentation benchmarks.\n","authors":["Zisheng Chen","Hongbin Xu","Weitao Chen","Zhipeng Zhou","Haihong Xiao","Baigui Sun","Xuansong Xie","Wenxiong Kang"],"pdf_url":"https://arxiv.org/pdf/2304.08965v5.pdf","comment":"Accepted by International Conference on Computer Vision (ICCV) 2023"},{"id":"http://arxiv.org/abs/2309.10399v3","updated":"2024-01-02T14:24:42Z","published":"2023-09-19T08:00:26Z","title":"Exploiting Causality Signals in Medical Images: A Pilot Study with\n  Empirical Results","summary":"  We present a novel technique to discover and exploit weak causal signals\ndirectly from images via neural networks for classification purposes. This way,\nwe model how the presence of a feature in one part of the image affects the\nappearance of another feature in a different part of the image. Our method\nconsists of a convolutional neural network backbone and a causality-factors\nextractor module, which computes weights to enhance each feature map according\nto its causal influence in the scene. We develop different architecture\nvariants and empirically evaluate all the models on two public datasets of\nprostate MRI images and breast histopathology slides for cancer diagnosis. We\nstudy the effectiveness of our module both in fully-supervised and few-shot\nlearning, we assess its addition to existing attention-based solutions, we\nconduct ablation studies, and investigate the explainability of our models via\nclass activation maps. Our findings show that our lightweight block extracts\nmeaningful information and improves the overall classification, together with\nproducing more robust predictions that focus on relevant parts of the image.\nThat is crucial in medical imaging, where accurate and reliable classifications\nare essential for effective diagnosis and treatment planning.\n","authors":["Gianluca Carloni","Sara Colantonio"],"pdf_url":"https://arxiv.org/pdf/2309.10399v3.pdf","comment":"Added experiments in which we integrate our Mulcat module to existing\n  models using Bottleneck Attention Modules, and added experiments in Few-Shot\n  Learning; 19 pages"},{"id":"http://arxiv.org/abs/2306.11300v5","updated":"2024-01-02T14:18:02Z","published":"2023-06-20T05:30:59Z","title":"RS5M and GeoRSCLIP: A Large Scale Vision-Language Dataset and A Large\n  Vision-Language Model for Remote Sensing","summary":"  Pre-trained Vision-Language Models (VLMs) utilizing extensive image-text\npaired data have demonstrated unprecedented image-text association\ncapabilities, achieving remarkable results across various downstream tasks. A\ncritical challenge is how to make use of existing large-scale pre-trained VLMs,\nwhich are trained on common objects, to perform the domain-specific transfer\nfor accomplishing domain-related downstream tasks. A critical challenge is how\nto make use of existing large-scale pre-trained VLMs, which are trained on\ncommon objects, to perform the domain-specific transfer for accomplishing\ndomain-related downstream tasks. In this paper, we propose a new framework that\nincludes the Domain pre-trained Vision-Language Model (DVLM), bridging the gap\nbetween the General Vision-Language Model (GVLM) and domain-specific downstream\ntasks. Moreover, we present an image-text paired dataset in the field of remote\nsensing (RS), RS5M, which has 5 million RS images with English descriptions.\nThe dataset is obtained from filtering publicly available image-text paired\ndatasets and captioning label-only RS datasets with pre-trained VLM. These\nconstitute the first large-scale RS image-text paired dataset. Additionally, we\nfine-tuned the CLIP model and tried several Parameter-Efficient Fine-Tuning\nmethods on RS5M to implement the DVLM. Experimental results show that our\nproposed dataset is highly effective for various tasks, and our model GeoRSCLIP\nimproves upon the baseline or previous state-of-the-art model by $3\\%\\sim20\\%$\nin Zero-shot Classification (ZSC), $3\\%\\sim6\\%$ in Remote Sensing Cross-Modal\nText-Image Retrieval (RSCTIR) and $4\\%\\sim5\\%$ in Semantic Localization (SeLo)\ntasks. Dataset and models have been released in:\n\\url{https://github.com/om-ai-lab/RS5M}.\n","authors":["Zilun Zhang","Tiancheng Zhao","Yulong Guo","Jianwei Yin"],"pdf_url":"https://arxiv.org/pdf/2306.11300v5.pdf","comment":"RS5M dataset v5"},{"id":"http://arxiv.org/abs/2401.01216v1","updated":"2024-01-02T14:10:21Z","published":"2024-01-02T14:10:21Z","title":"Noise-NeRF: Hide Information in Neural Radiance Fields using Trainable\n  Noise","summary":"  Neural radiance fields (NeRF) have been proposed as an innovative 3D\nrepresentation method. While attracting lots of attention, NeRF faces critical\nissues such as information confidentiality and security. Steganography is a\ntechnique used to embed information in another object as a means of protecting\ninformation security. Currently, there are few related studies on NeRF\nsteganography, facing challenges in low steganography quality, model weight\ndamage, and a limited amount of steganographic information. This paper proposes\na novel NeRF steganography method based on trainable noise: Noise-NeRF.\nFurthermore, we propose the Adaptive Pixel Selection strategy and Pixel\nPerturbation strategy to improve the steganography quality and efficiency. The\nextensive experiments on open-source datasets show that Noise-NeRF provides\nstate-of-the-art performances in both steganography quality and rendering\nquality, as well as effectiveness in super-resolution image steganography.\n","authors":["Qinglong Huang","Yong Liao","Yanbin Hao","Pengyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2401.01216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01214v1","updated":"2024-01-02T14:04:42Z","published":"2024-01-02T14:04:42Z","title":"YOLO algorithm with hybrid attention feature pyramid network for solder\n  joint defect detection","summary":"  Traditional manual detection for solder joint defect is no longer applied\nduring industrial production due to low efficiency, inconsistent evaluation,\nhigh cost and lack of real-time data. A new approach has been proposed to\naddress the issues of low accuracy, high false detection rates and\ncomputational cost of solder joint defect detection in surface mount technology\nof industrial scenarios. The proposed solution is a hybrid attention mechanism\ndesigned specifically for the solder joint defect detection algorithm to\nimprove quality control in the manufacturing process by increasing the accuracy\nwhile reducing the computational cost. The hybrid attention mechanism comprises\na proposed enhanced multi-head self-attention and coordinate attention\nmechanisms increase the ability of attention networks to perceive contextual\ninformation and enhances the utilization range of network features. The\ncoordinate attention mechanism enhances the connection between different\nchannels and reduces location information loss. The hybrid attention mechanism\nenhances the capability of the network to perceive long-distance position\ninformation and learn local features. The improved algorithm model has good\ndetection ability for solder joint defect detection, with mAP reaching 91.5%,\n4.3% higher than the You Only Look Once version 5 algorithm and better than\nother comparative algorithms. Compared to other versions, mean Average\nPrecision, Precision, Recall, and Frame per Seconds indicators have also\nimproved. The improvement of detection accuracy can be achieved while meeting\nreal-time detection requirements.\n","authors":["Li Ang","Siti Khatijah Nor Abdul Rahim","Raseeda Hamzah","Raihah Aminuddin","Gao Yousheng"],"pdf_url":"https://arxiv.org/pdf/2401.01214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.04718v2","updated":"2024-01-02T13:56:50Z","published":"2022-11-09T07:23:28Z","title":"On the Application of Efficient Neural Mapping to Real-Time Indoor\n  Localisation for Unmanned Ground Vehicles","summary":"  Global localisation from visual data is a challenging problem applicable to\nmany robotics domains. Prior works have shown that neural networks can be\ntrained to map images of an environment to absolute camera pose within that\nenvironment, learning an implicit neural mapping in the process. In this work\nwe evaluate the applicability of such an approach to real-world robotics\nscenarios, demonstrating that by constraining the problem to 2-dimensions and\nsignificantly increasing the quantity of training data, a compact model capable\nof real-time inference on embedded platforms can be used to achieve\nlocalisation accuracy of several centimetres. We deploy our trained model\nonboard a UGV platform, demonstrating its effectiveness in a waypoint\nnavigation task, wherein it is able to localise with a mean accuracy of 9cm at\na rate of 6fps running on the UGV onboard CPU, 35fps on an embedded GPU, or\n220fps on a desktop GPU. Along with this work we will release a novel\nlocalisation dataset comprising simulated and real environments, each with\ntraining samples numbering in the tens of thousands.\n","authors":["Christopher J. Holder","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2211.04718v2.pdf","comment":"\\c{opyright} 2024 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2401.00616v2","updated":"2024-01-02T13:47:19Z","published":"2024-01-01T00:08:39Z","title":"GD^2-NeRF: Generative Detail Compensation via GAN and Diffusion for\n  One-shot Generalizable Neural Radiance Fields","summary":"  In this paper, we focus on the One-shot Novel View Synthesis (O-NVS) task\nwhich targets synthesizing photo-realistic novel views given only one reference\nimage per scene. Previous One-shot Generalizable Neural Radiance Fields\n(OG-NeRF) methods solve this task in an inference-time finetuning-free manner,\nyet suffer the blurry issue due to the encoder-only architecture that highly\nrelies on the limited reference image. On the other hand, recent\ndiffusion-based image-to-3d methods show vivid plausible results via distilling\npre-trained 2D diffusion models into a 3D representation, yet require tedious\nper-scene optimization. Targeting these issues, we propose the GD$^2$-NeRF, a\nGenerative Detail compensation framework via GAN and Diffusion that is both\ninference-time finetuning-free and with vivid plausible details. In detail,\nfollowing a coarse-to-fine strategy, GD$^2$-NeRF is mainly composed of a\nOne-stage Parallel Pipeline (OPP) and a 3D-consistent Detail Enhancer\n(Diff3DE). At the coarse stage, OPP first efficiently inserts the GAN model\ninto the existing OG-NeRF pipeline for primarily relieving the blurry issue\nwith in-distribution priors captured from the training dataset, achieving a\ngood balance between sharpness (LPIPS, FID) and fidelity (PSNR, SSIM). Then, at\nthe fine stage, Diff3DE further leverages the pre-trained image diffusion\nmodels to complement rich out-distribution details while maintaining decent 3D\nconsistency. Extensive experiments on both the synthetic and real-world\ndatasets show that GD$^2$-NeRF noticeably improves the details while without\nper-scene finetuning.\n","authors":["Xiao Pan","Zongxin Yang","Shuai Bai","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2401.00616v2.pdf","comment":"Reading with Macbook Preview is recommended for best quality;\n  Submitted to Journal"},{"id":"http://arxiv.org/abs/2401.01208v1","updated":"2024-01-02T13:31:51Z","published":"2024-01-02T13:31:51Z","title":"FGENet: Fine-Grained Extraction Network for Congested Crowd Counting","summary":"  Crowd counting has gained significant popularity due to its practical\napplications. However, mainstream counting methods ignore precise individual\nlocalization and suffer from annotation noise because of counting from\nestimating density maps. Additionally, they also struggle with high-density\nimages.To address these issues, we propose an end-to-end model called\nFine-Grained Extraction Network (FGENet). Different from methods estimating\ndensity maps, FGENet directly learns the original coordinate points that\nrepresent the precise localization of individuals.This study designs a fusion\nmodule, named Fine-Grained Feature Pyramid(FGFP), that is used to fuse feature\nmaps extracted by the backbone of FGENet. The fused features are then passed to\nboth regression and classification heads, where the former provides predicted\npoint coordinates for a given image, and the latter determines the confidence\nlevel for each predicted point being an individual. At the end, FGENet\nestablishes correspondences between prediction points and ground truth points\nby employing the Hungarian algorithm. For training FGENet, we design a robust\nloss function, named Three-Task Combination (TTC), to mitigate the impact of\nannotation noise. Extensive experiments are conducted on four widely used crowd\ncounting datasets. Experimental results demonstrate the effectiveness of\nFGENet. Notably, our method achieves a remarkable improvement of 3.14 points in\nMean Absolute Error (MAE) on the ShanghaiTech Part A dataset, showcasing its\nsuperiority over the existing state-of-the-art methods. Even more impressively,\nFGENet surpasses previous benchmarks on the UCF\\_CC\\_50 dataset with an\nastounding enhancement of 30.16 points in MAE.\n","authors":["Hao-Yuan Ma","Li Zhang","Xiang-Yi Wei"],"pdf_url":"https://arxiv.org/pdf/2401.01208v1.pdf","comment":"Accepted by 30th International Conference on MultiMedia Modeling"},{"id":"http://arxiv.org/abs/2401.01207v1","updated":"2024-01-02T13:28:39Z","published":"2024-01-02T13:28:39Z","title":"Towards a Simultaneous and Granular Identity-Expression Control in\n  Personalized Face Generation","summary":"  In human-centric content generation, the pre-trained text-to-image models\nstruggle to produce user-wanted portrait images, which retain the identity of\nindividuals while exhibiting diverse expressions. This paper introduces our\nefforts towards personalized face generation. To this end, we propose a novel\nmulti-modal face generation framework, capable of simultaneous\nidentity-expression control and more fine-grained expression synthesis. Our\nexpression control is so sophisticated that it can be specialized by the\nfine-grained emotional vocabulary. We devise a novel diffusion model that can\nundertake the task of simultaneously face swapping and reenactment. Due to the\nentanglement of identity and expression, it's nontrivial to separately and\nprecisely control them in one framework, thus has not been explored yet. To\novercome this, we propose several innovative designs in the conditional\ndiffusion model, including balancing identity and expression encoder, improved\nmidpoint sampling, and explicitly background conditioning. Extensive\nexperiments have demonstrated the controllability and scalability of the\nproposed framework, in comparison with state-of-the-art text-to-image, face\nswapping, and face reenactment methods.\n","authors":["Renshuai Liu","Bowen Ma","Wei Zhang","Zhipeng Hu","Changjie Fan","Tangjie Lv","Yu Ding","Xuan Cheng"],"pdf_url":"https://arxiv.org/pdf/2401.01207v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04028v2","updated":"2024-01-02T13:18:34Z","published":"2023-12-07T03:53:53Z","title":"ImFace++: A Sophisticated Nonlinear 3D Morphable Face Model with\n  Implicit Neural Representations","summary":"  Accurate representations of 3D faces are of paramount importance in various\ncomputer vision and graphics applications. However, the challenges persist due\nto the limitations imposed by data discretization and model linearity, which\nhinder the precise capture of identity and expression clues in current studies.\nThis paper presents a novel 3D morphable face model, named ImFace++, to learn a\nsophisticated and continuous space with implicit neural representations.\nImFace++ first constructs two explicitly disentangled deformation fields to\nmodel complex shapes associated with identities and expressions, respectively,\nwhich simultaneously facilitate the automatic learning of correspondences\nacross diverse facial shapes. To capture more sophisticated facial details, a\nrefinement displacement field within the template space is further\nincorporated, enabling a fine-grained learning of individual-specific facial\ndetails. Furthermore, a Neural Blend-Field is designed to reinforce the\nrepresentation capabilities through adaptive blending of an array of local\nfields. In addition to ImFace++, we have devised an improved learning strategy\nto extend expression embeddings, allowing for a broader range of expression\nvariations. Comprehensive qualitative and quantitative evaluations demonstrate\nthat ImFace++ significantly advances the state-of-the-art in terms of both face\nreconstruction fidelity and correspondence accuracy.\n","authors":["Mingwu Zheng","Haiyu Zhang","Hongyu Yang","Liming Chen","Di Huang"],"pdf_url":"https://arxiv.org/pdf/2312.04028v2.pdf","comment":"Project page:\n  https://github.com/MingwuZheng/ImFace/tree/imface%2B%2B. arXiv admin note:\n  text overlap with arXiv:2203.14510"},{"id":"http://arxiv.org/abs/2307.01530v2","updated":"2024-01-02T13:13:49Z","published":"2023-07-04T07:33:53Z","title":"Tomato Maturity Recognition with Convolutional Transformers","summary":"  Tomatoes are a major crop worldwide, and accurately classifying their\nmaturity is important for many agricultural applications, such as harvesting,\ngrading, and quality control. In this paper, the authors propose a novel method\nfor tomato maturity classification using a convolutional transformer. The\nconvolutional transformer is a hybrid architecture that combines the strengths\nof convolutional neural networks (CNNs) and transformers. Additionally, this\nstudy introduces a new tomato dataset named KUTomaData, explicitly designed to\ntrain deep-learning models for tomato segmentation and classification.\nKUTomaData is a compilation of images sourced from a greenhouse in the UAE,\nwith approximately 700 images available for training and testing. The dataset\nis prepared under various lighting conditions and viewing perspectives and\nemploys different mobile camera sensors, distinguishing it from existing\ndatasets. The contributions of this paper are threefold:Firstly, the authors\npropose a novel method for tomato maturity classification using a modular\nconvolutional transformer. Secondly, the authors introduce a new tomato image\ndataset that contains images of tomatoes at different maturity levels. Lastly,\nthe authors show that the convolutional transformer outperforms\nstate-of-the-art methods for tomato maturity classification. The effectiveness\nof the proposed framework in handling cluttered and occluded tomato instances\nwas evaluated using two additional public datasets, Laboro Tomato and Rob2Pheno\nAnnotated Tomato, as benchmarks. The evaluation results across these three\ndatasets demonstrate the exceptional performance of our proposed framework,\nsurpassing the state-of-the-art by 58.14%, 65.42%, and 66.39% in terms of mean\naverage precision scores for KUTomaData, Laboro Tomato, and Rob2Pheno Annotated\nTomato, respectively.\n","authors":["Asim Khan","Taimur Hassan","Muhammad Shafay","Israa Fahmy","Naoufel Werghi","Lakmal Seneviratne","Irfan Hussain"],"pdf_url":"https://arxiv.org/pdf/2307.01530v2.pdf","comment":"23 pages, 6 figures and 8 Tables"},{"id":"http://arxiv.org/abs/2401.01201v1","updated":"2024-01-02T13:04:41Z","published":"2024-01-02T13:04:41Z","title":"Whole-examination AI estimation of fetal biometrics from 20-week\n  ultrasound scans","summary":"  The current approach to fetal anomaly screening is based on biometric\nmeasurements derived from individually selected ultrasound images. In this\npaper, we introduce a paradigm shift that attains human-level performance in\nbiometric measurement by aggregating automatically extracted biometrics from\nevery frame across an entire scan, with no need for operator intervention. We\nuse a convolutional neural network to classify each frame of an ultrasound\nvideo recording. We then measure fetal biometrics in every frame where\nappropriate anatomy is visible. We use a Bayesian method to estimate the true\nvalue of each biometric from a large number of measurements and\nprobabilistically reject outliers. We performed a retrospective experiment on\n1457 recordings (comprising 48 million frames) of 20-week ultrasound scans,\nestimated fetal biometrics in those scans and compared our estimates to the\nmeasurements sonographers took during the scan. Our method achieves human-level\nperformance in estimating fetal biometrics and estimates well-calibrated\ncredible intervals in which the true biometric value is expected to lie.\n","authors":["Lorenzo Venturini","Samuel Budd","Alfonso Farruggia","Robert Wright","Jacqueline Matthew","Thomas G. Day","Bernhard Kainz","Reza Razavi","Jo V. Hajnal"],"pdf_url":"https://arxiv.org/pdf/2401.01201v1.pdf","comment":"14 pages, 16 figures. Submitted to NPJ digital medicine. For\n  associated video file, see\n  http://wp.doc.ic.ac.uk/ifind/wp-content/uploads/sites/79/2023/12/realtime.gif"},{"id":"http://arxiv.org/abs/2401.01200v1","updated":"2024-01-02T13:03:39Z","published":"2024-01-02T13:03:39Z","title":"Skin cancer diagnosis using NIR spectroscopy data of skin lesions in\n  vivo using machine learning algorithms","summary":"  Skin lesions are classified in benign or malignant. Among the malignant,\nmelanoma is a very aggressive cancer and the major cause of deaths. So, early\ndiagnosis of skin cancer is very desired. In the last few years, there is a\ngrowing interest in computer aided diagnostic (CAD) using most image and\nclinical data of the lesion. These sources of information present limitations\ndue to their inability to provide information of the molecular structure of the\nlesion. NIR spectroscopy may provide an alternative source of information to\nautomated CAD of skin lesions. The most commonly used techniques and\nclassification algorithms used in spectroscopy are Principal Component Analysis\n(PCA), Partial Least Squares - Discriminant Analysis (PLS-DA), and Support\nVector Machines (SVM). Nonetheless, there is a growing interest in applying the\nmodern techniques of machine and deep learning (MDL) to spectroscopy. One of\nthe main limitations to apply MDL to spectroscopy is the lack of public\ndatasets. Since there is no public dataset of NIR spectral data to skin\nlesions, as far as we know, an effort has been made and a new dataset named\nNIR-SC-UFES, has been collected, annotated and analyzed generating the\ngold-standard for classification of NIR spectral data to skin cancer. Next, the\nmachine learning algorithms XGBoost, CatBoost, LightGBM, 1D-convolutional\nneural network (1D-CNN) were investigated to classify cancer and non-cancer\nskin lesions. Experimental results indicate the best performance obtained by\nLightGBM with pre-processing using standard normal variate (SNV), feature\nextraction providing values of 0.839 for balanced accuracy, 0.851 for recall,\n0.852 for precision, and 0.850 for F-score. The obtained results indicate the\nfirst steps in CAD of skin lesions aiming the automated triage of patients with\nskin lesions in vivo using NIR spectral data.\n","authors":["Flavio P. Loss","Pedro H. da Cunha","Matheus B. Rocha","Madson Poltronieri Zanoni","Leandro M. de Lima","Isadora Tavares Nascimento","Isabella Rezende","Tania R. P. Canuto","Luciana de Paula Vieira","Renan Rossoni","Maria C. S. Santos","Patricia Lyra Frasson","Wanderson Romão","Paulo R. Filgueiras","Renato A. Krohling"],"pdf_url":"https://arxiv.org/pdf/2401.01200v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01199v1","updated":"2024-01-02T13:03:29Z","published":"2024-01-02T13:03:29Z","title":"JMA: a General Algorithm to Craft Nearly Optimal Targeted Adversarial\n  Example","summary":"  Most of the approaches proposed so far to craft targeted adversarial examples\nagainst Deep Learning classifiers are highly suboptimal and typically rely on\nincreasing the likelihood of the target class, thus implicitly focusing on\none-hot encoding settings. In this paper, we propose a more general,\ntheoretically sound, targeted attack that resorts to the minimization of a\nJacobian-induced MAhalanobis distance (JMA) term, taking into account the\neffort (in the input space) required to move the latent space representation of\nthe input sample in a given direction. The minimization is solved by exploiting\nthe Wolfe duality theorem, reducing the problem to the solution of a\nNon-Negative Least Square (NNLS) problem. The proposed algorithm provides an\noptimal solution to a linearized version of the adversarial example problem\noriginally introduced by Szegedy et al. \\cite{szegedy2013intriguing}. The\nexperiments we carried out confirm the generality of the proposed attack which\nis proven to be effective under a wide variety of output encoding schemes.\nNoticeably, the JMA attack is also effective in a multi-label classification\nscenario, being capable to induce a targeted modification of up to half the\nlabels in a complex multilabel classification scenario with 20 labels, a\ncapability that is out of reach of all the attacks proposed so far. As a\nfurther advantage, the JMA attack usually requires very few iterations, thus\nresulting more efficient than existing methods.\n","authors":["Benedetta Tondi","Wei Guo","Mauro Barni"],"pdf_url":"https://arxiv.org/pdf/2401.01199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.02803v2","updated":"2024-01-02T12:33:45Z","published":"2023-04-11T14:53:57Z","title":"Tensor PCA from basis in tensor space","summary":"  The aim of this paper is to present a mathematical framework for tensor PCA.\nThe proposed approach is able to overcome the limitations of previous methods\nthat extract a low dimensional subspace by iteratively solving an optimization\nproblem. The core of the proposed approach is the derivation of a basis in\ntensor space from a real self-adjoint tensor operator, thus reducing the\nproblem of deriving a basis to an eigenvalue problem. Three different cases\nhave been studied to derive: i) a basis from a self-adjoint tensor operator;\nii) a rank-1 basis; iii) a basis in a subspace. In particular, the equivalence\nbetween eigenvalue equation for a real self-adjoint tensor operator and\nstandard matrix eigenvalue equation has been proven. For all the three cases\nconsidered, a subspace approach has been adopted to derive a tensor PCA.\nExperiments on image datasets validate the proposed mathematical framework.\n","authors":["Claudio Turchetti"],"pdf_url":"https://arxiv.org/pdf/2305.02803v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2208.03754v3","updated":"2024-01-02T12:27:46Z","published":"2022-08-07T15:57:18Z","title":"Exploring Long- and Short-Range Temporal Information for Learned Video\n  Compression","summary":"  Learned video compression methods have gained a variety of interest in the\nvideo coding community since they have matched or even exceeded the\nrate-distortion (RD) performance of traditional video codecs. However, many\ncurrent learning-based methods are dedicated to utilizing short-range temporal\ninformation, thus limiting their performance. In this paper, we focus on\nexploiting the unique characteristics of video content and further exploring\ntemporal information to enhance compression performance. Specifically, for\nlong-range temporal information exploitation, we propose temporal prior that\ncan update continuously within the group of pictures (GOP) during inference. In\nthat case temporal prior contains valuable temporal information of all decoded\nimages within the current GOP. As for short-range temporal information, we\npropose a progressive guided motion compensation to achieve robust and\neffective compensation. In detail, we design a hierarchical structure to\nachieve multi-scale compensation. More importantly, we use optical flow\nguidance to generate pixel offsets between feature maps at each scale, and the\ncompensation results at each scale will be used to guide the following scale's\ncompensation. Sufficient experimental results demonstrate that our method can\nobtain better RD performance than state-of-the-art video compression\napproaches. The code is publicly available on:\nhttps://github.com/Huairui/LSTVC.\n","authors":["Huairui Wang","Zhenzhong Chen"],"pdf_url":"https://arxiv.org/pdf/2208.03754v3.pdf","comment":"arXiv admin note: text overlap with arXiv:2207.04589"},{"id":"http://arxiv.org/abs/2312.07418v2","updated":"2024-01-02T12:24:25Z","published":"2023-12-12T16:39:12Z","title":"Attention Based Encoder Decoder Model for Video Captioning in Nepali\n  (2023)","summary":"  Video captioning in Nepali, a language written in the Devanagari script,\npresents a unique challenge due to the lack of existing academic work in this\ndomain. This work develops a novel encoder-decoder paradigm for Nepali video\ncaptioning to tackle this difficulty. LSTM and GRU sequence-to-sequence models\nare used in the model to produce related textual descriptions based on features\nretrieved from video frames using CNNs. Using Google Translate and manual\npost-editing, a Nepali video captioning dataset is generated from the Microsoft\nResearch Video Description Corpus (MSVD) dataset created using Google\nTranslate, and manual post-editing work. The efficacy of the model for\nDevanagari-scripted video captioning is demonstrated by BLEU, METOR, and ROUGE\nmeasures, which are used to assess its performance.\n","authors":["Kabita Parajuli","Shashidhar Ram Joshi"],"pdf_url":"https://arxiv.org/pdf/2312.07418v2.pdf","comment":"Result are wrong and took some time"},{"id":"http://arxiv.org/abs/2401.01181v1","updated":"2024-01-02T12:18:40Z","published":"2024-01-02T12:18:40Z","title":"Query-Based Knowledge Sharing for Open-Vocabulary Multi-Label\n  Classification","summary":"  Identifying labels that did not appear during training, known as multi-label\nzero-shot learning, is a non-trivial task in computer vision. To this end,\nrecent studies have attempted to explore the multi-modal knowledge of\nvision-language pre-training (VLP) models by knowledge distillation, allowing\nto recognize unseen labels in an open-vocabulary manner. However, experimental\nevidence shows that knowledge distillation is suboptimal and provides limited\nperformance gain in unseen label prediction. In this paper, a novel query-based\nknowledge sharing paradigm is proposed to explore the multi-modal knowledge\nfrom the pretrained VLP model for open-vocabulary multi-label classification.\nSpecifically, a set of learnable label-agnostic query tokens is trained to\nextract critical vision knowledge from the input image, and further shared\nacross all labels, allowing them to select tokens of interest as visual clues\nfor recognition. Besides, we propose an effective prompt pool for robust label\nembedding, and reformulate the standard ranking learning into a form of\nclassification to allow the magnitude of feature vectors for matching, which\nboth significantly benefit label recognition. Experimental results show that\nour framework significantly outperforms state-of-the-art methods on zero-shot\ntask by 5.9% and 4.5% in mAP on the NUS-WIDE and Open Images, respectively.\n","authors":["Xuelin Zhu","Jian Liu","Dongqi Tang","Jiawei Ge","Weijia Liu","Bo Liu","Jiuxin Cao"],"pdf_url":"https://arxiv.org/pdf/2401.01181v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01180v1","updated":"2024-01-02T12:16:01Z","published":"2024-01-02T12:16:01Z","title":"Accurate and Efficient Urban Street Tree Inventory with Deep Learning on\n  Mobile Phone Imagery","summary":"  Deforestation, a major contributor to climate change, poses detrimental\nconsequences such as agricultural sector disruption, global warming, flash\nfloods, and landslides. Conventional approaches to urban street tree inventory\nsuffer from inaccuracies and necessitate specialised equipment. To overcome\nthese challenges, this paper proposes an innovative method that leverages deep\nlearning techniques and mobile phone imaging for urban street tree inventory.\nOur approach utilises a pair of images captured by smartphone cameras to\naccurately segment tree trunks and compute the diameter at breast height (DBH).\nCompared to traditional methods, our approach exhibits several advantages,\nincluding superior accuracy, reduced dependency on specialised equipment, and\napplicability in hard-to-reach areas. We evaluated our method on a\ncomprehensive dataset of 400 trees and achieved a DBH estimation accuracy with\nan error rate of less than 2.5%. Our method holds significant potential for\nsubstantially improving forest management practices. By enhancing the accuracy\nand efficiency of tree inventory, our model empowers urban management to\nmitigate the adverse effects of deforestation and climate change.\n","authors":["Asim Khan","Umair Nawaz","Anwaar Ulhaq","Iqbal Gondal","Sajid Javed"],"pdf_url":"https://arxiv.org/pdf/2401.01180v1.pdf","comment":"8 Pages, 7 figures and 5 Tables"},{"id":"http://arxiv.org/abs/2401.01179v1","updated":"2024-01-02T12:14:41Z","published":"2024-01-02T12:14:41Z","title":"Freeze the backbones: A Parameter-Efficient Contrastive Approach to\n  Robust Medical Vision-Language Pre-training","summary":"  Modern healthcare often utilises radiographic images alongside textual\nreports for diagnostics, encouraging the use of Vision-Language Self-Supervised\nLearning (VL-SSL) with large pre-trained models to learn versatile medical\nvision representations. However, most existing VL-SSL frameworks are trained\nend-to-end, which is computation-heavy and can lose vital prior information\nembedded in pre-trained encoders. To address both issues, we introduce the\nbackbone-agnostic Adaptor framework, which preserves medical knowledge in\npre-trained image and text encoders by keeping them frozen, and employs a\nlightweight Adaptor module for cross-modal learning. Experiments on medical\nimage classification and segmentation tasks across three datasets reveal that\nour framework delivers competitive performance while cutting trainable\nparameters by over 90% compared to current pre-training approaches. Notably,\nwhen fine-tuned with just 1% of data, Adaptor outperforms several\nTransformer-based methods trained on full datasets in medical image\nsegmentation.\n","authors":["Jiuming Qin","Che Liu","Sibo Cheng","Yike Guo","Rossella Arcucci"],"pdf_url":"https://arxiv.org/pdf/2401.01179v1.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2401.01178v1","updated":"2024-01-02T12:13:35Z","published":"2024-01-02T12:13:35Z","title":"GBSS:a global building semantic segmentation dataset for large-scale\n  remote sensing building extraction","summary":"  Semantic segmentation techniques for extracting building footprints from\nhigh-resolution remote sensing images have been widely used in many fields such\nas urban planning. However, large-scale building extraction demands higher\ndiversity in training samples. In this paper, we construct a Global Building\nSemantic Segmentation (GBSS) dataset (The dataset will be released), which\ncomprises 116.9k pairs of samples (about 742k buildings) from six continents.\nThere are significant variations of building samples in terms of size and\nstyle, so the dataset can be a more challenging benchmark for evaluating the\ngeneralization and robustness of building semantic segmentation models. We\nvalidated through quantitative and qualitative comparisons between different\ndatasets, and further confirmed the potential application in the field of\ntransfer learning by conducting experiments on subsets.\n","authors":["Yuping Hu","Xin Huang","Jiayi Li","Zhen Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.01178v1.pdf","comment":"5 pages,6 figures"},{"id":"http://arxiv.org/abs/2401.01175v1","updated":"2024-01-02T12:09:06Z","published":"2024-01-02T12:09:06Z","title":"Learning Surface Scattering Parameters From SAR Images Using\n  Differentiable Ray Tracing","summary":"  Simulating high-resolution Synthetic Aperture Radar (SAR) images in complex\nscenes has consistently presented a significant research challenge. The\ndevelopment of a microwave-domain surface scattering model and its\nreversibility are poised to play a pivotal role in enhancing the authenticity\nof SAR image simulations and facilitating the reconstruction of target\nparameters. Drawing inspiration from the field of computer graphics, this paper\nproposes a surface microwave rendering model that comprehensively considers\nboth Specular and Diffuse contributions. The model is analytically represented\nby the coherent spatially varying bidirectional scattering distribution\nfunction (CSVBSDF) based on the Kirchhoff approximation (KA) and the\nperturbation method (SPM). And SAR imaging is achieved through the synergistic\ncombination of ray tracing and fast mapping projection techniques. Furthermore,\na differentiable ray tracing (DRT) engine based on SAR images was constructed\nfor CSVBSDF surface scattering parameter learning. Within this SAR image\nsimulation engine, the use of differentiable reverse ray tracing enables the\nrapid estimation of parameter gradients from SAR images. The effectiveness of\nthis approach has been validated through simulations and comparisons with real\nSAR images. By learning the surface scattering parameters, substantial\nenhancements in SAR image simulation performance under various observation\nconditions have been demonstrated.\n","authors":["Jiangtao Wei","Yixiang Luomei","Xu Zhang","Feng Xu"],"pdf_url":"https://arxiv.org/pdf/2401.01175v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01173v1","updated":"2024-01-02T12:06:31Z","published":"2024-01-02T12:06:31Z","title":"En3D: An Enhanced Generative Model for Sculpting 3D Humans from 2D\n  Synthetic Data","summary":"  We present En3D, an enhanced generative scheme for sculpting high-quality 3D\nhuman avatars. Unlike previous works that rely on scarce 3D datasets or limited\n2D collections with imbalanced viewing angles and imprecise pose priors, our\napproach aims to develop a zero-shot 3D generative scheme capable of producing\nvisually realistic, geometrically accurate and content-wise diverse 3D humans\nwithout relying on pre-existing 3D or 2D assets. To address this challenge, we\nintroduce a meticulously crafted workflow that implements accurate physical\nmodeling to learn the enhanced 3D generative model from synthetic 2D data.\nDuring inference, we integrate optimization modules to bridge the gap between\nrealistic appearances and coarse 3D shapes. Specifically, En3D comprises three\nmodules: a 3D generator that accurately models generalizable 3D humans with\nrealistic appearance from synthesized balanced, diverse, and structured human\nimages; a geometry sculptor that enhances shape quality using multi-view normal\nconstraints for intricate human anatomy; and a texturing module that\ndisentangles explicit texture maps with fidelity and editability, leveraging\nsemantical UV partitioning and a differentiable rasterizer. Experimental\nresults show that our approach significantly outperforms prior works in terms\nof image quality, geometry accuracy and content diversity. We also showcase the\napplicability of our generated avatars for animation and editing, as well as\nthe scalability of our approach for content-style free adaptation.\n","authors":["Yifang Men","Biwen Lei","Yuan Yao","Miaomiao Cui","Zhouhui Lian","Xuansong Xie"],"pdf_url":"https://arxiv.org/pdf/2401.01173v1.pdf","comment":"Project Page: https://menyifang.github.io/projects/En3D/index.html"},{"id":"http://arxiv.org/abs/2207.13165v3","updated":"2024-01-02T11:54:36Z","published":"2022-07-26T19:41:59Z","title":"YOLO and Mask R-CNN for Vehicle Number Plate Identification","summary":"  License plate scanners have grown in popularity in parking lots during the\npast few years. In order to quickly identify license plates, traditional plate\nrecognition devices used in parking lots employ a fixed source of light and\nshooting angles. For skewed angles, such as license plate images taken with\nultra-wide angle or fisheye lenses, deformation of the license plate\nrecognition plate can also be quite severe, impairing the ability of standard\nlicense plate recognition systems to identify the plate. Mask RCNN gadget that\nmay be utilised for oblique pictures and various shooting angles. The results\nof the experiments show that the suggested design will be capable of\nclassifying license plates with bevel angles larger than 0/60. Character\nrecognition using the suggested Mask R-CNN approach has advanced significantly\nas well. The proposed Mask R-CNN method has also achieved significant progress\nin character recognition, which is tilted more than 45 degrees as compared to\nthe strategy of employing the YOLOv2 model. Experiment results also suggest\nthat the methodology presented in the open data plate collecting is better than\nother techniques (known as the AOLP dataset).\n","authors":["Siddharth Ganjoo"],"pdf_url":"https://arxiv.org/pdf/2207.13165v3.pdf","comment":"changes to be done"}],"Robotics":[{"id":"http://arxiv.org/abs/2401.01466v1","updated":"2024-01-02T23:58:00Z","published":"2024-01-02T23:58:00Z","title":"Human Leading or Following Preferences: Effects on Human Perception of\n  the Robot and the Human-Robot Collaboration","summary":"  Achieving effective and seamless human-robot collaboration requires two key\noutcomes: enhanced team performance and fostering a positive human perception\nof both the robot and the collaboration. This paper investigates the capability\nof the proposed task planning framework to realize these objectives by\nintegrating human leading/following preference and performance into its task\nallocation and scheduling processes. We designed a collaborative scenario\nwherein the robot autonomously collaborates with participants. The outcomes of\nthe user study indicate that the proactive task planning framework successfully\nattains the aforementioned goals. We also explore the impact of participants'\nleadership and followership styles on their collaboration. The results reveal\nintriguing relationships between these factors, which warrant further\ninvestigation in future studies.\n","authors":["Ali Noormohammadi-Asl","Kevin Fan","Stephen L. Smith","Kerstin Dautenhahn"],"pdf_url":"https://arxiv.org/pdf/2401.01466v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.13702v2","updated":"2024-01-02T22:33:42Z","published":"2022-10-25T01:51:36Z","title":"DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to\n  Reality","summary":"  Recent work has demonstrated the ability of deep reinforcement learning (RL)\nalgorithms to learn complex robotic behaviours in simulation, including in the\ndomain of multi-fingered manipulation. However, such models can be challenging\nto transfer to the real world due to the gap between simulation and reality. In\nthis paper, we present our techniques to train a) a policy that can perform\nrobust dexterous manipulation on an anthropomorphic robot hand and b) a robust\npose estimator suitable for providing reliable real-time information on the\nstate of the object being manipulated. Our policies are trained to adapt to a\nwide range of conditions in simulation. Consequently, our vision-based policies\nsignificantly outperform the best vision policies in the literature on the same\nreorientation task and are competitive with policies that are given privileged\nstate information via motion capture systems. Our work reaffirms the\npossibilities of sim-to-real transfer for dexterous manipulation in diverse\nkinds of hardware and simulator setups, and in our case, with the Allegro Hand\nand Isaac Gym GPU-based simulation. Furthermore, it opens up possibilities for\nresearchers to achieve such results with commonly-available, affordable robot\nhands and cameras. Videos of the resulting policy and supplementary\ninformation, including experiments and demos, can be found at\nhttps://dextreme.org/\n","authors":["Ankur Handa","Arthur Allshire","Viktor Makoviychuk","Aleksei Petrenko","Ritvik Singh","Jingzhou Liu","Denys Makoviichuk","Karl Van Wyk","Alexander Zhurkevich","Balakumar Sundaralingam","Yashraj Narang","Jean-Francois Lafleche","Dieter Fox","Gavriel State"],"pdf_url":"https://arxiv.org/pdf/2210.13702v2.pdf","comment":"28 pages. A smaller version of this paper is accepted to ICRA 2023"},{"id":"http://arxiv.org/abs/2401.01445v1","updated":"2024-01-02T22:07:44Z","published":"2024-01-02T22:07:44Z","title":"Indoor Obstacle Discovery on Reflective Ground via Monocular Camera","summary":"  Visual obstacle discovery is a key step towards autonomous navigation of\nindoor mobile robots. Successful solutions have many applications in multiple\nscenes. One of the exceptions is the reflective ground. In this case, the\nreflections on the floor resemble the true world, which confuses the obstacle\ndiscovery and leaves navigation unsuccessful. We argue that the key to this\nproblem lies in obtaining discriminative features for reflections and\nobstacles. Note that obstacle and reflection can be separated by the ground\nplane in 3D space. With this observation, we firstly introduce a\npre-calibration based ground detection scheme that uses robot motion to predict\nthe ground plane. Due to the immunity of robot motion to reflection, this\nscheme avoids failed ground detection caused by reflection. Given the detected\nground, we design a ground-pixel parallax to describe the location of a pixel\nrelative to the ground. Based on this, a unified appearance-geometry feature\nrepresentation is proposed to describe objects inside rectangular boxes.\nEventually, based on segmenting by detection framework, an appearance-geometry\nfusion regressor is designed to utilize the proposed feature to discover the\nobstacles. It also prevents our model from concentrating too much on parts of\nobstacles instead of whole obstacles. For evaluation, we introduce a new\ndataset for Obstacle on Reflective Ground (ORG), which comprises 15 scenes with\nvarious ground reflections, a total of more than 200 image sequences and 3400\nRGB images. The pixel-wise annotations of ground and obstacle provide a\ncomparison to our method and other methods. By reducing the misdetection of the\nreflection, the proposed approach outperforms others. The source code and the\ndataset will be available at\nhttps://github.com/XuefengBUPT/IndoorObstacleDiscovery-RG.\n","authors":["Feng Xue","Yicong Chang","Tianxi Wang","Yu Zhou","Anlong Ming"],"pdf_url":"https://arxiv.org/pdf/2401.01445v1.pdf","comment":"International Journal of Computer Vision (IJCV) 2023. Project Page:\n  https://xuefeng-cvr.github.io/IODRG"},{"id":"http://arxiv.org/abs/2305.01072v2","updated":"2024-01-02T21:52:11Z","published":"2023-05-01T20:13:23Z","title":"Fast Path Planning Through Large Collections of Safe Boxes","summary":"  We present a fast algorithm for the design of smooth paths (or trajectories)\nthat are constrained to lie in a collection of axis-aligned boxes. We consider\nthe case where the number of these safe boxes is large, and basic preprocessing\nof them (such as finding their intersections) can be done offline. At runtime\nwe quickly generate a smooth path between given initial and terminal positions.\nOur algorithm designs trajectories that are guaranteed to be safe at all times,\nand detects infeasibility whenever such a trajectory does not exist. Our\nalgorithm is based on two subproblems that we can solve very efficiently:\nfinding a shortest path in a weighted graph, and solving (multiple) convex\noptimal-control problems. We demonstrate the proposed path planner on\nlarge-scale numerical examples, and we provide an efficient open-source\nsoftware implementation, fastpathplanning.\n","authors":["Tobia Marcucci","Parth Nobel","Russ Tedrake","Stephen Boyd"],"pdf_url":"https://arxiv.org/pdf/2305.01072v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01439v1","updated":"2024-01-02T21:27:43Z","published":"2024-01-02T21:27:43Z","title":"Off-Road LiDAR Intensity Based Semantic Segmentation","summary":"  LiDAR is used in autonomous driving to provide 3D spatial information and\nenable accurate perception in off-road environments, aiding in obstacle\ndetection, mapping, and path planning. Learning-based LiDAR semantic\nsegmentation utilizes machine learning techniques to automatically classify\nobjects and regions in LiDAR point clouds. Learning-based models struggle in\noff-road environments due to the presence of diverse objects with varying\ncolors, textures, and undefined boundaries, which can lead to difficulties in\naccurately classifying and segmenting objects using traditional geometric-based\nfeatures. In this paper, we address this problem by harnessing the LiDAR\nintensity parameter to enhance object segmentation in off-road environments.\nOur approach was evaluated in the RELLIS-3D data set and yielded promising\nresults as a preliminary analysis with improved mIoU for classes \"puddle\" and\n\"grass\" compared to more complex deep learning-based benchmarks. The\nmethodology was evaluated for compatibility across both Velodyne and Ouster\nLiDAR systems, assuring its cross-platform applicability. This analysis\nadvocates for the incorporation of calibrated intensity as a supplementary\ninput, aiming to enhance the prediction accuracy of learning based semantic\nsegmentation frameworks.\nhttps://github.com/MOONLABIISERB/lidar-intensity-predictor/tree/main\n","authors":["Kasi Viswanath","Peng Jiang","Sujit PB","Srikanth Saripalli"],"pdf_url":"https://arxiv.org/pdf/2401.01439v1.pdf","comment":"Accepted to ISER 2023"},{"id":"http://arxiv.org/abs/2401.01425v1","updated":"2024-01-02T20:28:06Z","published":"2024-01-02T20:28:06Z","title":"SwapTransformer: highway overtaking tactical planner model via imitation\n  learning on OSHA dataset","summary":"  This paper investigates the high-level decision-making problem in highway\nscenarios regarding lane changing and over-taking other slower vehicles. In\nparticular, this paper aims to improve the Travel Assist feature for automatic\novertaking and lane changes on highways. About 9 million samples including lane\nimages and other dynamic objects are collected in simulation. This data;\nOvertaking on Simulated HighwAys (OSHA) dataset is released to tackle this\nchallenge. To solve this problem, an architecture called SwapTransformer is\ndesigned and implemented as an imitation learning approach on the OSHA dataset.\nMoreover, auxiliary tasks such as future points and car distance network\npredictions are proposed to aid the model in better understanding the\nsurrounding environment. The performance of the proposed solution is compared\nwith a multi-layer perceptron (MLP) and multi-head self-attention networks as\nbaselines in a simulation environment. We also demonstrate the performance of\nthe model with and without auxiliary tasks. All models are evaluated based on\ndifferent metrics such as time to finish each lap, number of overtakes, and\nspeed difference with speed limit. The evaluation shows that the\nSwapTransformer model outperforms other models in different traffic densities\nin the inference phase.\n","authors":["Alireza Shamsoshoara","Safin B Salih","Pedram Aghazadeh"],"pdf_url":"https://arxiv.org/pdf/2401.01425v1.pdf","comment":"19 pages, 12 Figures, 1 Algorithm, 2 Tables"},{"id":"http://arxiv.org/abs/2401.01409v1","updated":"2024-01-02T19:15:36Z","published":"2024-01-02T19:15:36Z","title":"Design, Manufacturing and Open-Loop Control of a Soft Pneumatic Arm","summary":"  Soft Robots distinguish themselves from traditional robots by embracing\nflexible kinematics. Because of their recent emergence, there exist numerous\nuncharted territories, including novel actuators, manufacturing processes, and\nadvanced control methods. This research is centred on the design, fabrication,\nand control of a pneumatic soft robot. The principal objective is to develop a\nmodular soft robot featuring with multiple segments, each one of three degrees\nof freedom. This yields to tubular structure with five independent degrees of\nfreedom, enabling motion across three spatial dimensions. Physical construction\nleverages tin-cured silicone and a wax casting method, refined through\niterative processes. 3D-printed PLA moulds, filled with silicone, yield the\ndesired model, while bladder-like structures, are formed within using\nsolidified paraffin wax positive moulds. For control, an empirically fine-tuned\nopen-loop system is adopted. The project culminates in rigorous testing bending\nability and weight carrying capacity and possible applications are discussed.\n","authors":["Jorge Francisco García-Samartín","Adrián Rieker","Antonio Barrientos"],"pdf_url":"https://arxiv.org/pdf/2401.01409v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.14082v5","updated":"2024-01-02T18:23:59Z","published":"2021-09-28T23:00:30Z","title":"Sample-Efficient Safety Assurances using Conformal Prediction","summary":"  When deploying machine learning models in high-stakes robotics applications,\nthe ability to detect unsafe situations is crucial. Early warning systems can\nprovide alerts when an unsafe situation is imminent (in the absence of\ncorrective action). To reliably improve safety, these warning systems should\nhave a provable false negative rate; i.e. of the situations that are unsafe,\nfewer than $\\epsilon$ will occur without an alert. In this work, we present a\nframework that combines a statistical inference technique known as conformal\nprediction with a simulator of robot/environment dynamics, in order to tune\nwarning systems to provably achieve an $\\epsilon$ false negative rate using as\nfew as $1/\\epsilon$ data points. We apply our framework to a driver warning\nsystem and a robotic grasping application, and empirically demonstrate\nguaranteed false negative rate while also observing low false detection\n(positive) rate.\n","authors":["Rachel Luo","Shengjia Zhao","Jonathan Kuck","Boris Ivanovic","Silvio Savarese","Edward Schmerling","Marco Pavone"],"pdf_url":"https://arxiv.org/pdf/2109.14082v5.pdf","comment":"International Journal of Robotics Research, 2023"},{"id":"http://arxiv.org/abs/2312.15071v2","updated":"2024-01-02T18:22:32Z","published":"2023-12-22T21:18:02Z","title":"Independence in the Home: A Wearable Interface for a Person with\n  Quadriplegia to Teleoperate a Mobile Manipulator","summary":"  Teleoperation of mobile manipulators within a home environment can\nsignificantly enhance the independence of individuals with severe motor\nimpairments, allowing them to regain the ability to perform self-care and\nhousehold tasks. There is a critical need for novel teleoperation interfaces to\noffer effective alternatives for individuals with impairments who may encounter\nchallenges in using existing interfaces due to physical limitations. In this\nwork, we iterate on one such interface, HAT (Head-Worn Assistive\nTeleoperation), an inertial-based wearable integrated into any head-worn\ngarment. We evaluate HAT through a 7-day in-home study with Henry Evans, a\nnon-speaking individual with quadriplegia who has participated extensively in\nassistive robotics studies. We additionally evaluate HAT with a proposed shared\ncontrol method for mobile manipulators termed Driver Assistance and demonstrate\nhow the interface generalizes to other physical devices and contexts. Our\nresults show that HAT is a strong teleoperation interface across key metrics\nincluding efficiency, errors, learning curve, and workload. Code and videos are\nlocated on our project website.\n","authors":["Akhil Padmanabha","Janavi Gupta","Chen Chen","Jehan Yang","Vy Nguyen","Douglas J. Weber","Carmel Majidi","Zackory Erickson"],"pdf_url":"https://arxiv.org/pdf/2312.15071v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05332v3","updated":"2024-01-02T17:19:39Z","published":"2023-12-08T19:33:22Z","title":"Bridging the Gaps: Learning Verifiable Model-Free Quadratic Programming\n  Controllers Inspired by Model Predictive Control","summary":"  In this paper, we introduce a new class of parameterized controllers, drawing\ninspiration from Model Predictive Control (MPC). The controller resembles a\nQuadratic Programming (QP) solver of a linear MPC problem, with the parameters\nof the controller being trained via Deep Reinforcement Learning (DRL) rather\nthan derived from system models. This approach addresses the limitations of\ncommon controllers with Multi-Layer Perceptron (MLP) or other general neural\nnetwork architecture used in DRL, in terms of verifiability and performance\nguarantees, and the learned controllers possess verifiable properties like\npersistent feasibility and asymptotic stability akin to MPC. On the other hand,\nnumerical examples illustrate that the proposed controller empirically matches\nMPC and MLP controllers in terms of control performance and has superior\nrobustness against modeling uncertainty and noises. Furthermore, the proposed\ncontroller is significantly more computationally efficient compared to MPC and\nrequires fewer parameters to learn than MLP controllers. Real-world experiments\non vehicle drift maneuvering task demonstrate the potential of these\ncontrollers for robotics and other demanding control tasks.\n","authors":["Yiwen Lu","Zishuo Li","Yihan Zhou","Na Li","Yilin Mo"],"pdf_url":"https://arxiv.org/pdf/2312.05332v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.04718v2","updated":"2024-01-02T13:56:50Z","published":"2022-11-09T07:23:28Z","title":"On the Application of Efficient Neural Mapping to Real-Time Indoor\n  Localisation for Unmanned Ground Vehicles","summary":"  Global localisation from visual data is a challenging problem applicable to\nmany robotics domains. Prior works have shown that neural networks can be\ntrained to map images of an environment to absolute camera pose within that\nenvironment, learning an implicit neural mapping in the process. In this work\nwe evaluate the applicability of such an approach to real-world robotics\nscenarios, demonstrating that by constraining the problem to 2-dimensions and\nsignificantly increasing the quantity of training data, a compact model capable\nof real-time inference on embedded platforms can be used to achieve\nlocalisation accuracy of several centimetres. We deploy our trained model\nonboard a UGV platform, demonstrating its effectiveness in a waypoint\nnavigation task, wherein it is able to localise with a mean accuracy of 9cm at\na rate of 6fps running on the UGV onboard CPU, 35fps on an embedded GPU, or\n220fps on a desktop GPU. Along with this work we will release a novel\nlocalisation dataset comprising simulated and real environments, each with\ntraining samples numbering in the tens of thousands.\n","authors":["Christopher J. Holder","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2211.04718v2.pdf","comment":"\\c{opyright} 2024 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2309.04251v2","updated":"2024-01-02T13:03:38Z","published":"2023-09-08T10:29:52Z","title":"Toward Certifying Maps for Safe Registration-based Localization Under\n  Adverse Conditions","summary":"  In this paper, we propose a way to model the resilience of the Iterative\nClosest Point (ICP) algorithm in the presence of corrupted measurements. In the\ncontext of autonomous vehicles, certifying the safety of the localization\nprocess poses a significant challenge. As robots evolve in a complex world,\nvarious types of noise can impact the measurements. Conventionally, this noise\nhas been assumed to be distributed according to a zero-mean Gaussian\ndistribution. However, this assumption does not hold in numerous scenarios,\nincluding adverse weather conditions, occlusions caused by dynamic obstacles,\nor long-term changes in the map. In these cases, the measurements are instead\naffected by large and deterministic faults. This paper introduces a closed-form\nformula approximating the pose error resulting from an ICP algorithm when\nsubjected to the most detrimental adverse measurements. Using this formula, we\ndevelop a metric to certify and pinpoint specific regions within the\nenvironment where the robot is more vulnerable to localization failures in the\npresence of faults in the measurements.\n","authors":["Johann Laconte","Daniil Lisus","Timothy D. Barfoot"],"pdf_url":"https://arxiv.org/pdf/2309.04251v2.pdf","comment":"in IEEE Robotics and Automation Letters, 2023"},{"id":"http://arxiv.org/abs/2401.01189v1","updated":"2024-01-02T12:35:03Z","published":"2024-01-02T12:35:03Z","title":"NID-SLAM: Neural Implicit Representation-based RGB-D SLAM in dynamic\n  environments","summary":"  Neural implicit representations have been explored to enhance visual SLAM\nalgorithms, especially in providing high-fidelity dense map. Existing methods\noperate robustly in static scenes but struggle with the disruption caused by\nmoving objects. In this paper we present NID-SLAM, which significantly improves\nthe performance of neural SLAM in dynamic environments. We propose a new\napproach to enhance inaccurate regions in semantic masks, particularly in\nmarginal areas. Utilizing the geometric information present in depth images,\nthis method enables accurate removal of dynamic objects, thereby reducing the\nprobability of camera drift. Additionally, we introduce a keyframe selection\nstrategy for dynamic scenes, which enhances camera tracking robustness against\nlarge-scale objects and improves the efficiency of mapping. Experiments on\npublicly available RGB-D datasets demonstrate that our method outperforms\ncompetitive neural SLAM approaches in tracking accuracy and mapping quality in\ndynamic environments.\n","authors":["Ziheng Xu","Jianwei Niu","Qingfeng Li","Tao Ren","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2401.01189v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02641v3","updated":"2024-01-02T11:01:44Z","published":"2023-12-05T10:29:28Z","title":"Inertial Line-Of-Sight Stabilization Using a 3-DOF Spherical Parallel\n  Manipulator with Coaxial Input Shafts","summary":"  This article dives into the use of a 3-RRR Spherical Parallel Manipulator\n(SPM) for the purpose of inertial Line Of Sight (LOS) stabilization. Such a\nparallel robot provides three Degrees of Freedom (DOF) in orientation and is\nstudied from the kinematic point of view. In particular, one guarantees that\nthe singular loci (with the resulting numerical instabilities and inappropriate\nbehavior of the mechanism) are far away from the prescribed workspace. Once the\nkinematics of the device is certified, a control strategy needs to be\nimplemented in order to stabilize the LOS through the upper platform of the\nmechanism. Such a work is done with MATLAB Simulink using a SimMechanics model\nof our robot.\n","authors":["Alexandre Le","Guillaume Rance","Fabrice Rouillier","Damien Chablat"],"pdf_url":"https://arxiv.org/pdf/2312.02641v3.pdf","comment":"OPTRO Conference 2024 (11th International Symposium on Optronics in\n  Defense & Security, 2024), 11 pages, 16 figures"},{"id":"http://arxiv.org/abs/2401.01123v1","updated":"2024-01-02T09:39:34Z","published":"2024-01-02T09:39:34Z","title":"Symbolic Manipulation Planning with Discovered Object and Relational\n  Predicates","summary":"  Discovering the symbols and rules that can be used in long-horizon planning\nfrom a robot's unsupervised exploration of its environment and continuous\nsensorimotor experience is a challenging task. The previous studies proposed\nlearning symbols from single or paired object interactions and planning with\nthese symbols. In this work, we propose a system that learns rules with\ndiscovered object and relational symbols that encode an arbitrary number of\nobjects and the relations between them, converts those rules to Planning Domain\nDescription Language (PDDL), and generates plans that involve affordances of\nthe arbitrary number of objects to achieve tasks. We validated our system with\nbox-shaped objects in different sizes and showed that the system can develop a\nsymbolic knowledge of pick-up, carry, and place operations, taking into account\nobject compounds in different configurations, such as boxes would be carried\ntogether with a larger box that they are placed on. We also compared our method\nwith the state-of-the-art methods and showed that planning with the operators\ndefined over relational symbols gives better planning performance compared to\nthe baselines.\n","authors":["Alper Ahmetoglu","Erhan Oztop","Emre Ugur"],"pdf_url":"https://arxiv.org/pdf/2401.01123v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01081v1","updated":"2024-01-02T07:54:40Z","published":"2024-01-02T07:54:40Z","title":"PLE-SLAM: A Visual-Inertial SLAM Based on Point-Line Features and\n  Efficient IMU Initialization","summary":"  Visual-inertial SLAM is essential in various fields, such as AR/VR, uncrewed\naerial vehicles, industrial robots, and autonomous driving. The fusion of a\ncamera and inertial measurement unit (IMU) can make up for the shortcomings of\na signal sensor, which significantly improves the accuracy and robustness of\nlocalization in challenging environments. Robust tracking and accurate inertial\nparameter estimation are the basis for the stable operation of the system. This\narticle presents PLE-SLAM, an entirely precise and real-time visual-inertial\nSLAM algorithm based on point-line features and efficient IMU initialization.\nFirst, we introduce line features in a point-based visual-inertial SLAM system.\nWe use parallel computing methods to extract features and compute descriptors\nto ensure real-time performance. Second, the proposed system estimates\ngyroscope bias with rotation pre-integration and point and line observations.\nAccelerometer bias and gravity direction are solved by an analytical method.\nAfter initialization, all inertial parameters are refined through maximum a\nposteriori (MAP) estimation. Moreover, we open a dynamic feature elimination\nthread to improve the adaptability to dynamic environments and use CNN,\nbag-of-words and GNN to detect loops and match features. Excellent wide\nbaseline matching capability of DNN-based matching method and illumination\nrobustness significantly improve loop detection recall and loop inter-frame\npose estimation. The front-end and back-end are designed for hardware\nacceleration. The experiments are performed on public datasets, and the results\nshow that the proposed system is one of the state-of-the-art methods in complex\nscenarios.\n","authors":["Jiaming He","Mingrui Li","Yangyang Wang","Hongyu Wang"],"pdf_url":"https://arxiv.org/pdf/2401.01081v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05019v2","updated":"2024-01-02T06:13:16Z","published":"2023-12-08T12:57:13Z","title":"Vision-based Learning for Drones: A Survey","summary":"  Drones as advanced cyber-physical systems are undergoing a transformative\nshift with the advent of vision-based learning, a field that is rapidly gaining\nprominence due to its profound impact on drone autonomy and functionality.\nDifferent from existing task-specific surveys, this review offers a\ncomprehensive overview of vision-based learning in drones, emphasizing its\npivotal role in enhancing their operational capabilities under various\nscenarios. We start by elucidating the fundamental principles of vision-based\nlearning, highlighting how it significantly improves drones' visual perception\nand decision-making processes. We then categorize vision-based control methods\ninto indirect, semi-direct, and end-to-end approaches from the\nperception-control perspective. We further explore various applications of\nvision-based drones with learning capabilities, ranging from single-agent\nsystems to more complex multi-agent and heterogeneous system scenarios, and\nunderscore the challenges and innovations characterizing each area. Finally, we\nexplore open questions and potential solutions, paving the way for ongoing\nresearch and development in this dynamic and rapidly evolving field. With\ngrowing large language models (LLMs) and embodied intelligence, vision-based\nlearning for drones provides a promising but challenging road towards\nartificial general intelligence (AGI) in 3D physical world.\n","authors":["Jiaping Xiao","Rangya Zhang","Yuhang Zhang","Mir Feroskhan"],"pdf_url":"https://arxiv.org/pdf/2312.05019v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11460v3","updated":"2024-01-02T04:11:12Z","published":"2023-12-18T18:59:06Z","title":"Hybrid Internal Model: Learning Agile Legged Locomotion with Simulated\n  Robot Response","summary":"  Robust locomotion control depends on accurate state estimations. However, the\nsensors of most legged robots can only provide partial and noisy observations,\nmaking the estimation particularly challenging, especially for external states\nlike terrain frictions and elevation maps. Inspired by the classical Internal\nModel Control principle, we consider these external states as disturbances and\nintroduce Hybrid Internal Model (HIM) to estimate them according to the\nresponse of the robot. The response, which we refer to as the hybrid internal\nembedding, contains the robot's explicit velocity and implicit stability\nrepresentation, corresponding to two primary goals for locomotion tasks:\nexplicitly tracking velocity and implicitly maintaining stability. We use\ncontrastive learning to optimize the embedding to be close to the robot's\nsuccessor state, in which the response is naturally embedded. HIM has several\nappealing benefits: It only needs the robot's proprioceptions, i.e., those from\njoint encoders and IMU as observations. It innovatively maintains consistent\nobservations between simulation reference and reality that avoids information\nloss in mimicking learning. It exploits batch-level information that is more\nrobust to noises and keeps better sample efficiency. It only requires 1 hour of\ntraining on an RTX 4090 to enable a quadruped robot to traverse any terrain\nunder any disturbances. A wealth of real-world experiments demonstrates its\nagility, even in high-difficulty tasks and cases never occurred during the\ntraining process, revealing remarkable open-world generalizability.\n","authors":["Junfeng Long","Zirui Wang","Quanyi Li","Jiawei Gao","Liu Cao","Jiangmiao Pang"],"pdf_url":"https://arxiv.org/pdf/2312.11460v3.pdf","comment":"Use 1 hour to train a quadruped robot capable of traversing any\n  terrain under any disturbances in the open world, Project Page:\n  https://github.com/OpenRobotLab/HIMLoco"}],"Multimedia":[{"id":"http://arxiv.org/abs/2306.11300v5","updated":"2024-01-02T14:18:02Z","published":"2023-06-20T05:30:59Z","title":"RS5M and GeoRSCLIP: A Large Scale Vision-Language Dataset and A Large\n  Vision-Language Model for Remote Sensing","summary":"  Pre-trained Vision-Language Models (VLMs) utilizing extensive image-text\npaired data have demonstrated unprecedented image-text association\ncapabilities, achieving remarkable results across various downstream tasks. A\ncritical challenge is how to make use of existing large-scale pre-trained VLMs,\nwhich are trained on common objects, to perform the domain-specific transfer\nfor accomplishing domain-related downstream tasks. A critical challenge is how\nto make use of existing large-scale pre-trained VLMs, which are trained on\ncommon objects, to perform the domain-specific transfer for accomplishing\ndomain-related downstream tasks. In this paper, we propose a new framework that\nincludes the Domain pre-trained Vision-Language Model (DVLM), bridging the gap\nbetween the General Vision-Language Model (GVLM) and domain-specific downstream\ntasks. Moreover, we present an image-text paired dataset in the field of remote\nsensing (RS), RS5M, which has 5 million RS images with English descriptions.\nThe dataset is obtained from filtering publicly available image-text paired\ndatasets and captioning label-only RS datasets with pre-trained VLM. These\nconstitute the first large-scale RS image-text paired dataset. Additionally, we\nfine-tuned the CLIP model and tried several Parameter-Efficient Fine-Tuning\nmethods on RS5M to implement the DVLM. Experimental results show that our\nproposed dataset is highly effective for various tasks, and our model GeoRSCLIP\nimproves upon the baseline or previous state-of-the-art model by $3\\%\\sim20\\%$\nin Zero-shot Classification (ZSC), $3\\%\\sim6\\%$ in Remote Sensing Cross-Modal\nText-Image Retrieval (RSCTIR) and $4\\%\\sim5\\%$ in Semantic Localization (SeLo)\ntasks. Dataset and models have been released in:\n\\url{https://github.com/om-ai-lab/RS5M}.\n","authors":["Zilun Zhang","Tiancheng Zhao","Yulong Guo","Jianwei Yin"],"pdf_url":"https://arxiv.org/pdf/2306.11300v5.pdf","comment":"RS5M dataset v5"},{"id":"http://arxiv.org/abs/2401.01163v1","updated":"2024-01-02T11:46:42Z","published":"2024-01-02T11:46:42Z","title":"NU-Class Net: A Novel Deep Learning-based Approach for Video Quality\n  Enhancement","summary":"  Video content has experienced a surge in popularity, asserting its dominance\nover internet traffic and Internet of Things (IoT) networks. Video compression\nhas long been regarded as the primary means of efficiently managing the\nsubstantial multimedia traffic generated by video-capturing devices.\nNevertheless, video compression algorithms entail significant computational\ndemands in order to achieve substantial compression ratios. This complexity\npresents a formidable challenge when implementing efficient video coding\nstandards in resource-constrained embedded systems, such as IoT edge node\ncameras. To tackle this challenge, this paper introduces NU-Class Net, an\ninnovative deep-learning model designed to mitigate compression artifacts\nstemming from lossy compression codecs. This enhancement significantly elevates\nthe perceptible quality of low-bit-rate videos. By employing the NU-Class Net,\nthe video encoder within the video-capturing node can reduce output quality,\nthereby generating low-bit-rate videos and effectively curtailing both\ncomputation and bandwidth requirements at the edge. On the decoder side, which\nis typically less encumbered by resource limitations, NU-Class Net is applied\nafter the video decoder to compensate for artifacts and approximate the quality\nof the original video. Experimental results affirm the efficacy of the proposed\nmodel in enhancing the perceptible quality of videos, especially those streamed\nat low bit rates.\n","authors":["Parham Zilouchian Moghaddam","Mehdi Modarressi","MohammadAmin Sadeghi"],"pdf_url":"https://arxiv.org/pdf/2401.01163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10307v2","updated":"2024-01-02T02:36:22Z","published":"2023-12-16T03:50:13Z","title":"MusER: Musical Element-Based Regularization for Generating Symbolic\n  Music with Emotion","summary":"  Generating music with emotion is an important task in automatic music\ngeneration, in which emotion is evoked through a variety of musical elements\n(such as pitch and duration) that change over time and collaborate with each\nother. However, prior research on deep learning-based emotional music\ngeneration has rarely explored the contribution of different musical elements\nto emotions, let alone the deliberate manipulation of these elements to alter\nthe emotion of music, which is not conducive to fine-grained element-level\ncontrol over emotions. To address this gap, we present a novel approach\nemploying musical element-based regularization in the latent space to\ndisentangle distinct elements, investigate their roles in distinguishing\nemotions, and further manipulate elements to alter musical emotions.\nSpecifically, we propose a novel VQ-VAE-based model named MusER. MusER\nincorporates a regularization loss to enforce the correspondence between the\nmusical element sequences and the specific dimensions of latent variable\nsequences, providing a new solution for disentangling discrete sequences.\nTaking advantage of the disentangled latent vectors, a two-level decoding\nstrategy that includes multiple decoders attending to latent vectors with\ndifferent semantics is devised to better predict the elements. By visualizing\nlatent space, we conclude that MusER yields a disentangled and interpretable\nlatent space and gain insights into the contribution of distinct elements to\nthe emotional dimensions (i.e., arousal and valence). Experimental results\ndemonstrate that MusER outperforms the state-of-the-art models for generating\nemotional music in both objective and subjective evaluation. Besides, we\nrearrange music through element transfer and attempt to alter the emotion of\nmusic by transferring emotion-distinguishable elements.\n","authors":["Shulei Ji","Xinyu Yang"],"pdf_url":"https://arxiv.org/pdf/2312.10307v2.pdf","comment":"Accepted by AAAI 2024"}]},"2024-01-01T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2310.16033v2","updated":"2024-01-01T23:50:31Z","published":"2023-10-24T17:48:04Z","title":"ViCrop: Perceiving Small Visual Details in Zero-shot Visual Question\n  Answering with Multimodal Large Language Models","summary":"  Multimodal Large Language Models (MLLMs) have recently achieved promising\nzero-shot accuracy on visual question answering (VQA) -- a fundamental task\naffecting various downstream applications and domains. Given the great\npotential for the broad use of these models, it is important to investigate\ntheir limitations in dealing with different image and question properties. In\nthis work, we investigate whether MLLMs can perceive details as well as larger\ncomponents in images. In particular, we show that their zero-shot accuracy in\nanswering visual questions is very sensitive to the size of the visual subject\nrelated to the question, declining up to $45.91\\%$ with size. Furthermore, we\nshow that this effect is causal by observing that human visual cropping can\nsignificantly mitigate their sensitivity to size. To scale up the usefulness of\nhuman cropping, we propose ViCrop, a general framework that utilizes automatic\nvisual cropping to enhance zero-shot VQA of MLLMs. We construct five variants\nof ViCrop leveraging either external localization models or the decision\nprocess of the given MLLM itself. Our results show that ViCrop improves MLLMs'\nzero-shot accuracy across different VQA datasets, for example, enhances\nBLIP2-T5's performance by $32.23\\%$ on the TextVQA test set. To facilitate\nfurther investigation of MLLMs' behaviors, our code is publicly released.\n","authors":["Jiarui Zhang","Mahyar Khayatkhoei","Prateek Chhikara","Filip Ilievski"],"pdf_url":"https://arxiv.org/pdf/2310.16033v2.pdf","comment":"18 pages, 11 figures, 6 tables"},{"id":"http://arxiv.org/abs/2309.12269v4","updated":"2024-01-01T23:48:58Z","published":"2023-09-21T17:24:40Z","title":"The Cambridge Law Corpus: A Dataset for Legal AI Research","summary":"  We introduce the Cambridge Law Corpus (CLC), a dataset for legal AI research.\nIt consists of over 250 000 court cases from the UK. Most cases are from the\n21st century, but the corpus includes cases as old as the 16th century. This\npaper presents the first release of the corpus, containing the raw text and\nmeta-data. Together with the corpus, we provide annotations on case outcomes\nfor 638 cases, done by legal experts. Using our annotated data, we have trained\nand evaluated case outcome extraction with GPT-3, GPT-4 and RoBERTa models to\nprovide benchmarks. We include an extensive legal and ethical discussion to\naddress the potentially sensitive nature of this material. As a consequence,\nthe corpus will only be released for research purposes under certain\nrestrictions.\n","authors":["Andreas Östling","Holli Sargeant","Huiyuan Xie","Ludwig Bull","Alexander Terenin","Leif Jonsson","Måns Magnusson","Felix Steffek"],"pdf_url":"https://arxiv.org/pdf/2309.12269v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04469v2","updated":"2024-01-01T23:18:29Z","published":"2023-12-07T17:41:44Z","title":"On the Learnability of Watermarks for Language Models","summary":"  Watermarking of language model outputs enables statistical detection of\nmodel-generated text, which has many applications in the responsible deployment\nof language models. Existing watermarking strategies operate by altering the\ndecoder of an existing language model, and the ability for a language model to\ndirectly learn to generate the watermark would have significant implications\nfor the real-world deployment of watermarks. First, learned watermarks could be\nused to build open models that naturally generate watermarked text, allowing\nfor open models to benefit from watermarking. Second, if watermarking is used\nto determine the provenance of generated text, an adversary can hurt the\nreputation of a victim model by spoofing its watermark and generating damaging\nwatermarked text. To investigate the learnability of watermarks, we propose\nwatermark distillation, which trains a student model to behave like a teacher\nmodel that uses decoding-based watermarking. We test our approach on three\ndistinct decoding-based watermarking strategies and various hyperparameter\nsettings, finding that models can learn to generate watermarked text with high\ndetectability. We also find limitations to learnability, including the loss of\nwatermarking capabilities under fine-tuning on normal text and high sample\ncomplexity when learning low-distortion watermarks.\n","authors":["Chenchen Gu","Xiang Lisa Li","Percy Liang","Tatsunori Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2312.04469v2.pdf","comment":"22 pages, 6 figures"},{"id":"http://arxiv.org/abs/2305.17760v6","updated":"2024-01-01T21:06:57Z","published":"2023-05-28T16:04:48Z","title":"Language Models are Bounded Pragmatic Speakers: Understanding RLHF from\n  a Bayesian Cognitive Modeling Perspective","summary":"  How do language models \"think\"? This paper formulates a probabilistic\ncognitive model called the bounded pragmatic speaker, which can characterize\nthe operation of different variations of language models. Specifically, we\ndemonstrate that large language models fine-tuned with reinforcement learning\nfrom human feedback (Ouyang et al., 2022) embody a model of thought that\nconceptually resembles a fast-and-slow model (Kahneman, 2011), which\npsychologists have attributed to humans. We discuss the limitations of\nreinforcement learning from human feedback as a fast-and-slow model of thought\nand propose avenues for expanding this framework. In essence, our research\nhighlights the value of adopting a cognitive probabilistic modeling approach to\ngain insights into the comprehension, evaluation, and advancement of language\nmodels.\n","authors":["Khanh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2305.17760v6.pdf","comment":"Proceedings of the First Workshop on Theory of Mind in Communicating\n  Agents at (TOM @ ICML 2023)"},{"id":"http://arxiv.org/abs/2401.00824v1","updated":"2024-01-01T17:48:25Z","published":"2024-01-01T17:48:25Z","title":"Graph-Convolutional Autoencoder Ensembles for the Humanities,\n  Illustrated with a Study of the American Slave Trade","summary":"  We introduce a graph-aware autoencoder ensemble framework, with associated\nformalisms and tooling, designed to facilitate deep learning for scholarship in\nthe humanities. By composing sub-architectures to produce a model isomorphic to\na humanistic domain we maintain interpretability while providing function\nsignatures for each sub-architectural choice, allowing both traditional and\ncomputational researchers to collaborate without disrupting established\npractices. We illustrate a practical application of our approach to a\nhistorical study of the American post-Atlantic slave trade, and make several\nspecific technical contributions: a novel hybrid graph-convolutional\nautoencoder mechanism, batching policies for common graph topologies, and\nmasking techniques for particular use-cases. The effectiveness of the framework\nfor broadening participation of diverse domains is demonstrated by a growing\nsuite of two dozen studies, both collaborations with humanists and established\ntasks from machine learning literature, spanning a variety of fields and data\nmodalities. We make performance comparisons of several different architectural\nchoices and conclude with an ambitious list of imminent next steps for this\nresearch.\n","authors":["Tom Lippincott"],"pdf_url":"https://arxiv.org/pdf/2401.00824v1.pdf","comment":"More in-depth technical companion to \"A general neural ensemble\n  technique to support traditional scholarship\", Digital Humanities 2020"},{"id":"http://arxiv.org/abs/2401.00820v1","updated":"2024-01-01T17:32:28Z","published":"2024-01-01T17:32:28Z","title":"A Computational Framework for Behavioral Assessment of LLM Therapists","summary":"  The emergence of ChatGPT and other large language models (LLMs) has greatly\nincreased interest in utilizing LLMs as therapists to support individuals\nstruggling with mental health challenges. However, due to the lack of\nsystematic studies, our understanding of how LLM therapists behave, i.e., ways\nin which they respond to clients, is significantly limited. Understanding their\nbehavior across a wide range of clients and situations is crucial to accurately\nassess their capabilities and limitations in the high-risk setting of mental\nhealth, where undesirable behaviors can lead to severe consequences. In this\npaper, we propose BOLT, a novel computational framework to study the\nconversational behavior of LLMs when employed as therapists. We develop an\nin-context learning method to quantitatively measure the behavior of LLMs based\non 13 different psychotherapy techniques including reflections, questions,\nsolutions, normalizing, and psychoeducation. Subsequently, we compare the\nbehavior of LLM therapists against that of high- and low-quality human therapy,\nand study how their behavior can be modulated to better reflect behaviors\nobserved in high-quality therapy. Our analysis of GPT and Llama-variants\nreveals that these LLMs often resemble behaviors more commonly exhibited in\nlow-quality therapy rather than high-quality therapy, such as offering a higher\ndegree of problem-solving advice when clients share emotions, which is against\ntypical recommendations. At the same time, unlike low-quality therapy, LLMs\nreflect significantly more upon clients' needs and strengths. Our analysis\nframework suggests that despite the ability of LLMs to generate anecdotal\nexamples that appear similar to human therapists, LLM therapists are currently\nnot fully consistent with high-quality care, and thus require additional\nresearch to ensure quality care.\n","authors":["Yu Ying Chiu","Ashish Sharma","Inna Wanyin Lin","Tim Althoff"],"pdf_url":"https://arxiv.org/pdf/2401.00820v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00812v1","updated":"2024-01-01T16:51:20Z","published":"2024-01-01T16:51:20Z","title":"If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code\n  Empowers Large Language Models to Serve as Intelligent Agents","summary":"  The prominent large language models (LLMs) of today differ from past language\nmodels not only in size, but also in the fact that they are trained on a\ncombination of natural language and formal language (code). As a medium between\nhumans and computers, code translates high-level goals into executable steps,\nfeaturing standard syntax, logical consistency, abstraction, and modularity. In\nthis survey, we present an overview of the various benefits of integrating code\ninto LLMs' training data. Specifically, beyond enhancing LLMs in code\ngeneration, we observe that these unique properties of code help (i) unlock the\nreasoning ability of LLMs, enabling their applications to a range of more\ncomplex natural language tasks; (ii) steer LLMs to produce structured and\nprecise intermediate steps, which can then be connected to external execution\nends through function calls; and (iii) take advantage of code compilation and\nexecution environment, which also provides diverse feedback for model\nimprovement. In addition, we trace how these profound capabilities of LLMs,\nbrought by code, have led to their emergence as intelligent agents (IAs) in\nsituations where the ability to understand instructions, decompose goals, plan\nand execute actions, and refine from feedback are crucial to their success on\ndownstream tasks. Finally, we present several key challenges and future\ndirections of empowering LLMs with code.\n","authors":["Ke Yang","Jiateng Liu","John Wu","Chaoqi Yang","Yi R. Fung","Sha Li","Zixuan Huang","Xu Cao","Xingyao Wang","Yiquan Wang","Heng Ji","Chengxiang Zhai"],"pdf_url":"https://arxiv.org/pdf/2401.00812v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00811v1","updated":"2024-01-01T16:42:56Z","published":"2024-01-01T16:42:56Z","title":"PerSHOP -- A Persian dataset for shopping dialogue systems modeling","summary":"  Nowadays, dialogue systems are used in many fields of industry and research.\nThere are successful instances of these systems, such as Apple Siri, Google\nAssistant, and IBM Watson. Task-oriented dialogue system is a category of\nthese, that are used in specific tasks. They can perform tasks such as booking\nplane tickets or making restaurant reservations. Shopping is one of the most\npopular areas on these systems. The bot replaces the human salesperson and\ninteracts with the customers by speaking. To train the models behind the scenes\nof these systems, annotated data is needed. In this paper, we developed a\ndataset of dialogues in the Persian language through crowd-sourcing. We\nannotated these dialogues to train a model. This dataset contains nearly 22k\nutterances in 15 different domains and 1061 dialogues. This is the largest\nPersian dataset in this field, which is provided freely so that future\nresearchers can use it. Also, we proposed some baseline models for natural\nlanguage understanding (NLU) tasks. These models perform two tasks for NLU:\nintent classification and entity extraction. The F-1 score metric obtained for\nintent classification is around 91% and for entity extraction is around 93%,\nwhich can be a baseline for future research.\n","authors":["Keyvan Mahmoudi","Heshaam Faili"],"pdf_url":"https://arxiv.org/pdf/2401.00811v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00793v1","updated":"2024-01-01T15:40:35Z","published":"2024-01-01T15:40:35Z","title":"SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for\n  Large Language Models","summary":"  With the growing use of large language models hosted on cloud platforms to\noffer inference services, privacy concerns are escalating, especially\nconcerning sensitive data like investment plans and bank account details.\nSecure Multi-Party Computing (SMPC) emerges as a promising solution to protect\nthe privacy of inference data and model parameters. However, the application of\nSMPC in Privacy-Preserving Inference (PPI) for large language models,\nparticularly those based on the Transformer architecture, often leads to\nconsiderable slowdowns or declines in performance. This is largely due to the\nmultitude of nonlinear operations in the Transformer architecture, which are\nnot well-suited to SMPC and are difficult to circumvent or optimize\neffectively. To address this concern, we introduce an advanced optimization\nframework called SecFormer, designed to strike an optimal balance between\nperformance and efficiency in PPI for Transformer models. By implementing\nknowledge distillation techniques, we successfully eliminate the high-cost\nexponential and maximum operations in PPI without sacrificing model\nperformance. Additionally, we have developed a suite of efficient SMPC\nprotocols that utilize segmented polynomials and Goldschmidt's method to handle\nother complex nonlinear functions within PPI, such as GeLU, LayerNorm, and\nSoftmax. Our extensive experiments reveal that SecFormer outperforms MPCFormer\nin performance, showing improvements of $5.6\\%$ and $24.2\\%$ for\nBERT$_{\\text{BASE}}$ and BERT$_{\\text{LARGE}}$, respectively. In terms of\nefficiency, SecFormer is 3.4 and 3.2 times faster than Puma, demonstrating its\neffectiveness and speed.\n","authors":["Jinglong Luo","Yehong Zhang","Jiaqi Zhang","Xin Mu","Hui Wang","Yue Yu","Zenglin Xu"],"pdf_url":"https://arxiv.org/pdf/2401.00793v1.pdf","comment":"12pages, 15figures"},{"id":"http://arxiv.org/abs/2401.00788v1","updated":"2024-01-01T15:30:19Z","published":"2024-01-01T15:30:19Z","title":"Astraios: Parameter-Efficient Instruction Tuning Code Large Language\n  Models","summary":"  The high cost of full-parameter fine-tuning (FFT) of Large Language Models\n(LLMs) has led to a series of parameter-efficient fine-tuning (PEFT) methods.\nHowever, it remains unclear which methods provide the best cost-performance\ntrade-off at different model scales. We introduce Astraios, a suite of 28\ninstruction-tuned OctoCoder models using 7 tuning methods and 4 model sizes up\nto 16 billion parameters. Through investigations across 5 tasks and 8 different\ndatasets encompassing both code comprehension and code generation tasks, we\nfind that FFT generally leads to the best downstream performance across all\nscales, and PEFT methods differ significantly in their efficacy based on the\nmodel scale. LoRA usually offers the most favorable trade-off between cost and\nperformance. Further investigation into the effects of these methods on both\nmodel robustness and code security reveals that larger models tend to\ndemonstrate reduced robustness and less security. At last, we explore the\nrelationships among updated parameters, cross-entropy loss, and task\nperformance. We find that the tuning effectiveness observed in small models\ngeneralizes well to larger models, and the validation loss in instruction\ntuning can be a reliable indicator of overall downstream performance.\n","authors":["Terry Yue Zhuo","Armel Zebaze","Nitchakarn Suppattarachai","Leandro von Werra","Harm de Vries","Qian Liu","Niklas Muennighoff"],"pdf_url":"https://arxiv.org/pdf/2401.00788v1.pdf","comment":"25 pages (12 main), 19 figures, 8 tables"},{"id":"http://arxiv.org/abs/2401.00779v1","updated":"2024-01-01T14:58:53Z","published":"2024-01-01T14:58:53Z","title":"Temporal Validity Change Prediction","summary":"  Temporal validity is an important property of text that is useful for many\ndownstream applications, such as recommender systems, conversational AI, or\nstory understanding. Existing benchmarking tasks often require models to\nidentify the temporal validity duration of a single statement. However, in many\ncases, additional contextual information, such as sentences in a story or posts\non a social media profile, can be collected from the available text stream.\nThis contextual information may greatly alter the duration for which a\nstatement is expected to be valid. We propose Temporal Validity Change\nPrediction, a natural language processing task benchmarking the capability of\nmachine learning models to detect contextual statements that induce such\nchange. We create a dataset consisting of temporal target statements sourced\nfrom Twitter and crowdsource sample context statements. We then benchmark a set\nof transformer-based language models on our dataset. Finally, we experiment\nwith temporal validity duration prediction as an auxiliary task to improve the\nperformance of the state-of-the-art model.\n","authors":["Georg Wenzel","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2401.00779v1.pdf","comment":"9 pages, 9 figures, 3 tables"},{"id":"http://arxiv.org/abs/2401.00763v1","updated":"2024-01-01T14:06:55Z","published":"2024-01-01T14:06:55Z","title":"New Job, New Gender? Measuring the Social Bias in Image Generation\n  Models","summary":"  Image generation models can generate or edit images from a given text. Recent\nadvancements in image generation technology, exemplified by DALL-E and\nMidjourney, have been groundbreaking. These advanced models, despite their\nimpressive capabilities, are often trained on massive Internet datasets, making\nthem susceptible to generating content that perpetuates social stereotypes and\nbiases, which can lead to severe consequences. Prior research on assessing bias\nwithin image generation models suffers from several shortcomings, including\nlimited accuracy, reliance on extensive human labor, and lack of comprehensive\nanalysis. In this paper, we propose BiasPainter, a novel metamorphic testing\nframework that can accurately, automatically and comprehensively trigger social\nbias in image generation models. BiasPainter uses a diverse range of seed\nimages of individuals and prompts the image generation models to edit these\nimages using gender, race, and age-neutral queries. These queries span 62\nprofessions, 39 activities, 57 types of objects, and 70 personality traits. The\nframework then compares the edited images to the original seed images, focusing\non any changes related to gender, race, and age. BiasPainter adopts a testing\noracle that these characteristics should not be modified when subjected to\nneutral prompts. Built upon this design, BiasPainter can trigger the social\nbias and evaluate the fairness of image generation models. To evaluate the\neffectiveness of BiasPainter, we use BiasPainter to test five widely-used\ncommercial image generation software and models, such as stable diffusion and\nMidjourney. Experimental results show that 100\\% of the generated test cases\ncan successfully trigger social bias in image generation models.\n","authors":["Wenxuan Wang","Haonan Bai","Jen-tse Huang","Yuxuan Wan","Youliang Yuan","Haoyi Qiu","Nanyun Peng","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2401.00763v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00761v1","updated":"2024-01-01T14:02:27Z","published":"2024-01-01T14:02:27Z","title":"The Earth is Flat? Unveiling Factual Errors in Large Language Models","summary":"  Large Language Models (LLMs) like ChatGPT are foundational in various\napplications due to their extensive knowledge from pre-training and\nfine-tuning. Despite this, they are prone to generating factual and commonsense\nerrors, raising concerns in critical areas like healthcare, journalism, and\neducation to mislead users. Current methods for evaluating LLMs' veracity are\nlimited by test data leakage or the need for extensive human labor, hindering\nefficient and accurate error detection. To tackle this problem, we introduce a\nnovel, automatic testing framework, FactChecker, aimed at uncovering factual\ninaccuracies in LLMs. This framework involves three main steps: First, it\nconstructs a factual knowledge graph by retrieving fact triplets from a\nlarge-scale knowledge database. Then, leveraging the knowledge graph,\nFactChecker employs a rule-based approach to generates three types of questions\n(Yes-No, Multiple-Choice, and WH questions) that involve single-hop and\nmulti-hop relations, along with correct answers. Lastly, it assesses the LLMs'\nresponses for accuracy using tailored matching strategies for each question\ntype. Our extensive tests on six prominent LLMs, including text-davinci-002,\ntext-davinci-003, ChatGPT~(gpt-3.5-turbo, gpt-4), Vicuna, and LLaMA-2, reveal\nthat FactChecker can trigger factual errors in up to 45\\% of questions in these\nmodels. Moreover, we demonstrate that FactChecker's test cases can improve\nLLMs' factual accuracy through in-context learning and fine-tuning (e.g.,\nllama-2-13b-chat's accuracy increase from 35.3\\% to 68.5\\%). We are making all\ncode, data, and results available for future research endeavors.\n","authors":["Wenxuan Wang","Juluan Shi","Zhaopeng Tu","Youliang Yuan","Jen-tse Huang","Wenxiang Jiao","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2401.00761v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00757v1","updated":"2024-01-01T13:53:53Z","published":"2024-01-01T13:53:53Z","title":"A & B == B & A: Triggering Logical Reasoning Failures in Large Language\n  Models","summary":"  Recent advancements in large language models (LLMs) have propelled Artificial\nIntelligence (AI) to new heights, enabling breakthroughs in various tasks such\nas writing assistance, code generation, and machine translation. A significant\ndistinction of advanced LLMs, such as ChatGPT, is their demonstrated ability to\n\"reason.\" However, evaluating the reasoning ability of LLMs remains a challenge\nas most existing evaluations focus on their accuracy on the downstream tasks\nrather than directly assessing their reasoning processes. Efforts have been\nmade to develop benchmarks and metrics to assess reasoning in LLMs, but they\nsuffer from data leakage or limited scope. In this paper, we introduce\nLogicAsker, an automatic approach that comprehensively evaluates and improves\nthe logical reasoning abilities of LLMs under a set of atomic reasoning skills\nbased on propositional and predicate logic. The results provide insights into\nLLMs' reasoning abilities and reveal the logical rules the LLMs did not learn\nwell. We evaluate LogicAsker on six widely deployed LLMs, including GPT-3,\nChatGPT, GPT-4, Bard, Vicuna, and Guanaco. The results show that test cases\nfrom LogicAsker can find logical reasoning failures in different LLMs with a\nrate of 25\\% - 94\\%. In addition, the test cases of LogicAsker can be further\nused to design demonstration examples for in-context learning, which\neffectively improves the logical reasoning ability of LLMs, e.g., 10\\% for\nGPT-4. As far as we know, our work is the first to create prompts based on\ntesting results to improve LLMs' formal reasoning ability effectively. All the\ncode, data, and results will be released for reproduction and future research.\n","authors":["Yuxuan Wan","Wenxuan Wang","Yiliu Yang","Youliang Yuan","Jen-tse Huang","Pinjia He","Wenxiang Jiao","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2401.00757v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00751v1","updated":"2024-01-01T13:28:46Z","published":"2024-01-01T13:28:46Z","title":"Machine Translation Testing via Syntactic Tree Pruning","summary":"  Machine translation systems have been widely adopted in our daily life,\nmaking life easier and more convenient. Unfortunately, erroneous translations\nmay result in severe consequences, such as financial losses. This requires to\nimprove the accuracy and the reliability of machine translation systems.\nHowever, it is challenging to test machine translation systems because of the\ncomplexity and intractability of the underlying neural models. To tackle these\nchallenges, we propose a novel metamorphic testing approach by syntactic tree\npruning (STP) to validate machine translation systems. Our key insight is that\na pruned sentence should have similar crucial semantics compared with the\noriginal sentence. Specifically, STP (1) proposes a core semantics-preserving\npruning strategy by basic sentence structure and dependency relations on the\nlevel of syntactic tree representation; (2) generates source sentence pairs\nbased on the metamorphic relation; (3) reports suspicious issues whose\ntranslations break the consistency property by a bag-of-words model. We further\nevaluate STP on two state-of-the-art machine translation systems (i.e., Google\nTranslate and Bing Microsoft Translator) with 1,200 source sentences as inputs.\nThe results show that STP can accurately find 5,073 unique erroneous\ntranslations in Google Translate and 5,100 unique erroneous translations in\nBing Microsoft Translator (400% more than state-of-the-art techniques), with\n64.5% and 65.4% precision, respectively. The reported erroneous translations\nvary in types and more than 90% of them cannot be found by state-of-the-art\ntechniques. There are 9,393 erroneous translations unique to STP, which is\n711.9% more than state-of-the-art techniques. Moreover, STP is quite effective\nto detect translation errors for the original sentences with a recall reaching\n74.0%, improving state-of-the-art techniques by 55.1% on average.\n","authors":["Quanjun Zhang","Juan Zhai","Chunrong Fang","Jiawei Liu","Weisong Sun","Haichuan Hu","Qingyu Wang"],"pdf_url":"https://arxiv.org/pdf/2401.00751v1.pdf","comment":"Accepted to ACM Transactions on Software Engineering and Methodology\n  2024 (TOSEM'24)"},{"id":"http://arxiv.org/abs/2401.00741v1","updated":"2024-01-01T12:49:36Z","published":"2024-01-01T12:49:36Z","title":"ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of\n  Large Language Models in Real-world Scenarios","summary":"  Existing evaluations of tool learning primarily focus on validating the\nalignment of selected tools for large language models (LLMs) with expected\noutcomes. However, these approaches rely on a limited set of scenarios where\nanswers can be pre-determined, diverging from genuine needs. Furthermore, a\nsole emphasis on outcomes disregards the intricate capabilities essential for\nLLMs to effectively utilize tools. To tackle this issue, we propose ToolEyes, a\nfine-grained system tailored for the evaluation of the LLMs' tool learning\ncapabilities in authentic scenarios. The system meticulously examines seven\nreal-world scenarios, analyzing five dimensions crucial to LLMs in tool\nlearning: format alignment, intent comprehension, behavior planning, tool\nselection, and answer organization. Additionally, ToolEyes incorporates a tool\nlibrary boasting approximately 600 tools, serving as an intermediary between\nLLMs and the physical world. Evaluations involving ten LLMs across three\ncategories reveal a preference for specific scenarios and limited cognitive\nabilities in tool learning. Intriguingly, expanding the model size even\nexacerbates the hindrance to tool learning. These findings offer instructive\ninsights aimed at advancing the field of tool learning. The data is available\natt https://github.com/Junjie-Ye/ToolEyes.git.\n","authors":["Junjie Ye","Guanyu Li","Songyang Gao","Caishuang Huang","Yilong Wu","Sixian Li","Xiaoran Fan","Shihan Dou","Qi Zhang","Tao Gui","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2401.00741v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14557v2","updated":"2024-01-01T09:24:47Z","published":"2023-12-22T09:30:41Z","title":"Aurora:Activating Chinese chat capability for Mixtral-8x7B sparse\n  Mixture-of-Experts through Instruction-Tuning","summary":"  Existing research has demonstrated that refining large language models (LLMs)\nthrough the utilization of machine-generated instruction-following data\nempowers these models to exhibit impressive zero-shot capabilities for novel\ntasks, without requiring human-authored instructions. In this paper, we\nsystematically investigate, preprocess, and integrate three Chinese\ninstruction-following datasets with the aim of enhancing the Chinese\nconversational capabilities of Mixtral-8x7B sparse Mixture-of-Experts model.\nThrough instruction fine-tuning on this carefully processed dataset, we\nsuccessfully construct the Mixtral-8x7B sparse Mixture-of-Experts model named\n\"Aurora.\" To assess the performance of Aurora, we utilize three widely\nrecognized benchmark tests: C-Eval, MMLU, and CMMLU. Empirical studies validate\nthe effectiveness of instruction fine-tuning applied to Mixtral-8x7B sparse\nMixture-of-Experts model. This work is pioneering in the execution of\ninstruction fine-tuning on a sparse expert-mixed model, marking a significant\nbreakthrough in enhancing the capabilities of this model architecture. Our\ncode, data and model are publicly available at\n  https://github.com/WangRongsheng/Aurora\n","authors":["Rongsheng Wang","Haoming Chen","Ruizhe Zhou","Yaofei Duan","Kunyan Cai","Han Ma","Jiaxi Cui","Jian Li","Patrick Cheong-Iao Pang","Yapeng Wang","Tao Tan"],"pdf_url":"https://arxiv.org/pdf/2312.14557v2.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2312.17660v2","updated":"2024-01-01T08:59:40Z","published":"2023-12-29T15:56:24Z","title":"Normalization of Lithuanian Text Using Regular Expressions","summary":"  Text Normalization is an integral part of any text-to-speech synthesis\nsystem. In a natural language text, there are elements such as numbers, dates,\nabbreviations, etc. that belong to other semiotic classes. They are called\nnon-standard words (NSW) and need to be expanded into ordinary words. For this\npurpose, it is necessary to identify the semiotic class of each NSW. The\ntaxonomy of semiotic classes adapted to the Lithuanian language is presented in\nthe work. Sets of rules are created for detecting and expanding NSWs based on\nregular expressions. Experiments with three completely different data sets were\nperformed and the accuracy was assessed. Causes of errors are explained and\nrecommendations are given for the development of text normalization rules.\n","authors":["Pijus Kasparaitis"],"pdf_url":"https://arxiv.org/pdf/2312.17660v2.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2401.00698v1","updated":"2024-01-01T08:32:50Z","published":"2024-01-01T08:32:50Z","title":"Large Language Models aren't all that you need","summary":"  This paper describes the architecture and systems built towards solving the\nSemEval 2023 Task 2: MultiCoNER II (Multilingual Complex Named Entity\nRecognition) [1]. We evaluate two approaches (a) a traditional Conditional\nRandom Fields model and (b) a Large Language Model (LLM) fine-tuned with a\ncustomized head and compare the two approaches. The novel ideas explored are:\n1) Decaying auxiliary loss (with residual) - where we train the model on an\nauxiliary task of Coarse-Grained NER and include this task as a part of the\nloss function 2) Triplet token blending - where we explore ways of blending the\nembeddings of neighboring tokens in the final NER layer prior to prediction 3)\nTask-optimal heads - where we explore a variety of custom heads and learning\nrates for the final layer of the LLM. We also explore multiple LLMs including\nGPT-3 and experiment with a variety of dropout and other hyperparameter\nsettings before arriving at our final model which achieves micro & macro f1 of\n0.85/0.84 (on dev) and 0.67/0.61 on the test data . We show that while\npre-trained LLMs, by themselves, bring about a large improvement in scores as\ncompared to traditional models, we also demonstrate that tangible improvements\nto the Macro-F1 score can be made by augmenting the LLM with additional\nfeature/loss/model engineering techniques described above.\n","authors":["Kiran Voderhobli Holla","Chaithanya Kumar","Aryan Singh"],"pdf_url":"https://arxiv.org/pdf/2401.00698v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00690v1","updated":"2024-01-01T07:35:31Z","published":"2024-01-01T07:35:31Z","title":"Benchmarking Large Language Models on Controllable Generation under\n  Diversified Instructions","summary":"  While large language models (LLMs) have exhibited impressive\ninstruction-following capabilities, it is still unclear whether and to what\nextent they can respond to explicit constraints that might be entailed in\nvarious instructions. As a significant aspect of LLM alignment, it is thus\nimportant to formulate such a specialized set of instructions as well as\ninvestigate the resulting behavior of LLMs. To address this vacancy, we propose\na new benchmark CoDI-Eval to systematically and comprehensively evaluate LLMs'\nresponses to instructions with various constraints. We construct a large\ncollection of constraints-attributed instructions as a test suite focused on\nboth generalization and coverage. Specifically, we advocate an instruction\ndiversification process to synthesize diverse forms of constraint expression\nand also deliberate the candidate task taxonomy with even finer-grained\nsub-categories. Finally, we automate the entire evaluation process to\nfacilitate further developments. Different from existing studies on\ncontrollable text generation, CoDI-Eval extends the scope to the prevalent\ninstruction-following paradigm for the first time. We provide extensive\nevaluations of representative LLMs (e.g., ChatGPT, Vicuna) on CoDI-Eval,\nrevealing their limitations in following instructions with specific constraints\nand there is still a significant gap between open-source and commercial\nclosed-source LLMs. We believe this benchmark will facilitate research into\nimproving the controllability of LLMs' responses to instructions. Our data and\ncode are available at https://github.com/Xt-cyh/CoDI-Eval.\n","authors":["Yihan Chen","Benfeng Xu","Quan Wang","Yi Liu","Zhendong Mao"],"pdf_url":"https://arxiv.org/pdf/2401.00690v1.pdf","comment":"Accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2401.00689v1","updated":"2024-01-01T07:35:29Z","published":"2024-01-01T07:35:29Z","title":"Large language model for Bible sentiment analysis: Sermon on the Mount","summary":"  The revolution of natural language processing via large language models has\nmotivated its use in multidisciplinary areas that include social sciences and\nhumanities and more specifically, comparative religion. Sentiment analysis\nprovides a mechanism to study the emotions expressed in text. Recently,\nsentiment analysis has been used to study and compare translations of the\nBhagavad Gita, which is a fundamental and sacred Hindu text. In this study, we\nuse sentiment analysis for studying selected chapters of the Bible. These\nchapters are known as the Sermon on the Mount. We utilize a pre-trained\nlanguage model for sentiment analysis by reviewing five translations of the\nSermon on the Mount, which include the King James version, the New\nInternational Version, the New Revised Standard Version, the Lamsa Version, and\nthe Basic English Version. We provide a chapter-by-chapter and verse-by-verse\ncomparison using sentiment and semantic analysis and review the major\nsentiments expressed. Our results highlight the varying sentiments across the\nchapters and verses. We found that the vocabulary of the respective\ntranslations is significantly different. We detected different levels of\nhumour, optimism, and empathy in the respective chapters that were used by\nJesus to deliver his message.\n","authors":["Mahek Vora","Tom Blau","Vansh Kachhwal","Ashu M. G. Solo","Rohitash Chandra"],"pdf_url":"https://arxiv.org/pdf/2401.00689v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00676v1","updated":"2024-01-01T06:04:52Z","published":"2024-01-01T06:04:52Z","title":"Digger: Detecting Copyright Content Mis-usage in Large Language Model\n  Training","summary":"  Pre-training, which utilizes extensive and varied datasets, is a critical\nfactor in the success of Large Language Models (LLMs) across numerous\napplications. However, the detailed makeup of these datasets is often not\ndisclosed, leading to concerns about data security and potential misuse. This\nis particularly relevant when copyrighted material, still under legal\nprotection, is used inappropriately, either intentionally or unintentionally,\ninfringing on the rights of the authors.\n  In this paper, we introduce a detailed framework designed to detect and\nassess the presence of content from potentially copyrighted books within the\ntraining datasets of LLMs. This framework also provides a confidence estimation\nfor the likelihood of each content sample's inclusion. To validate our\napproach, we conduct a series of simulated experiments, the results of which\naffirm the framework's effectiveness in identifying and addressing instances of\ncontent misuse in LLM training processes. Furthermore, we investigate the\npresence of recognizable quotes from famous literary works within these\ndatasets. The outcomes of our study have significant implications for ensuring\nthe ethical use of copyrighted materials in the development of LLMs,\nhighlighting the need for more transparent and responsible data management\npractices in this field.\n","authors":["Haodong Li","Gelei Deng","Yi Liu","Kailong Wang","Yuekang Li","Tianwei Zhang","Yang Liu","Guoai Xu","Guosheng Xu","Haoyu Wang"],"pdf_url":"https://arxiv.org/pdf/2401.00676v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00642v1","updated":"2024-01-01T03:04:14Z","published":"2024-01-01T03:04:14Z","title":"Predicting Anti-microbial Resistance using Large Language Models","summary":"  During times of increasing antibiotic resistance and the spread of infectious\ndiseases like COVID-19, it is important to classify genes related to antibiotic\nresistance. As natural language processing has advanced with transformer-based\nlanguage models, many language models that learn characteristics of nucleotide\nsequences have also emerged. These models show good performance in classifying\nvarious features of nucleotide sequences. When classifying nucleotide\nsequences, not only the sequence itself, but also various background knowledge\nis utilized. In this study, we use not only a nucleotide sequence-based\nlanguage model but also a text language model based on PubMed articles to\nreflect more biological background knowledge in the model. We propose a method\nto fine-tune the nucleotide sequence language model and the text language model\nbased on various databases of antibiotic resistance genes. We also propose an\nLLM-based augmentation technique to supplement the data and an ensemble method\nto effectively combine the two models. We also propose a benchmark for\nevaluating the model. Our method achieved better performance than the\nnucleotide sequence language model in the drug resistance class prediction.\n","authors":["Hyunwoo Yoo","Bahrad Sokhansanj","James R. Brown","Gail Rosen"],"pdf_url":"https://arxiv.org/pdf/2401.00642v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.02478v2","updated":"2024-01-01T02:10:43Z","published":"2023-03-10T14:36:47Z","title":"Exploring AI-Generated Text in Student Writing: How Does AI Help?","summary":"  English as foreign language_EFL_students' use of text generated from\nartificial intelligence_AI_natural language generation_NLG_tools may improve\ntheir writing quality. However, it remains unclear to what extent AI-generated\ntext in these students' writing might lead to higher-quality writing. We\nexplored 23 Hong Kong secondary school students' attempts to write stories\ncomprising their own words and AI-generated text. Human experts scored the\nstories for dimensions of content, language and organization. We analyzed the\nbasic organization and structure and syntactic complexity of the stories'\nAI-generated text and performed multiple linear regression and cluster\nanalyses. The results show the number of human words and the number of\nAI-generated words contribute significantly to scores. Besides, students can be\ngrouped into competent and less competent writers who use more AI-generated\ntext or less AI-generated text compared to their peers. Comparisons of clusters\nreveal some benefit of AI-generated text in improving the quality of both\nhigh-scoring students' and low-scoring students' writing. The findings can\ninform pedagogical strategies to use AI-generated text for EFL students'\nwriting and to address digital divides. This study contributes designs of NLG\ntools and writing activities to implement AI-generated text in schools.\n","authors":["David James Woo","Hengky Susanto","Chi Ho Yeung","Kai Guo","April Ka Yeng Fung"],"pdf_url":"https://arxiv.org/pdf/2304.02478v2.pdf","comment":"45 pages, 11 figures, 3 tables"},{"id":"http://arxiv.org/abs/2208.09709v2","updated":"2024-01-01T01:35:08Z","published":"2022-08-20T15:21:35Z","title":"BSpell: A CNN-Blended BERT Based Bangla Spell Checker","summary":"  Bangla typing is mostly performed using English keyboard and can be highly\nerroneous due to the presence of compound and similarly pronounced letters.\nSpelling correction of a misspelled word requires understanding of word typing\npattern as well as the context of the word usage. A specialized BERT model\nnamed BSpell has been proposed in this paper targeted towards word for word\ncorrection in sentence level. BSpell contains an end-to-end trainable CNN\nsub-model named SemanticNet along with specialized auxiliary loss. This allows\nBSpell to specialize in highly inflected Bangla vocabulary in the presence of\nspelling errors. Furthermore, a hybrid pretraining scheme has been proposed for\nBSpell that combines word level and character level masking. Comparison on two\nBangla and one Hindi spelling correction dataset shows the superiority of our\nproposed approach. BSpell is available as a Bangla spell checking tool via\nGitHub: https://github.com/Hasiburshanto/Bangla-Spell-Checker\n","authors":["Chowdhury Rafeed Rahman","MD. Hasibur Rahman","Samiha Zakir","Mohammad Rafsan","Mohammed Eunus Ali"],"pdf_url":"https://arxiv.org/pdf/2208.09709v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02938v1","updated":"2024-01-01T23:10:23Z","published":"2024-01-01T23:10:23Z","title":"Fast and Optimal Weight Update for Pruned Large Language Models","summary":"  Pruning large language models (LLMs) is a challenging task due to their\nenormous size. The primary difficulty is fine-tuning the model after pruning,\nwhich is needed to recover the lost performance caused by dropping weights.\nRecent approaches have either ignored fine-tuning entirely, focusing on\nefficient pruning criteria, or attempted layer-wise weight updates, preserving\nthe behavior of each layer. However, even layer-wise weight updates can be\ncostly for LLMs, and previous works have resorted to various approximations.\n  In our paper, we propose a fast and optimal weight update algorithm for\npruned layers based on the Alternating Direction Method of Multipliers (ADMM).\nCoupled with a simple iterative pruning mask selection, our algorithm achieves\nstate-of-the-art pruning performance across a wide range of LLMs. Code is\navailable at https://github.com/fmfi-compbio/admm-pruning.\n","authors":["Vladimír Boža"],"pdf_url":"https://arxiv.org/pdf/2401.02938v1.pdf","comment":null}],"Robotics":[{"id":"http://arxiv.org/abs/2310.16542v2","updated":"2024-01-01T18:26:05Z","published":"2023-10-25T10:45:38Z","title":"ParisLuco3D: A high-quality target dataset for domain generalization of\n  LiDAR perception","summary":"  LiDAR is an essential sensor for autonomous driving by collecting precise\ngeometric information regarding a scene. As the performance of various LiDAR\nperception tasks has improved, generalizations to new environments and sensors\nhas emerged to test these optimized models in real-world conditions.\nUnfortunately, the various annotation strategies of data providers complicate\nthe computation of cross-domain performances.\n  This paper provides a novel dataset, ParisLuco3D, specifically designed for\ncross-domain evaluation to make it easier to evaluate the performance utilizing\nvarious source datasets. Alongside the dataset, online benchmarks for LiDAR\nsemantic segmentation, LiDAR object detection, and LiDAR tracking are provided\nto ensure a fair comparison across methods.\n  The ParisLuco3D dataset, evaluation scripts, and links to benchmarks can be\nfound at the following website: https://npm3d.fr/parisluco3d\n","authors":["Jules Sanchez","Louis Soum-Fontez","Jean-Emmanuel Deschaud","Francois Goulette"],"pdf_url":"https://arxiv.org/pdf/2310.16542v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00929v1","updated":"2024-01-01T18:20:43Z","published":"2024-01-01T18:20:43Z","title":"GenH2R: Learning Generalizable Human-to-Robot Handover via Scalable\n  Simulation, Demonstration, and Imitation","summary":"  This paper presents GenH2R, a framework for learning generalizable\nvision-based human-to-robot (H2R) handover skills. The goal is to equip robots\nwith the ability to reliably receive objects with unseen geometry handed over\nby humans in various complex trajectories. We acquire such generalizability by\nlearning H2R handover at scale with a comprehensive solution including\nprocedural simulation assets creation, automated demonstration generation, and\neffective imitation learning. We leverage large-scale 3D model repositories,\ndexterous grasp generation methods, and curve-based 3D animation to create an\nH2R handover simulation environment named \\simabbns, surpassing the number of\nscenes in existing simulators by three orders of magnitude. We further\nintroduce a distillation-friendly demonstration generation method that\nautomatically generates a million high-quality demonstrations suitable for\nlearning. Finally, we present a 4D imitation learning method augmented by a\nfuture forecasting objective to distill demonstrations into a visuo-motor\nhandover policy. Experimental evaluations in both simulators and the real world\ndemonstrate significant improvements (at least +10\\% success rate) over\nbaselines in all cases. The project page is https://GenH2R.github.io/.\n","authors":["Zifan Wang","Junyu Chen","Ziqing Chen","Pengwei Xie","Rui Chen","Li Yi"],"pdf_url":"https://arxiv.org/pdf/2401.00929v1.pdf","comment":"The project page is https://GenH2R.github.io/"},{"id":"http://arxiv.org/abs/2401.00776v1","updated":"2024-01-01T14:45:19Z","published":"2024-01-01T14:45:19Z","title":"Edge Computing based Human-Robot Cognitive Fusion: A Medical Case Study\n  in the Autism Spectrum Disorder Therapy","summary":"  In recent years, edge computing has served as a paradigm that enables many\nfuture technologies like AI, Robotics, IoT, and high-speed wireless sensor\nnetworks (like 5G) by connecting cloud computing facilities and services to the\nend users. Especially in medical and healthcare applications, it provides\nremote patient monitoring and increases voluminous multimedia. From the\nrobotics angle, robot-assisted therapy (RAT) is an active-assistive robotic\ntechnology in rehabilitation robotics, attracting many researchers to study and\nbenefit people with disability like autism spectrum disorder (ASD) children.\nHowever, the main challenge of RAT is that the model capable of detecting the\naffective states of ASD people exists and can recall individual preferences.\nMoreover, involving expert diagnosis and recommendations to guide robots in\nupdating the therapy approach to adapt to different statuses and scenarios is a\ncrucial part of the ASD therapy process. This paper proposes the architecture\nof edge cognitive computing by combining human experts and assisted robots\ncollaborating in the same framework to help ASD patients with long-term\nsupport. By integrating the real-time computing and analysis of a new cognitive\nrobotic model for ASD therapy, the proposed architecture can achieve a seamless\nremote diagnosis, round-the-clock symptom monitoring, emergency warning,\ntherapy alteration, and advanced assistance.\n","authors":["Qin Yang"],"pdf_url":"https://arxiv.org/pdf/2401.00776v1.pdf","comment":"This paper was accepted by the 38th AAAI 2024 workshop: \"Cooperative\n  Multi-Agent Systems Decision-Making and Learning: From Individual Needs to\n  Swarm Intelligence\""},{"id":"http://arxiv.org/abs/2304.08842v3","updated":"2024-01-01T13:56:49Z","published":"2023-04-18T09:13:52Z","title":"UDTIRI: An Online Open-Source Intelligent Road Inspection Benchmark\n  Suite","summary":"  In the nascent domain of urban digital twins (UDT), the prospects for\nleveraging cutting-edge deep learning techniques are vast and compelling.\nParticularly within the specialized area of intelligent road inspection (IRI),\na noticeable gap exists, underscored by the current dearth of dedicated\nresearch efforts and the lack of large-scale well-annotated datasets. To foster\nadvancements in this burgeoning field, we have launched an online open-source\nbenchmark suite, referred to as UDTIRI. Along with this article, we introduce\nthe road pothole detection task, the first online competition published within\nthis benchmark suite. This task provides a well-annotated dataset, comprising\n1,000 RGB images and their pixel/instance-level ground-truth annotations,\ncaptured in diverse real-world scenarios under different illumination and\nweather conditions. Our benchmark provides a systematic and thorough evaluation\nof state-of-the-art object detection, semantic segmentation, and instance\nsegmentation networks, developed based on either convolutional neural networks\nor Transformers. We anticipate that our benchmark will serve as a catalyst for\nthe integration of advanced UDT techniques into IRI. By providing algorithms\nwith a more comprehensive understanding of diverse road conditions, we seek to\nunlock their untapped potential and foster innovation in this critical domain.\n","authors":["Sicen Guo","Jiahang Li","Yi Feng","Dacheng Zhou","Denghuang Zhang","Chen Chen","Shuai Su","Xingyi Zhu","Qijun Chen","Rui Fan"],"pdf_url":"https://arxiv.org/pdf/2304.08842v3.pdf","comment":"Database webpage: https://www.udtiri.com/, Kaggle webpage:\n  https://www.kaggle.com/datasets/jiahangli617/udtiri"},{"id":"http://arxiv.org/abs/2401.00917v1","updated":"2024-01-01T07:03:15Z","published":"2024-01-01T07:03:15Z","title":"Fast and Continual Learning for Hybrid Control Policies using\n  Generalized Benders Decomposition","summary":"  Hybrid model predictive control with both continuous and discrete variables\nis widely applicable to robotic control tasks, especially those involving\ncontact with the environment. Due to the combinatorial complexity, the solving\nspeed of hybrid MPC can be insufficient for real-time applications. In this\npaper, we proposed a hybrid MPC solver based on Generalized Benders\nDecomposition (GBD). The algorithm enumerates and stores cutting planes online\ninside a finite buffer. After a short cold-start phase, the stored cuts provide\nwarm-starts for the new problem instances to enhance the solving speed. Despite\nthe disturbance and randomly changing environment, the solving speed maintains.\nLeveraging on the sparsity of feasibility cuts, we also propose a fast\nalgorithm for Benders master problems. Our solver is validated through\ncontrolling a cart-pole system with randomly moving soft contact walls, and a\nfree-flying robot navigating around obstacles. The results show that with\nsignificantly less data than previous works, the solver reaches competitive\nspeeds to the off-the-shelf solver Gurobi despite the Python overhead.\n","authors":["Xuan Lin"],"pdf_url":"https://arxiv.org/pdf/2401.00917v1.pdf","comment":"A more complete version of the previous paper \"Generalized Benders\n  Decomposition with Continual Learning for Hybrid Model Predictive Control in\n  Dynamic Environment\". arXiv admin note: substantial text overlap with\n  arXiv:2310.03344"},{"id":"http://arxiv.org/abs/2401.00678v1","updated":"2024-01-01T06:15:16Z","published":"2024-01-01T06:15:16Z","title":"General-purpose foundation models for increased autonomy in\n  robot-assisted surgery","summary":"  The dominant paradigm for end-to-end robot learning focuses on optimizing\ntask-specific objectives that solve a single robotic problem such as picking up\nan object or reaching a target position. However, recent work on high-capacity\nmodels in robotics has shown promise toward being trained on large collections\nof diverse and task-agnostic datasets of video demonstrations. These models\nhave shown impressive levels of generalization to unseen circumstances,\nespecially as the amount of data and the model complexity scale. Surgical robot\nsystems that learn from data have struggled to advance as quickly as other\nfields of robot learning for a few reasons: (1) there is a lack of existing\nlarge-scale open-source data to train models, (2) it is challenging to model\nthe soft-body deformations that these robots work with during surgery because\nsimulation cannot match the physical and visual complexity of biological\ntissue, and (3) surgical robots risk harming patients when tested in clinical\ntrials and require more extensive safety measures. This perspective article\naims to provide a path toward increasing robot autonomy in robot-assisted\nsurgery through the development of a multi-modal, multi-task,\nvision-language-action model for surgical robots. Ultimately, we argue that\nsurgical robots are uniquely positioned to benefit from general-purpose models\nand provide three guiding actions toward increased autonomy in robot-assisted\nsurgery.\n","authors":["Samuel Schmidgall","Ji Woong Kim","Alan Kuntz","Ahmed Ezzat Ghazi","Axel Krieger"],"pdf_url":"https://arxiv.org/pdf/2401.00678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00637v1","updated":"2024-01-01T02:25:39Z","published":"2024-01-01T02:25:39Z","title":"Nonlinear vibration of a dipteran flight robot system with rotational\n  geometric nonlinearity","summary":"  The dipteran flight mechanism of the insects is commonly used to design the\nnonlinear flight robot system. However, the dynamic response of the click\nmechanism of the nonlinear robot system with multiple stability still unclear.\nIn this paper, a novel dipteran robot model with click mechanism proposed based\non the multiple stability of snap-through buckling. The motion of equation of\nthe nonlinear flight robot system is obtained by using the Euler-Lagrange\nequation. The nonlinear potential energy, the elastic force, equilibrium\nbifurcation, as well as equilibrium stability are investigated to show the\nmultiple stability characteristics. The transient sets of bifurcation and\npersistent set of regions in the system parameter plane and the corresponding\nphase portraits are obtained with multiple stability of single and double well\nbehaviors. Then, the periodic free vibration response are defined by the\nanalytical solution of three kinds of elliptical functions, as well as the\namplitude frequency responses are investigated by numerical integration. Based\non the topological equivalent method, the chaotic thresholds of the homo-clinic\norbits for the chaotic vibration of harmonic forced robot system are derived to\nshow the chaotic parametric condition. Finally, the prototype of nonlinear\nflapping robot is manufactured and the experimental system is setup. The\nnonlinear static moment of force curves, periodic response and dynamic flight\nvibration of dipteran robot system are carried out. It is shown that the test\nresults are agree well with the theoretical analysis and numerical simulation.\nThose result have the potential application for the structure design of the\nefficient flight robot.\n","authors":["Yanwei Han","Zijian Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.00637v1.pdf","comment":"30 pages, 24 figure"}],"Multimedia":[{"id":"http://arxiv.org/abs/2309.14181v3","updated":"2024-01-01T14:48:48Z","published":"2023-09-25T14:43:43Z","title":"Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level\n  Vision","summary":"  The rapid evolution of Multi-modality Large Language Models (MLLMs) has\ncatalyzed a shift in computer vision from specialized models to general-purpose\nfoundation models. Nevertheless, there is still an inadequacy in assessing the\nabilities of MLLMs on low-level visual perception and understanding. To address\nthis gap, we present Q-Bench, a holistic benchmark crafted to systematically\nevaluate potential abilities of MLLMs on three realms: low-level visual\nperception, low-level visual description, and overall visual quality\nassessment. a) To evaluate the low-level perception ability, we construct the\nLLVisionQA dataset, consisting of 2,990 diverse-sourced images, each equipped\nwith a human-asked question focusing on its low-level attributes. We then\nmeasure the correctness of MLLMs on answering these questions. b) To examine\nthe description ability of MLLMs on low-level information, we propose the\nLLDescribe dataset consisting of long expert-labelled golden low-level text\ndescriptions on 499 images, and a GPT-involved comparison pipeline between\noutputs of MLLMs and the golden descriptions. c) Besides these two tasks, we\nfurther measure their visual quality assessment ability to align with human\nopinion scores. Specifically, we design a softmax-based strategy that enables\nMLLMs to predict quantifiable quality scores, and evaluate them on various\nexisting image quality assessment (IQA) datasets. Our evaluation across the\nthree abilities confirms that MLLMs possess preliminary low-level visual\nskills. However, these skills are still unstable and relatively imprecise,\nindicating the need for specific enhancements on MLLMs towards these abilities.\nWe hope that our benchmark can encourage the research community to delve deeper\nto discover and enhance these untapped potentials of MLLMs. Project Page:\nhttps://q-future.github.io/Q-Bench.\n","authors":["Haoning Wu","Zicheng Zhang","Erli Zhang","Chaofeng Chen","Liang Liao","Annan Wang","Chunyi Li","Wenxiu Sun","Qiong Yan","Guangtao Zhai","Weisi Lin"],"pdf_url":"https://arxiv.org/pdf/2309.14181v3.pdf","comment":"27 pages, 11 tables, with updated results"},{"id":"http://arxiv.org/abs/2401.00763v1","updated":"2024-01-01T14:06:55Z","published":"2024-01-01T14:06:55Z","title":"New Job, New Gender? Measuring the Social Bias in Image Generation\n  Models","summary":"  Image generation models can generate or edit images from a given text. Recent\nadvancements in image generation technology, exemplified by DALL-E and\nMidjourney, have been groundbreaking. These advanced models, despite their\nimpressive capabilities, are often trained on massive Internet datasets, making\nthem susceptible to generating content that perpetuates social stereotypes and\nbiases, which can lead to severe consequences. Prior research on assessing bias\nwithin image generation models suffers from several shortcomings, including\nlimited accuracy, reliance on extensive human labor, and lack of comprehensive\nanalysis. In this paper, we propose BiasPainter, a novel metamorphic testing\nframework that can accurately, automatically and comprehensively trigger social\nbias in image generation models. BiasPainter uses a diverse range of seed\nimages of individuals and prompts the image generation models to edit these\nimages using gender, race, and age-neutral queries. These queries span 62\nprofessions, 39 activities, 57 types of objects, and 70 personality traits. The\nframework then compares the edited images to the original seed images, focusing\non any changes related to gender, race, and age. BiasPainter adopts a testing\noracle that these characteristics should not be modified when subjected to\nneutral prompts. Built upon this design, BiasPainter can trigger the social\nbias and evaluate the fairness of image generation models. To evaluate the\neffectiveness of BiasPainter, we use BiasPainter to test five widely-used\ncommercial image generation software and models, such as stable diffusion and\nMidjourney. Experimental results show that 100\\% of the generated test cases\ncan successfully trigger social bias in image generation models.\n","authors":["Wenxuan Wang","Haonan Bai","Jen-tse Huang","Yuxuan Wan","Youliang Yuan","Haoyi Qiu","Nanyun Peng","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2401.00763v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00736v1","updated":"2024-01-01T12:25:57Z","published":"2024-01-01T12:25:57Z","title":"Diffusion Models, Image Super-Resolution And Everything: A Survey","summary":"  Diffusion Models (DMs) represent a significant advancement in image\nSuper-Resolution (SR), aligning technical image quality more closely with human\npreferences and expanding SR applications. DMs address critical limitations of\nprevious methods, enhancing overall realism and details in SR images. However,\nDMs suffer from color-shifting issues, and their high computational costs call\nfor efficient sampling alternatives, underscoring the challenge of balancing\ncomputational efficiency and image quality. This survey gives an overview of\nDMs applied to image SR and offers a detailed analysis that underscores the\nunique characteristics and methodologies within this domain, distinct from\nbroader existing reviews in the field. It presents a unified view of DM\nfundamentals and explores research directions, including alternative input\ndomains, conditioning strategies, guidance, corruption spaces, and zero-shot\nmethods. This survey provides insights into the evolution of image SR with DMs,\naddressing current trends, challenges, and future directions in this rapidly\nevolving field.\n","authors":["Brian B. Moser","Arundhati S. Shanbhag","Federico Raue","Stanislav Frolov","Sebastian Palacio","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2401.00736v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00658v1","updated":"2024-01-01T04:11:55Z","published":"2024-01-01T04:11:55Z","title":"Point Cloud in the Air","summary":"  Acquisition and processing of point clouds (PCs) is a crucial enabler for\nmany emerging applications reliant on 3D spatial data, such as robot\nnavigation, autonomous vehicles, and augmented reality. In most scenarios, PCs\nacquired by remote sensors must be transmitted to an edge server for fusion,\nsegmentation, or inference. Wireless transmission of PCs not only puts on\nincreased burden on the already congested wireless spectrum, but also confronts\na unique set of challenges arising from the irregular and unstructured nature\nof PCs. In this paper, we meticulously delineate these challenges and offer a\ncomprehensive examination of existing solutions while candidly acknowledging\ntheir inherent limitations. In response to these intricacies, we proffer four\npragmatic solution frameworks, spanning advanced techniques, hybrid schemes,\nand distributed data aggregation approaches. In doing so, our goal is to chart\na path toward efficient, reliable, and low-latency wireless PC transmission.\n","authors":["Yulin Shao","Chenghong Bian","Li Yang","Qianqian Yang","Zhaoyang Zhang","Deniz Gunduz"],"pdf_url":"https://arxiv.org/pdf/2401.00658v1.pdf","comment":null}]},"2024-01-04T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2401.02417v1","updated":"2024-01-04T18:59:31Z","published":"2024-01-04T18:59:31Z","title":"Task Oriented Dialogue as a Catalyst for Self-Supervised Automatic\n  Speech Recognition","summary":"  While word error rates of automatic speech recognition (ASR) systems have\nconsistently fallen, natural language understanding (NLU) applications built on\ntop of ASR systems still attribute significant numbers of failures to\nlow-quality speech recognition results. Existing assistant systems collect\nlarge numbers of these unsuccessful interactions, but these systems usually\nfail to learn from these interactions, even in an offline fashion. In this\nwork, we introduce CLC: Contrastive Learning for Conversations, a family of\nmethods for contrastive fine-tuning of models in a self-supervised fashion,\nmaking use of easily detectable artifacts in unsuccessful conversations with\nassistants. We demonstrate that our CLC family of approaches can improve the\nperformance of ASR models on OD3, a new public large-scale semi-synthetic\nmeta-dataset of audio task-oriented dialogues, by up to 19.2%. These gains\ntransfer to real-world systems as well, where we show that CLC can help to\nimprove performance by up to 6.7% over baselines. We make OD3 publicly\navailable at https://github.com/amazon-science/amazon-od3 .\n","authors":["David M. Chan","Shalini Ghosh","Hitesh Tulsiani","Ariya Rastrow","Björn Hoffmeister"],"pdf_url":"https://arxiv.org/pdf/2401.02417v1.pdf","comment":"To appear in ICASSP 2024"},{"id":"http://arxiv.org/abs/2401.02415v1","updated":"2024-01-04T18:59:12Z","published":"2024-01-04T18:59:12Z","title":"LLaMA Pro: Progressive LLaMA with Block Expansion","summary":"  Humans generally acquire new skills without compromising the old; however,\nthe opposite holds for Large Language Models (LLMs), e.g., from LLaMA to\nCodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with\nan expansion of Transformer blocks. We tune the expanded blocks using only new\ncorpus, efficiently and effectively improving the model's knowledge without\ncatastrophic forgetting. In this paper, we experiment on the corpus of code and\nmath, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from\nLLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro\nand its instruction-following counterpart (LLaMA Pro-Instruct) achieve advanced\nperformance among various benchmarks, demonstrating superiority over existing\nopen models in the LLaMA family and the immense potential of reasoning and\naddressing diverse tasks as an intelligent agent. Our findings provide valuable\ninsights into integrating natural and programming languages, laying a solid\nfoundation for developing advanced language agents that operate effectively in\nvarious environments.\n","authors":["Chengyue Wu","Yukang Gan","Yixiao Ge","Zeyu Lu","Jiahao Wang","Ye Feng","Ping Luo","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2401.02415v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02412v1","updated":"2024-01-04T18:53:01Z","published":"2024-01-04T18:53:01Z","title":"LLM Augmented LLMs: Expanding Capabilities through Composition","summary":"  Foundational models with billions of parameters which have been trained on\nlarge corpora of data have demonstrated non-trivial skills in a variety of\ndomains. However, due to their monolithic structure, it is challenging and\nexpensive to augment them or impart new skills. On the other hand, due to their\nadaptation abilities, several new instances of these models are being trained\ntowards new domains and tasks. In this work, we study the problem of efficient\nand practical composition of existing foundation models with more specific\nmodels to enable newer capabilities. To this end, we propose CALM --\nComposition to Augment Language Models -- which introduces cross-attention\nbetween models to compose their representations and enable new capabilities.\nSalient features of CALM are: (i) Scales up LLMs on new tasks by 're-using'\nexisting LLMs along with a few additional parameters and data, (ii) Existing\nmodel weights are kept intact, and hence preserves existing capabilities, and\n(iii) Applies to diverse domains and settings. We illustrate that augmenting\nPaLM2-S with a smaller model trained on low-resource languages results in an\nabsolute improvement of up to 13\\% on tasks like translation into English and\narithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is\naugmented with a code-specific model, we see a relative improvement of 40\\%\nover the base model for code generation and explanation tasks -- on-par with\nfully fine-tuned counterparts.\n","authors":["Rachit Bansal","Bidisha Samanta","Siddharth Dalmia","Nitish Gupta","Shikhar Vashishth","Sriram Ganapathy","Abhishek Bapna","Prateek Jain","Partha Talukdar"],"pdf_url":"https://arxiv.org/pdf/2401.02412v1.pdf","comment":"17 pages, 2 figures, 8 tables"},{"id":"http://arxiv.org/abs/2312.11671v2","updated":"2024-01-04T18:46:39Z","published":"2023-12-18T19:27:09Z","title":"Evaluating Language-Model Agents on Realistic Autonomous Tasks","summary":"  In this report, we explore the ability of language model agents to acquire\nresources, create copies of themselves, and adapt to novel challenges they\nencounter in the wild. We refer to this cluster of capabilities as \"autonomous\nreplication and adaptation\" or ARA. We believe that systems capable of ARA\ncould have wide-reaching and hard-to-anticipate consequences, and that\nmeasuring and forecasting ARA may be useful for informing measures around\nsecurity, monitoring, and alignment. Additionally, once a system is capable of\nARA, placing bounds on a system's capabilities may become significantly more\ndifficult.\n  We construct four simple example agents that combine language models with\ntools that allow them to take actions in the world. We then evaluate these\nagents on 12 tasks relevant to ARA. We find that these language model agents\ncan only complete the easiest tasks from this list, although they make some\nprogress on the more challenging tasks. Unfortunately, these evaluations are\nnot adequate to rule out the possibility that near-future agents will be\ncapable of ARA. In particular, we do not think that these evaluations provide\ngood assurance that the ``next generation'' of language models (e.g. 100x\neffective compute scaleup on existing models) will not yield agents capable of\nARA, unless intermediate evaluations are performed during pretraining.\nRelatedly, we expect that fine-tuning of the existing models could produce\nsubstantially more competent agents, even if the fine-tuning is not directly\ntargeted at ARA.\n","authors":["Megan Kinniment","Lucas Jun Koba Sato","Haoxing Du","Brian Goodrich","Max Hasin","Lawrence Chan","Luke Harold Miles","Tao R. Lin","Hjalmar Wijk","Joel Burget","Aaron Ho","Elizabeth Barnes","Paul Christiano"],"pdf_url":"https://arxiv.org/pdf/2312.11671v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2312.10302v3","updated":"2024-01-04T18:00:11Z","published":"2023-12-16T03:33:12Z","title":"One Shot Learning as Instruction Data Prospector for Large Language\n  Models","summary":"  Aligning large language models(LLMs) with human is a critical step in\neffectively utilizing their pre-trained capabilities across a wide array of\nlanguage tasks. Current instruction tuning practices often rely on expanding\ndataset size without a clear strategy for ensuring data quality, which can\ninadvertently introduce noise and degrade model performance. To address this\nchallenge, we introduce Nuggets, a novel and efficient methodology that employs\none shot learning to select high-quality instruction data from expansive\ndatasets. Nuggets assesses the potential of individual instruction examples to\nact as effective one shot examples, thereby identifying those that can\nsignificantly enhance diverse task performance. Nuggets utilizes a scoring\nsystem based on the impact of candidate examples on the perplexity of a diverse\nanchor set, facilitating the selection of the most beneficial data for\ninstruction tuning. Through rigorous testing on two benchmarks, including\nMT-Bench and Alpaca-Eval, we demonstrate that instruction tuning with the top\n1% of Nuggets-curated examples substantially outperforms conventional methods\nthat use the full dataset. These findings advocate for a data selection\nparadigm that prioritizes quality, offering a more efficient pathway to align\nLLMs with humans.\n","authors":["Yunshui Li","Binyuan Hui","Xiaobo Xia","Jiaxi Yang","Min Yang","Lei Zhang","Shuzheng Si","Junhao Liu","Tongliang Liu","Fei Huang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2312.10302v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02385v1","updated":"2024-01-04T17:54:59Z","published":"2024-01-04T17:54:59Z","title":"TinyLlama: An Open-Source Small Language Model","summary":"  We present TinyLlama, a compact 1.1B language model pretrained on around 1\ntrillion tokens for approximately 3 epochs. Building on the architecture and\ntokenizer of Llama 2, TinyLlama leverages various advances contributed by the\nopen-source community (e.g., FlashAttention), achieving better computational\nefficiency. Despite its relatively small size, TinyLlama demonstrates\nremarkable performance in a series of downstream tasks. It significantly\noutperforms existing open-source language models with comparable sizes. Our\nmodel checkpoints and code are publicly available on GitHub at\nhttps://github.com/jzhang38/TinyLlama.\n","authors":["Peiyuan Zhang","Guangtao Zeng","Tianduo Wang","Wei Lu"],"pdf_url":"https://arxiv.org/pdf/2401.02385v1.pdf","comment":"Technical Report"},{"id":"http://arxiv.org/abs/2401.01078v3","updated":"2024-01-04T17:29:47Z","published":"2024-01-02T07:46:34Z","title":"Vietnamese Poem Generation & The Prospect Of Cross-Language Poem-To-Poem\n  Translation","summary":"  Poetry generation has been a challenging task in the field of Natural\nLanguage Processing, as it requires the model to understand the nuances of\nlanguage, sentiment, and style. In this paper, we propose using Large Language\nModels to generate Vietnamese poems of various genres from natural language\nprompts, thereby facilitating an intuitive process with enhanced content\ncontrol. Our most efficacious model, the GPT-3 Babbage variant, achieves a\ncustom evaluation score of 0.8, specifically tailored to the \"luc bat\" genre of\nVietnamese poetry. Furthermore, we also explore the idea of paraphrasing poems\ninto normal text prompts and yield a relatively high score of 0.781 in the \"luc\nbat\" genre. This experiment presents the potential for cross-Language\npoem-to-poem translation with translated poems as the inputs while concurrently\nmaintaining complete control over the generated content.\n","authors":["Triet Minh Huynh","Quan Le Bao"],"pdf_url":"https://arxiv.org/pdf/2401.01078v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02369v1","updated":"2024-01-04T17:23:44Z","published":"2024-01-04T17:23:44Z","title":"SPEER: Sentence-Level Planning of Long Clinical Summaries via Embedded\n  Entity Retrieval","summary":"  Clinician must write a lengthy summary each time a patient is discharged from\nthe hospital. This task is time-consuming due to the sheer number of unique\nclinical concepts covered in the admission. Identifying and covering salient\nentities is vital for the summary to be clinically useful. We fine-tune\nopen-source LLMs (Mistral-7B-Instruct and Zephyr-7B-\\b{eta}) on the task and\nfind that they generate incomplete and unfaithful summaries. To increase entity\ncoverage, we train a smaller, encoder-only model to predict salient entities,\nwhich are treated as content-plans to guide the LLM. To encourage the LLM to\nfocus on specific mentions in the source notes, we propose SPEER:\nSentence-level Planning via Embedded Entity Retrieval. Specifically, we mark\neach salient entity span with special \"{{ }}\" boundary tags and instruct the\nLLM to retrieve marked spans before generating each sentence. Sentence-level\nplanning acts as a form of state tracking in that the model is explicitly\nrecording the entities it uses. We fine-tune Mistral and Zephyr variants on a\nlarge-scale, diverse dataset of ~167k in-patient hospital admissions and\nevaluate on 3 datasets. SPEER shows gains in both coverage and faithfulness\nmetrics over non-guided and guided baselines.\n","authors":["Griffin Adams","Jason Zucker","Noémie Elhadad"],"pdf_url":"https://arxiv.org/pdf/2401.02369v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2312.15228v2","updated":"2024-01-04T16:20:42Z","published":"2023-12-23T11:37:09Z","title":"Adversarial Data Poisoning for Fake News Detection: How to Make a Model\n  Misclassify a Target News without Modifying It","summary":"  Fake news detection models are critical to countering disinformation but can\nbe manipulated through adversarial attacks. In this position paper, we analyze\nhow an attacker can compromise the performance of an online learning detector\non specific news content without being able to manipulate the original target\nnews. In some contexts, such as social networks, where the attacker cannot\nexert complete control over all the information, this scenario can indeed be\nquite plausible. Therefore, we show how an attacker could potentially introduce\npoisoning data into the training data to manipulate the behavior of an online\nlearning method. Our initial findings reveal varying susceptibility of logistic\nregression models based on complexity and attack type.\n","authors":["Federico Siciliano","Luca Maiano","Lorenzo Papa","Federica Baccini","Irene Amerini","Fabrizio Silvestri"],"pdf_url":"https://arxiv.org/pdf/2312.15228v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02333v1","updated":"2024-01-04T16:16:14Z","published":"2024-01-04T16:16:14Z","title":"Beyond Extraction: Contextualising Tabular Data for Efficient\n  Summarisation by Language Models","summary":"  The conventional use of the Retrieval-Augmented Generation (RAG) architecture\nhas proven effective for retrieving information from diverse documents.\nHowever, challenges arise in handling complex table queries, especially within\nPDF documents containing intricate tabular structures.This research introduces\nan innovative approach to enhance the accuracy of complex table queries in\nRAG-based systems. Our methodology involves storing PDFs in the retrieval\ndatabase and extracting tabular content separately. The extracted tables\nundergo a process of context enrichment, concatenating headers with\ncorresponding values. To ensure a comprehensive understanding of the enriched\ndata, we employ a fine-tuned version of the Llama-2-chat language model for\nsummarisation within the RAG architecture. Furthermore, we augment the tabular\ndata with contextual sense using the ChatGPT 3.5 API through a one-shot prompt.\nThis enriched data is then fed into the retrieval database alongside other\nPDFs. Our approach aims to significantly improve the precision of complex table\nqueries, offering a promising solution to a longstanding challenge in\ninformation retrieval.\n","authors":["Uday Allu","Biddwan Ahmed","Vishesh Tripathi"],"pdf_url":"https://arxiv.org/pdf/2401.02333v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02330v1","updated":"2024-01-04T16:07:43Z","published":"2024-01-04T16:07:43Z","title":"LLaVA-$φ$: Efficient Multi-Modal Assistant with Small Language Model","summary":"  In this paper, we introduce LLaVA-$\\phi$ (LLaVA-Phi), an efficient\nmulti-modal assistant that harnesses the power of the recently advanced small\nlanguage model, Phi-2, to facilitate multi-modal dialogues. LLaVA-Phi marks a\nnotable advancement in the realm of compact multi-modal models. It demonstrates\nthat even smaller language models, with as few as 2.7B parameters, can\neffectively engage in intricate dialogues that integrate both textual and\nvisual elements, provided they are trained with high-quality corpora. Our model\ndelivers commendable performance on publicly available benchmarks that\nencompass visual comprehension, reasoning, and knowledge-based perception.\nBeyond its remarkable performance in multi-modal dialogue tasks, our model\nopens new avenues for applications in time-sensitive environments and systems\nthat require real-time interaction, such as embodied agents. It highlights the\npotential of smaller language models to achieve sophisticated levels of\nunderstanding and interaction, while maintaining greater resource\nefficiency.The project is available at {https://github.com/zhuyiche/llava-phi}.\n","authors":["Yichen Zhu","Minjie Zhu","Ning Liu","Zhicai Ou","Xiaofeng Mou","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2401.02330v1.pdf","comment":"technique report"},{"id":"http://arxiv.org/abs/2401.02297v1","updated":"2024-01-04T14:36:38Z","published":"2024-01-04T14:36:38Z","title":"Are LLMs Robust for Spoken Dialogues?","summary":"  Large Pre-Trained Language Models have demonstrated state-of-the-art\nperformance in different downstream tasks, including dialogue state tracking\nand end-to-end response generation. Nevertheless, most of the publicly\navailable datasets and benchmarks on task-oriented dialogues focus on written\nconversations. Consequently, the robustness of the developed models to spoken\ninteractions is unknown. In this work, we have evaluated the performance of\nLLMs for spoken task-oriented dialogues on the DSTC11 test sets. Due to the\nlack of proper spoken dialogue datasets, we have automatically transcribed a\ndevelopment set of spoken dialogues with a state-of-the-art ASR engine. We have\ncharacterized the ASR-error types and their distributions and simulated these\nerrors in a large dataset of dialogues. We report the intrinsic (perplexity)\nand extrinsic (human evaluation) performance of fine-tuned GPT-2 and T5 models\nin two subtasks of response generation and dialogue state tracking,\nrespectively. The results show that LLMs are not robust to spoken noise by\ndefault, however, fine-tuning/training such models on a proper dataset of\nspoken TODs can result in a more robust performance.\n","authors":["Seyed Mahed Mousavi","Gabriel Roccabruna","Simone Alghisi","Massimo Rizzoli","Mirco Ravanelli","Giuseppe Riccardi"],"pdf_url":"https://arxiv.org/pdf/2401.02297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.02106v2","updated":"2024-01-04T13:32:06Z","published":"2022-04-05T10:55:33Z","title":"How do media talk about the Covid-19 pandemic? Metaphorical thematic\n  clustering in Italian online newspapers","summary":"  The contribution presents a study on figurative language of the first months\nof the COVID-19 crisis in Italian online newspapers. Particularly, we contrast\ntopics and metaphorical language used by journalists in the first and second\nphase of the government response to the pandemic in Spring 2020. The analysis\nis conducted on a journalistic corpus collected between February 24th and June\n3rd, 2020. The analysis is performed using both quantitative and qualitative\napproaches, combining Structural Topic Modelling (Roberts et al. 2016),\nConceptual Metaphor Theory (Lakoff & Johnson, 1980), and qualitative-corpus\nbased metaphor analysis (Charteris-Black, 2004). We find a significant shift in\ntopics discussed across Phase 1 and Phase 2, and interesting overlaps in\ntopic-specific metaphors. Using qualitative corpus analysis, we present a more\nin-depth case study discussing metaphorical collocations of the topics of\nEconomy and Society\n","authors":["Lucia Busso","Ottavia Tordini"],"pdf_url":"https://arxiv.org/pdf/2204.02106v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02256v1","updated":"2024-01-04T13:15:41Z","published":"2024-01-04T13:15:41Z","title":"Rethinking Response Evaluation from Interlocutor's Eye for Open-Domain\n  Dialogue Systems","summary":"  Open-domain dialogue systems have started to engage in continuous\nconversations with humans. Those dialogue systems are required to be adjusted\nto the human interlocutor and evaluated in terms of their perspective. However,\nit is questionable whether the current automatic evaluation methods can\napproximate the interlocutor's judgments. In this study, we analyzed and\nexamined what features are needed in an automatic response evaluator from the\ninterlocutor's perspective. The first experiment on the Hazumi dataset revealed\nthat interlocutor awareness plays a critical role in making automatic response\nevaluation correlate with the interlocutor's judgments. The second experiment\nusing massive conversations on X (formerly Twitter) confirmed that dialogue\ncontinuity prediction can train an interlocutor-aware response evaluator\nwithout human feedback while revealing the difficulty in evaluating generated\nresponses compared to human responses.\n","authors":["Yuma Tsuta","Naoki Yoshinaga","Shoetsu Sato","Masashi Toyoda"],"pdf_url":"https://arxiv.org/pdf/2401.02256v1.pdf","comment":"9 pages, 3 figures, 5 tables, Accepted by IJCNLP-AACL 2023 SRW"},{"id":"http://arxiv.org/abs/2401.02254v1","updated":"2024-01-04T13:11:17Z","published":"2024-01-04T13:11:17Z","title":"L3Cube-IndicNews: News-based Short Text and Long Document Classification\n  Datasets in Indic Languages","summary":"  In this work, we introduce L3Cube-IndicNews, a multilingual text\nclassification corpus aimed at curating a high-quality dataset for Indian\nregional languages, with a specific focus on news headlines and articles. We\nhave centered our work on 10 prominent Indic languages, including Hindi,\nBengali, Marathi, Telugu, Tamil, Gujarati, Kannada, Odia, Malayalam, and\nPunjabi. Each of these news datasets comprises 10 or more classes of news\narticles. L3Cube-IndicNews offers 3 distinct datasets tailored to handle\ndifferent document lengths that are classified as: Short Headlines\nClassification (SHC) dataset containing the news headline and news category,\nLong Document Classification (LDC) dataset containing the whole news article\nand the news category, and Long Paragraph Classification (LPC) containing\nsub-articles of the news and the news category. We maintain consistent labeling\nacross all 3 datasets for in-depth length-based analysis. We evaluate each of\nthese Indic language datasets using 4 different models including monolingual\nBERT, multilingual Indic Sentence BERT (IndicSBERT), and IndicBERT. This\nresearch contributes significantly to expanding the pool of available text\nclassification datasets and also makes it possible to develop topic\nclassification models for Indian regional languages. This also serves as an\nexcellent resource for cross-lingual analysis owing to the high overlap of\nlabels among languages. The datasets and models are shared publicly at\nhttps://github.com/l3cube-pune/indic-nlp\n","authors":["Aishwarya Mirashi","Srushti Sonavane","Purva Lingayat","Tejas Padhiyar","Raviraj Joshi"],"pdf_url":"https://arxiv.org/pdf/2401.02254v1.pdf","comment":"Accepted at the International Conference on Natural Language\n  Processing (ICON 2023)"},{"id":"http://arxiv.org/abs/2401.02212v1","updated":"2024-01-04T11:34:39Z","published":"2024-01-04T11:34:39Z","title":"Joint Multi-Facts Reasoning Network For Complex Temporal Question\n  Answering Over Knowledge Graph","summary":"  Temporal Knowledge Graph (TKG) is an extension of regular knowledge graph by\nattaching the time scope. Existing temporal knowledge graph question answering\n(TKGQA) models solely approach simple questions, owing to the prior assumption\nthat each question only contains a single temporal fact with explicit/implicit\ntemporal constraints. Hence, they perform poorly on questions which own\nmultiple temporal facts. In this paper, we propose \\textbf{\\underline{J}}oint\n\\textbf{\\underline{M}}ulti \\textbf{\\underline{F}}acts\n\\textbf{\\underline{R}}easoning \\textbf{\\underline{N}}etwork (JMFRN), to jointly\nreasoning multiple temporal facts for accurately answering \\emph{complex}\ntemporal questions. Specifically, JMFRN first retrieves question-related\ntemporal facts from TKG for each entity of the given complex question. For\njoint reasoning, we design two different attention (\\ie entity-aware and\ntime-aware) modules, which are suitable for universal settings, to aggregate\nentities and timestamps information of retrieved facts. Moreover, to filter\nincorrect type answers, we introduce an additional answer type discrimination\ntask. Extensive experiments demonstrate our proposed method significantly\noutperforms the state-of-art on the well-known complex temporal question\nbenchmark TimeQuestions.\n","authors":["Rikui Huang","Wei Wei","Xiaoye Qu","Wenfeng Xie","Xianling Mao","Dangyang Chen"],"pdf_url":"https://arxiv.org/pdf/2401.02212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02208v1","updated":"2024-01-04T11:27:48Z","published":"2024-01-04T11:27:48Z","title":"DIALIGHT: Lightweight Multilingual Development and Evaluation of\n  Task-Oriented Dialogue Systems with Large Language Models","summary":"  We present DIALIGHT, a toolkit for developing and evaluating multilingual\nTask-Oriented Dialogue (ToD) systems which facilitates systematic evaluations\nand comparisons between ToD systems using fine-tuning of Pretrained Language\nModels (PLMs) and those utilising the zero-shot and in-context learning\ncapabilities of Large Language Models (LLMs). In addition to automatic\nevaluation, this toolkit features (i) a secure, user-friendly web interface for\nfine-grained human evaluation at both local utterance level and global dialogue\nlevel, and (ii) a microservice-based backend, improving efficiency and\nscalability. Our evaluations reveal that while PLM fine-tuning leads to higher\naccuracy and coherence, LLM-based systems excel in producing diverse and\nlikeable responses. However, we also identify significant challenges of LLMs in\nadherence to task-specific instructions and generating outputs in multiple\nlanguages, highlighting areas for future research. We hope this open-sourced\ntoolkit will serve as a valuable resource for researchers aiming to develop and\nproperly evaluate multilingual ToD systems and will lower, currently still\nhigh, entry barriers in the field.\n","authors":["Songbo Hu","Xiaobin Wang","Zhangdie Yuan","Anna Korhonen","Ivan Vulić"],"pdf_url":"https://arxiv.org/pdf/2401.02208v1.pdf","comment":"17 pages, 7 tables, 9 figures"},{"id":"http://arxiv.org/abs/2307.05300v3","updated":"2024-01-04T10:51:24Z","published":"2023-07-11T14:45:19Z","title":"Unleashing the Emergent Cognitive Synergy in Large Language Models: A\n  Task-Solving Agent through Multi-Persona Self-Collaboration","summary":"  Human intelligence thrives on cognitive synergy, where collaboration among\ndifferent minds yield superior outcomes compared to isolated individuals. In\nthis work, we propose Solo Performance Prompting (SPP), which transforms a\nsingle LLM into a cognitive synergist by engaging in multi-turn\nself-collaboration with multiple personas. A cognitive synergist is an\nintelligent agent that collaboratively combines multiple minds' strengths and\nknowledge to enhance problem-solving in complex tasks. By dynamically\nidentifying and simulating different personas based on task inputs, SPP\nunleashes the potential of cognitive synergy in LLMs. Our in-depth analysis\nshows that assigning multiple fine-grained personas in LLMs improves\nproblem-solving abilities compared to using a single or fixed number of\npersonas. We evaluate SPP on three challenging tasks: Trivia Creative Writing,\nCodenames Collaborative, and Logic Grid Puzzle, encompassing both\nknowledge-intensive and reasoning-intensive types. Unlike previous works, such\nas Chain-of-Thought, that solely enhance the reasoning abilities in LLMs,\nexperimental results demonstrate that SPP effectively reduces factual\nhallucination, and maintains strong reasoning capabilities. Additionally,\ncomparative experiments show that cognitive synergy only emerges in GPT-4 and\ndoes not appear in less capable models, such as GPT-3.5-turbo and\nLlama2-13b-chat, which draws an interesting analogy to human development. Code,\ndata, and prompts can be found at:\nhttps://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.\n","authors":["Zhenhailong Wang","Shaoguang Mao","Wenshan Wu","Tao Ge","Furu Wei","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2307.05300v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.03656v3","updated":"2024-01-04T10:41:12Z","published":"2023-08-07T15:18:30Z","title":"Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using\n  EmotionBench","summary":"  Evaluating Large Language Models' (LLMs) anthropomorphic capabilities has\nbecome increasingly important in contemporary discourse. Utilizing the emotion\nappraisal theory from psychology, we propose to evaluate the empathy ability of\nLLMs, i.e., how their feelings change when presented with specific situations.\nAfter a careful and comprehensive survey, we collect a dataset containing over\n400 situations that have proven effective in eliciting the eight emotions\ncentral to our study. Categorizing the situations into 36 factors, we conduct a\nhuman evaluation involving more than 1,200 subjects worldwide. With the human\nevaluation results as references, our evaluation includes five LLMs, covering\nboth commercial and open-source models, including variations in model sizes,\nfeaturing the latest iterations, such as GPT-4 and LLaMA-2. We find that,\ndespite several misalignments, LLMs can generally respond appropriately to\ncertain situations. Nevertheless, they fall short in alignment with the\nemotional behaviors of human beings and cannot establish connections between\nsimilar situations. Our collected dataset of situations, the human evaluation\nresults, and the code of our testing framework, dubbed EmotionBench, is made\nopenly accessible via https://github.com/CUHK-ARISE/EmotionBench. We aspire to\ncontribute to the advancement of LLMs regarding better alignment with the\nemotional behaviors of human beings, thereby enhancing their utility and\napplicability as intelligent assistants.\n","authors":["Jen-tse Huang","Man Ho Lam","Eric John Li","Shujie Ren","Wenxuan Wang","Wenxiang Jiao","Zhaopeng Tu","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2308.03656v3.pdf","comment":"16 pages. Added demographic distribution of the user study. Added\n  ethics statements and limitations"},{"id":"http://arxiv.org/abs/2401.02187v1","updated":"2024-01-04T10:39:58Z","published":"2024-01-04T10:39:58Z","title":"Location Aware Modular Biencoder for Tourism Question Answering","summary":"  Answering real-world tourism questions that seek Point-of-Interest (POI)\nrecommendations is challenging, as it requires both spatial and non-spatial\nreasoning, over a large candidate pool. The traditional method of encoding each\npair of question and POI becomes inefficient when the number of candidates\nincreases, making it infeasible for real-world applications. To overcome this,\nwe propose treating the QA task as a dense vector retrieval problem, where we\nencode questions and POIs separately and retrieve the most relevant POIs for a\nquestion by utilizing embedding space similarity. We use pretrained language\nmodels (PLMs) to encode textual information, and train a location encoder to\ncapture spatial information of POIs. Experiments on a real-world tourism QA\ndataset demonstrate that our approach is effective, efficient, and outperforms\nprevious methods across all metrics. Enabled by the dense retrieval\narchitecture, we further build a global evaluation baseline, expanding the\nsearch space by 20 times compared to previous work. We also explore several\nfactors that impact on the model's performance through follow-up experiments.\nOur code and model are publicly available at https://github.com/haonan-li/LAMB.\n","authors":["Haonan Li","Martin Tomko","Timothy Baldwin"],"pdf_url":"https://arxiv.org/pdf/2401.02187v1.pdf","comment":"Accepted at AACL 2023"},{"id":"http://arxiv.org/abs/2305.08372v2","updated":"2024-01-04T10:15:28Z","published":"2023-05-15T06:14:36Z","title":"Hierarchical Aligned Multimodal Learning for NER on Tweet Posts","summary":"  Mining structured knowledge from tweets using named entity recognition (NER)\ncan be beneficial for many down stream applications such as recommendation and\nintention understanding. With tweet posts tending to be multimodal, multimodal\nnamed entity recognition (MNER) has attracted more attention. In this paper, we\npropose a novel approach, which can dynamically align the image and text\nsequence and achieve the multi-level cross-modal learning to augment textual\nword representation for MNER improvement. To be specific, our framework can be\nsplit into three main stages: the first stage focuses on intra-modality\nrepresentation learning to derive the implicit global and local knowledge of\neach modality, the second evaluates the relevance between the text and its\naccompanying image and integrates different grained visual information based on\nthe relevance, the third enforces semantic refinement via iterative cross-modal\ninteractions and co-attention. We conduct experiments on two open datasets, and\nthe results and detailed analysis demonstrate the advantage of our model.\n","authors":["Peipei Liu","Hong Li","Yimo Ren","Jie Liu","Shuaizong Si","Hongsong Zhu","Limin Sun"],"pdf_url":"https://arxiv.org/pdf/2305.08372v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.06374v2","updated":"2024-01-04T10:12:18Z","published":"2023-12-11T13:32:11Z","title":"UstanceBR: a multimodal language resource for stance prediction","summary":"  This work introduces UstanceBR, a multimodal corpus in the Brazilian\nPortuguese Twitter domain for target-based stance prediction. The corpus\ncomprises 86.8 k labelled stances towards selected target topics, and extensive\nnetwork information about the users who published these stances on social\nmedia. In this article we describe the corpus multimodal data, and a number of\nusage examples in both in-domain and zero-shot stance prediction based on text-\nand network-related information, which are intended to provide initial baseline\nresults for future studies in the field.\n","authors":["Camila Pereira","Matheus Pavan","Sungwon Yoon","Ricelli Ramos","Pablo Costa","Lais Cavalheiro","Ivandre Paraboni"],"pdf_url":"https://arxiv.org/pdf/2312.06374v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02158v1","updated":"2024-01-04T09:13:18Z","published":"2024-01-04T09:13:18Z","title":"Shayona@SMM4H23: COVID-19 Self diagnosis classification using BERT and\n  LightGBM models","summary":"  This paper describes approaches and results for shared Task 1 and 4 of\nSMMH4-23 by Team Shayona. Shared Task-1 was binary classification of english\ntweets self-reporting a COVID-19 diagnosis, and Shared Task-4 was Binary\nclassification of English Reddit posts self-reporting a social anxiety disorder\ndiagnosis. Our team has achieved the highest f1-score 0.94 in Task-1 among all\nparticipants. We have leveraged the Transformer model (BERT) in combination\nwith the LightGBM model for both tasks.\n","authors":["Rushi Chavda","Darshan Makwana","Vraj Patel","Anupam Shukla"],"pdf_url":"https://arxiv.org/pdf/2401.02158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02147v1","updated":"2024-01-04T08:53:08Z","published":"2024-01-04T08:53:08Z","title":"Exploring Boundary of GPT-4V on Marine Analysis: A Preliminary Case\n  Study","summary":"  Large language models (LLMs) have demonstrated a powerful ability to answer\nvarious queries as a general-purpose assistant. The continuous multi-modal\nlarge language models (MLLM) empower LLMs with the ability to perceive visual\nsignals. The launch of GPT-4 (Generative Pre-trained Transformers) has\ngenerated significant interest in the research communities. GPT-4V(ison) has\ndemonstrated significant power in both academia and industry fields, as a focal\npoint in a new artificial intelligence generation. Though significant success\nwas achieved by GPT-4V, exploring MLLMs in domain-specific analysis (e.g.,\nmarine analysis) that required domain-specific knowledge and expertise has\ngained less attention. In this study, we carry out the preliminary and\ncomprehensive case study of utilizing GPT-4V for marine analysis. This report\nconducts a systematic evaluation of existing GPT-4V, assessing the performance\nof GPT-4V on marine research and also setting a new standard for future\ndevelopments in MLLMs. The experimental results of GPT-4V show that the\nresponses generated by GPT-4V are still far away from satisfying the\ndomain-specific requirements of the marine professions. All images and prompts\nused in this study will be available at\nhttps://github.com/hkust-vgd/Marine_GPT-4V_Eval\n","authors":["Ziqiang Zheng","Yiwei Chen","Jipeng Zhang","Tuan-Anh Vu","Huimin Zeng","Yue Him Wong Tim","Sai-Kit Yeung"],"pdf_url":"https://arxiv.org/pdf/2401.02147v1.pdf","comment":"51 pages, 36 figures, Repository:\n  https://github.com/hkust-vgd/Marine_GPT-4V_Eval"},{"id":"http://arxiv.org/abs/2401.02132v1","updated":"2024-01-04T08:34:16Z","published":"2024-01-04T08:34:16Z","title":"DCR-Consistency: Divide-Conquer-Reasoning for Consistency Evaluation and\n  Improvement of Large Language Models","summary":"  Evaluating the quality and variability of text generated by Large Language\nModels (LLMs) poses a significant, yet unresolved research challenge.\nTraditional evaluation methods, such as ROUGE and BERTScore, which measure\ntoken similarity, often fail to capture the holistic semantic equivalence. This\nresults in a low correlation with human judgments and intuition, which is\nespecially problematic in high-stakes applications like healthcare and finance\nwhere reliability, safety, and robust decision-making are highly critical. This\nwork proposes DCR, an automated framework for evaluating and improving the\nconsistency of LLM-generated texts using a divide-conquer-reasoning approach.\nUnlike existing LLM-based evaluators that operate at the paragraph level, our\nmethod employs a divide-and-conquer evaluator (DCE) that breaks down the\nparagraph-to-paragraph comparison between two generated responses into\nindividual sentence-to-paragraph comparisons, each evaluated based on\npredefined criteria. To facilitate this approach, we introduce an automatic\nmetric converter (AMC) that translates the output from DCE into an\ninterpretable numeric score. Beyond the consistency evaluation, we further\npresent a reason-assisted improver (RAI) that leverages the analytical reasons\nwith explanations identified by DCE to generate new responses aimed at reducing\nthese inconsistencies. Through comprehensive and systematic empirical analysis,\nwe show that our approach outperforms state-of-the-art methods by a large\nmargin (e.g., +19.3% and +24.3% on the SummEval dataset) in evaluating the\nconsistency of LLM generation across multiple benchmarks in semantic, factual,\nand summarization consistency tasks. Our approach also substantially reduces\nnearly 90% of output inconsistencies, showing promise for effective\nhallucination mitigation.\n","authors":["Wendi Cui","Jiaxin Zhang","Zhuohang Li","Lopez Damien","Kamalika Das","Bradley Malin","Sricharan Kumar"],"pdf_url":"https://arxiv.org/pdf/2401.02132v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02122v1","updated":"2024-01-04T08:11:33Z","published":"2024-01-04T08:11:33Z","title":"PEFT for Speech: Unveiling Optimal Placement, Merging Strategies, and\n  Ensemble Techniques","summary":"  Parameter-Efficient Fine-Tuning (PEFT) is increasingly recognized as an\neffective method in speech processing. However, the optimal approach and the\nplacement of PEFT methods remain inconclusive. Our study conducts extensive\nexperiments to compare different PEFT methods and their layer-wise placement\nadapting Differentiable Architecture Search (DARTS). We also explore the use of\nensemble learning to leverage diverse PEFT strategies. The results reveal that\nDARTS does not outperform the baseline approach, which involves inserting the\nsame PEFT method into all layers of a Self-Supervised Learning (SSL) model. In\ncontrast, an ensemble learning approach, particularly one employing majority\nvoting, demonstrates superior performance. Our statistical evidence indicates\nthat different PEFT methods learn in varied ways. This variation might explain\nwhy the synergistic integration of various PEFT methods through ensemble\nlearning can harness their unique learning capabilities more effectively\ncompared to individual layer-wise optimization.\n","authors":["Tzu-Han Lin","How-Shing Wang","Hao-Yung Weng","Kuang-Chen Peng","Zih-Ching Chen","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2401.02122v1.pdf","comment":"Submitted to ICASSP 2024 Self-supervision in Audio, Speech and Beyond\n  workshop"},{"id":"http://arxiv.org/abs/2401.02115v1","updated":"2024-01-04T07:50:24Z","published":"2024-01-04T07:50:24Z","title":"Using LLM to select the right SQL Query from candidates","summary":"  Text-to-SQL models can generate a list of candidate SQL queries, and the best\nquery is often in the candidate list, but not at the top of the list. An\neffective re-rank method can select the right SQL query from the candidate list\nand improve the model's performance. Previous studies on code generation\nautomatically generate test cases and use them to re-rank candidate codes.\nHowever, automatic test case generation for text-to-SQL is an understudied\nfield. We propose an automatic test case generation method that first generates\na database and then uses LLMs to predict the ground truth, which is the\nexpected execution results of the ground truth SQL query on this database. To\nreduce the difficulty for LLMs to predict, we conduct experiments to search for\nways to generate easy databases for LLMs and design easy-to-understand prompts.\nBased on our test case generation method, we propose a re-rank method to select\nthe right SQL query from the candidate list. Given a candidate list, our method\ncan generate test cases and re-rank the candidate list according to their pass\nnumbers on these test cases and their generation probabilities. The experiment\nresults on the validation dataset of Spider show that the performance of some\nstate-of-the-art models can get a 3.6\\% improvement after applying our re-rank\nmethod.\n","authors":["Zhenwen Li","Tao Xie"],"pdf_url":"https://arxiv.org/pdf/2401.02115v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2311.04589v3","updated":"2024-01-04T07:31:07Z","published":"2023-11-08T10:34:16Z","title":"TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models","summary":"  Despite Multi-modal Large Language Models (MM-LLMs) have made exciting\nstrides recently, they are still struggling to efficiently model the\ninteractions among multi-modal inputs and the generation in non-textual\nmodalities. In this work, we propose TEAL (Tokenize and Embed ALl)}, an\napproach to treat the input from any modality as a token sequence and learn a\njoint embedding space for all modalities. Specifically, for the input from any\nmodality, TEAL first discretizes it into a token sequence with the\noff-the-shelf tokenizer and embeds the token sequence into a joint embedding\nspace with a learnable embedding matrix. MM-LLMs just need to predict the\nmulti-modal tokens autoregressively as the textual LLMs do. Finally, the\ncorresponding de-tokenizer is applied to generate the output in each modality\nbased on the predicted token sequence. With the joint embedding space, TEAL\nenables the frozen LLMs to perform both understanding and generation tasks\ninvolving non-textual modalities, such as image and audio. Thus, the textual\nLLM can just work as an interface and maintain its high performance in textual\nunderstanding and generation. Experiments show that TEAL achieves substantial\nimprovements in multi-modal understanding, and implements a simple scheme for\nmulti-modal generations.\n","authors":["Zhen Yang","Yingxue Zhang","Fandong Meng","Jie Zhou"],"pdf_url":"https://arxiv.org/pdf/2311.04589v3.pdf","comment":"Multi-modal, Large Language Models, Tokenizer, Understanding and\n  Generation"},{"id":"http://arxiv.org/abs/2401.02088v1","updated":"2024-01-04T06:23:22Z","published":"2024-01-04T06:23:22Z","title":"Re-evaluating the Memory-balanced Pipeline Parallelism: BPipe","summary":"  Pipeline parallelism is an essential technique in the training of large-scale\nTransformer models. However, it suffers from imbalanced memory consumption,\nleading to insufficient memory utilization. The BPipe technique was proposed to\naddress this issue and has proven effective in the GPT-3 model. Nevertheless,\nour experiments have not yielded similar benefits for LLaMA training.\nAdditionally, BPipe only yields negligible benefits for GPT-3 training when\napplying flash attention. We analyze the underlying causes of the divergent\nperformance of BPipe on GPT-3 and LLaMA. Furthermore, we introduce a novel\nmethod to estimate the performance of BPipe.\n","authors":["Mincong Huang","Chao Wang","Chi Ma","Yineng Zhang","Peng Zhang","Lei Yu"],"pdf_url":"https://arxiv.org/pdf/2401.02088v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02072v1","updated":"2024-01-04T05:47:41Z","published":"2024-01-04T05:47:41Z","title":"ICE-GRT: Instruction Context Enhancement by Generative Reinforcement\n  based Transformers","summary":"  The emergence of Large Language Models (LLMs) such as ChatGPT and LLaMA\nencounter limitations in domain-specific tasks, with these models often lacking\ndepth and accuracy in specialized areas, and exhibiting a decrease in general\ncapabilities when fine-tuned, particularly analysis ability in small sized\nmodels. To address these gaps, we introduce ICE-GRT, utilizing Reinforcement\nLearning from Human Feedback (RLHF) grounded in Proximal Policy Optimization\n(PPO), demonstrating remarkable ability in in-domain scenarios without\ncompromising general task performance. Our exploration of ICE-GRT highlights\nits understanding and reasoning ability to not only generate robust answers but\nalso to provide detailed analyses of the reasons behind the answer. This\ncapability marks a significant progression beyond the scope of Supervised\nFine-Tuning models. The success of ICE-GRT is dependent on several crucial\nfactors, including Appropriate Data, Reward Size Scaling, KL-Control, Advantage\nNormalization, etc. The ICE-GRT model exhibits state-of-the-art performance in\ndomain-specific tasks and across 12 general Language tasks against equivalent\nsize and even larger size LLMs, highlighting the effectiveness of our approach.\nWe provide a comprehensive analysis of the ICE-GRT, underscoring the\nsignificant advancements it brings to the field of LLM.\n","authors":["Chen Zheng","Ke Sun","Da Tang","Yukun Ma","Yuyu Zhang","Chenguang Xi","Xun Zhou"],"pdf_url":"https://arxiv.org/pdf/2401.02072v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14033v2","updated":"2024-01-04T05:11:22Z","published":"2023-12-21T17:02:06Z","title":"T-Eval: Evaluating the Tool Utilization Capability Step by Step","summary":"  Large language models (LLM) have achieved remarkable performance on various\nNLP tasks and are augmented by tools for broader applications. Yet, how to\nevaluate and analyze the tool-utilization capability of LLMs is still\nunder-explored. In contrast to previous works that evaluate models\nholistically, we comprehensively decompose the tool utilization into multiple\nsub-processes, including instruction following, planning, reasoning, retrieval,\nunderstanding, and review. Based on that, we further introduce T-Eval to\nevaluate the tool utilization capability step by step. T-Eval disentangles the\ntool utilization evaluation into several sub-domains along model capabilities,\nfacilitating the inner understanding of both holistic and isolated competency\nof LLMs. We conduct extensive experiments on T-Eval and in-depth analysis of\nvarious LLMs. T-Eval not only exhibits consistency with the outcome-oriented\nevaluation but also provides a more fine-grained analysis of the capabilities\nof LLMs, providing a new perspective in LLM evaluation on tool-utilization\nability. The benchmark will be available at\nhttps://github.com/open-compass/T-Eval.\n","authors":["Zehui Chen","Weihua Du","Wenwei Zhang","Kuikun Liu","Jiangning Liu","Miao Zheng","Jingming Zhuo","Songyang Zhang","Dahua Lin","Kai Chen","Feng Zhao"],"pdf_url":"https://arxiv.org/pdf/2312.14033v2.pdf","comment":"Code: https://github.com/open-compass/T-Eval; Website:\n  https://open-compass.github.io/T-Eval"},{"id":"http://arxiv.org/abs/2312.17432v2","updated":"2024-01-04T03:08:53Z","published":"2023-12-29T01:56:17Z","title":"Video Understanding with Large Language Models: A Survey","summary":"  With the burgeoning growth of online video platforms and the escalating\nvolume of video content, the demand for proficient video understanding tools\nhas intensified markedly. Given the remarkable capabilities of Large Language\nModels (LLMs) in language and multimodal tasks, this survey provides a detailed\noverview of the recent advancements in video understanding harnessing the power\nof LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly\nadvanced, particularly their ability for open-ended spatial-temporal reasoning\ncombined with commonsense knowledge, suggesting a promising path for future\nvideo understanding. We examine the unique characteristics and capabilities of\nVid-LLMs, categorizing the approaches into four main types: LLM-based Video\nAgents, Vid-LLMs Pretraining, Vid-LLMs Instruction Tuning, and Hybrid Methods.\nFurthermore, this survey presents a comprehensive study of the tasks, datasets,\nand evaluation methodologies for Vid-LLMs. Additionally, it explores the\nexpansive applications of Vid-LLMs across various domains, highlighting their\nremarkable scalability and versatility in real-world video understanding\nchallenges. Finally, it summarizes the limitations of existing Vid-LLMs and\noutlines directions for future research. For more information, readers are\nrecommended to visit the repository at\nhttps://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding.\n","authors":["Yunlong Tang","Jing Bi","Siting Xu","Luchuan Song","Susan Liang","Teng Wang","Daoan Zhang","Jie An","Jingyang Lin","Rongyi Zhu","Ali Vosoughi","Chao Huang","Zeliang Zhang","Feng Zheng","Jianguo Zhang","Ping Luo","Jiebo Luo","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2312.17432v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14504v2","updated":"2024-01-04T02:49:21Z","published":"2023-12-22T08:08:45Z","title":"Theory of Hallucinations based on Equivariance","summary":"  This study aims to acquire knowledge for creating very large language models\nthat are immune to hallucinations. Hallucinations in contemporary large\nlanguage models are often attributed to a misunderstanding of real-world social\nrelationships. Therefore, I hypothesize that very large language models capable\nof thoroughly grasping all these relationships will be free from\nhallucinations. Additionally, I propose that certain types of equivariant\nlanguage models are adept at learning and understanding these relationships.\nBuilding on this, I have developed a specialized cross-entropy error function\nto create a hallucination scale for language models, which measures their\nextent of equivariance acquisition. Utilizing this scale, I tested language\nmodels for their ability to acquire character-level equivariance. In\nparticular, I introduce and employ a novel technique based on T5 (Text To Text\nTransfer Transformer) that efficiently understands permuted input texts without\nthe need for explicit dictionaries to convert token IDs (integers) to texts\n(strings). This T5 model demonstrated a moderate ability to acquire\ncharacter-level equivariance. Additionally, I discovered scale laws that can\naid in developing hallucination-free language models at the character level.\nThis methodology can be extended to assess equivariance acquisition at the word\nlevel, paving the way for very large language models that can comprehensively\nunderstand relationships and, consequently, avoid hallucinations.\n","authors":["Hisaichi Shibata"],"pdf_url":"https://arxiv.org/pdf/2312.14504v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02038v1","updated":"2024-01-04T02:43:57Z","published":"2024-01-04T02:43:57Z","title":"Understanding LLMs: A Comprehensive Overview from Training to Inference","summary":"  The introduction of ChatGPT has led to a significant increase in the\nutilization of Large Language Models (LLMs) for addressing downstream tasks.\nThere's an increasing focus on cost-efficient training and deployment within\nthis context. Low-cost training and deployment of LLMs represent the future\ndevelopment trend. This paper reviews the evolution of large language model\ntraining techniques and inference deployment technologies aligned with this\nemerging trend. The discussion on training includes various aspects, including\ndata preprocessing, training architecture, pre-training tasks, parallel\ntraining, and relevant content related to model fine-tuning. On the inference\nside, the paper covers topics such as model compression, parallel computation,\nmemory scheduling, and structural optimization. It also explores LLMs'\nutilization and provides insights into their future development.\n","authors":["Yiheng Liu","Hao He","Tianle Han","Xu Zhang","Mengyuan Liu","Jiaming Tian","Yutong Zhang","Jiaqi Wang","Xiaohui Gao","Tianyang Zhong","Yi Pan","Shaochen Xu","Zihao Wu","Zhengliang Liu","Xin Zhang","Shu Zhang","Xintao Hu","Tuo Zhang","Ning Qiang","Tianming Liu","Bao Ge"],"pdf_url":"https://arxiv.org/pdf/2401.02038v1.pdf","comment":"30 pages,6 figures"},{"id":"http://arxiv.org/abs/2401.02034v1","updated":"2024-01-04T02:33:38Z","published":"2024-01-04T02:33:38Z","title":"Text2MDT: Extracting Medical Decision Trees from Medical Texts","summary":"  Knowledge of the medical decision process, which can be modeled as medical\ndecision trees (MDTs), is critical to build clinical decision support systems.\nHowever, the current MDT construction methods rely heavily on time-consuming\nand laborious manual annotation. In this work, we propose a novel task,\nText2MDT, to explore the automatic extraction of MDTs from medical texts such\nas medical guidelines and textbooks. We normalize the form of the MDT and\ncreate an annotated Text-to-MDT dataset in Chinese with the participation of\nmedical experts. We investigate two different methods for the Text2MDT tasks:\n(a) an end-to-end framework which only relies on a GPT style large language\nmodels (LLM) instruction tuning to generate all the node information and tree\nstructures. (b) The pipeline framework which decomposes the Text2MDT task to\nthree subtasks. Experiments on our Text2MDT dataset demonstrate that: (a) the\nend-to-end method basd on LLMs (7B parameters or larger) show promising\nresults, and successfully outperform the pipeline methods. (b) The\nchain-of-thought (COT) prompting method \\cite{Wei2022ChainOT} can improve the\nperformance of the fine-tuned LLMs on the Text2MDT test set. (c) the\nlightweight pipelined method based on encoder-based pretrained models can\nperform comparably with LLMs with model complexity two magnititudes smaller.\nOur Text2MDT dataset is open-sourced at\n\\url{https://tianchi.aliyun.com/dataset/95414}, and the source codes are\nopen-sourced at \\url{https://github.com/michael-wzhu/text2dt}.\n","authors":["Wei Zhu","Wenfeng Li","Xing Tian","Pengfei Wang","Xiaoling Wang","Jin Chen","Yuanbin Wu","Yuan Ni","Guotong Xie"],"pdf_url":"https://arxiv.org/pdf/2401.02034v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.06911v2","updated":"2024-01-04T02:22:07Z","published":"2023-08-14T03:12:29Z","title":"GIT-Mol: A Multi-modal Large Language Model for Molecular Science with\n  Graph, Image, and Text","summary":"  Large language models have made significant strides in natural language\nprocessing, enabling innovative applications in molecular science by processing\ntextual representations of molecules. However, most existing language models\ncannot capture the rich information with complex molecular structures or\nimages. In this paper, we introduce GIT-Mol, a multi-modal large language model\nthat integrates the Graph, Image, and Text information. To facilitate the\nintegration of multi-modal molecular data, we propose GIT-Former, a novel\narchitecture that is capable of aligning all modalities into a unified latent\nspace. We achieve a 5%-10% accuracy increase in properties prediction and a\n20.2% boost in molecule generation validity compared to the baselines. With the\nany-to-language molecular translation strategy, our model has the potential to\nperform more downstream tasks, such as compound name recognition and chemical\nreaction prediction.\n","authors":["Pengfei Liu","Yiming Ren","Zhixiang Ren"],"pdf_url":"https://arxiv.org/pdf/2308.06911v2.pdf","comment":"14 pages, 4 figures"},{"id":"http://arxiv.org/abs/2305.06355v2","updated":"2024-01-04T02:06:07Z","published":"2023-05-10T17:59:04Z","title":"VideoChat: Chat-Centric Video Understanding","summary":"  In this paper, we initiate an attempt of developing an end-to-end\nchat-centric video understanding system, coined as VideoChat. It integrates\nvideo foundation models and large language models via a learnable neural\ninterface, excelling in spatiotemporal reasoning, event localization, and\ncausal relationship inference. To instructively tune this system, we build a\nvideo-centric instruction dataset, composed of thousands of videos associated\nwith detailed descriptions and conversations. This dataset emphasizes\nspatiotemporal reasoning and captures causal relationships, providing a\nvaluable asset for training our chat-centric video understanding system.\nPreliminary qualitative experiments demonstrate the potential of our system\nacross a broad spectrum of video applications, which could serve as a simple\nprototype system for future research on chat-centric video understanding.\nAccess our code and data at https://github.com/OpenGVLab/Ask-Anything\n","authors":["KunChang Li","Yinan He","Yi Wang","Yizhuo Li","Wenhai Wang","Ping Luo","Yali Wang","Limin Wang","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2305.06355v2.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2401.01761v2","updated":"2024-01-04T01:30:36Z","published":"2024-01-03T14:28:55Z","title":"Cross-target Stance Detection by Exploiting Target Analytical\n  Perspectives","summary":"  Cross-target stance detection (CTSD) is an important task, which infers the\nattitude of the destination target by utilizing annotated data derived from the\nsource target. One important approach in CTSD is to extract domain-invariant\nfeatures to bridge the knowledge gap between multiple targets. However, the\nanalysis of informal and short text structure, and implicit expressions,\ncomplicate the extraction of domain-invariant knowledge. In this paper, we\npropose a Multi-Perspective Prompt-Tuning (MPPT) model for CTSD that uses the\nanalysis perspective as a bridge to transfer knowledge. First, we develop a\ntwo-stage instruct-based chain-of-thought method (TsCoT) to elicit target\nanalysis perspectives and provide natural language explanations (NLEs) from\nmultiple viewpoints by formulating instructions based on large language model\n(LLM). Second, we propose a multi-perspective prompt-tuning framework\n(MultiPLN) to fuse the NLEs into the stance predictor. Extensive experiments\nresults demonstrate the superiority of MPPT against the state-of-the-art\nbaseline methods.\n","authors":["Daijun Ding","Rong Chen","Liwen Jing","Bowen Zhang","Xu Huang","Li Dong","Xiaowen Zhao","Ge Song"],"pdf_url":"https://arxiv.org/pdf/2401.01761v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02009v1","updated":"2024-01-04T00:32:33Z","published":"2024-01-04T00:32:33Z","title":"Self-Contrast: Better Reflection Through Inconsistent Solving\n  Perspectives","summary":"  The reflection capacity of Large Language Model (LLM) has garnered extensive\nattention. A post-hoc prompting strategy, e.g., reflexion and self-refine,\nrefines LLM's response based on self-evaluated or external feedback. However,\nrecent research indicates without external feedback, LLM's intrinsic reflection\nis unstable. Our investigation unveils that the key bottleneck is the quality\nof the self-evaluated feedback. We find LLMs often exhibit overconfidence or\nhigh randomness when self-evaluate, offering stubborn or inconsistent feedback,\nwhich causes poor reflection. To remedy this, we advocate Self-Contrast: It\nadaptively explores diverse solving perspectives tailored to the request,\ncontrasts the differences, and summarizes these discrepancies into a checklist\nwhich could be used to re-examine and eliminate discrepancies. Our method\nendows LLM with diverse perspectives to alleviate stubborn biases. Moreover,\ntheir discrepancies indicate potential errors or inherent uncertainties that\nLLM often overlooks. Reflecting upon these can catalyze more accurate and\nstable reflection. Experiments conducted on a series of reasoning and\ntranslation tasks with different LLMs serve to underscore the effectiveness and\ngenerality of our strategy.\n","authors":["Wenqi Zhang","Yongliang Shen","Linjuan Wu","Qiuying Peng","Jun Wang","Yueting Zhuang","Weiming Lu"],"pdf_url":"https://arxiv.org/pdf/2401.02009v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.07372v5","updated":"2024-01-04T23:54:41Z","published":"2023-05-12T10:45:29Z","title":"Interactive Text-to-SQL Generation via Editable Step-by-Step\n  Explanations","summary":"  Relational databases play an important role in business, science, and more.\nHowever, many users cannot fully unleash the analytical power of relational\ndatabases, because they are not familiar with database languages such as SQL.\nMany techniques have been proposed to automatically generate SQL from natural\nlanguage, but they suffer from two issues: (1) they still make many mistakes,\nparticularly for complex queries, and (2) they do not provide a flexible way\nfor non-expert users to validate and refine incorrect queries. To address these\nissues, we introduce a new interaction mechanism that allows users to directly\nedit a step-by-step explanation of a query to fix errors. Our experiments on\nmultiple datasets, as well as a user study with 24 participants, demonstrate\nthat our approach can achieve better performance than multiple SOTA approaches.\nOur code and datasets are available at https://github.com/magic-YuanTian/STEPS.\n","authors":["Yuan Tian","Zheng Zhang","Zheng Ning","Toby Jia-Jun Li","Jonathan K. Kummerfeld","Tianyi Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.07372v5.pdf","comment":"Accepted to EMNLP 2023"},{"id":"http://arxiv.org/abs/2312.11514v2","updated":"2024-01-04T22:28:37Z","published":"2023-12-12T18:57:08Z","title":"LLM in a flash: Efficient Large Language Model Inference with Limited\n  Memory","summary":"  Large language models (LLMs) are central to modern natural language\nprocessing, delivering exceptional performance in various tasks. However, their\nsubstantial computational and memory requirements present challenges,\nespecially for devices with limited DRAM capacity. This paper tackles the\nchallenge of efficiently running LLMs that exceed the available DRAM capacity\nby storing the model parameters in flash memory, but bringing them on demand to\nDRAM. Our method involves constructing an inference cost model that takes into\naccount the characteristics of flash memory, guiding us to optimize in two\ncritical areas: reducing the volume of data transferred from flash and reading\ndata in larger, more contiguous chunks. Within this hardware-informed\nframework, we introduce two principal techniques. First, \"windowing\"\nstrategically reduces data transfer by reusing previously activated neurons,\nand second, \"row-column bundling\", tailored to the sequential data access\nstrengths of flash memory, increases the size of data chunks read from flash\nmemory. These methods collectively enable running models up to twice the size\nof the available DRAM, with a 4-5x and 20-25x increase in inference speed\ncompared to naive loading approaches in CPU and GPU, respectively. Our\nintegration of sparsity awareness, context-adaptive loading, and a\nhardware-oriented design paves the way for effective inference of LLMs on\ndevices with limited memory.\n","authors":["Keivan Alizadeh","Iman Mirzadeh","Dmitry Belenko","Karen Khatamifard","Minsik Cho","Carlo C Del Mundo","Mohammad Rastegari","Mehrdad Farajtabar"],"pdf_url":"https://arxiv.org/pdf/2312.11514v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2401.02509v1","updated":"2024-01-04T19:44:03Z","published":"2024-01-04T19:44:03Z","title":"Memory, Consciousness and Large Language Model","summary":"  With the development in cognitive science and Large Language Models (LLMs),\nincreasing connections have come to light between these two distinct fields.\nBuilding upon these connections, we propose a conjecture suggesting the\nexistence of a duality between LLMs and Tulving's theory of memory. We identify\na potential correspondence between Tulving's synergistic ecphory model (SEM) of\nretrieval and the emergent abilities observed in LLMs, serving as supporting\nevidence for our conjecture. Furthermore, we speculate that consciousness may\nbe considered a form of emergent ability based on this duality. We also discuss\nhow other theories of consciousness intersect with our research.\n","authors":["Jitang Li","Jinzheng Li"],"pdf_url":"https://arxiv.org/pdf/2401.02509v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2301.11329v2","updated":"2024-01-04T18:59:56Z","published":"2023-01-26T18:59:33Z","title":"Anatomy-aware and acquisition-agnostic joint registration with\n  SynthMorph","summary":"  Affine image registration is a cornerstone of medical-image analysis. While\nclassical algorithms can achieve excellent accuracy, they solve a\ntime-consuming optimization for every image pair. Deep-learning (DL) methods\nlearn a function that maps an image pair to an output transform. Evaluating the\nfunction is fast, but capturing large transforms can be challenging, and\nnetworks tend to struggle if a test-image characteristic shifts from the\ntraining domain, such as resolution. Most affine methods are agnostic to\nanatomy, meaning the registration will be inaccurate if algorithms consider all\nstructures in the image.\n  We address these shortcomings with SynthMorph, an easy-to-use DL tool for\njoint affine-deformable registration of any brain image without preprocessing,\nright off the MRI scanner. First, we leverage a strategy to train networks with\nwildly varying images synthesized from label maps, yielding robust performance\nacross acquisition specifics unseen at training. Second, we optimize the\nspatial overlap of select anatomical labels. This enables networks to\ndistinguish anatomy of interest from irrelevant structures, removing the need\nfor preprocessing that excludes content which would impinge on anatomy-specific\nregistration. Third, we combine the affine model with a deformable hypernetwork\nthat lets users choose the optimal deformation-field regularity for their\nspecific data, at registration time, in a fraction of the time required by\nclassical methods.\n  We rigorously analyze how competing architectures learn affine transforms and\ncompare state-of-the-art registration tools across an extremely diverse set of\nneuroimaging data, aiming to truly capture the behavior of methods in the real\nworld. SynthMorph demonstrates consistent and improved accuracy. It is\navailable at https://w3id.org/synthmorph, as a single complete end-to-end\nsolution for registration of brain MRI.\n","authors":["Malte Hoffmann","Andrew Hoopes","Douglas N. Greve","Bruce Fischl","Adrian V. Dalca"],"pdf_url":"https://arxiv.org/pdf/2301.11329v2.pdf","comment":"33 pages, 22 figures, 4 tables, affine registration, deformable\n  registration, deep learning, hypernetwork, domain shift, neuroimaging"},{"id":"http://arxiv.org/abs/2401.02418v1","updated":"2024-01-04T18:59:49Z","published":"2024-01-04T18:59:49Z","title":"Learning to Prompt with Text Only Supervision for Vision-Language Models","summary":"  Foundational vision-language models such as CLIP are becoming a new paradigm\nin vision, due to their excellent generalization abilities. However, adapting\nthese models for downstream tasks while maintaining their generalization\nremains a challenge. In literature, one branch of methods adapts CLIP by\nlearning prompts using visual information. While effective, most of these works\nrequire labeled data which is not practical, and often struggle to generalize\ntowards new datasets due to over-fitting on the source data. An alternative\napproach resorts to training-free methods by generating class descriptions from\nlarge language models (LLMs) and perform prompt ensembling. However, these\nmethods often generate class specific prompts that cannot be transferred to\nother classes, which incur higher costs by generating LLM descriptions for each\nclass separately. In this work, we propose to combine the strengths of these\nboth streams of methods by learning prompts using only text data derived from\nLLMs. As supervised training of prompts is not trivial due to absence of\nimages, we develop a training approach that allows prompts to extract rich\ncontextual knowledge from LLM data. Moreover, with LLM contextual data mapped\nwithin the learned prompts, it enables zero-shot transfer of prompts to new\nclasses and datasets potentially cutting the LLM prompt engineering cost. To\nthe best of our knowledge, this is the first work that learns generalized\nprompts using text only data. We perform extensive evaluations on 4 benchmarks\nwhere our method improves over prior ensembling works while being competitive\nto those utilizing labeled images. Our code and pre-trained models are\navailable at https://github.com/muzairkhattak/ProText.\n","authors":["Muhammad Uzair Khattak","Muhammad Ferjad Naeem","Muzammal Naseer","Luc Van Gool","Federico Tombari"],"pdf_url":"https://arxiv.org/pdf/2401.02418v1.pdf","comment":"Project Page: https://muzairkhattak.github.io/ProText/"},{"id":"http://arxiv.org/abs/2401.02416v1","updated":"2024-01-04T18:59:25Z","published":"2024-01-04T18:59:25Z","title":"ODIN: A Single Model for 2D and 3D Perception","summary":"  State-of-the-art models on contemporary 3D perception benchmarks like ScanNet\nconsume and label dataset-provided 3D point clouds, obtained through post\nprocessing of sensed multiview RGB-D images. They are typically trained\nin-domain, forego large-scale 2D pre-training and outperform alternatives that\nfeaturize the posed RGB-D multiview images instead. The gap in performance\nbetween methods that consume posed images versus post-processed 3D point clouds\nhas fueled the belief that 2D and 3D perception require distinct model\narchitectures. In this paper, we challenge this view and propose ODIN\n(Omni-Dimensional INstance segmentation), a model that can segment and label\nboth 2D RGB images and 3D point clouds, using a transformer architecture that\nalternates between 2D within-view and 3D cross-view information fusion. Our\nmodel differentiates 2D and 3D feature operations through the positional\nencodings of the tokens involved, which capture pixel coordinates for 2D patch\ntokens and 3D coordinates for 3D feature tokens. ODIN achieves state-of-the-art\nperformance on ScanNet200, Matterport3D and AI2THOR 3D instance segmentation\nbenchmarks, and competitive performance on ScanNet, S3DIS and COCO. It\noutperforms all previous works by a wide margin when the sensed 3D point cloud\nis used in place of the point cloud sampled from 3D mesh. When used as the 3D\nperception engine in an instructable embodied agent architecture, it sets a new\nstate-of-the-art on the TEACh action-from-dialogue benchmark. Our code and\ncheckpoints can be found at the project website: https://odin-seg.github.io.\n","authors":["Ayush Jain","Pushkal Katara","Nikolaos Gkanatsios","Adam W. Harley","Gabriel Sarch","Kriti Aggarwal","Vishrav Chaudhary","Katerina Fragkiadaki"],"pdf_url":"https://arxiv.org/pdf/2401.02416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02414v1","updated":"2024-01-04T18:55:01Z","published":"2024-01-04T18:55:01Z","title":"Bring Metric Functions into Diffusion Models","summary":"  We introduce a Cascaded Diffusion Model (Cas-DM) that improves a Denoising\nDiffusion Probabilistic Model (DDPM) by effectively incorporating additional\nmetric functions in training. Metric functions such as the LPIPS loss have been\nproven highly effective in consistency models derived from the score matching.\nHowever, for the diffusion counterparts, the methodology and efficacy of adding\nextra metric functions remain unclear. One major challenge is the mismatch\nbetween the noise predicted by a DDPM at each step and the desired clean image\nthat the metric function works well on. To address this problem, we propose\nCas-DM, a network architecture that cascades two network modules to effectively\napply metric functions to the diffusion model training. The first module,\nsimilar to a standard DDPM, learns to predict the added noise and is unaffected\nby the metric function. The second cascaded module learns to predict the clean\nimage, thereby facilitating the metric function computation. Experiment results\nshow that the proposed diffusion model backbone enables the effective use of\nthe LPIPS loss, leading to state-of-the-art image quality (FID, sFID, IS) on\nvarious established benchmarks.\n","authors":["Jie An","Zhengyuan Yang","Jianfeng Wang","Linjie Li","Zicheng Liu","Lijuan Wang","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2401.02414v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02412v1","updated":"2024-01-04T18:53:01Z","published":"2024-01-04T18:53:01Z","title":"LLM Augmented LLMs: Expanding Capabilities through Composition","summary":"  Foundational models with billions of parameters which have been trained on\nlarge corpora of data have demonstrated non-trivial skills in a variety of\ndomains. However, due to their monolithic structure, it is challenging and\nexpensive to augment them or impart new skills. On the other hand, due to their\nadaptation abilities, several new instances of these models are being trained\ntowards new domains and tasks. In this work, we study the problem of efficient\nand practical composition of existing foundation models with more specific\nmodels to enable newer capabilities. To this end, we propose CALM --\nComposition to Augment Language Models -- which introduces cross-attention\nbetween models to compose their representations and enable new capabilities.\nSalient features of CALM are: (i) Scales up LLMs on new tasks by 're-using'\nexisting LLMs along with a few additional parameters and data, (ii) Existing\nmodel weights are kept intact, and hence preserves existing capabilities, and\n(iii) Applies to diverse domains and settings. We illustrate that augmenting\nPaLM2-S with a smaller model trained on low-resource languages results in an\nabsolute improvement of up to 13\\% on tasks like translation into English and\narithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is\naugmented with a code-specific model, we see a relative improvement of 40\\%\nover the base model for code generation and explanation tasks -- on-par with\nfully fine-tuned counterparts.\n","authors":["Rachit Bansal","Bidisha Samanta","Siddharth Dalmia","Nitish Gupta","Shikhar Vashishth","Sriram Ganapathy","Abhishek Bapna","Prateek Jain","Partha Talukdar"],"pdf_url":"https://arxiv.org/pdf/2401.02412v1.pdf","comment":"17 pages, 2 figures, 8 tables"},{"id":"http://arxiv.org/abs/2401.02411v1","updated":"2024-01-04T18:50:38Z","published":"2024-01-04T18:50:38Z","title":"What You See is What You GAN: Rendering Every Pixel for High-Fidelity\n  Geometry in 3D GANs","summary":"  3D-aware Generative Adversarial Networks (GANs) have shown remarkable\nprogress in learning to generate multi-view-consistent images and 3D geometries\nof scenes from collections of 2D images via neural volume rendering. Yet, the\nsignificant memory and computational costs of dense sampling in volume\nrendering have forced 3D GANs to adopt patch-based training or employ\nlow-resolution rendering with post-processing 2D super resolution, which\nsacrifices multiview consistency and the quality of resolved geometry.\nConsequently, 3D GANs have not yet been able to fully resolve the rich 3D\ngeometry present in 2D images. In this work, we propose techniques to scale\nneural volume rendering to the much higher resolution of native 2D images,\nthereby resolving fine-grained 3D geometry with unprecedented detail. Our\napproach employs learning-based samplers for accelerating neural rendering for\n3D GAN training using up to 5 times fewer depth samples. This enables us to\nexplicitly \"render every pixel\" of the full-resolution image during training\nand inference without post-processing superresolution in 2D. Together with our\nstrategy to learn high-quality surface geometry, our method synthesizes\nhigh-resolution 3D geometry and strictly view-consistent images while\nmaintaining image quality on par with baselines relying on post-processing\nsuper resolution. We demonstrate state-of-the-art 3D gemetric quality on FFHQ\nand AFHQ, setting a new standard for unsupervised learning of 3D shapes in 3D\nGANs.\n","authors":["Alex Trevithick","Matthew Chan","Towaki Takikawa","Umar Iqbal","Shalini De Mello","Manmohan Chandraker","Ravi Ramamoorthi","Koki Nagano"],"pdf_url":"https://arxiv.org/pdf/2401.02411v1.pdf","comment":"See our project page: https://research.nvidia.com/labs/nxp/wysiwyg/"},{"id":"http://arxiv.org/abs/2401.02402v1","updated":"2024-01-04T18:39:32Z","published":"2024-01-04T18:39:32Z","title":"3D Open-Vocabulary Panoptic Segmentation with 2D-3D Vision-Language\n  Distillation","summary":"  3D panoptic segmentation is a challenging perception task, which aims to\npredict both semantic and instance annotations for 3D points in a scene.\nAlthough prior 3D panoptic segmentation approaches have achieved great\nperformance on closed-set benchmarks, generalizing to novel categories remains\nan open problem. For unseen object categories, 2D open-vocabulary segmentation\nhas achieved promising results that solely rely on frozen CLIP backbones and\nensembling multiple classification outputs. However, we find that simply\nextending these 2D models to 3D does not achieve good performance due to poor\nper-mask classification quality on novel categories. In this paper, we propose\nthe first method to tackle 3D open-vocabulary panoptic segmentation. Our model\ntakes advantage of the fusion between learnable LiDAR features and dense frozen\nvision CLIP features, using a single classification head to make predictions\nfor both base and novel classes. To further improve the classification\nperformance on novel classes and leverage the CLIP model, we propose two novel\nloss functions: object-level distillation loss and voxel-level distillation\nloss. Our experiments on the nuScenes and SemanticKITTI datasets show that our\nmethod outperforms strong baselines by a large margin.\n","authors":["Zihao Xiao","Longlong Jing","Shangxuan Wu","Alex Zihao Zhu","Jingwei Ji","Chiyu Max Jiang","Wei-Chih Hung","Thomas Funkhouser","Weicheng Kuo","Anelia Angelova","Yin Zhou","Shiwei Sheng"],"pdf_url":"https://arxiv.org/pdf/2401.02402v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.13991v3","updated":"2024-01-04T18:35:21Z","published":"2023-02-27T17:30:00Z","title":"Learning to Generalize towards Unseen Domains via a Content-Aware Style\n  Invariant Model for Disease Detection from Chest X-rays","summary":"  Performance degradation due to distribution discrepancy is a longstanding\nchallenge in intelligent imaging, particularly for chest X-rays (CXRs). Recent\nstudies have demonstrated that CNNs are biased toward styles (e.g.,\nuninformative textures) rather than content (e.g., shape), in stark contrast to\nthe human vision system. Radiologists tend to learn visual cues from CXRs and\nthus perform well across multiple domains. Motivated by this, we employ the\nnovel on-the-fly style randomization modules at both image (SRM-IL) and feature\n(SRM-FL) levels to create rich style perturbed features while keeping the\ncontent intact for robust cross-domain performance. Previous methods simulate\nunseen domains by constructing new styles via interpolation or swapping styles\nfrom existing data, limiting them to available source domains during training.\nHowever, SRM-IL samples the style statistics from the possible value range of a\nCXR image instead of the training data to achieve more diversified\naugmentations. Moreover, we utilize pixel-wise learnable parameters in the\nSRM-FL compared to pre-defined channel-wise mean and standard deviations as\nstyle embeddings for capturing more representative style features.\nAdditionally, we leverage consistency regularizations on global semantic\nfeatures and predictive distributions from with and without style-perturbed\nversions of the same CXR to tweak the model's sensitivity toward content\nmarkers for accurate predictions. Our proposed method, trained on CheXpert and\nMIMIC-CXR datasets, achieves 77.32$\\pm$0.35, 88.38$\\pm$0.19, 82.63$\\pm$0.13\nAUCs(%) on the unseen domain test datasets, i.e., BRAX, VinDr-CXR, and NIH\nchest X-ray14, respectively, compared to 75.56$\\pm$0.80, 87.57$\\pm$0.46,\n82.07$\\pm$0.19 from state-of-the-art models on five-fold cross-validation with\nstatistically significant results in thoracic disease classification.\n","authors":["Mohammad Zunaed","Md. Aynal Haque","Taufiq Hasan"],"pdf_url":"https://arxiv.org/pdf/2302.13991v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02400v1","updated":"2024-01-04T18:32:48Z","published":"2024-01-04T18:32:48Z","title":"Learning the 3D Fauna of the Web","summary":"  Learning 3D models of all animals on the Earth requires massively scaling up\nexisting solutions. With this ultimate goal in mind, we develop 3D-Fauna, an\napproach that learns a pan-category deformable 3D animal model for more than\n100 animal species jointly. One crucial bottleneck of modeling animals is the\nlimited availability of training data, which we overcome by simply learning\nfrom 2D Internet images. We show that prior category-specific attempts fail to\ngeneralize to rare species with limited training images. We address this\nchallenge by introducing the Semantic Bank of Skinned Models (SBSM), which\nautomatically discovers a small set of base animal shapes by combining\ngeometric inductive priors with semantic knowledge implicitly captured by an\noff-the-shelf self-supervised feature extractor. To train such a model, we also\ncontribute a new large-scale dataset of diverse animal species. At inference\ntime, given a single image of any quadruped animal, our model reconstructs an\narticulated 3D mesh in a feed-forward fashion within seconds.\n","authors":["Zizhang Li","Dor Litvak","Ruining Li","Yunzhi Zhang","Tomas Jakab","Christian Rupprecht","Shangzhe Wu","Andrea Vedaldi","Jiajun Wu"],"pdf_url":"https://arxiv.org/pdf/2401.02400v1.pdf","comment":"The first two authors contributed equally to this work. The last\n  three authors contributed equally. Project page:\n  https://kyleleey.github.io/3DFauna/"},{"id":"http://arxiv.org/abs/2312.06661v2","updated":"2024-01-04T17:59:04Z","published":"2023-12-11T18:59:55Z","title":"UpFusion: Novel View Diffusion from Unposed Sparse View Observations","summary":"  We propose UpFusion, a system that can perform novel view synthesis and infer\n3D representations for an object given a sparse set of reference images without\ncorresponding pose information. Current sparse-view 3D inference methods\ntypically rely on camera poses to geometrically aggregate information from\ninput views, but are not robust in-the-wild when such information is\nunavailable/inaccurate. In contrast, UpFusion sidesteps this requirement by\nlearning to implicitly leverage the available images as context in a\nconditional generative model for synthesizing novel views. We incorporate two\ncomplementary forms of conditioning into diffusion models for leveraging the\ninput views: a) via inferring query-view aligned features using a scene-level\ntransformer, b) via intermediate attentional layers that can directly observe\nthe input image tokens. We show that this mechanism allows generating\nhigh-fidelity novel views while improving the synthesis quality given\nadditional (unposed) images. We evaluate our approach on the Co3Dv2 and Google\nScanned Objects datasets and demonstrate the benefits of our method over\npose-reliant sparse-view methods as well as single-view methods that cannot\nleverage additional views. Finally, we also show that our learned model can\ngeneralize beyond the training categories and even allow reconstruction from\nself-captured images of generic objects in-the-wild.\n","authors":["Bharath Raj Nagoor Kani","Hsin-Ying Lee","Sergey Tulyakov","Shubham Tulsiani"],"pdf_url":"https://arxiv.org/pdf/2312.06661v2.pdf","comment":"Project Page: https://upfusion3d.github.io/ v2: Fixed a citation\n  mistake"},{"id":"http://arxiv.org/abs/2401.02384v1","updated":"2024-01-04T17:51:48Z","published":"2024-01-04T17:51:48Z","title":"ChartAssisstant: A Universal Chart Multimodal Language Model via\n  Chart-to-Table Pre-training and Multitask Instruction Tuning","summary":"  Charts play a vital role in data visualization, understanding data patterns,\nand informed decision-making. However, their unique combination of graphical\nelements (e.g., bars, lines) and textual components (e.g., labels, legends)\nposes challenges for general-purpose multimodal models. While vision-language\nmodels trained on chart data excel in comprehension, they struggle with\ngeneralization and require task-specific fine-tuning. To address these\nchallenges, we propose ChartAssistant, a chart-based vision-language model for\nuniversal chart comprehension and reasoning. ChartAssistant leverages ChartSFT,\na comprehensive dataset covering diverse chart-related tasks with basic and\nspecialized chart types. It undergoes a two-stage training process, starting\nwith pre-training on chart-to-table parsing to align chart and text, followed\nby multitask instruction-following fine-tuning. This approach enables\nChartAssistant to achieve competitive performance across various chart tasks\nwithout task-specific fine-tuning. Experimental results demonstrate significant\nperformance gains over the state-of-the-art UniChart method, outperforming\nOpenAI's GPT-4V(ision) on real-world chart data. The code and data are\navailable at https://github.com/OpenGVLab/ChartAst.\n","authors":["Fanqing Meng","Wenqi Shao","Quanfeng Lu","Peng Gao","Kaipeng Zhang","Yu Qiao","Ping Luo"],"pdf_url":"https://arxiv.org/pdf/2401.02384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02383v1","updated":"2024-01-04T17:51:44Z","published":"2024-01-04T17:51:44Z","title":"Survey of 3D Human Body Pose and Shape Estimation Methods for\n  Contemporary Dance Applications","summary":"  3D human body shape and pose estimation from RGB images is a challenging\nproblem with potential applications in augmented/virtual reality, healthcare\nand fitness technology and virtual retail. Recent solutions have focused on\nthree types of inputs: i) single images, ii) multi-view images and iii) videos.\nIn this study, we surveyed and compared 3D body shape and pose estimation\nmethods for contemporary dance and performing arts, with a special focus on\nhuman body pose and dressing, camera viewpoint, illumination conditions and\nbackground conditions. We demonstrated that multi-frame methods, such as PHALP,\nprovide better results than single-frame method for pose estimation when\ndancers are performing contemporary dances.\n","authors":["Darshan Venkatrayappa","Alain Tremeau","Damien Muselet","Philippe Colantoni"],"pdf_url":"https://arxiv.org/pdf/2401.02383v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2008.09062 by other authors"},{"id":"http://arxiv.org/abs/2401.02361v1","updated":"2024-01-04T17:00:49Z","published":"2024-01-04T17:00:49Z","title":"An Open and Comprehensive Pipeline for Unified Object Grounding and\n  Detection","summary":"  Grounding-DINO is a state-of-the-art open-set detection model that tackles\nmultiple vision tasks including Open-Vocabulary Detection (OVD), Phrase\nGrounding (PG), and Referring Expression Comprehension (REC). Its effectiveness\nhas led to its widespread adoption as a mainstream architecture for various\ndownstream applications. However, despite its significance, the original\nGrounding-DINO model lacks comprehensive public technical details due to the\nunavailability of its training code. To bridge this gap, we present\nMM-Grounding-DINO, an open-source, comprehensive, and user-friendly baseline,\nwhich is built with the MMDetection toolbox. It adopts abundant vision datasets\nfor pre-training and various detection and grounding datasets for fine-tuning.\nWe give a comprehensive analysis of each reported result and detailed settings\nfor reproduction. The extensive experiments on the benchmarks mentioned\ndemonstrate that our MM-Grounding-DINO-Tiny outperforms the Grounding-DINO-Tiny\nbaseline. We release all our models to the research community. Codes and\ntrained models are released at\nhttps://github.com/open-mmlab/mmdetection/configs/mm_grounding_dino.\n","authors":["Xiangyu Zhao","Yicheng Chen","Shilin Xu","Xiangtai Li","Xinjiang Wang","Yining Li","Haian Huang"],"pdf_url":"https://arxiv.org/pdf/2401.02361v1.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2401.02358v1","updated":"2024-01-04T16:58:31Z","published":"2024-01-04T16:58:31Z","title":"A novel method to enhance pneumonia detection via a model-level\n  ensembling of CNN and vision transformer","summary":"  Pneumonia remains a leading cause of morbidity and mortality worldwide. Chest\nX-ray (CXR) imaging is a fundamental diagnostic tool, but traditional analysis\nrelies on time-intensive expert evaluation. Recently, deep learning has shown\nimmense potential for automating pneumonia detection from CXRs. This paper\nexplores applying neural networks to improve CXR-based pneumonia diagnosis. We\ndeveloped a novel model fusing Convolution Neural networks (CNN) and Vision\nTransformer networks via model-level ensembling. Our fusion architecture\ncombines a ResNet34 variant and a Multi-Axis Vision Transformer small model.\nBoth base models are initialized with ImageNet pre-trained weights. The output\nlayers are removed, and features are combined using a flattening layer before\nfinal classification. Experiments used the Kaggle pediatric pneumonia dataset\ncontaining 1,341 normal and 3,875 pneumonia CXR images. We compared our model\nagainst standalone ResNet34, Vision Transformer, and Swin Transformer Tiny\nbaseline models using identical training procedures. Extensive data\naugmentation, Adam optimization, learning rate warmup, and decay were employed.\nThe fusion model achieved a state-of-the-art accuracy of 94.87%, surpassing the\nbaselines. We also attained excellent sensitivity, specificity, kappa score,\nand positive predictive value. Confusion matrix analysis confirms fewer\nmisclassifications. The ResNet34 and Vision Transformer combination enables\njointly learning robust features from CNNs and Transformer paradigms. This\nmodel-level ensemble technique effectively integrates their complementary\nstrengths for enhanced pneumonia classification.\n","authors":["Sandeep Angara","Nishith Reddy Mannuru","Aashrith Mannuru","Sharath Thirunagaru"],"pdf_url":"https://arxiv.org/pdf/2401.02358v1.pdf","comment":"NA"},{"id":"http://arxiv.org/abs/2401.02357v1","updated":"2024-01-04T16:57:56Z","published":"2024-01-04T16:57:56Z","title":"Fit-NGP: Fitting Object Models to Neural Graphics Primitives","summary":"  Accurate 3D object pose estimation is key to enabling many robotic\napplications that involve challenging object interactions. In this work, we\nshow that the density field created by a state-of-the-art efficient radiance\nfield reconstruction method is suitable for highly accurate and robust pose\nestimation for objects with known 3D models, even when they are very small and\nwith challenging reflective surfaces. We present a fully automatic object pose\nestimation system based on a robot arm with a single wrist-mounted camera,\nwhich can scan a scene from scratch, detect and estimate the 6-Degrees of\nFreedom (DoF) poses of multiple objects within a couple of minutes of\noperation. Small objects such as bolts and nuts are estimated with accuracy on\norder of 1mm.\n","authors":["Marwan Taher","Ignacio Alzugaray","Andrew J. Davison"],"pdf_url":"https://arxiv.org/pdf/2401.02357v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.05922v3","updated":"2024-01-04T16:52:34Z","published":"2022-12-09T17:34:53Z","title":"Audiovisual Masked Autoencoders","summary":"  Can we leverage the audiovisual information already present in video to\nimprove self-supervised representation learning? To answer this question, we\nstudy various pretraining architectures and objectives within the masked\nautoencoding framework, motivated by the success of similar methods in natural\nlanguage and image understanding. We show that we can achieve significant\nimprovements on audiovisual downstream classification tasks, surpassing the\nstate-of-the-art on VGGSound and AudioSet. Furthermore, we can leverage our\naudiovisual pretraining scheme for multiple unimodal downstream tasks using a\nsingle audiovisual pretrained model. We additionally demonstrate the\ntransferability of our representations, achieving state-of-the-art audiovisual\nresults on Epic Kitchens without pretraining specifically for this dataset.\n","authors":["Mariana-Iuliana Georgescu","Eduardo Fonseca","Radu Tudor Ionescu","Mario Lucic","Cordelia Schmid","Anurag Arnab"],"pdf_url":"https://arxiv.org/pdf/2212.05922v3.pdf","comment":"ICCV 2023"},{"id":"http://arxiv.org/abs/2401.02347v1","updated":"2024-01-04T16:43:46Z","published":"2024-01-04T16:43:46Z","title":"Mining Fine-Grained Image-Text Alignment for Zero-Shot Captioning via\n  Text-Only Training","summary":"  Image captioning aims at generating descriptive and meaningful textual\ndescriptions of images, enabling a broad range of vision-language applications.\nPrior works have demonstrated that harnessing the power of Contrastive Image\nLanguage Pre-training (CLIP) offers a promising approach to achieving zero-shot\ncaptioning, eliminating the need for expensive caption annotations. However,\nthe widely observed modality gap in the latent space of CLIP harms the\nperformance of zero-shot captioning by breaking the alignment between paired\nimage-text features. To address this issue, we conduct an analysis on the CLIP\nlatent space which leads to two findings. Firstly, we observe that the CLIP's\nvisual feature of image subregions can achieve closer proximity to the paired\ncaption due to the inherent information loss in text descriptions. In addition,\nwe show that the modality gap between a paired image-text can be empirically\nmodeled as a zero-mean Gaussian distribution. Motivated by the findings, we\npropose a novel zero-shot image captioning framework with text-only training to\nreduce the modality gap. In particular, we introduce a subregion feature\naggregation to leverage local region information, which produces a compact\nvisual representation for matching text representation. Moreover, we\nincorporate a noise injection and CLIP reranking strategy to boost captioning\nperformance. We also extend our framework to build a zero-shot VQA pipeline,\ndemonstrating its generality. Through extensive experiments on common\ncaptioning and VQA datasets such as MSCOCO, Flickr30k and VQAV2, we show that\nour method achieves remarkable performance improvements. Code is available at\nhttps://github.com/Artanic30/MacCap.\n","authors":["Longtian Qiu","Shan Ning","Xuming He"],"pdf_url":"https://arxiv.org/pdf/2401.02347v1.pdf","comment":"AAAI 2024.Open sourced, Code and Model Available"},{"id":"http://arxiv.org/abs/2401.02335v1","updated":"2024-01-04T16:19:52Z","published":"2024-01-04T16:19:52Z","title":"Linguistic Profiling of Deepfakes: An Open Database for Next-Generation\n  Deepfake Detection","summary":"  The emergence of text-to-image generative models has revolutionized the field\nof deepfakes, enabling the creation of realistic and convincing visual content\ndirectly from textual descriptions. However, this advancement presents\nconsiderably greater challenges in detecting the authenticity of such content.\nExisting deepfake detection datasets and methods often fall short in\neffectively capturing the extensive range of emerging deepfakes and offering\nsatisfactory explanatory information for detection. To address the significant\nissue, this paper introduces a deepfake database (DFLIP-3K) for the development\nof convincing and explainable deepfake detection. It encompasses about 300K\ndiverse deepfake samples from approximately 3K generative models, which boasts\nthe largest number of deepfake models in the literature. Moreover, it collects\naround 190K linguistic footprints of these deepfakes. The two distinguished\nfeatures enable DFLIP-3K to develop a benchmark that promotes progress in\nlinguistic profiling of deepfakes, which includes three sub-tasks namely\ndeepfake detection, model identification, and prompt prediction. The deepfake\nmodel and prompt are two essential components of each deepfake, and thus\ndissecting them linguistically allows for an invaluable exploration of\ntrustworthy and interpretable evidence in deepfake detection, which we believe\nis the key for the next-generation deepfake detection. Furthermore, DFLIP-3K is\nenvisioned as an open database that fosters transparency and encourages\ncollaborative efforts to further enhance its growth. Our extensive experiments\non the developed benchmark verify that our DFLIP-3K database is capable of\nserving as a standardized resource for evaluating and comparing\nlinguistic-based deepfake detection, identification, and prompt prediction\ntechniques.\n","authors":["Yabin Wang","Zhiwu Huang","Zhiheng Ma","Xiaopeng Hong"],"pdf_url":"https://arxiv.org/pdf/2401.02335v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02330v1","updated":"2024-01-04T16:07:43Z","published":"2024-01-04T16:07:43Z","title":"LLaVA-$φ$: Efficient Multi-Modal Assistant with Small Language Model","summary":"  In this paper, we introduce LLaVA-$\\phi$ (LLaVA-Phi), an efficient\nmulti-modal assistant that harnesses the power of the recently advanced small\nlanguage model, Phi-2, to facilitate multi-modal dialogues. LLaVA-Phi marks a\nnotable advancement in the realm of compact multi-modal models. It demonstrates\nthat even smaller language models, with as few as 2.7B parameters, can\neffectively engage in intricate dialogues that integrate both textual and\nvisual elements, provided they are trained with high-quality corpora. Our model\ndelivers commendable performance on publicly available benchmarks that\nencompass visual comprehension, reasoning, and knowledge-based perception.\nBeyond its remarkable performance in multi-modal dialogue tasks, our model\nopens new avenues for applications in time-sensitive environments and systems\nthat require real-time interaction, such as embodied agents. It highlights the\npotential of smaller language models to achieve sophisticated levels of\nunderstanding and interaction, while maintaining greater resource\nefficiency.The project is available at {https://github.com/zhuyiche/llava-phi}.\n","authors":["Yichen Zhu","Minjie Zhu","Ning Liu","Zhicai Ou","Xiaofeng Mou","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2401.02330v1.pdf","comment":"technique report"},{"id":"http://arxiv.org/abs/2401.02326v1","updated":"2024-01-04T15:54:45Z","published":"2024-01-04T15:54:45Z","title":"ClassWise-SAM-Adapter: Parameter Efficient Fine-tuning Adapts Segment\n  Anything to SAR Domain for Semantic Segmentation","summary":"  In the realm of artificial intelligence, the emergence of foundation models,\nbacked by high computing capabilities and extensive data, has been\nrevolutionary. Segment Anything Model (SAM), built on the Vision Transformer\n(ViT) model with millions of parameters and vast training dataset SA-1B, excels\nin various segmentation scenarios relying on its significance of semantic\ninformation and generalization ability. Such achievement of visual foundation\nmodel stimulates continuous researches on specific downstream tasks in computer\nvision. The ClassWise-SAM-Adapter (CWSAM) is designed to adapt the\nhigh-performing SAM for landcover classification on space-borne Synthetic\nAperture Radar (SAR) images. The proposed CWSAM freezes most of SAM's\nparameters and incorporates lightweight adapters for parameter efficient\nfine-tuning, and a classwise mask decoder is designed to achieve semantic\nsegmentation task. This adapt-tuning method allows for efficient landcover\nclassification of SAR images, balancing the accuracy with computational demand.\nIn addition, the task specific input module injects low frequency information\nof SAR images by MLP-based layers to improve the model performance. Compared to\nconventional state-of-the-art semantic segmentation algorithms by extensive\nexperiments, CWSAM showcases enhanced performance with fewer computing\nresources, highlighting the potential of leveraging foundational models like\nSAM for specific downstream tasks in the SAR domain. The source code is\navailable at: https://github.com/xypu98/CWSAM.\n","authors":["Xinyang Pu","Hecheng Jia","Linghao Zheng","Feng Wang","Feng Xu"],"pdf_url":"https://arxiv.org/pdf/2401.02326v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2204.03842v3","updated":"2024-01-04T15:50:25Z","published":"2022-04-08T05:11:04Z","title":"From 2D Images to 3D Model:Weakly Supervised Multi-View Face\n  Reconstruction with Deep Fusion","summary":"  While weakly supervised multi-view face reconstruction (MVR) is garnering\nincreased attention, one critical issue still remains open: how to effectively\nfuse multiple image information to reconstruct high-precision 3D models. In\nthis regard, we propose a novel model called Deep Fusion MVR (DF-MVR) and\ndesign a multi-view encoding to single decoding framework with skip\nconnections, able to extract, integrate, and compensate deep features with\nattention from multi-view images. Furthermore, we adopt the involution kernel\nto enrich deep fusion features with channel features. In addition, we develop\nthe face parse network to learn, identify, and emphasize the critical common\nface area within multi-view images. Experiments on Pixel-Face and Bosphorus\ndatasets indicate the superiority of our model. Without 3D annotation, DF-MVR\nachieves 5.2% and 3.0% RMSE improvement over the existing weakly supervised\nMVRs respectively on Pixel-Face and Bosphorus dataset. Code will be available\npublicly at https://github.com/weiguangzhao/DF_MVR.\n","authors":["Weiguang Zhao","Chaolong Yang","Jianan Ye","Rui Zhang","Yuyao Yan","Xi Yang","Bin Dong","Amir Hussain","Kaizhu Huang"],"pdf_url":"https://arxiv.org/pdf/2204.03842v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02317v1","updated":"2024-01-04T15:34:44Z","published":"2024-01-04T15:34:44Z","title":"BA-SAM: Scalable Bias-Mode Attention Mask for Segment Anything Model","summary":"  In this paper, we address the challenge of image resolution variation for the\nSegment Anything Model (SAM). SAM, known for its zero-shot generalizability,\nexhibits a performance degradation when faced with datasets with varying image\nsizes. Previous approaches tend to resize the image to a fixed size or adopt\nstructure modifications, hindering the preservation of SAM's rich prior\nknowledge. Besides, such task-specific tuning necessitates a complete\nretraining of the model, which is cost-expensive and unacceptable for\ndeployment in the downstream tasks. In this paper, we reformulate this issue as\na length extrapolation problem, where token sequence length varies while\nmaintaining a consistent patch size for images of different sizes. To this end,\nwe propose Scalable Bias-Mode Attention Mask (BA-SAM) to enhance SAM's\nadaptability to varying image resolutions while eliminating the need for\nstructure modifications. Firstly, we introduce a new scaling factor to ensure\nconsistent magnitude in the attention layer's dot product values when the token\nsequence length changes. Secondly, we present a bias-mode attention mask that\nallows each token to prioritize neighboring information, mitigating the impact\nof untrained distant information. Our BA-SAM demonstrates efficacy in two\nscenarios: zero-shot and fine-tuning. Extensive evaluation on diverse datasets,\nincluding DIS5K, DUTS, ISIC, COD10K, and COCO, reveals its ability to\nsignificantly mitigate performance degradation in the zero-shot setting and\nachieve state-of-the-art performance with minimal fine-tuning. Furthermore, we\npropose a generalized model and benchmark, showcasing BA-SAM's generalizability\nacross all four datasets simultaneously.\n","authors":["Yiran Song","Qianyu Zhou","Xiangtai Li","Deng-Ping Fan","Xuequan Lu","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2401.02317v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02313v1","updated":"2024-01-04T15:21:53Z","published":"2024-01-04T15:21:53Z","title":"SuperEdge: Towards a Generalization Model for Self-Supervised Edge\n  Detection","summary":"  Edge detection is a fundamental technique in various computer vision tasks.\nEdges are indeed effectively delineated by pixel discontinuity and can offer\nreliable structural information even in textureless areas. State-of-the-art\nheavily relies on pixel-wise annotations, which are labor-intensive and subject\nto inconsistencies when acquired manually. In this work, we propose a novel\nself-supervised approach for edge detection that employs a multi-level,\nmulti-homography technique to transfer annotations from synthetic to real-world\ndatasets. To fully leverage the generated edge annotations, we developed\nSuperEdge, a streamlined yet efficient model capable of concurrently extracting\nedges at pixel-level and object-level granularity. Thanks to self-supervised\ntraining, our method eliminates the dependency on manual annotated edge labels,\nthereby enhancing its generalizability across diverse datasets. Comparative\nevaluations reveal that SuperEdge advances edge detection, demonstrating\nimprovements of 4.9% in ODS and 3.3% in OIS over the existing STEdge method on\nBIPEDv2.\n","authors":["Leng Kai","Zhang Zhijie","Liu Jie","Zed Boukhers","Sui Wei","Cong Yang","Li Zhijun"],"pdf_url":"https://arxiv.org/pdf/2401.02313v1.pdf","comment":"7pages"},{"id":"http://arxiv.org/abs/2305.17033v4","updated":"2024-01-04T15:10:34Z","published":"2023-05-26T15:40:11Z","title":"The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics\n  (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs)","summary":"  Pediatric tumors of the central nervous system are the most common cause of\ncancer-related death in children. The five-year survival rate for high-grade\ngliomas in children is less than 20\\%. Due to their rarity, the diagnosis of\nthese entities is often delayed, their treatment is mainly based on historic\ntreatment concepts, and clinical trials require multi-institutional\ncollaborations. The MICCAI Brain Tumor Segmentation (BraTS) Challenge is a\nlandmark community benchmark event with a successful history of 12 years of\nresource creation for the segmentation and analysis of adult glioma. Here we\npresent the CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023 challenge, which\nrepresents the first BraTS challenge focused on pediatric brain tumors with\ndata acquired across multiple international consortia dedicated to pediatric\nneuro-oncology and clinical trials. The BraTS-PEDs 2023 challenge focuses on\nbenchmarking the development of volumentric segmentation algorithms for\npediatric brain glioma through standardized quantitative performance evaluation\nmetrics utilized across the BraTS 2023 cluster of challenges. Models gaining\nknowledge from the BraTS-PEDs multi-parametric structural MRI (mpMRI) training\ndata will be evaluated on separate validation and unseen test mpMRI dataof\nhigh-grade pediatric glioma. The CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023\nchallenge brings together clinicians and AI/imaging scientists to lead to\nfaster development of automated segmentation techniques that could benefit\nclinical trials, and ultimately the care of children with brain tumors.\n","authors":["Anahita Fathi Kazerooni","Nastaran Khalili","Xinyang Liu","Debanjan Haldar","Zhifan Jiang","Syed Muhammed Anwar","Jake Albrecht","Maruf Adewole","Udunna Anazodo","Hannah Anderson","Sina Bagheri","Ujjwal Baid","Timothy Bergquist","Austin J. Borja","Evan Calabrese","Verena Chung","Gian-Marco Conte","Farouk Dako","James Eddy","Ivan Ezhov","Ariana Familiar","Keyvan Farahani","Shuvanjan Haldar","Juan Eugenio Iglesias","Anastasia Janas","Elaine Johansen","Blaise V Jones","Florian Kofler","Dominic LaBella","Hollie Anne Lai","Koen Van Leemput","Hongwei Bran Li","Nazanin Maleki","Aaron S McAllister","Zeke Meier","Bjoern Menze","Ahmed W Moawad","Khanak K Nandolia","Julija Pavaine","Marie Piraud","Tina Poussaint","Sanjay P Prabhu","Zachary Reitman","Andres Rodriguez","Jeffrey D Rudie","Ibraheem Salman Shaikh","Lubdha M. Shah","Nakul Sheth","Russel Taki Shinohara","Wenxin Tu","Karthik Viswanathan","Chunhao Wang","Jeffrey B Ware","Benedikt Wiestler","Walter Wiggins","Anna Zapaishchykova","Mariam Aboian","Miriam Bornhorst","Peter de Blank","Michelle Deutsch","Maryam Fouladi","Lindsey Hoffman","Benjamin Kann","Margot Lazow","Leonie Mikael","Ali Nabavizadeh","Roger Packer","Adam Resnick","Brian Rood","Arastoo Vossough","Spyridon Bakas","Marius George Linguraru"],"pdf_url":"https://arxiv.org/pdf/2305.17033v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.02535v3","updated":"2024-01-04T15:06:19Z","published":"2023-08-01T10:02:26Z","title":"Learning to Generate Training Datasets for Robust Semantic Segmentation","summary":"  Semantic segmentation methods have advanced significantly. Still, their\nrobustness to real-world perturbations and object types not seen during\ntraining remains a challenge, particularly in safety-critical applications. We\npropose a novel approach to improve the robustness of semantic segmentation\ntechniques by leveraging the synergy between label-to-image generators and\nimage-to-label segmentation models. Specifically, we design Robusta, a novel\nrobust conditional generative adversarial network to generate realistic and\nplausible perturbed images that can be used to train reliable segmentation\nmodels. We conduct in-depth studies of the proposed generative model, assess\nthe performance and robustness of the downstream segmentation network, and\ndemonstrate that our approach can significantly enhance the robustness in the\nface of real-world perturbations, distribution shifts, and out-of-distribution\nsamples. Our results suggest that this approach could be valuable in\nsafety-critical applications, where the reliability of perception modules such\nas semantic segmentation is of utmost importance and comes with a limited\ncomputational budget in inference. We release our code at\nhttps://github.com/ENSTA-U2IS/robusta.\n","authors":["Marwane Hariat","Olivier Laurent","Rémi Kazmierczak","Shihao Zhang","Andrei Bursuc","Angela Yao","Gianni Franchi"],"pdf_url":"https://arxiv.org/pdf/2308.02535v3.pdf","comment":"Published as a conference paper at WACV 2024"},{"id":"http://arxiv.org/abs/2301.13656v2","updated":"2024-01-04T14:58:13Z","published":"2023-01-31T14:18:19Z","title":"A Survey and Benchmark of Automatic Surface Reconstruction from Point\n  Clouds","summary":"  We present a comprehensive survey and benchmark of both traditional and\nlearning-based methods for surface reconstruction from point clouds. This task\nis particularly challenging for real-world acquisitions due to factors like\nnoise, outliers, non-uniform sampling, and missing data. Traditional approaches\noften simplify the problem by imposing handcrafted priors on either the input\npoint clouds or the resulting surface, a process that can necessitate tedious\nhyperparameter tuning. Conversely, deep learning models have the capability to\ndirectly learn the properties of input point clouds and desired surfaces from\ndata. We study the influence of these handcrafted and learned priors on the\nprecision and robustness of surface reconstruction techniques. We evaluate\nvarious time-tested and contemporary methods in a standardized manner. When\nboth trained and evaluated on point clouds with identical characteristics, the\nlearning-based models consistently produce superior surfaces compared to their\ntraditional counterparts$\\unicode{x2013}$even in scenarios involving novel\nshape categories. However, traditional methods demonstrate greater resilience\nto the diverse array of point cloud anomalies commonly found in real-world 3D\nacquisitions. For the benefit of the research community, we make our code and\ndatasets available, inviting further enhancements to learning-based surface\nreconstruction. This can be accessed at\nhttps://github.com/raphaelsulzer/dsr-benchmark .\n","authors":["Raphael Sulzer","Renaud Marlet","Bruno Vallet","Loic Landrieu"],"pdf_url":"https://arxiv.org/pdf/2301.13656v2.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2401.02309v1","updated":"2024-01-04T14:55:57Z","published":"2024-01-04T14:55:57Z","title":"TR-DETR: Task-Reciprocal Transformer for Joint Moment Retrieval and\n  Highlight Detection","summary":"  Video moment retrieval (MR) and highlight detection (HD) based on natural\nlanguage queries are two highly related tasks, which aim to obtain relevant\nmoments within videos and highlight scores of each video clip. Recently,\nseveral methods have been devoted to building DETR-based networks to solve both\nMR and HD jointly. These methods simply add two separate task heads after\nmulti-modal feature extraction and feature interaction, achieving good\nperformance. Nevertheless, these approaches underutilize the reciprocal\nrelationship between two tasks. In this paper, we propose a task-reciprocal\ntransformer based on DETR (TR-DETR) that focuses on exploring the inherent\nreciprocity between MR and HD. Specifically, a local-global multi-modal\nalignment module is first built to align features from diverse modalities into\na shared latent space. Subsequently, a visual feature refinement is designed to\neliminate query-irrelevant information from visual features for modal\ninteraction. Finally, a task cooperation module is constructed to refine the\nretrieval pipeline and the highlight score prediction process by utilizing the\nreciprocity between MR and HD. Comprehensive experiments on QVHighlights,\nCharades-STA and TVSum datasets demonstrate that TR-DETR outperforms existing\nstate-of-the-art methods. Codes are available at\n\\url{https://github.com/mingyao1120/TR-DETR}.\n","authors":["Hao Sun","Mingyao Zhou","Wenjing Chen","Wei Xie"],"pdf_url":"https://arxiv.org/pdf/2401.02309v1.pdf","comment":"Accepted by AAAI-24"},{"id":"http://arxiv.org/abs/2401.02292v1","updated":"2024-01-04T14:31:56Z","published":"2024-01-04T14:31:56Z","title":"GridFormer: Point-Grid Transformer for Surface Reconstruction","summary":"  Implicit neural networks have emerged as a crucial technology in 3D surface\nreconstruction. To reconstruct continuous surfaces from discrete point clouds,\nencoding the input points into regular grid features (plane or volume) has been\ncommonly employed in existing approaches. However, these methods typically use\nthe grid as an index for uniformly scattering point features. Compared with the\nirregular point features, the regular grid features may sacrifice some\nreconstruction details but improve efficiency. To take full advantage of these\ntwo types of features, we introduce a novel and high-efficiency attention\nmechanism between the grid and point features named Point-Grid Transformer\n(GridFormer). This mechanism treats the grid as a transfer point connecting the\nspace and point cloud. Our method maximizes the spatial expressiveness of grid\nfeatures and maintains computational efficiency. Furthermore, optimizing\npredictions over the entire space could potentially result in blurred\nboundaries. To address this issue, we further propose a boundary optimization\nstrategy incorporating margin binary cross-entropy loss and boundary sampling.\nThis approach enables us to achieve a more precise representation of the object\nstructure. Our experiments validate that our method is effective and\noutperforms the state-of-the-art approaches under widely used benchmarks by\nproducing more precise geometry reconstructions. The code is available at\nhttps://github.com/list17/GridFormer.\n","authors":["Shengtao Li","Ge Gao","Yudong Liu","Yu-Shen Liu","Ming Gu"],"pdf_url":"https://arxiv.org/pdf/2401.02292v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01822v2","updated":"2024-01-04T14:28:02Z","published":"2024-01-03T16:38:56Z","title":"HawkRover: An Autonomous mmWave Vehicular Communication Testbed with\n  Multi-sensor Fusion and Deep Learning","summary":"  Connected and automated vehicles (CAVs) have become a transformative\ntechnology that can change our daily life. Currently, millimeter-wave (mmWave)\nbands are identified as the promising CAV connectivity solution. While it can\nprovide high data rate, their realization faces many challenges such as high\nattenuation during mmWave signal propagation and mobility management. Existing\nsolution has to initiate pilot signal to measure channel information, then\napply signal processing to calculate the best narrow beam towards the receiver\nend to guarantee sufficient signal power. This process takes significant\noverhead and time, hence not suitable for vehicles. In this study, we propose\nan autonomous and low-cost testbed to collect extensive co-located mmWave\nsignal and other sensors data such as LiDAR (Light Detection and Ranging),\ncameras, ultrasonic, etc, traditionally for ``automated'', to facilitate mmWave\nvehicular communications. Intuitively, these sensors can build a 3D map around\nthe vehicle and signal propagation path can be estimated, eliminating iterative\nthe process via pilot signals. This multimodal data fusion, together with AI,\nis expected to bring significant advances in ``connected'' research.\n","authors":["Ethan Zhu","Haijian Sun","Mingyue Ji"],"pdf_url":"https://arxiv.org/pdf/2401.01822v2.pdf","comment":"submitted to IEEE conferences for future publications"},{"id":"http://arxiv.org/abs/2401.02287v1","updated":"2024-01-04T14:10:38Z","published":"2024-01-04T14:10:38Z","title":"Distillation-based fabric anomaly detection","summary":"  Unsupervised texture anomaly detection has been a concerning topic in a vast\namount of industrial processes. Patterned textures inspection, particularly in\nthe context of fabric defect detection, is indeed a widely encountered use\ncase. This task involves handling a diverse spectrum of colors and textile\ntypes, encompassing a wide range of fabrics. Given the extensive variability in\ncolors, textures, and defect types, fabric defect detection poses a complex and\nchallenging problem in the field of patterned textures inspection. In this\narticle, we propose a knowledge distillation-based approach tailored\nspecifically for addressing the challenge of unsupervised anomaly detection in\ntextures resembling fabrics. Our method aims to redefine the recently\nintroduced reverse distillation approach, which advocates for an\nencoder-decoder design to mitigate classifier bias and to prevent the student\nfrom reconstructing anomalies. In this study, we present a new reverse\ndistillation technique for the specific task of fabric defect detection. Our\napproach involves a meticulous design selection that strategically highlights\nhigh-level features. To demonstrate the capabilities of our approach both in\nterms of performance and inference speed, we conducted a series of experiments\non multiple texture datasets, including MVTEC AD, AITEX, and TILDA, alongside\nconducting experiments on a dataset acquired from a textile manufacturing\nfacility. The main contributions of this paper are the following: a robust\ntexture anomaly detector utilizing a reverse knowledge-distillation technique\nsuitable for both anomaly detection and domain generalization and a novel\ndataset encompassing a diverse range of fabrics and defects.\n","authors":["Simon Thomine","Hichem Snoussi"],"pdf_url":"https://arxiv.org/pdf/2401.02287v1.pdf","comment":"Textile Research Journal. 2023;0(0)"},{"id":"http://arxiv.org/abs/2306.07716v3","updated":"2024-01-04T13:58:50Z","published":"2023-06-13T12:07:01Z","title":"Dynamically Masked Discriminator for Generative Adversarial Networks","summary":"  Training Generative Adversarial Networks (GANs) remains a challenging\nproblem. The discriminator trains the generator by learning the distribution of\nreal/generated data. However, the distribution of generated data changes\nthroughout the training process, which is difficult for the discriminator to\nlearn. In this paper, we propose a novel method for GANs from the viewpoint of\nonline continual learning. We observe that the discriminator model, trained on\nhistorically generated data, often slows down its adaptation to the changes in\nthe new arrival generated data, which accordingly decreases the quality of\ngenerated results. By treating the generated data in training as a stream, we\npropose to detect whether the discriminator slows down the learning of new\nknowledge in generated data. Therefore, we can explicitly enforce the\ndiscriminator to learn new knowledge fast. Particularly, we propose a new\ndiscriminator, which automatically detects its retardation and then dynamically\nmasks its features, such that the discriminator can adaptively learn the\ntemporally-vary distribution of generated data. Experimental results show our\nmethod outperforms the state-of-the-art approaches.\n","authors":["Wentian Zhang","Haozhe Liu","Bing Li","Jinheng Xie","Yawen Huang","Yuexiang Li","Yefeng Zheng","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2306.07716v3.pdf","comment":"Updated v2 -- NeurIPS 2023 camera ready version"},{"id":"http://arxiv.org/abs/2401.02281v1","updated":"2024-01-04T13:58:14Z","published":"2024-01-04T13:58:14Z","title":"PEGASUS: Physically Enhanced Gaussian Splatting Simulation System for\n  6DOF Object Pose Dataset Generation","summary":"  We introduce Physically Enhanced Gaussian Splatting Simulation System\n(PEGASUS) for 6DOF object pose dataset generation, a versatile dataset\ngenerator based on 3D Gaussian Splatting. Environment and object\nrepresentations can be easily obtained using commodity cameras to reconstruct\nwith Gaussian Splatting. PEGASUS allows the composition of new scenes by\nmerging the respective underlying Gaussian Splatting point cloud of an\nenvironment with one or multiple objects. Leveraging a physics engine enables\nthe simulation of natural object placement within a scene through interaction\nbetween meshes extracted for the objects and the environment. Consequently, an\nextensive amount of new scenes - static or dynamic - can be created by\ncombining different environments and objects. By rendering scenes from various\nperspectives, diverse data points such as RGB images, depth maps, semantic\nmasks, and 6DoF object poses can be extracted. Our study demonstrates that\ntraining on data generated by PEGASUS enables pose estimation networks to\nsuccessfully transfer from synthetic data to real-world data. Moreover, we\nintroduce the Ramen dataset, comprising 30 Japanese cup noodle items. This\ndataset includes spherical scans that captures images from both object\nhemisphere and the Gaussian Splatting reconstruction, making them compatible\nwith PEGASUS.\n","authors":["Lukas Meyer","Floris Erich","Yusuke Yoshiyasu","Marc Stamminger","Noriaki Ando","Yukiyasu Domae"],"pdf_url":"https://arxiv.org/pdf/2401.02281v1.pdf","comment":"Project Page: https://meyerls.github.io/pegasus_web"},{"id":"http://arxiv.org/abs/2401.02278v1","updated":"2024-01-04T13:56:54Z","published":"2024-01-04T13:56:54Z","title":"Lightweight Fish Classification Model for Sustainable Marine Management:\n  Indonesian Case","summary":"  The enormous demand for seafood products has led to exploitation of marine\nresources and near-extinction of some species. In particular, overfishing is\none the main issues in sustainable marine development. In alignment with the\nprotection of marine resources and sustainable fishing, this study proposes to\nadvance fish classification techniques that support identifying protected fish\nspecies using state-of-the-art machine learning. We use a custom modification\nof the MobileNet model to design a lightweight classifier called M-MobileNet\nthat is capable of running on limited hardware. As part of the study, we\ncompiled a labeled dataset of 37,462 images of fish found in the waters of the\nIndonesian archipelago. The proposed model is trained on the dataset to\nclassify images of the captured fish into their species and give\nrecommendations on whether they are consumable or not. Our modified MobileNet\nmodel uses only 50\\% of the top layer parameters with about 42% GTX 860M\nutility and achieves up to 97% accuracy in fish classification and determining\nits consumability. Given the limited computing capacity available on many\nfishing vessels, the proposed model provides a practical solution to on-site\nfish classification. In addition, synchronized implementation of the proposed\nmodel on multiple vessels can supply valuable information about the movement\nand location of different species of fish.\n","authors":["Febrian Kurniawan","Gandeva Bayu Satrya","Firuz Kamalov"],"pdf_url":"https://arxiv.org/pdf/2401.02278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02274v1","updated":"2024-01-04T13:49:45Z","published":"2024-01-04T13:49:45Z","title":"ShapeAug: Occlusion Augmentation for Event Camera Data","summary":"  Recently, Dynamic Vision Sensors (DVSs) sparked a lot of interest due to\ntheir inherent advantages over conventional RGB cameras. These advantages\ninclude a low latency, a high dynamic range and a low energy consumption.\nNevertheless, the processing of DVS data using Deep Learning (DL) methods\nremains a challenge, particularly since the availability of event training data\nis still limited. This leads to a need for event data augmentation techniques\nin order to improve accuracy as well as to avoid over-fitting on the training\ndata. Another challenge especially in real world automotive applications is\nocclusion, meaning one object is hindering the view onto the object behind it.\nIn this paper, we present a novel event data augmentation approach, which\naddresses this problem by introducing synthetic events for randomly moving\nobjects in a scene. We test our method on multiple DVS classification datasets,\nresulting in an relative improvement of up to 6.5 % in top1-accuracy. Moreover,\nwe apply our augmentation technique on the real world Gen1 Automotive Event\nDataset for object detection, where we especially improve the detection of\npedestrians by up to 5 %.\n","authors":["Katharina Bendig","René Schuster","Didier Stricker"],"pdf_url":"https://arxiv.org/pdf/2401.02274v1.pdf","comment":"Accepted at ICPRAM 2024"},{"id":"http://arxiv.org/abs/2401.01736v2","updated":"2024-01-04T13:24:48Z","published":"2024-01-03T13:19:14Z","title":"Few-shot Adaptation of Multi-modal Foundation Models: A Survey","summary":"  Multi-modal (vision-language) models, such as CLIP, are replacing traditional\nsupervised pre-training models (e.g., ImageNet-based pre-training) as the new\ngeneration of visual foundation models. These models with robust and aligned\nsemantic representations learned from billions of internet image-text pairs and\ncan be applied to various downstream tasks in a zero-shot manner. However, in\nsome fine-grained domains like medical imaging and remote sensing, the\nperformance of multi-modal foundation models often leaves much to be desired.\nConsequently, many researchers have begun to explore few-shot adaptation\nmethods for these models, gradually deriving three main technical approaches:\n1) prompt-based methods, 2) adapter-based methods, and 3) external\nknowledge-based methods. Nevertheless, this rapidly developing field has\nproduced numerous results without a comprehensive survey to systematically\norganize the research progress. Therefore, in this survey, we introduce and\nanalyze the research advancements in few-shot adaptation methods for\nmulti-modal models, summarizing commonly used datasets and experimental setups,\nand comparing the results of different methods. In addition, due to the lack of\nreliable theoretical support for existing methods, we derive the few-shot\nadaptation generalization error bound for multi-modal models. The theorem\nreveals that the generalization error of multi-modal foundation models is\nconstrained by three factors: domain gap, model capacity, and sample size.\nBased on this, we propose three possible solutions from the following aspects:\n1) adaptive domain generalization, 2) adaptive model selection, and 3) adaptive\nknowledge utilization.\n","authors":["Fan Liu","Tianshu Zhang","Wenwen Dai","Wenwen Cai","Xiaocong Zhou","Delong Chen"],"pdf_url":"https://arxiv.org/pdf/2401.01736v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02241v1","updated":"2024-01-04T12:52:48Z","published":"2024-01-04T12:52:48Z","title":"Slot-guided Volumetric Object Radiance Fields","summary":"  We present a novel framework for 3D object-centric representation learning.\nOur approach effectively decomposes complex scenes into individual objects from\na single image in an unsupervised fashion. This method, called slot-guided\nVolumetric Object Radiance Fields (sVORF), composes volumetric object radiance\nfields with object slots as a guidance to implement unsupervised 3D scene\ndecomposition. Specifically, sVORF obtains object slots from a single image via\na transformer module, maps these slots to volumetric object radiance fields\nwith a hypernetwork and composes object radiance fields with the guidance of\nobject slots at a 3D location. Moreover, sVORF significantly reduces memory\nrequirement due to small-sized pixel rendering during training. We demonstrate\nthe effectiveness of our approach by showing top results in scene decomposition\nand generation tasks of complex synthetic datasets (e.g., Room-Diverse).\nFurthermore, we also confirm the potential of sVORF to segment objects in\nreal-world scenes (e.g., the LLFF dataset). We hope our approach can provide\npreliminary understanding of the physical world and help ease future research\nin 3D object-centric representation learning.\n","authors":["Di Qi","Tong Yang","Xiangyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.02241v1.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2301.02008v2","updated":"2024-01-04T12:20:15Z","published":"2023-01-05T11:17:19Z","title":"Expressive Speech-driven Facial Animation with controllable emotions","summary":"  It is in high demand to generate facial animation with high realism, but it\nremains a challenging task. Existing approaches of speech-driven facial\nanimation can produce satisfactory mouth movement and lip synchronization, but\nshow weakness in dramatic emotional expressions and flexibility in emotion\ncontrol. This paper presents a novel deep learning-based approach for\nexpressive facial animation generation from speech that can exhibit\nwide-spectrum facial expressions with controllable emotion type and intensity.\nWe propose an emotion controller module to learn the relationship between the\nemotion variations (e.g., types and intensity) and the corresponding facial\nexpression parameters. It enables emotion-controllable facial animation, where\nthe target expression can be continuously adjusted as desired. The qualitative\nand quantitative evaluations show that the animation generated by our method is\nrich in facial emotional expressiveness while retaining accurate lip movement,\noutperforming other state-of-the-art methods.\n","authors":["Yutong Chen","Junhong Zhao","Wei-Qiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2301.02008v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.04967v2","updated":"2024-01-04T11:42:20Z","published":"2023-09-10T09:00:28Z","title":"Towards Fully Decoupled End-to-End Person Search","summary":"  End-to-end person search aims to jointly detect and re-identify a target\nperson in raw scene images with a unified model. The detection task unifies all\npersons while the re-id task discriminates different identities, resulting in\nconflict optimal objectives. Existing works proposed to decouple end-to-end\nperson search to alleviate such conflict. Yet these methods are still\nsub-optimal on one or two of the sub-tasks due to their partially decoupled\nmodels, which limits the overall person search performance. In this paper, we\npropose to fully decouple person search towards optimal person search. A\ntask-incremental person search network is proposed to incrementally construct\nan end-to-end model for the detection and re-id sub-task, which decouples the\nmodel architecture for the two sub-tasks. The proposed task-incremental network\nallows task-incremental training for the two conflicting tasks. This enables\nindependent learning for different objectives thus fully decoupled the model\nfor persons earch. Comprehensive experimental evaluations demonstrate the\neffectiveness of the proposed fully decoupled models for end-to-end person\nsearch.\n","authors":["Pengcheng Zhang","Xiao Bai","Jin Zheng","Xin Ning"],"pdf_url":"https://arxiv.org/pdf/2309.04967v2.pdf","comment":"DICTA 2023 Best Student Paper"},{"id":"http://arxiv.org/abs/2401.02192v1","updated":"2024-01-04T10:54:05Z","published":"2024-01-04T10:54:05Z","title":"Nodule detection and generation on chest X-rays: NODE21 Challenge","summary":"  Pulmonary nodules may be an early manifestation of lung cancer, the leading\ncause of cancer-related deaths among both men and women. Numerous studies have\nestablished that deep learning methods can yield high-performance levels in the\ndetection of lung nodules in chest X-rays. However, the lack of gold-standard\npublic datasets slows down the progression of the research and prevents\nbenchmarking of methods for this task. To address this, we organized a public\nresearch challenge, NODE21, aimed at the detection and generation of lung\nnodules in chest X-rays. While the detection track assesses state-of-the-art\nnodule detection systems, the generation track determines the utility of nodule\ngeneration algorithms to augment training data and hence improve the\nperformance of the detection systems. This paper summarizes the results of the\nNODE21 challenge and performs extensive additional experiments to examine the\nimpact of the synthetically generated nodule training images on the detection\nalgorithm performance.\n","authors":["Ecem Sogancioglu","Bram van Ginneken","Finn Behrendt","Marcel Bengs","Alexander Schlaefer","Miron Radu","Di Xu","Ke Sheng","Fabien Scalzo","Eric Marcus","Samuele Papa","Jonas Teuwen","Ernst Th. Scholten","Steven Schalekamp","Nils Hendrix","Colin Jacobs","Ward Hendrix","Clara I Sánchez","Keelin Murphy"],"pdf_url":"https://arxiv.org/pdf/2401.02192v1.pdf","comment":"15 pages, 5 figures"},{"id":"http://arxiv.org/abs/2401.02173v1","updated":"2024-01-04T09:55:15Z","published":"2024-01-04T09:55:15Z","title":"Prompt Decoupling for Text-to-Image Person Re-identification","summary":"  Text-to-image person re-identification (TIReID) aims to retrieve the target\nperson from an image gallery via a textual description query. Recently,\npre-trained vision-language models like CLIP have attracted significant\nattention and have been widely utilized for this task due to their robust\ncapacity for semantic concept learning and rich multi-modal knowledge. However,\nrecent CLIP-based TIReID methods commonly rely on direct fine-tuning of the\nentire network to adapt the CLIP model for the TIReID task. Although these\nmethods show competitive performance on this topic, they are suboptimal as they\nnecessitate simultaneous domain adaptation and task adaptation. To address this\nissue, we attempt to decouple these two processes during the training stage.\nSpecifically, we introduce the prompt tuning strategy to enable domain\nadaptation and propose a two-stage training approach to disentangle domain\nadaptation from task adaptation. In the first stage, we freeze the two encoders\nfrom CLIP and solely focus on optimizing the prompts to alleviate domain gap\nbetween the original training data of CLIP and downstream tasks. In the second\nstage, we maintain the fixed prompts and fine-tune the CLIP model to prioritize\ncapturing fine-grained information, which is more suitable for TIReID task.\nFinally, we evaluate the effectiveness of our method on three widely used\ndatasets. Compared to the directly fine-tuned approach, our method achieves\nsignificant improvements.\n","authors":["Weihao Li","Lei Tan","Pingyang Dai","Yan Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.02173v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09024v2","updated":"2024-01-04T09:54:46Z","published":"2023-11-15T15:14:16Z","title":"Fast Certification of Vision-Language Models Using Incremental\n  Randomized Smoothing","summary":"  A key benefit of deep vision-language models such as CLIP is that they enable\nzero-shot open vocabulary classification; the user has the ability to define\nnovel class labels via natural language prompts at inference time. However,\nwhile CLIP-based zero-shot classifiers have demonstrated competitive\nperformance across a range of domain shifts, they remain highly vulnerable to\nadversarial attacks. Therefore, ensuring the robustness of such models is\ncrucial for their reliable deployment in the wild.\n  In this work, we introduce Open Vocabulary Certification (OVC), a fast\ncertification method designed for open-vocabulary models like CLIP via\nrandomized smoothing techniques. Given a base \"training\" set of prompts and\ntheir corresponding certified CLIP classifiers, OVC relies on the observation\nthat a classifier with a novel prompt can be viewed as a perturbed version of\nnearby classifiers in the base training set. Therefore, OVC can rapidly certify\nthe novel classifier using a variation of incremental randomized smoothing. By\nusing a caching trick, we achieve approximately two orders of magnitude\nacceleration in the certification process for novel prompts. To achieve further\n(heuristic) speedups, OVC approximates the embedding space at a given input\nusing a multivariate normal distribution bypassing the need for sampling via\nforward passes through the vision backbone. We demonstrate the effectiveness of\nOVC on through experimental evaluation using multiple vision-language backbones\non the CIFAR-10 and ImageNet test datasets.\n","authors":["A K Nirala","A Joshi","C Hegde","S Sarkar"],"pdf_url":"https://arxiv.org/pdf/2311.09024v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00440v2","updated":"2024-01-04T09:43:33Z","published":"2023-12-31T09:38:53Z","title":"TSGAN: An Optical-to-SAR Dual Conditional GAN for Optical based SAR\n  Temporal Shifting","summary":"  In contrast to the well-investigated field of SAR-to-Optical translation,\nthis study explores the lesser-investigated domain of Optical-to-SAR\ntranslation, a challenging field due to the ill-posed nature of this\ntranslation. The complexity arises as a single optical data can have multiple\nSAR representations based on the SAR viewing geometry. We propose a novel\napproach, termed SAR Temporal Shifting, which inputs an optical data from the\ndesired timestamp along with a SAR data from a different temporal point but\nwith a consistent viewing geometry as the expected SAR data, both complemented\nwith a change map of optical data during the intervening period. This model\nmodifies the SAR data based on the changes observed in optical data to generate\nthe SAR data for the desired timestamp. Our model, a dual conditional\nGenerative Adversarial Network (GAN), named Temporal Shifting GAN (TSGAN),\nincorporates a siamese encoder in both the Generator and the Discriminator. To\nprevent the model from overfitting on the input SAR data, we employed a change\nweighted loss function. Our approach surpasses traditional translation methods\nby eliminating the GAN's fiction phenomenon, particularly in unchanged regions,\nresulting in higher SSIM and PSNR in these areas. Additionally, modifications\nto the Pix2Pix architecture and the inclusion of attention mechanisms have\nenhanced the model's performance on all regions of the data. This research\npaves the way for leveraging legacy optical datasets, the most abundant and\nlongstanding source of Earth imagery data, extending their use to SAR domains\nand temporal analyses. To foster further research, we provide the code,\ndatasets used in our study, and a framework for generating paired SAR-Optical\ndatasets for new regions of interest. These resources are available on\ngithub.com/moienr/TemporalGAN\n","authors":["Moien Rangzan","Sara Attarchi","Richard Gloaguen","Seyed Kazem Alavipanah"],"pdf_url":"https://arxiv.org/pdf/2401.00440v2.pdf","comment":"Comments: Added acknowledgments and corrected a typo. No changes to\n  the main content"},{"id":"http://arxiv.org/abs/2312.12789v2","updated":"2024-01-04T09:34:08Z","published":"2023-12-20T06:22:21Z","title":"SLP-Net:An efficient lightweight network for segmentation of skin\n  lesions","summary":"  Prompt treatment for melanoma is crucial. To assist physicians in identifying\nlesion areas precisely in a quick manner, we propose a novel skin lesion\nsegmentation technique namely SLP-Net, an ultra-lightweight segmentation\nnetwork based on the spiking neural P(SNP) systems type mechanism. Most\nexisting convolutional neural networks achieve high segmentation accuracy while\nneglecting the high hardware cost. SLP-Net, on the contrary, has a very small\nnumber of parameters and a high computation speed. We design a lightweight\nmulti-scale feature extractor without the usual encoder-decoder structure.\nRather than a decoder, a feature adaptation module is designed to replace it\nand implement multi-scale information decoding. Experiments at the ISIC2018\nchallenge demonstrate that the proposed model has the highest Acc and DSC among\nthe state-of-the-art methods, while experiments on the PH2 dataset also\ndemonstrate a favorable generalization ability. Finally, we compare the\ncomputational complexity as well as the computational speed of the models in\nexperiments, where SLP-Net has the highest overall superiority\n","authors":["Bo Yang","Hong Peng","Chenggang Guo","Xiaohui Luo","Jun Wang","Xianzhong Long"],"pdf_url":"https://arxiv.org/pdf/2312.12789v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.12804v2","updated":"2024-01-04T09:28:50Z","published":"2023-12-20T06:52:38Z","title":"Multi-stages attention Breast cancer classification based on nonlinear\n  spiking neural P neurons with autapses","summary":"  Breast cancer(BC) is a prevalent type of malignant tumor in women. Early\ndiagnosis and treatment are vital for enhancing the patients' survival rate.\nDownsampling in deep networks may lead to loss of information, so for\ncompensating the detail and edge information and allowing convolutional neural\nnetworks to pay more attention to seek the lesion region, we propose a\nmulti-stages attention architecture based on NSNP neurons with autapses. First,\nunlike the single-scale attention acquisition methods of existing methods, we\nset up spatial attention acquisition at each feature map scale of the\nconvolutional network to obtain an fusion global information on attention\nguidance. Then we introduce a new type of NSNP variants called NSNP neurons\nwith autapses. Specifically, NSNP systems are modularized as feature encoders,\nrecoding the features extracted from convolutional neural network as well as\nthe fusion of attention information and preserve the key characteristic\nelements in feature maps. This ensures the retention of valuable data while\ngradually transforming high-dimensional complicated info into low-dimensional\nones. The proposed method is evaluated on the public dataset BreakHis at\nvarious magnifications and classification tasks. It achieves a classification\naccuracy of 96.32% at all magnification cases, outperforming state-of-the-art\nmethods. Ablation studies are also performed, verifying the proposed model's\nefficacy. The source code is available at\nXhuBobYoung/Breast-cancer-Classification.\n","authors":["Bo Yang","Hong Peng","Xiaohui Luo","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2312.12804v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.15403v2","updated":"2024-01-04T09:23:07Z","published":"2023-03-27T17:19:50Z","title":"Training-free Content Injection using h-space in Diffusion Models","summary":"  Diffusion models (DMs) synthesize high-quality images in various domains.\nHowever, controlling their generative process is still hazy because the\nintermediate variables in the process are not rigorously studied. Recently, the\nbottleneck feature of the U-Net, namely $h$-space, is found to convey the\nsemantics of the resulting image. It enables StyleCLIP-like latent editing\nwithin DMs. In this paper, we explore further usage of $h$-space beyond\nattribute editing, and introduce a method to inject the content of one image\ninto another image by combining their features in the generative processes.\nBriefly, given the original generative process of the other image, 1) we\ngradually blend the bottleneck feature of the content with proper\nnormalization, and 2) we calibrate the skip connections to match the injected\ncontent. Unlike custom-diffusion approaches, our method does not require\ntime-consuming optimization or fine-tuning. Instead, our method manipulates\nintermediate features within a feed-forward generative process. Furthermore,\nour method does not require supervision from external networks. The code is\navailable at https://curryjung.github.io/InjectFusion/\n","authors":["Jaeseok Jeong","Mingi Kwon","Youngjung Uh"],"pdf_url":"https://arxiv.org/pdf/2303.15403v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02162v1","updated":"2024-01-04T09:19:54Z","published":"2024-01-04T09:19:54Z","title":"Frequency Domain Nuances Mining for Visible-Infrared Person\n  Re-identification","summary":"  The key of visible-infrared person re-identification (VIReID) lies in how to\nminimize the modality discrepancy between visible and infrared images. Existing\nmethods mainly exploit the spatial information while ignoring the\ndiscriminative frequency information. To address this issue, this paper aims to\nreduce the modality discrepancy from the frequency domain perspective.\nSpecifically, we propose a novel Frequency Domain Nuances Mining (FDNM) method\nto explore the cross-modality frequency domain information, which mainly\nincludes an amplitude guided phase (AGP) module and an amplitude nuances mining\n(ANM) module. These two modules are mutually beneficial to jointly explore\nfrequency domain visible-infrared nuances, thereby effectively reducing the\nmodality discrepancy in the frequency domain. Besides, we propose a\ncenter-guided nuances mining loss to encourage the ANM module to preserve\ndiscriminative identity information while discovering diverse cross-modality\nnuances. To the best of our knowledge, this is the first work that explores the\npotential frequency information for VIReID research. Extensive experiments show\nthat the proposed FDNM has significant advantages in improving the performance\nof VIReID. Specifically, our method outperforms the second-best method by 5.2\\%\nin Rank-1 accuracy and 5.8\\% in mAP on the SYSU-MM01 dataset under the indoor\nsearch mode, respectively. Besides, we also validate the effectiveness and\ngeneralization of our method on the challenging visible-infrared face\nrecognition task. \\textcolor{magenta}{The code will be available.}\n","authors":["Yukang Zhang","Yang Lu","Yan Yan","Hanzi Wang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2401.02162v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02161v1","updated":"2024-01-04T09:18:31Z","published":"2024-01-04T09:18:31Z","title":"Enhancing RAW-to-sRGB with Decoupled Style Structure in Fourier Domain","summary":"  RAW to sRGB mapping, which aims to convert RAW images from smartphones into\nRGB form equivalent to that of Digital Single-Lens Reflex (DSLR) cameras, has\nbecome an important area of research. However, current methods often ignore the\ndifference between cell phone RAW images and DSLR camera RGB images, a\ndifference that goes beyond the color matrix and extends to spatial structure\ndue to resolution variations. Recent methods directly rebuild color mapping and\nspatial structure via shared deep representation, limiting optimal performance.\nInspired by Image Signal Processing (ISP) pipeline, which distinguishes image\nrestoration and enhancement, we present a novel Neural ISP framework, named\nFourierISP. This approach breaks the image down into style and structure within\nthe frequency domain, allowing for independent optimization. FourierISP is\ncomprised of three subnetworks: Phase Enhance Subnet for structural refinement,\nAmplitude Refine Subnet for color learning, and Color Adaptation Subnet for\nblending them in a smooth manner. This approach sharpens both color and\nstructure, and extensive evaluations across varied datasets confirm that our\napproach realizes state-of-the-art results. Code will be available at\n~\\url{https://github.com/alexhe101/FourierISP}.\n","authors":["Xuanhua He","Tao Hu","Guoli Wang","Zejin Wang","Run Wang","Qian Zhang","Keyu Yan","Ziyi Chen","Rui Li","Chenjun Xie","Jie Zhang","Man Zhou"],"pdf_url":"https://arxiv.org/pdf/2401.02161v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2011.14956v5","updated":"2024-01-04T09:17:42Z","published":"2020-11-25T09:40:34Z","title":"Handling Noisy Labels via One-Step Abductive Multi-Target Learning and\n  Its Application to Helicobacter Pylori Segmentation","summary":"  Learning from noisy labels is an important concern in plenty of real-world\nscenarios. Various approaches for this concern first make corrections\ncorresponding to potentially noisy-labeled instances, and then update\npredictive model with information of the made corrections. However, in specific\nareas, such as medical histopathology whole slide image analysis (MHWSIA), it\nis often difficult or impossible for experts to manually achieve the noisy-free\nground-truth labels which leads to labels with complex noise. This situation\nraises two more difficult problems: 1) the methodology of approaches making\ncorrections corresponding to potentially noisy-labeled instances has\nlimitations due to the complex noise existing in labels; and 2) the appropriate\nevaluation strategy for validation/testing is unclear because of the great\ndifficulty in collecting the noisy-free ground-truth labels. For the problem\n1), we present one-step abductive multi-target learning (OSAMTL) that imposes a\none-step logical reasoning upon machine learning via a multi-target learning\nprocedure to constrain the predictions of the learning model to be subject to\nour prior knowledge about the true target. For the problem 2), we propose a\nlogical assessment formula (LAF) that evaluates the logical rationality of the\noutputs of an approach by estimating the consistencies between the predictions\nof the learning model and the logical facts narrated from the results of the\none-step logical reasoning of OSAMTL. Based on the Helicobacter pylori (H.\npylori) segmentation task in MHWSIA, we show that OSAMTL enables the machine\nlearning model achieving logically more rational predictions, which is beyond\nvarious state-of-the-art approaches in handling complex noisy labels.\n","authors":["Yongquan Yang","Yiming Yang","Jie Chen","Jiayi Zheng","Zhongxi Zheng"],"pdf_url":"https://arxiv.org/pdf/2011.14956v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09077v2","updated":"2024-01-04T09:04:44Z","published":"2023-11-15T16:19:13Z","title":"Spiking NeRF: Representing the Real-World Geometry by a Discontinuous\n  Representation","summary":"  A crucial reason for the success of existing NeRF-based methods is to build a\nneural density field for the geometry representation via multiple perceptron\nlayers (MLPs). MLPs are continuous functions, however, real geometry or density\nfield is frequently discontinuous at the interface between the air and the\nsurface. Such a contrary brings the problem of unfaithful geometry\nrepresentation. To this end, this paper proposes spiking NeRF, which leverages\nspiking neurons and a hybrid Artificial Neural Network (ANN)-Spiking Neural\nNetwork (SNN) framework to build a discontinuous density field for faithful\ngeometry representation. Specifically, we first demonstrate the reason why\ncontinuous density fields will bring inaccuracy. Then, we propose to use the\nspiking neurons to build a discontinuous density field. We conduct a\ncomprehensive analysis for the problem of existing spiking neuron models and\nthen provide the numerical relationship between the parameter of the spiking\nneuron and the theoretical accuracy of geometry. Based on this, we propose a\nbounded spiking neuron to build the discontinuous density field. Our method\nachieves SOTA performance. The source code and the supplementary material are\navailable at https://github.com/liaozhanfeng/Spiking-NeRF.\n","authors":["Zhanfeng Liao","Qian Zheng","Yan Liu","Gang Pan"],"pdf_url":"https://arxiv.org/pdf/2311.09077v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.17358v3","updated":"2024-01-04T09:00:11Z","published":"2023-06-30T01:32:16Z","title":"Shadow Generation with Decomposed Mask Prediction and Attentive Shadow\n  Filling","summary":"  Image composition refers to inserting a foreground object into a background\nimage to obtain a composite image. In this work, we focus on generating\nplausible shadows for the inserted foreground object to make the composite\nimage more realistic. To supplement the existing small-scale dataset, we create\na large-scale dataset called RdSOBA with rendering techniques. Moreover, we\ndesign a two-stage network named DMASNet with decomposed mask prediction and\nattentive shadow filling. Specifically, in the first stage, we decompose shadow\nmask prediction into box prediction and shape prediction. In the second stage,\nwe attend to reference background shadow pixels to fill the foreground shadow.\nAbundant experiments prove that our DMASNet achieves better visual effects and\ngeneralizes well to real composite images.\n","authors":["Xinhao Tao","Junyan Cao","Yan Hong","Li Niu"],"pdf_url":"https://arxiv.org/pdf/2306.17358v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02151v1","updated":"2024-01-04T08:58:25Z","published":"2024-01-04T08:58:25Z","title":"Frequency-Adaptive Pan-Sharpening with Mixture of Experts","summary":"  Pan-sharpening involves reconstructing missing high-frequency information in\nmulti-spectral images with low spatial resolution, using a higher-resolution\npanchromatic image as guidance. Although the inborn connection with frequency\ndomain, existing pan-sharpening research has not almost investigated the\npotential solution upon frequency domain. To this end, we propose a novel\nFrequency Adaptive Mixture of Experts (FAME) learning framework for\npan-sharpening, which consists of three key components: the Adaptive Frequency\nSeparation Prediction Module, the Sub-Frequency Learning Expert Module, and the\nExpert Mixture Module. In detail, the first leverages the discrete cosine\ntransform to perform frequency separation by predicting the frequency mask. On\nthe basis of generated mask, the second with low-frequency MOE and\nhigh-frequency MOE takes account for enabling the effective low-frequency and\nhigh-frequency information reconstruction. Followed by, the final fusion module\ndynamically weights high-frequency and low-frequency MOE knowledge to adapt to\nremote sensing images with significant content variations. Quantitative and\nqualitative experiments over multiple datasets demonstrate that our method\nperforms the best against other state-of-the-art ones and comprises a strong\ngeneralization ability for real-world scenes. Code will be made publicly at\n\\url{https://github.com/alexhe101/FAME-Net}.\n","authors":["Xuanhua He","Keyu Yan","Rui Li","Chengjun Xie","Jie Zhang","Man Zhou"],"pdf_url":"https://arxiv.org/pdf/2401.02151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02150v1","updated":"2024-01-04T08:57:09Z","published":"2024-01-04T08:57:09Z","title":"Marginal Debiased Network for Fair Visual Recognition","summary":"  Deep neural networks (DNNs) are often prone to learn the spurious\ncorrelations between target classes and bias attributes, like gender and race,\ninherent in a major portion of training data (bias-aligned samples), thus\nshowing unfair behavior and arising controversy in the modern pluralistic and\negalitarian society. In this paper, we propose a novel marginal debiased\nnetwork (MDN) to learn debiased representations. More specifically, a marginal\nsoftmax loss (MSL) is designed by introducing the idea of margin penalty into\nthe fairness problem, which assigns a larger margin for bias-conflicting\nsamples (data without spurious correlations) than for bias-aligned ones, so as\nto deemphasize the spurious correlations and improve generalization on unbiased\ntest criteria. To determine the margins, our MDN is optimized through a meta\nlearning framework. We propose a meta equalized loss (MEL) to perceive the\nmodel fairness, and adaptively update the margin parameters by metaoptimization\nwhich requires the trained model guided by the optimal margins should minimize\nMEL computed on an unbiased meta-validation set. Extensive experiments on\nBiasedMNIST, Corrupted CIFAR-10, CelebA and UTK-Face datasets demonstrate that\nour MDN can achieve a remarkable performance on under-represented samples and\nobtain superior debiased results against the previous approaches.\n","authors":["Mei Wang","Weihong Deng","Sen Su"],"pdf_url":"https://arxiv.org/pdf/2401.02150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02147v1","updated":"2024-01-04T08:53:08Z","published":"2024-01-04T08:53:08Z","title":"Exploring Boundary of GPT-4V on Marine Analysis: A Preliminary Case\n  Study","summary":"  Large language models (LLMs) have demonstrated a powerful ability to answer\nvarious queries as a general-purpose assistant. The continuous multi-modal\nlarge language models (MLLM) empower LLMs with the ability to perceive visual\nsignals. The launch of GPT-4 (Generative Pre-trained Transformers) has\ngenerated significant interest in the research communities. GPT-4V(ison) has\ndemonstrated significant power in both academia and industry fields, as a focal\npoint in a new artificial intelligence generation. Though significant success\nwas achieved by GPT-4V, exploring MLLMs in domain-specific analysis (e.g.,\nmarine analysis) that required domain-specific knowledge and expertise has\ngained less attention. In this study, we carry out the preliminary and\ncomprehensive case study of utilizing GPT-4V for marine analysis. This report\nconducts a systematic evaluation of existing GPT-4V, assessing the performance\nof GPT-4V on marine research and also setting a new standard for future\ndevelopments in MLLMs. The experimental results of GPT-4V show that the\nresponses generated by GPT-4V are still far away from satisfying the\ndomain-specific requirements of the marine professions. All images and prompts\nused in this study will be available at\nhttps://github.com/hkust-vgd/Marine_GPT-4V_Eval\n","authors":["Ziqiang Zheng","Yiwei Chen","Jipeng Zhang","Tuan-Anh Vu","Huimin Zeng","Yue Him Wong Tim","Sai-Kit Yeung"],"pdf_url":"https://arxiv.org/pdf/2401.02147v1.pdf","comment":"51 pages, 36 figures, Repository:\n  https://github.com/hkust-vgd/Marine_GPT-4V_Eval"},{"id":"http://arxiv.org/abs/2401.02142v1","updated":"2024-01-04T08:48:21Z","published":"2024-01-04T08:48:21Z","title":"GUESS:GradUally Enriching SyntheSis for Text-Driven Human Motion\n  Generation","summary":"  In this paper, we propose a novel cascaded diffusion-based generative\nframework for text-driven human motion synthesis, which exploits a strategy\nnamed GradUally Enriching SyntheSis (GUESS as its abbreviation). The strategy\nsets up generation objectives by grouping body joints of detailed skeletons in\nclose semantic proximity together and then replacing each of such joint group\nwith a single body-part node. Such an operation recursively abstracts a human\npose to coarser and coarser skeletons at multiple granularity levels. With\ngradually increasing the abstraction level, human motion becomes more and more\nconcise and stable, significantly benefiting the cross-modal motion synthesis\ntask. The whole text-driven human motion synthesis problem is then divided into\nmultiple abstraction levels and solved with a multi-stage generation framework\nwith a cascaded latent diffusion model: an initial generator first generates\nthe coarsest human motion guess from a given text description; then, a series\nof successive generators gradually enrich the motion details based on the\ntextual description and the previous synthesized results. Notably, we further\nintegrate GUESS with the proposed dynamic multi-condition fusion mechanism to\ndynamically balance the cooperative effects of the given textual condition and\nsynthesized coarse motion prompt in different generation stages. Extensive\nexperiments on large-scale datasets verify that GUESS outperforms existing\nstate-of-the-art methods by large margins in terms of accuracy, realisticness,\nand diversity. Code is available at https://github.com/Xuehao-Gao/GUESS.\n","authors":["Xuehao Gao","Yang Yang","Zhenyu Xie","Shaoyi Du","Zhongqian Sun","Yang Wu"],"pdf_url":"https://arxiv.org/pdf/2401.02142v1.pdf","comment":"Accepted by IEEE Transactions on Visualization and Computer Graphics\n  (2024)"},{"id":"http://arxiv.org/abs/2401.02141v1","updated":"2024-01-04T08:46:39Z","published":"2024-01-04T08:46:39Z","title":"Bayesian Intrinsic Groupwise Image Registration: Unsupervised\n  Disentanglement of Anatomy and Geometry","summary":"  This article presents a general Bayesian learning framework for multi-modal\ngroupwise registration on medical images. The method builds on probabilistic\nmodelling of the image generative process, where the underlying common anatomy\nand geometric variations of the observed images are explicitly disentangled as\nlatent variables. Thus, groupwise registration is achieved through the solution\nto Bayesian inference. We propose a novel hierarchical variational\nauto-encoding architecture to realize the inference procedure of the latent\nvariables, where the registration parameters can be calculated in a\nmathematically interpretable fashion. Remarkably, this new paradigm can learn\ngroupwise registration in an unsupervised closed-loop self-reconstruction\nprocess, sparing the burden of designing complex intensity-based similarity\nmeasures. The computationally efficient disentangled architecture is also\ninherently scalable and flexible, allowing for groupwise registration on\nlarge-scale image groups with variable sizes. Furthermore, the inferred\nstructural representations from disentanglement learning are capable of\ncapturing the latent anatomy of the observations with visual semantics.\nExtensive experiments were conducted to validate the proposed framework,\nincluding four datasets from cardiac, brain and abdominal medical images. The\nresults have demonstrated the superiority of our method over conventional\nsimilarity-based approaches in terms of accuracy, efficiency, scalability and\ninterpretability.\n","authors":["Xinzhe Luo","Xin Wang","Linda Shapiro","Chun Yuan","Jianfeng Feng","Xiahai Zhuang"],"pdf_url":"https://arxiv.org/pdf/2401.02141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02138v1","updated":"2024-01-04T08:43:41Z","published":"2024-01-04T08:43:41Z","title":"Explore Human Parsing Modality for Action Recognition","summary":"  Multimodal-based action recognition methods have achieved high success using\npose and RGB modality. However, skeletons sequences lack appearance depiction\nand RGB images suffer irrelevant noise due to modality limitations. To address\nthis, we introduce human parsing feature map as a novel modality, since it can\nselectively retain effective semantic features of the body parts, while\nfiltering out most irrelevant noise. We propose a new dual-branch framework\ncalled Ensemble Human Parsing and Pose Network (EPP-Net), which is the first to\nleverage both skeletons and human parsing modalities for action recognition.\nThe first human pose branch feeds robust skeletons in graph convolutional\nnetwork to model pose features, while the second human parsing branch also\nleverages depictive parsing feature maps to model parsing festures via\nconvolutional backbones. The two high-level features will be effectively\ncombined through a late fusion strategy for better action recognition.\nExtensive experiments on NTU RGB+D and NTU RGB+D 120 benchmarks consistently\nverify the effectiveness of our proposed EPP-Net, which outperforms the\nexisting action recognition methods. Our code is available at:\nhttps://github.com/liujf69/EPP-Net-Action.\n","authors":["Jinfu Liu","Runwei Ding","Yuhang Wen","Nan Dai","Fanyang Meng","Shen Zhao","Mengyuan Liu"],"pdf_url":"https://arxiv.org/pdf/2401.02138v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2307.07977"},{"id":"http://arxiv.org/abs/2401.02137v1","updated":"2024-01-04T08:42:36Z","published":"2024-01-04T08:42:36Z","title":"SyCoCa: Symmetrizing Contrastive Captioners with Attentive Masking for\n  Multimodal Alignment","summary":"  Multimodal alignment between language and vision is the fundamental topic in\ncurrent vision-language model research. Contrastive Captioners (CoCa), as a\nrepresentative method, integrates Contrastive Language-Image Pretraining (CLIP)\nand Image Caption (IC) into a unified framework, resulting in impressive\nresults. CLIP imposes a bidirectional constraints on global representation of\nentire images and sentences. Although IC conducts an unidirectional\nimage-to-text generation on local representation, it lacks any constraint on\nlocal text-to-image reconstruction, which limits the ability to understand\nimages at a fine-grained level when aligned with texts. To achieve multimodal\nalignment from both global and local perspectives, this paper proposes\nSymmetrizing Contrastive Captioners (SyCoCa), which introduces bidirectional\ninteractions on images and texts across the global and local representation\nlevels. Specifically, we expand a Text-Guided Masked Image Modeling (TG-MIM)\nhead based on ITC and IC heads. The improved SyCoCa can further leverage\ntextual cues to reconstruct contextual images and visual cues to predict\ntextual contents. When implementing bidirectional local interactions, the local\ncontents of images tend to be cluttered or unrelated to their textual\ndescriptions. Thus, we employ an attentive masking strategy to select effective\nimage patches for interaction. Extensive experiments on five vision-language\ntasks, including image-text retrieval, image-captioning, visual question\nanswering, and zero-shot/finetuned image classification, validate the\neffectiveness of our proposed method.\n","authors":["Ziping Ma","Furong Xu","Jian Liu","Ming Yang","Qingpei Guo"],"pdf_url":"https://arxiv.org/pdf/2401.02137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02126v1","updated":"2024-01-04T08:21:30Z","published":"2024-01-04T08:21:30Z","title":"Unified Diffusion-Based Rigid and Non-Rigid Editing with Text and Image\n  Guidance","summary":"  Existing text-to-image editing methods tend to excel either in rigid or\nnon-rigid editing but encounter challenges when combining both, resulting in\nmisaligned outputs with the provided text prompts. In addition, integrating\nreference images for control remains challenging. To address these issues, we\npresent a versatile image editing framework capable of executing both rigid and\nnon-rigid edits, guided by either textual prompts or reference images. We\nleverage a dual-path injection scheme to handle diverse editing scenarios and\nintroduce an integrated self-attention mechanism for fusion of appearance and\nstructural information. To mitigate potential visual artifacts, we further\nemploy latent fusion techniques to adjust intermediate latents. Compared to\nprevious work, our approach represents a significant advance in achieving\nprecise and versatile image editing. Comprehensive experiments validate the\nefficacy of our method, showcasing competitive or superior results in\ntext-based editing and appearance transfer tasks, encompassing both rigid and\nnon-rigid settings.\n","authors":["Jiacheng Wang","Ping Liu","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2401.02126v1.pdf","comment":"15 pages, 13 figures"},{"id":"http://arxiv.org/abs/2312.06978v3","updated":"2024-01-04T08:21:26Z","published":"2023-12-12T04:38:30Z","title":"CLASS-M: Adaptive stain separation-based contrastive learning with\n  pseudo-labeling for histopathological image classification","summary":"  Histopathological image classification is an important task in medical image\nanalysis. Recent approaches generally rely on weakly supervised learning due to\nthe ease of acquiring case-level labels from pathology reports. However,\npatch-level classification is preferable in applications where only a limited\nnumber of cases are available or when local prediction accuracy is critical. On\nthe other hand, acquiring extensive datasets with localized labels for training\nis not feasible. In this paper, we propose a semi-supervised patch-level\nhistopathological image classification model, named CLASS-M, that does not\nrequire extensively labeled datasets. CLASS-M is formed by two main parts: a\ncontrastive learning module that uses separated Hematoxylin and Eosin images\ngenerated through an adaptive stain separation process, and a module with\npseudo-labels using MixUp. We compare our model with other state-of-the-art\nmodels on two clear cell renal cell carcinoma datasets. We demonstrate that our\nCLASS-M model has the best performance on both datasets. Our code is available\nat github.com/BzhangURU/Paper_CLASS-M/tree/main\n","authors":["Bodong Zhang","Hamid Manoochehri","Man Minh Ho","Fahimeh Fooladgar","Yosep Chong","Beatrice S. Knudsen","Deepika Sirohi","Tolga Tasdizen"],"pdf_url":"https://arxiv.org/pdf/2312.06978v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13398v3","updated":"2024-01-04T08:19:16Z","published":"2023-11-22T13:53:04Z","title":"Depth-Regularized Optimization for 3D Gaussian Splatting in Few-Shot\n  Images","summary":"  In this paper, we present a method to optimize Gaussian splatting with a\nlimited number of images while avoiding overfitting. Representing a 3D scene by\ncombining numerous Gaussian splats has yielded outstanding visual quality.\nHowever, it tends to overfit the training views when only a small number of\nimages are available. To address this issue, we introduce a dense depth map as\na geometry guide to mitigate overfitting. We obtained the depth map using a\npre-trained monocular depth estimation model and aligning the scale and offset\nusing sparse COLMAP feature points. The adjusted depth aids in the color-based\noptimization of 3D Gaussian splatting, mitigating floating artifacts, and\nensuring adherence to geometric constraints. We verify the proposed method on\nthe NeRF-LLFF dataset with varying numbers of few images. Our approach\ndemonstrates robust geometry compared to the original method that relies solely\non images. Project page: robot0321.github.io/DepthRegGS\n","authors":["Jaeyoung Chung","Jeongtaek Oh","Kyoung Mu Lee"],"pdf_url":"https://arxiv.org/pdf/2311.13398v3.pdf","comment":"10 pages, 5 figures; Project page: robot0321.github.io/DepthRegGS"},{"id":"http://arxiv.org/abs/2305.17423v3","updated":"2024-01-04T08:10:13Z","published":"2023-05-27T09:14:03Z","title":"Accelerating Text-to-Image Editing via Cache-Enabled Sparse Diffusion\n  Inference","summary":"  Due to the recent success of diffusion models, text-to-image generation is\nbecoming increasingly popular and achieves a wide range of applications. Among\nthem, text-to-image editing, or continuous text-to-image generation, attracts\nlots of attention and can potentially improve the quality of generated images.\nIt's common to see that users may want to slightly edit the generated image by\nmaking minor modifications to their input textual descriptions for several\nrounds of diffusion inference. However, such an image editing process suffers\nfrom the low inference efficiency of many existing diffusion models even using\nGPU accelerators. To solve this problem, we introduce Fast Image Semantically\nEdit (FISEdit), a cached-enabled sparse diffusion model inference engine for\nefficient text-to-image editing. The key intuition behind our approach is to\nutilize the semantic mapping between the minor modifications on the input text\nand the affected regions on the output image. For each text editing step,\nFISEdit can automatically identify the affected image regions and utilize the\ncached unchanged regions' feature map to accelerate the inference process.\nExtensive empirical results show that FISEdit can be $3.4\\times$ and\n$4.4\\times$ faster than existing methods on NVIDIA TITAN RTX and A100 GPUs\nrespectively, and even generates more satisfactory images.\n","authors":["Zihao Yu","Haoyang Li","Fangcheng Fu","Xupeng Miao","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2305.17423v3.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2308.04669v4","updated":"2024-01-04T08:00:37Z","published":"2023-08-09T02:27:23Z","title":"A General Implicit Framework for Fast NeRF Composition and Rendering","summary":"  A variety of Neural Radiance Fields (NeRF) methods have recently achieved\nremarkable success in high render speed. However, current accelerating methods\nare specialized and incompatible with various implicit methods, preventing\nreal-time composition over various types of NeRF works. Because NeRF relies on\nsampling along rays, it is possible to provide general guidance for\nacceleration. To that end, we propose a general implicit pipeline for composing\nNeRF objects quickly. Our method enables the casting of dynamic shadows within\nor between objects using analytical light sources while allowing multiple NeRF\nobjects to be seamlessly placed and rendered together with any arbitrary rigid\ntransformations. Mainly, our work introduces a new surface representation known\nas Neural Depth Fields (NeDF) that quickly determines the spatial relationship\nbetween objects by allowing direct intersection computation between rays and\nimplicit surfaces. It leverages an intersection neural network to query NeRF\nfor acceleration instead of depending on an explicit spatial structure.Our\nproposed method is the first to enable both the progressive and interactive\ncomposition of NeRF objects. Additionally, it also serves as a previewing\nplugin for a range of existing NeRF works.\n","authors":["Xinyu Gao","Ziyi Yang","Yunlu Zhao","Yuxiang Sun","Xiaogang Jin","Changqing Zou"],"pdf_url":"https://arxiv.org/pdf/2308.04669v4.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2401.02117v1","updated":"2024-01-04T07:55:53Z","published":"2024-01-04T07:55:53Z","title":"Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost\n  Whole-Body Teleoperation","summary":"  Imitation learning from human demonstrations has shown impressive performance\nin robotics. However, most results focus on table-top manipulation, lacking the\nmobility and dexterity necessary for generally useful tasks. In this work, we\ndevelop a system for imitating mobile manipulation tasks that are bimanual and\nrequire whole-body control. We first present Mobile ALOHA, a low-cost and\nwhole-body teleoperation system for data collection. It augments the ALOHA\nsystem with a mobile base, and a whole-body teleoperation interface. Using data\ncollected with Mobile ALOHA, we then perform supervised behavior cloning and\nfind that co-training with existing static ALOHA datasets boosts performance on\nmobile manipulation tasks. With 50 demonstrations for each task, co-training\ncan increase success rates by up to 90%, allowing Mobile ALOHA to autonomously\ncomplete complex mobile manipulation tasks such as sauteing and serving a piece\nof shrimp, opening a two-door wall cabinet to store heavy cooking pots, calling\nand entering an elevator, and lightly rinsing a used pan using a kitchen\nfaucet. Project website: https://mobile-aloha.github.io\n","authors":["Zipeng Fu","Tony Z. Zhao","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2401.02117v1.pdf","comment":"Project website: https://mobile-aloha.github.io (Zipeng Fu and Tony\n  Z. Zhao are project co-leads, Chelsea Finn is the advisor)"},{"id":"http://arxiv.org/abs/2401.02113v1","updated":"2024-01-04T07:49:32Z","published":"2024-01-04T07:49:32Z","title":"Source-Free Online Domain Adaptive Semantic Segmentation of Satellite\n  Images under Image Degradation","summary":"  Online adaptation to distribution shifts in satellite image segmentation\nstands as a crucial yet underexplored problem. In this paper, we address\nsource-free and online domain adaptation, i.e., test-time adaptation (TTA), for\nsatellite images, with the focus on mitigating distribution shifts caused by\nvarious forms of image degradation. Towards achieving this goal, we propose a\nnovel TTA approach involving two effective strategies. First, we progressively\nestimate the global Batch Normalization (BN) statistics of the target\ndistribution with incoming data stream. Leveraging these statistics during\ninference has the ability to effectively reduce domain gap. Furthermore, we\nenhance prediction quality by refining the predicted masks using global class\ncenters. Both strategies employ dynamic momentum for fast and stable\nconvergence. Notably, our method is backpropagation-free and hence fast and\nlightweight, making it highly suitable for on-the-fly adaptation to new domain.\nThrough comprehensive experiments across various domain adaptation scenarios,\nwe demonstrate the robust performance of our method.\n","authors":["Fahim Faisal Niloy","Kishor Kumar Bhaumik","Simon S. Woo"],"pdf_url":"https://arxiv.org/pdf/2401.02113v1.pdf","comment":"ICASSP 2024"},{"id":"http://arxiv.org/abs/2401.02110v1","updated":"2024-01-04T07:43:40Z","published":"2024-01-04T07:43:40Z","title":"Significance of Anatomical Constraints in Virtual Try-On","summary":"  The system of Virtual Try-ON (VTON) allows a user to try a product virtually.\nIn general, a VTON system takes a clothing source and a person's image to\npredict the try-on output of the person in the given clothing. Although\nexisting methods perform well for simple poses, in case of bent or crossed arms\nposture or when there is a significant difference between the alignment of the\nsource clothing and the pose of the target person, these methods fail by\ngenerating inaccurate clothing deformations. In the VTON methods that employ\nThin Plate Spline (TPS) based clothing transformations, this mainly occurs for\ntwo reasons - (1)~the second-order smoothness constraint of TPS that restricts\nthe bending of the object plane. (2)~Overlaps among different clothing parts\n(e.g., sleeves and torso) can not be modeled by a single TPS transformation, as\nit assumes the clothing as a single planar object; therefore, disregards the\nindependence of movement of different clothing parts. To this end, we make two\nmajor contributions. Concerning the bending limitations of TPS, we propose a\nhuman AnaTomy-Aware Geometric (ATAG) transformation. Regarding the overlap\nissue, we propose a part-based warping approach that divides the clothing into\nindependently warpable parts to warp them separately and later combine them.\nExtensive analysis shows the efficacy of this approach.\n","authors":["Debapriya Roy","Sanchayan Santra","Diganta Mukherjee","Bhabatosh Chanda"],"pdf_url":"https://arxiv.org/pdf/2401.02110v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2208.08076"},{"id":"http://arxiv.org/abs/2311.17515v2","updated":"2024-01-04T07:42:31Z","published":"2023-11-29T10:38:42Z","title":"Fusion of Single and Integral Multispectral Aerial Images","summary":"  A novel hybrid (model- and learning-based) architecture is presented for\nfusing the most significant features from conventional aerial images with the\nones from integral aerial images that are the result of synthetic aperture\nsensing for removing occlusion. It combines the environment's spatial\nreferences with features of unoccluded targets that would normally be hidden by\ndense vegetation. Our method out-beats state-of-the-art two-channel and\nmulti-channel fusion approaches visually and quantitatively in common metrics,\nsuch as mutual information, visual information fidelity, and peak\nsignal-to-noise ratio. The proposed model does not require manually tuned\nparameters, can be extended to an arbitrary number and combinations of spectral\nchannels, and is reconfigurable for addressing different use cases.\n","authors":["Mohamed Youssef","Oliver Bimber"],"pdf_url":"https://arxiv.org/pdf/2311.17515v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.15707v3","updated":"2024-01-04T07:42:19Z","published":"2023-12-25T12:12:36Z","title":"High-Fidelity Diffusion-based Image Editing","summary":"  Diffusion models have attained remarkable success in the domains of image\ngeneration and editing. It is widely recognized that employing larger inversion\nand denoising steps in diffusion model leads to improved image reconstruction\nquality. However, the editing performance of diffusion models tends to be no\nmore satisfactory even with increasing denoising steps. The deficiency in\nediting could be attributed to the conditional Markovian property of the\nediting process, where errors accumulate throughout denoising steps. To tackle\nthis challenge, we first propose an innovative framework where a rectifier\nmodule is incorporated to modulate diffusion model weights with residual\nfeatures, thereby providing compensatory information to bridge the fidelity\ngap. Furthermore, we introduce a novel learning paradigm aimed at minimizing\nerror propagation during the editing process, which trains the editing\nprocedure in a manner similar to denoising score-matching. Extensive\nexperiments demonstrate that our proposed framework and training strategy\nachieve high-fidelity reconstruction and editing results across various levels\nof denoising steps, meanwhile exhibits exceptional performance in terms of both\nquantitative metric and qualitative assessments. Moreover, we explore our\nmodel's generalization through several applications like image-to-image\ntranslation and out-of-domain image editing.\n","authors":["Chen Hou","Guoqiang Wei","Zhibo Chen"],"pdf_url":"https://arxiv.org/pdf/2312.15707v3.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2401.02099v1","updated":"2024-01-04T07:11:16Z","published":"2024-01-04T07:11:16Z","title":"CLAPP: Contrastive Language-Audio Pre-training in Passive Underwater\n  Vessel Classification","summary":"  Existing research on audio classification faces challenges in recognizing\nattributes of passive underwater vessel scenarios and lacks well-annotated\ndatasets due to data privacy concerns. In this study, we introduce CLAPP\n(Contrastive Language-Audio Pre-training in Passive Underwater Vessel\nClassification), a novel model. Our aim is to train a neural network using a\nwide range of vessel audio and vessel state text pairs obtained from an\noceanship dataset. CLAPP is capable of directly learning from raw vessel audio\ndata and, when available, from carefully curated labels, enabling improved\nrecognition of vessel attributes in passive underwater vessel scenarios.\nModel's zero-shot capability allows predicting the most relevant vessel state\ndescription for a given vessel audio, without directly optimizing for the task.\nOur approach aims to solve 2 challenges: vessel audio-text classification and\npassive underwater vessel audio attribute recognition. The proposed method\nachieves new state-of-the-art results on both Deepship and Shipsear public\ndatasets, with a notable margin of about 7%-13% for accuracy compared to prior\nmethods on zero-shot task.\n","authors":["Zeyu Li","Jingsheng Gao","Tong Yu","Suncheng Xiang","Jiacheng Ruan","Ting Liu","Yuzhuo Fu"],"pdf_url":"https://arxiv.org/pdf/2401.02099v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02097v1","updated":"2024-01-04T06:55:49Z","published":"2024-01-04T06:55:49Z","title":"Preserving Image Properties Through Initializations in Diffusion Models","summary":"  Retail photography imposes specific requirements on images. For instance,\nimages may need uniform background colors, consistent model poses, centered\nproducts, and consistent lighting. Minor deviations from these standards impact\na site's aesthetic appeal, making the images unsuitable for use. We show that\nStable Diffusion methods, as currently applied, do not respect these\nrequirements. The usual practice of training the denoiser with a very noisy\nimage and starting inference with a sample of pure noise leads to inconsistent\ngenerated images during inference. This inconsistency occurs because it is easy\nto tell the difference between samples of the training and inference\ndistributions. As a result, a network trained with centered retail product\nimages with uniform backgrounds generates images with erratic backgrounds. The\nproblem is easily fixed by initializing inference with samples from an\napproximation of noisy images. However, in using such an approximation, the\njoint distribution of text and noisy image at inference time still slightly\ndiffers from that at training time. This discrepancy is corrected by training\nthe network with samples from the approximate noisy image distribution.\nExtensive experiments on real application data show significant qualitative and\nquantitative improvements in performance from adopting these procedures.\nFinally, our procedure can interact well with other control-based methods to\nfurther enhance the controllability of diffusion-based methods.\n","authors":["Jeffrey Zhang","Shao-Yu Chang","Kedan Li","David Forsyth"],"pdf_url":"https://arxiv.org/pdf/2401.02097v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01886v2","updated":"2024-01-04T06:48:15Z","published":"2023-12-04T13:40:05Z","title":"InstructTA: Instruction-Tuned Targeted Attack for Large Vision-Language\n  Models","summary":"  Large vision-language models (LVLMs) have demonstrated their incredible\ncapability in image understanding and response generation. However, this rich\nvisual interaction also makes LVLMs vulnerable to adversarial examples. In this\npaper, we formulate a novel and practical gray-box attack scenario that the\nadversary can only access the visual encoder of the victim LVLM, without the\nknowledge of its prompts (which are often proprietary for service providers and\nnot publicly available) and its underlying large language model (LLM). This\npractical setting poses challenges to the cross-prompt and cross-model\ntransferability of targeted adversarial attack, which aims to confuse the LVLM\nto output a response that is semantically similar to the attacker's chosen\ntarget text. To this end, we propose an instruction-tuned targeted attack\n(dubbed InstructTA) to deliver the targeted adversarial attack on LVLMs with\nhigh transferability. Initially, we utilize a public text-to-image generative\nmodel to \"reverse\" the target response into a target image, and employ GPT-4 to\ninfer a reasonable instruction $\\boldsymbol{p}^\\prime$ from the target\nresponse. We then form a local surrogate model (sharing the same visual encoder\nwith the victim LVLM) to extract instruction-aware features of an adversarial\nimage example and the target image, and minimize the distance between these two\nfeatures to optimize the adversarial example. To further improve the\ntransferability, we augment the instruction $\\boldsymbol{p}^\\prime$ with\ninstructions paraphrased from an LLM. Extensive experiments demonstrate the\nsuperiority of our proposed method in targeted attack performance and\ntransferability.\n","authors":["Xunguang Wang","Zhenlan Ji","Pingchuan Ma","Zongjie Li","Shuai Wang"],"pdf_url":"https://arxiv.org/pdf/2312.01886v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.13505v4","updated":"2024-01-04T06:46:53Z","published":"2023-09-24T00:05:39Z","title":"Rewrite Caption Semantics: Bridging Semantic Gaps for\n  Language-Supervised Semantic Segmentation","summary":"  Vision-Language Pre-training has demonstrated its remarkable zero-shot\nrecognition ability and potential to learn generalizable visual representations\nfrom language supervision. Taking a step ahead, language-supervised semantic\nsegmentation enables spatial localization of textual inputs by learning pixel\ngrouping solely from image-text pairs. Nevertheless, the state-of-the-art\nsuffers from clear semantic gaps between visual and textual modality: plenty of\nvisual concepts appeared in images are missing in their paired captions. Such\nsemantic misalignment circulates in pre-training, leading to inferior zero-shot\nperformance in dense predictions due to insufficient visual concepts captured\nin textual representations. To close such semantic gap, we propose Concept\nCuration (CoCu), a pipeline that leverages CLIP to compensate for the missing\nsemantics. For each image-text pair, we establish a concept archive that\nmaintains potential visually-matched concepts with our proposed vision-driven\nexpansion and text-to-vision-guided ranking. Relevant concepts can thus be\nidentified via cluster-guided sampling and fed into pre-training, thereby\nbridging the gap between visual and textual semantics. Extensive experiments\nover a broad suite of 8 segmentation benchmarks show that CoCu achieves superb\nzero-shot transfer performance and greatly boosts language-supervised\nsegmentation baseline by a large margin, suggesting the value of bridging\nsemantic gap in pre-training data.\n","authors":["Yun Xing","Jian Kang","Aoran Xiao","Jiahao Nie","Ling Shao","Shijian Lu"],"pdf_url":"https://arxiv.org/pdf/2309.13505v4.pdf","comment":"NeurIPS 2023. Code is available at\n  https://github.com/xing0047/rewrite"},{"id":"http://arxiv.org/abs/2401.02094v1","updated":"2024-01-04T06:46:19Z","published":"2024-01-04T06:46:19Z","title":"Federated Class-Incremental Learning with Prototype Guided Transformer","summary":"  Existing federated learning methods have effectively addressed decentralized\nlearning in scenarios involving data privacy and non-IID data. However, in\nreal-world situations, each client dynamically learns new classes, requiring\nthe global model to maintain discriminative capabilities for both new and old\nclasses. To effectively mitigate the effects of catastrophic forgetting and\ndata heterogeneity under low communication costs, we designed a simple and\neffective method named PLoRA. On the one hand, we adopt prototype learning to\nlearn better feature representations and leverage the heuristic information\nbetween prototypes and class features to design a prototype re-weight module to\nsolve the classifier bias caused by data heterogeneity without retraining the\nclassification layer. On the other hand, our approach utilizes a pre-trained\nmodel as the backbone and utilizes LoRA to fine-tune with a tiny amount of\nparameters when learning new classes. Moreover, PLoRA does not rely on\nsimilarity-based module selection strategies, thereby further reducing\ncommunication overhead. Experimental results on standard datasets indicate that\nour method outperforms the state-of-the-art approaches significantly. More\nimportantly, our method exhibits strong robustness and superiority in various\nscenarios and degrees of data heterogeneity. Our code will be publicly\navailable.\n","authors":["Haiyang Guo","Fei Zhu","Wenzhuo Liu","Xu-Yao Zhang","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2401.02094v1.pdf","comment":"11 pages, 4 figures, conference"},{"id":"http://arxiv.org/abs/2312.11973v2","updated":"2024-01-04T06:26:36Z","published":"2023-12-19T09:11:49Z","title":"Continual Learning: Forget-free Winning Subnetworks for Video\n  Representations","summary":"  Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the\nexistence of efficient subnetworks within larger, dense networks, a\nhigh-performing Winning Subnetwork (WSN) in terms of task performance under\nappropriate sparsity conditions is considered for various continual learning\ntasks. It leverages pre-existing weights from dense networks to achieve\nefficient learning in Task Incremental Learning (TIL) scenarios. In Few-Shot\nClass Incremental Learning (FSCIL), a variation of WSN referred to as the Soft\nsubnetwork (SoftNet) is designed to prevent overfitting when the data samples\nare scarce. Furthermore, the sparse reuse of WSN weights is considered for\nVideo Incremental Learning (VIL). The use of Fourier Subneural Operator (FSO)\nwithin WSN is considered. It enables compact encoding of videos and identifies\nreusable subnetworks across varying bandwidths. We have integrated FSO into\ndifferent architectural frameworks for continual learning, including VIL, TIL,\nand FSCIL. Our comprehensive experiments demonstrate FSO's effectiveness,\nsignificantly improving task performance at various convolutional\nrepresentational levels. Specifically, FSO enhances higher-layer performance in\nTIL and FSCIL and lower-layer performance in VIL\n","authors":["Haeyong Kang","Jaehong Yoon","Sung Ju Hwang","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2312.11973v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2303.14962,\n  arXiv:2306.11305"},{"id":"http://arxiv.org/abs/2312.08774v3","updated":"2024-01-04T06:01:35Z","published":"2023-12-14T09:50:09Z","title":"VSFormer: Visual-Spatial Fusion Transformer for Correspondence Pruning","summary":"  Correspondence pruning aims to find correct matches (inliers) from an initial\nset of putative correspondences, which is a fundamental task for many\napplications. The process of finding is challenging, given the varying inlier\nratios between scenes/image pairs due to significant visual differences.\nHowever, the performance of the existing methods is usually limited by the\nproblem of lacking visual cues (\\eg texture, illumination, structure) of\nscenes. In this paper, we propose a Visual-Spatial Fusion Transformer\n(VSFormer) to identify inliers and recover camera poses accurately. Firstly, we\nobtain highly abstract visual cues of a scene with the cross attention between\nlocal features of two-view images. Then, we model these visual cues and\ncorrespondences by a joint visual-spatial fusion module, simultaneously\nembedding visual cues into correspondences for pruning. Additionally, to mine\nthe consistency of correspondences, we also design a novel module that combines\nthe KNN-based graph and the transformer, effectively capturing both local and\nglobal contexts. Extensive experiments have demonstrated that the proposed\nVSFormer outperforms state-of-the-art methods on outdoor and indoor benchmarks.\nOur code is provided at the following repository:\nhttps://github.com/sugar-fly/VSFormer.\n","authors":["Tangfei Liao","Xiaoqin Zhang","Li Zhao","Tao Wang","Guobao Xiao"],"pdf_url":"https://arxiv.org/pdf/2312.08774v3.pdf","comment":"Accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2312.17492v2","updated":"2024-01-04T05:57:15Z","published":"2023-12-29T06:46:37Z","title":"HEAP: Unsupervised Object Discovery and Localization with Contrastive\n  Grouping","summary":"  Unsupervised object discovery and localization aims to detect or segment\nobjects in an image without any supervision. Recent efforts have demonstrated a\nnotable potential to identify salient foreground objects by utilizing\nself-supervised transformer features. However, their scopes only build upon\npatch-level features within an image, neglecting region/image-level and\ncross-image relationships at a broader scale. Moreover, these methods cannot\ndifferentiate various semantics from multiple instances. To address these\nproblems, we introduce Hierarchical mErging framework via contrAstive grouPing\n(HEAP). Specifically, a novel lightweight head with cross-attention mechanism\nis designed to adaptively group intra-image patches into semantically coherent\nregions based on correlation among self-supervised features. Further, to ensure\nthe distinguishability among various regions, we introduce a region-level\ncontrastive clustering loss to pull closer similar regions across images. Also,\nan image-level contrastive loss is present to push foreground and background\nrepresentations apart, with which foreground objects and background are\naccordingly discovered. HEAP facilitates efficient hierarchical image\ndecomposition, which contributes to more accurate object discovery while also\nenabling differentiation among objects of various classes. Extensive\nexperimental results on semantic segmentation retrieval, unsupervised object\ndiscovery, and saliency detection tasks demonstrate that HEAP achieves\nstate-of-the-art performance.\n","authors":["Xin Zhang","Jinheng Xie","Yuan Yuan","Michael Bi Mi","Robby T. Tan"],"pdf_url":"https://arxiv.org/pdf/2312.17492v2.pdf","comment":"Accepted by AAAI24"},{"id":"http://arxiv.org/abs/2401.02076v1","updated":"2024-01-04T05:56:38Z","published":"2024-01-04T05:56:38Z","title":"Leveraging SAM for Single-Source Domain Generalization in Medical Image\n  Segmentation","summary":"  Domain Generalization (DG) aims to reduce domain shifts between domains to\nachieve promising performance on the unseen target domain, which has been\nwidely practiced in medical image segmentation. Single-source domain\ngeneralization (SDG) is the most challenging setting that trains on only one\nsource domain. Although existing methods have made considerable progress on SDG\nof medical image segmentation, the performances are still far from the\napplicable standards when faced with a relatively large domain shift. In this\npaper, we leverage the Segment Anything Model (SAM) to SDG to greatly improve\nthe ability of generalization. Specifically, we introduce a parallel framework,\nthe source images are sent into the SAM module and normal segmentation module\nrespectively. To reduce the calculation resources, we apply a merging strategy\nbefore sending images to the SAM module. We extract the bounding boxes from the\nsegmentation module and send the refined version as prompts to the SAM module.\nWe evaluate our model on a classic DG dataset and achieve competitive results\ncompared to other state-of-the-art DG methods. Furthermore, We conducted a\nseries of ablation experiments to prove the effectiveness of the proposed\nmethod. The code is publicly available at https://github.com/SARIHUST/SAMMed.\n","authors":["Hanhui Wang","Huaize Ye","Yi Xia","Xueyan Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.02076v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00926v2","updated":"2024-01-04T05:18:15Z","published":"2024-01-01T16:28:30Z","title":"Accurate Leukocyte Detection Based on Deformable-DETR and Multi-Level\n  Feature Fusion for Aiding Diagnosis of Blood Diseases","summary":"  In standard hospital blood tests, the traditional process requires doctors to\nmanually isolate leukocytes from microscopic images of patients' blood using\nmicroscopes. These isolated leukocytes are then categorized via automatic\nleukocyte classifiers to determine the proportion and volume of different types\nof leukocytes present in the blood samples, aiding disease diagnosis. This\nmethodology is not only time-consuming and labor-intensive, but it also has a\nhigh propensity for errors due to factors such as image quality and\nenvironmental conditions, which could potentially lead to incorrect subsequent\nclassifications and misdiagnosis. To address these issues, this paper proposes\nan innovative method of leukocyte detection: the Multi-level Feature Fusion and\nDeformable Self-attention DETR (MFDS-DETR). To tackle the issue of leukocyte\nscale disparity, we designed the High-level Screening-feature Fusion Pyramid\n(HS-FPN), enabling multi-level fusion. This model uses high-level features as\nweights to filter low-level feature information via a channel attention module\nand then merges the screened information with the high-level features, thus\nenhancing the model's feature expression capability. Further, we address the\nissue of leukocyte feature scarcity by incorporating a multi-scale deformable\nself-attention module in the encoder and using the self-attention and\ncross-deformable attention mechanisms in the decoder, which aids in the\nextraction of the global features of the leukocyte feature maps. The\neffectiveness, superiority, and generalizability of the proposed MFDS-DETR\nmethod are confirmed through comparisons with other cutting-edge leukocyte\ndetection models using the private WBCDD, public LISC and BCCD datasets. Our\nsource code and private WBCCD dataset are available at\nhttps://github.com/JustlfC03/MFDS-DETR.\n","authors":["Yifei Chen","Chenyan Zhang","Ben Chen","Yiyu Huang","Yifei Sun","Changmiao Wang","Xianjun Fu","Yuxing Dai","Feiwei Qin","Yong Peng","Yu Gao"],"pdf_url":"https://arxiv.org/pdf/2401.00926v2.pdf","comment":"15 pages, 11 figures, accept Computers in Biology and Medicine 2024"},{"id":"http://arxiv.org/abs/2307.06942v2","updated":"2024-01-04T05:00:34Z","published":"2023-07-13T17:58:32Z","title":"InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding\n  and Generation","summary":"  This paper introduces InternVid, a large-scale video-centric multimodal\ndataset that enables learning powerful and transferable video-text\nrepresentations for multimodal understanding and generation. The InternVid\ndataset contains over 7 million videos lasting nearly 760K hours, yielding 234M\nvideo clips accompanied by detailed descriptions of total 4.1B words. Our core\ncontribution is to develop a scalable approach to autonomously build a\nhigh-quality video-text dataset with large language models (LLM), thereby\nshowcasing its efficacy in learning video-language representation at scale.\nSpecifically, we utilize a multi-scale approach to generate video-related\ndescriptions. Furthermore, we introduce ViCLIP, a video-text representation\nlearning model based on ViT-L. Learned on InternVid via contrastive learning,\nthis model demonstrates leading zero-shot action recognition and competitive\nvideo retrieval performance. Beyond basic video understanding tasks like\nrecognition and retrieval, our dataset and model have broad applications. They\nare particularly beneficial for generating interleaved video-text data for\nlearning a video-centric dialogue system, advancing video-to-text and\ntext-to-video generation research. These proposed resources provide a tool for\nresearchers and practitioners interested in multimodal video understanding and\ngeneration.\n","authors":["Yi Wang","Yinan He","Yizhuo Li","Kunchang Li","Jiashuo Yu","Xin Ma","Xinhao Li","Guo Chen","Xinyuan Chen","Yaohui Wang","Conghui He","Ping Luo","Ziwei Liu","Yali Wang","Limin Wang","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2307.06942v2.pdf","comment":"Data and Code:\n  https://github.com/OpenGVLab/InternVideo/tree/main/Data/InternVid"},{"id":"http://arxiv.org/abs/2401.01642v2","updated":"2024-01-04T03:23:42Z","published":"2024-01-03T09:37:03Z","title":"BLADE: Box-Level Supervised Amodal Segmentation through Directed\n  Expansion","summary":"  Perceiving the complete shape of occluded objects is essential for human and\nmachine intelligence. While the amodal segmentation task is to predict the\ncomplete mask of partially occluded objects, it is time-consuming and\nlabor-intensive to annotate the pixel-level ground truth amodal masks.\nBox-level supervised amodal segmentation addresses this challenge by relying\nsolely on ground truth bounding boxes and instance classes as supervision,\nthereby alleviating the need for exhaustive pixel-level annotations.\nNevertheless, current box-level methodologies encounter limitations in\ngenerating low-resolution masks and imprecise boundaries, failing to meet the\ndemands of practical real-world applications. We present a novel solution to\ntackle this problem by introducing a directed expansion approach from visible\nmasks to corresponding amodal masks. Our approach involves a hybrid end-to-end\nnetwork based on the overlapping region - the area where different instances\nintersect. Diverse segmentation strategies are applied for overlapping regions\nand non-overlapping regions according to distinct characteristics. To guide the\nexpansion of visible masks, we introduce an elaborately-designed connectivity\nloss for overlapping regions, which leverages correlations with visible masks\nand facilitates accurate amodal segmentation. Experiments are conducted on\nseveral challenging datasets and the results show that our proposed method can\noutperform existing state-of-the-art methods with large margins.\n","authors":["Zhaochen Liu","Zhixuan Li","Tingting Jiang"],"pdf_url":"https://arxiv.org/pdf/2401.01642v2.pdf","comment":"Accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2401.01839v2","updated":"2024-01-04T03:23:04Z","published":"2024-01-03T17:11:27Z","title":"Frequency Domain Modality-invariant Feature Learning for\n  Visible-infrared Person Re-Identification","summary":"  Visible-infrared person re-identification (VI-ReID) is challenging due to the\nsignificant cross-modality discrepancies between visible and infrared images.\nWhile existing methods have focused on designing complex network architectures\nor using metric learning constraints to learn modality-invariant features, they\noften overlook which specific component of the image causes the modality\ndiscrepancy problem. In this paper, we first reveal that the difference in the\namplitude component of visible and infrared images is the primary factor that\ncauses the modality discrepancy and further propose a novel Frequency Domain\nmodality-invariant feature learning framework (FDMNet) to reduce modality\ndiscrepancy from the frequency domain perspective. Our framework introduces two\nnovel modules, namely the Instance-Adaptive Amplitude Filter (IAF) module and\nthe Phrase-Preserving Normalization (PPNorm) module, to enhance the\nmodality-invariant amplitude component and suppress the modality-specific\ncomponent at both the image- and feature-levels. Extensive experimental results\non two standard benchmarks, SYSU-MM01 and RegDB, demonstrate the superior\nperformance of our FDMNet against state-of-the-art methods.\n","authors":["Yulin Li","Tianzhu Zhang","Yongdong Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.01839v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2401.02044v1","updated":"2024-01-04T03:09:39Z","published":"2024-01-04T03:09:39Z","title":"Generalizable vision-language pre-training for annotation-free pathology\n  localization","summary":"  Locating pathologies automatically from medical images aids the understanding\nof the emergence and progression of diseases, and such an ability can\nsignificantly benefit clinical diagnostics. However, existing deep learning\nmodels heavily rely on expert annotations and lack generalization capabilities\nin open clinical environments. In this study, we present a generalizable\nvision-language pre-training model for Annotation-Free pathology Localization\n(AFLoc). The core strength of AFLoc lies in its image annotation-free\nmulti-level semantic structure-based contrastive learning, which\ncomprehensively aligns multi-granularity medical concepts from reports with\nabundant image features, to adapt to the diverse expressions of observed and\nemerging unseen pathologies. We conducted extensive experimental validation\nacross 4 distinct external datasets, encompassing 11 types of chest\npathologies, to verify its generalization ability. The results demonstrate that\nAFLoc surpasses 6 state-of-the-art methods and even outperforms the human\nbenchmark in locating 5 different pathologies, underscoring its suitability for\ncomplex clinical environments.\n","authors":["Hao Yang","Hong-Yu Zhou","Cheng Li","Weijian Huang","Jiarun Liu","Shanshan Wang"],"pdf_url":"https://arxiv.org/pdf/2401.02044v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.17432v2","updated":"2024-01-04T03:08:53Z","published":"2023-12-29T01:56:17Z","title":"Video Understanding with Large Language Models: A Survey","summary":"  With the burgeoning growth of online video platforms and the escalating\nvolume of video content, the demand for proficient video understanding tools\nhas intensified markedly. Given the remarkable capabilities of Large Language\nModels (LLMs) in language and multimodal tasks, this survey provides a detailed\noverview of the recent advancements in video understanding harnessing the power\nof LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly\nadvanced, particularly their ability for open-ended spatial-temporal reasoning\ncombined with commonsense knowledge, suggesting a promising path for future\nvideo understanding. We examine the unique characteristics and capabilities of\nVid-LLMs, categorizing the approaches into four main types: LLM-based Video\nAgents, Vid-LLMs Pretraining, Vid-LLMs Instruction Tuning, and Hybrid Methods.\nFurthermore, this survey presents a comprehensive study of the tasks, datasets,\nand evaluation methodologies for Vid-LLMs. Additionally, it explores the\nexpansive applications of Vid-LLMs across various domains, highlighting their\nremarkable scalability and versatility in real-world video understanding\nchallenges. Finally, it summarizes the limitations of existing Vid-LLMs and\noutlines directions for future research. For more information, readers are\nrecommended to visit the repository at\nhttps://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding.\n","authors":["Yunlong Tang","Jing Bi","Siting Xu","Luchuan Song","Susan Liang","Teng Wang","Daoan Zhang","Jie An","Jingyang Lin","Rongyi Zhu","Ali Vosoughi","Chao Huang","Zeliang Zhang","Feng Zheng","Jianguo Zhang","Ping Luo","Jiebo Luo","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2312.17432v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.02742v2","updated":"2024-01-04T03:08:36Z","published":"2023-09-06T05:56:30Z","title":"MLN-net: A multi-source medical image segmentation method for clustered\n  microcalcifications using multiple layer normalization","summary":"  Accurate segmentation of clustered microcalcifications in mammography is\ncrucial for the diagnosis and treatment of breast cancer. Despite exhibiting\nexpert-level accuracy, recent deep learning advancements in medical image\nsegmentation provide insufficient contribution to practical applications, due\nto the domain shift resulting from differences in patient postures, individual\ngland density, and imaging modalities of mammography etc. In this paper, a\nnovel framework named MLN-net, which can accurately segment multi-source images\nusing only single source images, is proposed for clustered microcalcification\nsegmentation. We first propose a source domain image augmentation method to\ngenerate multi-source images, leading to improved generalization. And a\nstructure of multiple layer normalization (LN) layers is used to construct the\nsegmentation network, which can be found efficient for clustered\nmicrocalcification segmentation in different domains. Additionally, a branch\nselection strategy is designed for measuring the similarity of the source\ndomain data and the target domain data. To validate the proposed MLN-net,\nextensive analyses including ablation experiments are performed, comparison of\n12 baseline methods. Extensive experiments validate the effectiveness of\nMLN-net in segmenting clustered microcalcifications from different domains and\nthe its segmentation accuracy surpasses state-of-the-art methods. Code will be\navailable at https://github.com/yezanting/MLN-NET-VERSON1.\n","authors":["Ke Wang","Zanting Ye","Xiang Xie","Haidong Cui","Tao Chen","Banteng Liu"],"pdf_url":"https://arxiv.org/pdf/2309.02742v2.pdf","comment":"17 pages, 9 figures, 3 tables"},{"id":"http://arxiv.org/abs/2308.04074v2","updated":"2024-01-04T03:04:44Z","published":"2023-08-08T06:16:37Z","title":"Exploiting Spatial-Temporal Context for Interacting Hand Reconstruction\n  on Monocular RGB Video","summary":"  Reconstructing interacting hands from monocular RGB data is a challenging\ntask, as it involves many interfering factors, e.g. self- and mutual occlusion\nand similar textures. Previous works only leverage information from a single\nRGB image without modeling their physically plausible relation, which leads to\ninferior reconstruction results. In this work, we are dedicated to explicitly\nexploiting spatial-temporal information to achieve better interacting hand\nreconstruction. On one hand, we leverage temporal context to complement\ninsufficient information provided by the single frame, and design a novel\ntemporal framework with a temporal constraint for interacting hand motion\nsmoothness. On the other hand, we further propose an interpenetration detection\nmodule to produce kinetically plausible interacting hands without physical\ncollisions. Extensive experiments are performed to validate the effectiveness\nof our proposed framework, which achieves new state-of-the-art performance on\npublic benchmarks.\n","authors":["Weichao Zhao","Hezhen Hu","Wengang Zhou","Li li","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2308.04074v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2401.02041v1","updated":"2024-01-04T02:56:50Z","published":"2024-01-04T02:56:50Z","title":"Efficient Cloud-edge Collaborative Inference for Object\n  Re-identification","summary":"  Current object re-identification (ReID) system follows the centralized\nprocessing paradigm, i.e., all computations are conducted in the cloud server\nand edge devices are only used to capture and send images. As the number of\nvideos experiences a rapid escalation, this paradigm has become impractical due\nto the finite computational resources. In such a scenario, the ReID system\nshould be converted to fit in the cloud-edge collaborative processing paradigm,\nwhich is crucial to boost the scalability and practicality of ReID systems.\nHowever, current relevant work lacks research on this issue, making it\nchallenging for ReID methods to be adapted effectively. Therefore, we pioneer a\ncloud-edge collaborative inference framework for ReID systems and particularly\npropose a distribution-aware correlation modeling network (DaCM) to make the\ndesired image return to the cloud server as soon as possible via learning to\nmodel the spatial-temporal correlations among instances. DaCM embeds the\nspatial-temporal correlations implicitly included in the timestamps into a\ngraph structure, and it can be applied in the cloud to regulate the size of the\nupload window and on the edge device to adjust the sequence of images,\nrespectively. Traditional ReID methods can be combined with DaCM seamlessly,\nenabling their application within our proposed edge-cloud collaborative\nframework. Extensive experiments demonstrate that our method obviously reduces\ntransmission overhead and significantly improves performance. We will release\nour code and model.\n","authors":["Chuanming Wang","Yuxin Yang","Mengshi Qi","Huadong Ma"],"pdf_url":"https://arxiv.org/pdf/2401.02041v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02032v1","updated":"2024-01-04T02:20:54Z","published":"2024-01-04T02:20:54Z","title":"DiffusionEdge: Diffusion Probabilistic Model for Crisp Edge Detection","summary":"  Limited by the encoder-decoder architecture, learning-based edge detectors\nusually have difficulty predicting edge maps that satisfy both correctness and\ncrispness. With the recent success of the diffusion probabilistic model (DPM),\nwe found it is especially suitable for accurate and crisp edge detection since\nthe denoising process is directly applied to the original image size.\nTherefore, we propose the first diffusion model for the task of general edge\ndetection, which we call DiffusionEdge. To avoid expensive computational\nresources while retaining the final performance, we apply DPM in the latent\nspace and enable the classic cross-entropy loss which is uncertainty-aware in\npixel level to directly optimize the parameters in latent space in a\ndistillation manner. We also adopt a decoupled architecture to speed up the\ndenoising process and propose a corresponding adaptive Fourier filter to adjust\nthe latent features of specific frequencies. With all the technical designs,\nDiffusionEdge can be stably trained with limited resources, predicting crisp\nand accurate edge maps with much fewer augmentation strategies. Extensive\nexperiments on four edge detection benchmarks demonstrate the superiority of\nDiffusionEdge both in correctness and crispness. On the NYUDv2 dataset,\ncompared to the second best, we increase the ODS, OIS (without post-processing)\nand AC by 30.2%, 28.1% and 65.1%, respectively. Code:\nhttps://github.com/GuHuangAI/DiffusionEdge.\n","authors":["Yunfan Ye","Kai Xu","Yuhang Huang","Renjiao Yi","Zhiping Cai"],"pdf_url":"https://arxiv.org/pdf/2401.02032v1.pdf","comment":"AAAI 2024"},{"id":"http://arxiv.org/abs/2401.02031v1","updated":"2024-01-04T02:15:09Z","published":"2024-01-04T02:15:09Z","title":"Spy-Watermark: Robust Invisible Watermarking for Backdoor Attack","summary":"  Backdoor attack aims to deceive a victim model when facing backdoor instances\nwhile maintaining its performance on benign data. Current methods use manual\npatterns or special perturbations as triggers, while they often overlook the\nrobustness against data corruption, making backdoor attacks easy to defend in\npractice. To address this issue, we propose a novel backdoor attack method\nnamed Spy-Watermark, which remains effective when facing data collapse and\nbackdoor defense. Therein, we introduce a learnable watermark embedded in the\nlatent domain of images, serving as the trigger. Then, we search for a\nwatermark that can withstand collapse during image decoding, cooperating with\nseveral anti-collapse operations to further enhance the resilience of our\ntrigger against data corruption. Extensive experiments are conducted on\nCIFAR10, GTSRB, and ImageNet datasets, demonstrating that Spy-Watermark\novertakes ten state-of-the-art methods in terms of robustness and stealthiness.\n","authors":["Ruofei Wang","Renjie Wan","Zongyu Guo","Qing Guo","Rui Huang"],"pdf_url":"https://arxiv.org/pdf/2401.02031v1.pdf","comment":"Accepted by ICASSP2024"},{"id":"http://arxiv.org/abs/2305.06355v2","updated":"2024-01-04T02:06:07Z","published":"2023-05-10T17:59:04Z","title":"VideoChat: Chat-Centric Video Understanding","summary":"  In this paper, we initiate an attempt of developing an end-to-end\nchat-centric video understanding system, coined as VideoChat. It integrates\nvideo foundation models and large language models via a learnable neural\ninterface, excelling in spatiotemporal reasoning, event localization, and\ncausal relationship inference. To instructively tune this system, we build a\nvideo-centric instruction dataset, composed of thousands of videos associated\nwith detailed descriptions and conversations. This dataset emphasizes\nspatiotemporal reasoning and captures causal relationships, providing a\nvaluable asset for training our chat-centric video understanding system.\nPreliminary qualitative experiments demonstrate the potential of our system\nacross a broad spectrum of video applications, which could serve as a simple\nprototype system for future research on chat-centric video understanding.\nAccess our code and data at https://github.com/OpenGVLab/Ask-Anything\n","authors":["KunChang Li","Yinan He","Yi Wang","Yizhuo Li","Wenhai Wang","Ping Luo","Yali Wang","Limin Wang","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2305.06355v2.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2306.00876v2","updated":"2024-01-04T02:02:51Z","published":"2023-06-01T16:37:50Z","title":"Quantifying Deep Learning Model Uncertainty in Conformal Prediction","summary":"  Precise estimation of predictive uncertainty in deep neural networks is a\ncritical requirement for reliable decision-making in machine learning and\nstatistical modeling, particularly in the context of medical AI. Conformal\nPrediction (CP) has emerged as a promising framework for representing the model\nuncertainty by providing well-calibrated confidence levels for individual\npredictions. However, the quantification of model uncertainty in conformal\nprediction remains an active research area, yet to be fully addressed. In this\npaper, we explore state-of-the-art CP methodologies and their theoretical\nfoundations. We propose a probabilistic approach in quantifying the model\nuncertainty derived from the produced prediction sets in conformal prediction\nand provide certified boundaries for the computed uncertainty. By doing so, we\nallow model uncertainty measured by CP to be compared by other uncertainty\nquantification methods such as Bayesian (e.g., MC-Dropout and DeepEnsemble) and\nEvidential approaches.\n","authors":["Hamed Karimi","Reza Samavi"],"pdf_url":"https://arxiv.org/pdf/2306.00876v2.pdf","comment":"Accepted in AAAI Second Symposium on Human Partnership with Medical\n  AI: Design, Operationalization, and Ethics"},{"id":"http://arxiv.org/abs/2401.02020v1","updated":"2024-01-04T01:33:33Z","published":"2024-01-04T01:33:33Z","title":"Spikformer V2: Join the High Accuracy Club on ImageNet with an SNN\n  Ticket","summary":"  Spiking Neural Networks (SNNs), known for their biologically plausible\narchitecture, face the challenge of limited performance. The self-attention\nmechanism, which is the cornerstone of the high-performance Transformer and\nalso a biologically inspired structure, is absent in existing SNNs. To this\nend, we explore the potential of leveraging both self-attention capability and\nbiological properties of SNNs, and propose a novel Spiking Self-Attention (SSA)\nand Spiking Transformer (Spikformer). The SSA mechanism eliminates the need for\nsoftmax and captures the sparse visual feature employing spike-based Query,\nKey, and Value. This sparse computation without multiplication makes SSA\nefficient and energy-saving. Further, we develop a Spiking Convolutional Stem\n(SCS) with supplementary convolutional layers to enhance the architecture of\nSpikformer. The Spikformer enhanced with the SCS is referred to as Spikformer\nV2. To train larger and deeper Spikformer V2, we introduce a pioneering\nexploration of Self-Supervised Learning (SSL) within the SNN. Specifically, we\npre-train Spikformer V2 with masking and reconstruction style inspired by the\nmainstream self-supervised Transformer, and then finetune the Spikformer V2 on\nthe image classification on ImageNet. Extensive experiments show that\nSpikformer V2 outperforms other previous surrogate training and ANN2SNN\nmethods. An 8-layer Spikformer V2 achieves an accuracy of 80.38% using 4 time\nsteps, and after SSL, a 172M 16-layer Spikformer V2 reaches an accuracy of\n81.10% with just 1 time step. To the best of our knowledge, this is the first\ntime that the SNN achieves 80+% accuracy on ImageNet. The code will be\navailable at Spikformer V2.\n","authors":["Zhaokun Zhou","Kaiwei Che","Wei Fang","Keyu Tian","Yuesheng Zhu","Shuicheng Yan","Yonghong Tian","Li Yuan"],"pdf_url":"https://arxiv.org/pdf/2401.02020v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.17599v3","updated":"2024-01-04T01:30:50Z","published":"2023-03-30T17:59:25Z","title":"Zero-Shot Video Editing Using Off-The-Shelf Image Diffusion Models","summary":"  Large-scale text-to-image diffusion models achieve unprecedented success in\nimage generation and editing. However, how to extend such success to video\nediting is unclear. Recent initial attempts at video editing require\nsignificant text-to-video data and computation resources for training, which is\noften not accessible. In this work, we propose vid2vid-zero, a simple yet\neffective method for zero-shot video editing. Our vid2vid-zero leverages\noff-the-shelf image diffusion models, and doesn't require training on any\nvideo. At the core of our method is a null-text inversion module for\ntext-to-video alignment, a cross-frame modeling module for temporal\nconsistency, and a spatial regularization module for fidelity to the original\nvideo. Without any training, we leverage the dynamic nature of the attention\nmechanism to enable bi-directional temporal modeling at test time. Experiments\nand analyses show promising results in editing attributes, subjects, places,\netc., in real-world videos. Code is made available at\n\\url{https://github.com/baaivision/vid2vid-zero}.\n","authors":["Wen Wang","Yan Jiang","Kangyang Xie","Zide Liu","Hao Chen","Yue Cao","Xinlong Wang","Chunhua Shen"],"pdf_url":"https://arxiv.org/pdf/2303.17599v3.pdf","comment":"Add customized video editing. Under Review"},{"id":"http://arxiv.org/abs/2401.02015v1","updated":"2024-01-04T01:10:56Z","published":"2024-01-04T01:10:56Z","title":"Improving Diffusion-Based Image Synthesis with Context Prediction","summary":"  Diffusion models are a new class of generative models, and have dramatically\npromoted image generation with unprecedented quality and diversity. Existing\ndiffusion models mainly try to reconstruct input image from a corrupted one\nwith a pixel-wise or feature-wise constraint along spatial axes. However, such\npoint-based reconstruction may fail to make each predicted pixel/feature fully\npreserve its neighborhood context, impairing diffusion-based image synthesis.\nAs a powerful source of automatic supervisory signal, context has been well\nstudied for learning representations. Inspired by this, we for the first time\npropose ConPreDiff to improve diffusion-based image synthesis with context\nprediction. We explicitly reinforce each point to predict its neighborhood\ncontext (i.e., multi-stride features/tokens/pixels) with a context decoder at\nthe end of diffusion denoising blocks in training stage, and remove the decoder\nfor inference. In this way, each point can better reconstruct itself by\npreserving its semantic connections with neighborhood context. This new\nparadigm of ConPreDiff can generalize to arbitrary discrete and continuous\ndiffusion backbones without introducing extra parameters in sampling procedure.\nExtensive experiments are conducted on unconditional image generation,\ntext-to-image generation and image inpainting tasks. Our ConPreDiff\nconsistently outperforms previous methods and achieves a new SOTA text-to-image\ngeneration results on MS-COCO, with a zero-shot FID score of 6.21.\n","authors":["Ling Yang","Jingwei Liu","Shenda Hong","Zhilong Zhang","Zhilin Huang","Zheming Cai","Wentao Zhang","Bin Cui"],"pdf_url":"https://arxiv.org/pdf/2401.02015v1.pdf","comment":"Accepted by NeurIPS 2023"},{"id":"http://arxiv.org/abs/2309.09431v4","updated":"2024-01-04T01:05:16Z","published":"2023-09-18T02:05:52Z","title":"FactoFormer: Factorized Hyperspectral Transformers with Self-Supervised\n  Pretraining","summary":"  Hyperspectral images (HSIs) contain rich spectral and spatial information.\nMotivated by the success of transformers in the field of natural language\nprocessing and computer vision where they have shown the ability to learn long\nrange dependencies within input data, recent research has focused on using\ntransformers for HSIs. However, current state-of-the-art hyperspectral\ntransformers only tokenize the input HSI sample along the spectral dimension,\nresulting in the under-utilization of spatial information. Moreover,\ntransformers are known to be data-hungry and their performance relies heavily\non large-scale pretraining, which is challenging due to limited annotated\nhyperspectral data. Therefore, the full potential of HSI transformers has not\nbeen fully realized. To overcome these limitations, we propose a novel\nfactorized spectral-spatial transformer that incorporates factorized\nself-supervised pretraining procedures, leading to significant improvements in\nperformance. The factorization of the inputs allows the spectral and spatial\ntransformers to better capture the interactions within the hyperspectral data\ncubes. Inspired by masked image modeling pretraining, we also devise efficient\nmasking strategies for pretraining each of the spectral and spatial\ntransformers. We conduct experiments on six publicly available datasets for HSI\nclassification task and demonstrate that our model achieves state-of-the-art\nperformance in all the datasets. The code for our model will be made available\nat https://github.com/csiro-robotics/factoformer.\n","authors":["Shaheer Mohamed","Maryam Haghighat","Tharindu Fernando","Sridha Sridharan","Clinton Fookes","Peyman Moghadam"],"pdf_url":"https://arxiv.org/pdf/2309.09431v4.pdf","comment":"Accepted to IEEE Transactions on Geoscience and Remote Sensing in\n  December 2023"},{"id":"http://arxiv.org/abs/2306.17019v2","updated":"2024-01-04T22:54:18Z","published":"2023-06-29T15:11:20Z","title":"Histopathology Slide Indexing and Search: Are We There Yet?","summary":"  The search and retrieval of digital histopathology slides is an important\ntask that has yet to be solved. In this case study, we investigate the clinical\nreadiness of three state-of-the-art histopathology slide search engines,\nYottixel, SISH, and RetCCL, on three patients with solid tumors. We provide a\nqualitative assessment of each model's performance in providing retrieval\nresults that are reliable and useful to pathologists. We found that all three\nimage search engines fail to produce consistently reliable results and have\ndifficulties in capturing granular and subtle features of malignancy, limiting\ntheir diagnostic accuracy. Based on our findings, we also propose a minimal set\nof requirements to further advance the development of accurate and reliable\nhistopathology image search engines for successful clinical adoption.\n","authors":["Helen H. Shang","Mohammad Sadegh Nasr","Jai Prakash Veerla","Parisa Boodaghi Malidarreh","MD Jillur Rahman Saurav","Amir Hajighasemi","Manfred Huber","Chace Moleta","Jitin Makker","Jacob M. Luber"],"pdf_url":"https://arxiv.org/pdf/2306.17019v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02565v1","updated":"2024-01-04T22:49:15Z","published":"2024-01-04T22:49:15Z","title":"Vulnerabilities Unveiled: Adversarially Attacking a Multimodal Vision\n  Langauge Model for Pathology Imaging","summary":"  In the dynamic landscape of medical artificial intelligence, this study\nexplores the vulnerabilities of the Pathology Language-Image Pretraining (PLIP)\nmodel, a Vision Language Foundation model, under targeted adversarial\nconditions. Leveraging the Kather Colon dataset with 7,180 H&E images across\nnine tissue types, our investigation employs Projected Gradient Descent (PGD)\nadversarial attacks to intentionally induce misclassifications. The outcomes\nreveal a 100% success rate in manipulating PLIP's predictions, underscoring its\nsusceptibility to adversarial perturbations. The qualitative analysis of\nadversarial examples delves into the interpretability challenges, shedding\nlight on nuanced changes in predictions induced by adversarial manipulations.\nThese findings contribute crucial insights into the interpretability, domain\nadaptation, and trustworthiness of Vision Language Models in medical imaging.\nThe study emphasizes the pressing need for robust defenses to ensure the\nreliability of AI models.\n","authors":["Jai Prakash Veerla","Poojitha Thota","Partha Sai Guttikonda","Shirin Nilizadeh","Jacob M. Luber"],"pdf_url":"https://arxiv.org/pdf/2401.02565v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02564v1","updated":"2024-01-04T22:37:56Z","published":"2024-01-04T22:37:56Z","title":"Predicting Future States with Spatial Point Processes in Single Molecule\n  Resolution Spatial Transcriptomics","summary":"  In this paper, we introduce a pipeline based on Random Forest Regression to\npredict the future distribution of cells that are expressed by the Sog-D gene\n(active cells) in both the Anterior to posterior (AP) and the Dorsal to Ventral\n(DV) axis of the Drosophila in embryogenesis process. This method provides\ninsights about how cells and living organisms control gene expression in super\nresolution whole embryo spatial transcriptomics imaging at sub cellular, single\nmolecule resolution. A Random Forest Regression model was used to predict the\nnext stage active distribution based on the previous one. To achieve this goal,\nwe leveraged temporally resolved, spatial point processes by including Ripley's\nK-function in conjunction with the cell's state in each stage of embryogenesis,\nand found average predictive accuracy of active cell distribution. This tool is\nanalogous to RNA Velocity for spatially resolved developmental biology, from\none data point we can predict future spatially resolved gene expression using\nfeatures from the spatial point processes.\n","authors":["Parisa Boodaghi Malidarreh","Biraaj Rout","Mohammad Sadegh Nasr","Priyanshi Borad","Jillur Rahman Saurav","Jai Prakash Veerla","Kelli Fenelon","Theodora Koromila","Jacob M. Luber"],"pdf_url":"https://arxiv.org/pdf/2401.02564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.09310v3","updated":"2024-01-04T22:05:15Z","published":"2022-11-17T02:58:55Z","title":"Language-Assisted Deep Learning for Autistic Behaviors Recognition","summary":"  Correctly recognizing the behaviors of children with Autism Spectrum Disorder\n(ASD) is of vital importance for the diagnosis of Autism and timely early\nintervention. However, the observation and recording during the treatment from\nthe parents of autistic children may not be accurate and objective. In such\ncases, automatic recognition systems based on computer vision and machine\nlearning (in particular deep learning) technology can alleviate this issue to a\nlarge extent. Existing human action recognition models can now achieve\npersuasive performance on challenging activity datasets, e.g. daily activity,\nand sports activity. However, problem behaviors in children with ASD are very\ndifferent from these general activities, and recognizing these problem\nbehaviors via computer vision is less studied. In this paper, we first evaluate\na strong baseline for action recognition, i.e. Video Swin Transformer, on two\nautism behaviors datasets (SSBD and ESBD) and show that it can achieve high\naccuracy and outperform the previous methods by a large margin, demonstrating\nthe feasibility of vision-based problem behaviors recognition. Moreover, we\npropose language-assisted training to further enhance the action recognition\nperformance. Specifically, we develop a two-branch multimodal deep learning\nframework by incorporating the \"freely available\" language description for each\ntype of problem behavior. Experimental results demonstrate that incorporating\nadditional language supervision can bring an obvious performance boost for the\nautism problem behaviors recognition task as compared to using the video\ninformation only (i.e. 3.49% improvement on ESBD and 1.46% on SSBD).\n","authors":["Andong Deng","Taojiannan Yang","Chen Chen","Qian Chen","Leslie Neely","Sakiko Oyama"],"pdf_url":"https://arxiv.org/pdf/2211.09310v3.pdf","comment":"Smart Health Journal"},{"id":"http://arxiv.org/abs/2401.02550v1","updated":"2024-01-04T21:47:56Z","published":"2024-01-04T21:47:56Z","title":"OptFlow: Fast Optimization-based Scene Flow Estimation without\n  Supervision","summary":"  Scene flow estimation is a crucial component in the development of autonomous\ndriving and 3D robotics, providing valuable information for environment\nperception and navigation. Despite the advantages of learning-based scene flow\nestimation techniques, their domain specificity and limited generalizability\nacross varied scenarios pose challenges. In contrast, non-learning\noptimization-based methods, incorporating robust priors or regularization,\noffer competitive scene flow estimation performance, require no training, and\nshow extensive applicability across datasets, but suffer from lengthy inference\ntimes. In this paper, we present OptFlow, a fast optimization-based scene flow\nestimation method. Without relying on learning or any labeled datasets, OptFlow\nachieves state-of-the-art performance for scene flow estimation on popular\nautonomous driving benchmarks. It integrates a local correlation weight matrix\nfor correspondence matching, an adaptive correspondence threshold limit for\nnearest-neighbor search, and graph prior rigidity constraints, resulting in\nexpedited convergence and improved point correspondence identification.\nMoreover, we demonstrate how integrating a point cloud registration function\nwithin our objective function bolsters accuracy and differentiates between\nstatic and dynamic points without relying on external odometry data.\nConsequently, OptFlow outperforms the baseline graph-prior method by\napproximately 20% and the Neural Scene Flow Prior method by 5%-7% in accuracy,\nall while offering the fastest inference time among all non-learning scene flow\nestimation methods.\n","authors":["Rahul Ahuja","Chris Baker","Wilko Schwarting"],"pdf_url":"https://arxiv.org/pdf/2401.02550v1.pdf","comment":"Accepted at the proceedings of the IEEE/CVF Winter Conference on\n  Applications of Computer Vision (WACV), 2024"},{"id":"http://arxiv.org/abs/2310.03937v2","updated":"2024-01-04T21:39:25Z","published":"2023-10-05T23:00:27Z","title":"Diffusion Models as Masked Audio-Video Learners","summary":"  Over the past several years, the synchronization between audio and visual\nsignals has been leveraged to learn richer audio-visual representations. Aided\nby the large availability of unlabeled videos, many unsupervised training\nframeworks have demonstrated impressive results in various downstream audio and\nvideo tasks. Recently, Masked Audio-Video Learners (MAViL) has emerged as a\nstate-of-the-art audio-video pre-training framework. MAViL couples contrastive\nlearning with masked autoencoding to jointly reconstruct audio spectrograms and\nvideo frames by fusing information from both modalities. In this paper, we\nstudy the potential synergy between diffusion models and MAViL, seeking to\nderive mutual benefits from these two frameworks. The incorporation of\ndiffusion into MAViL, combined with various training efficiency methodologies\nthat include the utilization of a masking ratio curriculum and adaptive batch\nsizing, results in a notable 32% reduction in pre-training Floating-Point\nOperations (FLOPS) and an 18% decrease in pre-training wall clock time.\nCrucially, this enhanced efficiency does not compromise the model's performance\nin downstream audio-classification tasks when compared to MAViL's performance.\n","authors":["Elvis Nunez","Yanzi Jin","Mohammad Rastegari","Sachin Mehta","Maxwell Horton"],"pdf_url":"https://arxiv.org/pdf/2310.03937v2.pdf","comment":"Camera-ready version for the Machine Learning for Audio Workshop at\n  NeurIPS 2023"},{"id":"http://arxiv.org/abs/2202.02952v3","updated":"2024-01-04T21:34:33Z","published":"2022-02-07T05:29:16Z","title":"Supervision by Denoising for Medical Image Segmentation","summary":"  Learning-based image reconstruction models, such as those based on the U-Net,\nrequire a large set of labeled images if good generalization is to be\nguaranteed. In some imaging domains, however, labeled data with pixel- or\nvoxel-level label accuracy are scarce due to the cost of acquiring them. This\nproblem is exacerbated further in domains like medical imaging, where there is\nno single ground truth label, resulting in large amounts of repeat variability\nin the labels. Therefore, training reconstruction networks to generalize better\nby learning from both labeled and unlabeled examples (called semi-supervised\nlearning) is problem of practical and theoretical interest. However,\ntraditional semi-supervised learning methods for image reconstruction often\nnecessitate handcrafting a differentiable regularizer specific to some given\nimaging problem, which can be extremely time-consuming. In this work, we\npropose \"supervision by denoising\" (SUD), a framework that enables us to\nsupervise reconstruction models using their own denoised output as soft labels.\nSUD unifies stochastic averaging and spatial denoising techniques under a\nspatio-temporal denoising framework and alternates denoising and model weight\nupdate steps in an optimization framework for semi-supervision. As example\napplications, we apply SUD to two problems arising from biomedical imaging --\nanatomical brain reconstruction (3D) and cortical parcellation (2D) -- to\ndemonstrate a significant improvement in the image reconstructions over\nsupervised-only and stochastic averaging baselines.\n","authors":["Sean I. Young","Adrian V. Dalca","Enzo Ferrante","Polina Golland","Christopher A. Metzler","Bruce Fischl","Juan Eugenio Iglesias"],"pdf_url":"https://arxiv.org/pdf/2202.02952v3.pdf","comment":"To appear in the IEEE Transactions on Pattern Analysis and Machine\n  Intelligence"},{"id":"http://arxiv.org/abs/2401.02539v1","updated":"2024-01-04T21:02:39Z","published":"2024-01-04T21:02:39Z","title":"Robot-Assisted Deep Venous Thrombosis Ultrasound Examination using\n  Virtual Fixture","summary":"  Deep Venous Thrombosis (DVT) is a common vascular disease with blood clots\ninside deep veins, which may block blood flow or even cause a life-threatening\npulmonary embolism. A typical exam for DVT using ultrasound (US) imaging is by\npressing the target vein until its lumen is fully compressed. However, the\ncompression exam is highly operator-dependent. To alleviate intra- and\ninter-variations, we present a robotic US system with a novel hybrid force\nmotion control scheme ensuring position and force tracking accuracy, and soft\nlanding of the probe onto the target surface. In addition, a path-based virtual\nfixture is proposed to realize easy human-robot interaction for repeat\ncompression operation at the lesion location. To ensure the biometric\nmeasurements obtained in different examinations are comparable, the 6D scanning\npath is determined in a coarse-to-fine manner using both an external RGBD\ncamera and US images. The RGBD camera is first used to extract a rough scanning\npath on the object. Then, the segmented vascular lumen from US images are used\nto optimize the scanning path to ensure the visibility of the target object. To\ngenerate a continuous scan path for developing virtual fixtures, an arc-length\nbased path fitting model considering both position and orientation is proposed.\nFinally, the whole system is evaluated on a human-like arm phantom with an\nuneven surface.\n","authors":["Dianye Huang","Chenguang Yang","Mingchuan Zhou","Angelos Karlas","Nassir Navab","Zhongliang Jiang"],"pdf_url":"https://arxiv.org/pdf/2401.02539v1.pdf","comment":"Accepted Paper IEEE T-ASE"},{"id":"http://arxiv.org/abs/2401.02537v1","updated":"2024-01-04T20:57:25Z","published":"2024-01-04T20:57:25Z","title":"Using Singular Value Decomposition in a Convolutional Neural Network to\n  Improve Brain Tumor Segmentation Accuracy","summary":"  A brain tumor consists of cells showing abnormal brain growth. The area of\nthe brain tumor significantly affects choosing the type of treatment and\nfollowing the course of the disease during the treatment. At the same time,\npictures of Brain MRIs are accompanied by noise. Eliminating existing noises\ncan significantly impact the better segmentation and diagnosis of brain tumors.\nIn this work, we have tried using the analysis of eigenvalues. We have used the\nMSVD algorithm, reducing the image noise and then using the deep neural network\nto segment the tumor in the images. The proposed method's accuracy was\nincreased by 2.4% compared to using the original images. With Using the MSVD\nmethod, convergence speed has also increased, showing the proposed method's\neffectiveness\n","authors":["Pegah Ahadian","Maryam Babaei","Kourosh Parand"],"pdf_url":"https://arxiv.org/pdf/2401.02537v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02536v1","updated":"2024-01-04T20:53:43Z","published":"2024-01-04T20:53:43Z","title":"Novel End-to-End Production-Ready Machine Learning Flow for\n  Nanolithography Modeling and Correction","summary":"  Optical lithography is the main enabler to semiconductor manufacturing. It\nrequires extensive processing to perform the Resolution Enhancement Techniques\n(RETs) required to transfer the design data to a working Integrated Circuits\n(ICs). The processing power and computational runtime for RETs tasks is ever\nincreasing due to the continuous reduction of the feature size and the\nexpansion of the chip area. State-of-the-art research sought Machine Learning\n(ML) technologies to reduce runtime and computational power, however they are\nstill not used in production yet. In this study, we analyze the reasons holding\nback ML computational lithography from being production ready and present a\nnovel highly scalable end-to-end flow that enables production ready ML-RET\ncorrection.\n","authors":["Mohamed S. E. Habib","Hossam A. H. Fahmy","Mohamed F. Abu-ElYazeed"],"pdf_url":"https://arxiv.org/pdf/2401.02536v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02526v1","updated":"2024-01-04T20:29:44Z","published":"2024-01-04T20:29:44Z","title":"Branched Variational Autoencoder Classifiers","summary":"  This paper introduces a modified variational autoencoder (VAEs) that contains\nan additional neural network branch. The resulting branched VAE (BVAE)\ncontributes a classification component based on the class labels to the total\nloss and therefore imparts categorical information to the latent\nrepresentation. As a result, the latent space distributions of the input\nclasses are separated and ordered, thereby enhancing the classification\naccuracy. The degree of improvement is quantified by numerical calculations\nemploying the benchmark MNIST dataset for both unrotated and rotated digits.\nThe proposed technique is then compared to and then incorporated into a VAE\nwith fixed output distributions. This procedure is found to yield improved\nperformance for a wide range of output distributions.\n","authors":["Ahmed Salah","David Yevick"],"pdf_url":"https://arxiv.org/pdf/2401.02526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02524v1","updated":"2024-01-04T20:23:51Z","published":"2024-01-04T20:23:51Z","title":"Comprehensive Exploration of Synthetic Data Generation: A Survey","summary":"  Recent years have witnessed a surge in the popularity of Machine Learning\n(ML), applied across diverse domains. However, progress is impeded by the\nscarcity of training data due to expensive acquisition and privacy legislation.\nSynthetic data emerges as a solution, but the abundance of released models and\nlimited overview literature pose challenges for decision-making. This work\nsurveys 417 Synthetic Data Generation (SDG) models over the last decade,\nproviding a comprehensive overview of model types, functionality, and\nimprovements. Common attributes are identified, leading to a classification and\ntrend analysis. The findings reveal increased model performance and complexity,\nwith neural network-based approaches prevailing, except for privacy-preserving\ndata generation. Computer vision dominates, with GANs as primary generative\nmodels, while diffusion models, transformers, and RNNs compete. Implications\nfrom our performance evaluation highlight the scarcity of common metrics and\ndatasets, making comparisons challenging. Additionally, the neglect of training\nand computational costs in literature necessitates attention in future\nresearch. This work serves as a guide for SDG model selection and identifies\ncrucial areas for future exploration.\n","authors":["André Bauer","Simon Trapp","Michael Stenger","Robert Leppich","Samuel Kounev","Mark Leznik","Kyle Chard","Ian Foster"],"pdf_url":"https://arxiv.org/pdf/2401.02524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.10159v3","updated":"2024-01-04T20:23:39Z","published":"2023-06-16T20:02:51Z","title":"Vision-Language Models can Identify Distracted Driver Behavior from\n  Naturalistic Videos","summary":"  Recognizing the activities causing distraction in real-world driving\nscenarios is critical for ensuring the safety and reliability of both drivers\nand pedestrians on the roadways. Conventional computer vision techniques are\ntypically data-intensive and require a large volume of annotated training data\nto detect and classify various distracted driving behaviors, thereby limiting\ntheir efficiency and scalability. We aim to develop a generalized framework\nthat showcases robust performance with access to limited or no annotated\ntraining data. Recently, vision-language models have offered large-scale\nvisual-textual pretraining that can be adapted to task-specific learning like\ndistracted driving activity recognition. Vision-language pretraining models,\nsuch as CLIP, have shown significant promise in learning natural\nlanguage-guided visual representations. This paper proposes a CLIP-based driver\nactivity recognition approach that identifies driver distraction from\nnaturalistic driving images and videos. CLIP's vision embedding offers\nzero-shot transfer and task-based finetuning, which can classify distracted\nactivities from driving video data. Our results show that this framework offers\nstate-of-the-art performance on zero-shot transfer and video-based CLIP for\npredicting the driver's state on two public datasets. We propose both\nframe-based and video-based frameworks developed on top of the CLIP's visual\nrepresentation for distracted driving detection and classification tasks and\nreport the results.\n","authors":["Md Zahid Hasan","Jiajing Chen","Jiyang Wang","Mohammed Shaiqur Rahman","Ameya Joshi","Senem Velipasalar","Chinmay Hegde","Anuj Sharma","Soumik Sarkar"],"pdf_url":"https://arxiv.org/pdf/2306.10159v3.pdf","comment":"18 pages, 10 figures"},{"id":"http://arxiv.org/abs/2401.02523v1","updated":"2024-01-04T20:17:25Z","published":"2024-01-04T20:17:25Z","title":"Image-based Deep Learning for Smart Digital Twins: a Review","summary":"  Smart Digital twins (SDTs) are being increasingly used to virtually replicate\nand predict the behaviors of complex physical systems through continual data\nassimilation enabling the optimization of the performance of these systems by\ncontrolling the actions of systems. Recently, deep learning (DL) models have\nsignificantly enhanced the capabilities of SDTs, particularly for tasks such as\npredictive maintenance, anomaly detection, and optimization. In many domains,\nincluding medicine, engineering, and education, SDTs use image data\n(image-based SDTs) to observe and learn system behaviors and control their\nbehaviors. This paper focuses on various approaches and associated challenges\nin developing image-based SDTs by continually assimilating image data from\nphysical systems. The paper also discusses the challenges involved in designing\nand implementing DL models for SDTs, including data acquisition, processing,\nand interpretation. In addition, insights into the future directions and\nopportunities for developing new image-based DL approaches to develop robust\nSDTs are provided. This includes the potential for using generative models for\ndata augmentation, developing multi-modal DL models, and exploring the\nintegration of DL with other technologies, including 5G, edge computing, and\nIoT. In this paper, we describe the image-based SDTs, which enable broader\nadoption of the digital twin DT paradigms across a broad spectrum of areas and\nthe development of new methods to improve the abilities of SDTs in replicating,\npredicting, and optimizing the behavior of complex systems.\n","authors":["Md Ruman Islam","Mahadevan Subramaniam","Pei-Chi Huang"],"pdf_url":"https://arxiv.org/pdf/2401.02523v1.pdf","comment":"12 pages, 2 figures, and 3 tables"},{"id":"http://arxiv.org/abs/2306.05411v2","updated":"2024-01-04T19:31:50Z","published":"2023-06-08T17:56:46Z","title":"R-MAE: Regions Meet Masked Autoencoders","summary":"  In this work, we explore regions as a potential visual analogue of words for\nself-supervised image representation learning. Inspired by Masked Autoencoding\n(MAE), a generative pre-training baseline, we propose masked region\nautoencoding to learn from groups of pixels or regions. Specifically, we design\nan architecture which efficiently addresses the one-to-many mapping between\nimages and regions, while being highly effective especially with high-quality\nregions. When integrated with MAE, our approach (R-MAE) demonstrates consistent\nimprovements across various pre-training datasets and downstream detection and\nsegmentation benchmarks, with negligible computational overheads. Beyond the\nquantitative evaluation, our analysis indicates the models pre-trained with\nmasked region autoencoding unlock the potential for interactive segmentation.\nThe code is provided at https://github.com/facebookresearch/r-mae.\n","authors":["Duy-Kien Nguyen","Vaibhav Aggarwal","Yanghao Li","Martin R. Oswald","Alexander Kirillov","Cees G. M. Snoek","Xinlei Chen"],"pdf_url":"https://arxiv.org/pdf/2306.05411v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02501v1","updated":"2024-01-04T19:25:00Z","published":"2024-01-04T19:25:00Z","title":"The cell signaling structure function","summary":"  Live cell microscopy captures 5-D $(x,y,z,channel,time)$ movies that display\npatterns of cellular motion and signaling dynamics. We present here an approach\nto finding spatiotemporal patterns of cell signaling dynamics in 5-D live cell\nmicroscopy movies unique in requiring no \\emph{a priori} knowledge of expected\npattern dynamics, and no training data. The proposed cell signaling structure\nfunction (SSF) is a Kolmogorov structure function that optimally measures cell\nsignaling state as nuclear intensity w.r.t. surrounding cytoplasm, a\nsignificant improvement compared to the current state-of-the-art cytonuclear\nratio. SSF kymographs store at each spatiotemporal cell centroid the SSF value,\nor a functional output such as velocity. Patterns of similarity are identified\nvia the metric normalized compression distance (NCD). The NCD is a reproducing\nkernel for a Hilbert space that represents the input SSF kymographs as points\nin a low dimensional embedding that optimally captures the pattern similarity\nidentified by the NCD throughout the space. The only parameter is the expected\ncell radii ($\\mu m$). A new formulation of the cluster structure function\noptimally estimates how meaningful an embedding from the RKHS representation.\nResults are presented quantifying the impact of ERK and AKT signaling between\ndifferent oncogenic mutations, and by the relation between ERK signaling and\ncellular velocity patterns for movies of 2-D monolayers of human breast\nepithelial (MCF10A) cells, 3-D MCF10A spheroids under optogenetic manipulation\nof ERK, and human induced pluripotent stem cells .\n","authors":["Layton Aho","Mark Winter","Marc DeCarlo","Agne Frismantiene","Yannick Blum","Paolo Armando Gagliardi","Olivier Pertz","Andrew R. Cohen"],"pdf_url":"https://arxiv.org/pdf/2401.02501v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.13301v4","updated":"2024-01-04T19:11:25Z","published":"2023-05-22T17:57:41Z","title":"Training Diffusion Models with Reinforcement Learning","summary":"  Diffusion models are a class of flexible generative models trained with an\napproximation to the log-likelihood objective. However, most use cases of\ndiffusion models are not concerned with likelihoods, but instead with\ndownstream objectives such as human-perceived image quality or drug\neffectiveness. In this paper, we investigate reinforcement learning methods for\ndirectly optimizing diffusion models for such objectives. We describe how\nposing denoising as a multi-step decision-making problem enables a class of\npolicy gradient algorithms, which we refer to as denoising diffusion policy\noptimization (DDPO), that are more effective than alternative reward-weighted\nlikelihood approaches. Empirically, DDPO is able to adapt text-to-image\ndiffusion models to objectives that are difficult to express via prompting,\nsuch as image compressibility, and those derived from human feedback, such as\naesthetic quality. Finally, we show that DDPO can improve prompt-image\nalignment using feedback from a vision-language model without the need for\nadditional data collection or human annotation. The project's website can be\nfound at http://rl-diffusion.github.io .\n","authors":["Kevin Black","Michael Janner","Yilun Du","Ilya Kostrikov","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/2305.13301v4.pdf","comment":"23 pages, 16 figures"},{"id":"http://arxiv.org/abs/2401.02473v1","updated":"2024-01-04T18:59:24Z","published":"2024-01-04T18:59:24Z","title":"VASE: Object-Centric Appearance and Shape Manipulation of Real Videos","summary":"  Recently, several works tackled the video editing task fostered by the\nsuccess of large-scale text-to-image generative models. However, most of these\nmethods holistically edit the frame using the text, exploiting the prior given\nby foundation diffusion models and focusing on improving the temporal\nconsistency across frames. In this work, we introduce a framework that is\nobject-centric and is designed to control both the object's appearance and,\nnotably, to execute precise and explicit structural modifications on the\nobject. We build our framework on a pre-trained image-conditioned diffusion\nmodel, integrate layers to handle the temporal dimension, and propose training\nstrategies and architectural modifications to enable shape control. We evaluate\nour method on the image-driven video editing task showing similar performance\nto the state-of-the-art, and showcasing novel shape-editing capabilities.\nFurther details, code and examples are available on our project page:\nhttps://helia95.github.io/vase-website/\n","authors":["Elia Peruzzo","Vidit Goel","Dejia Xu","Xingqian Xu","Yifan Jiang","Zhangyang Wang","Humphrey Shi","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2401.02473v1.pdf","comment":"Project Page https://helia95.github.io/vase-website/"},{"id":"http://arxiv.org/abs/2401.02329v1","updated":"2024-01-04T16:06:31Z","published":"2024-01-04T16:06:31Z","title":"Not all Minorities are Equal: Empty-Class-Aware Distillation for\n  Heterogeneous Federated Learning","summary":"  Data heterogeneity, characterized by disparities in local data distribution\nacross clients, poses a significant challenge in federated learning.\nSubstantial efforts have been devoted to addressing the heterogeneity in local\nlabel distribution. As minority classes suffer from worse accuracy due to\noverfitting on local imbalanced data, prior methods often incorporate\nclass-balanced learning techniques during local training. Despite the improved\nmean accuracy across all classes, we observe that empty classes-referring to\ncategories absent from a client's data distribution-are still not well\nrecognized. This paper introduces FedED, a novel approach in heterogeneous\nfederated learning that integrates both empty-class distillation and logit\nsuppression simultaneously. Specifically, empty-class distillation leverages\nknowledge distillation during local training on each client to retain essential\ninformation related to empty classes from the global model. Moreover, logit\nsuppression directly penalizes network logits for non-label classes,\neffectively addressing misclassifications in minority classes that may be\nbiased toward majority classes. Extensive experiments validate the efficacy of\nFedED, surpassing previous state-of-the-art methods across diverse datasets\nwith varying degrees of label distribution shift.\n","authors":["Kuangpu Guo","Yuhe Ding","Jian Liang","Ran He","Zilei Wang","Tieniu Tan"],"pdf_url":"https://arxiv.org/pdf/2401.02329v1.pdf","comment":null}],"Robotics":[{"id":"http://arxiv.org/abs/2401.02416v1","updated":"2024-01-04T18:59:25Z","published":"2024-01-04T18:59:25Z","title":"ODIN: A Single Model for 2D and 3D Perception","summary":"  State-of-the-art models on contemporary 3D perception benchmarks like ScanNet\nconsume and label dataset-provided 3D point clouds, obtained through post\nprocessing of sensed multiview RGB-D images. They are typically trained\nin-domain, forego large-scale 2D pre-training and outperform alternatives that\nfeaturize the posed RGB-D multiview images instead. The gap in performance\nbetween methods that consume posed images versus post-processed 3D point clouds\nhas fueled the belief that 2D and 3D perception require distinct model\narchitectures. In this paper, we challenge this view and propose ODIN\n(Omni-Dimensional INstance segmentation), a model that can segment and label\nboth 2D RGB images and 3D point clouds, using a transformer architecture that\nalternates between 2D within-view and 3D cross-view information fusion. Our\nmodel differentiates 2D and 3D feature operations through the positional\nencodings of the tokens involved, which capture pixel coordinates for 2D patch\ntokens and 3D coordinates for 3D feature tokens. ODIN achieves state-of-the-art\nperformance on ScanNet200, Matterport3D and AI2THOR 3D instance segmentation\nbenchmarks, and competitive performance on ScanNet, S3DIS and COCO. It\noutperforms all previous works by a wide margin when the sensed 3D point cloud\nis used in place of the point cloud sampled from 3D mesh. When used as the 3D\nperception engine in an instructable embodied agent architecture, it sets a new\nstate-of-the-art on the TEACh action-from-dialogue benchmark. Our code and\ncheckpoints can be found at the project website: https://odin-seg.github.io.\n","authors":["Ayush Jain","Pushkal Katara","Nikolaos Gkanatsios","Adam W. Harley","Gabriel Sarch","Kriti Aggarwal","Vishrav Chaudhary","Katerina Fragkiadaki"],"pdf_url":"https://arxiv.org/pdf/2401.02416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.09863v2","updated":"2024-01-04T18:20:35Z","published":"2023-08-19T00:43:36Z","title":"StROL: Stabilized and Robust Online Learning from Humans","summary":"  Robots often need to learn the human's reward function online, during the\ncurrent interaction. This real-time learning requires fast but approximate\nlearning rules: when the human's behavior is noisy or suboptimal, current\napproximations can result in unstable robot learning. Accordingly, in this\npaper we seek to enhance the robustness and convergence properties of gradient\ndescent learning rules when inferring the human's reward parameters. We model\nthe robot's learning algorithm as a dynamical system over the human preference\nparameters, where the human's true (but unknown) preferences are the\nequilibrium point. This enables us to perform Lyapunov stability analysis to\nderive the conditions under which the robot's learning dynamics converge. Our\nproposed algorithm (StROL) uses these conditions to learn robust-by-design\nlearning rules: given the original learning dynamics, StROL outputs a modified\nlearning rule that now converges to the human's true parameters under a larger\nset of human inputs. In practice, these autonomously generated learning rules\ncan correctly infer what the human is trying to convey, even when the human is\nnoisy, biased, and suboptimal. Across simulations and a user study we find that\nStROL results in a more accurate estimate and less regret than state-of-the-art\napproaches for online reward learning. See videos and code here:\nhttps://github.com/VT-Collab/StROL_RAL\n","authors":["Shaunak A. Mehta","Forrest Meng","Andrea Bajcsy","Dylan P. Losey"],"pdf_url":"https://arxiv.org/pdf/2308.09863v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.15346v2","updated":"2024-01-04T18:19:26Z","published":"2023-12-23T20:51:37Z","title":"Learning Multi-Step Manipulation Tasks from A Single Human Demonstration","summary":"  Learning from human demonstrations has exhibited remarkable achievements in\nrobot manipulation. However, the challenge remains to develop a robot system\nthat matches human capabilities and data efficiency in learning and\ngeneralizability, particularly in complex, unstructured real-world scenarios.\nWe propose a system that processes RGBD videos to translate human actions to\nrobot primitives and identifies task-relevant key poses of objects using\nGrounded Segment Anything. We then address challenges for robots in replicating\nhuman actions, considering the human-robot differences in kinematics and\ncollision geometry. To test the effectiveness of our system, we conducted\nexperiments focusing on manual dishwashing. With a single human demonstration\nrecorded in a mockup kitchen, the system achieved 50-100% success for each step\nand up to a 40% success rate for the whole task with different objects in a\nhome kitchen. Videos are available at https://robot-dishwashing.github.io\n","authors":["Dingkun Guo"],"pdf_url":"https://arxiv.org/pdf/2312.15346v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02386v1","updated":"2024-01-04T17:55:17Z","published":"2024-01-04T17:55:17Z","title":"Direction of Arrival Estimation Using Microphone Array Processing for\n  Moving Humanoid Robots","summary":"  The auditory system of humanoid robots has gained increased attention in\nrecent years. This system typically acquires the surrounding sound field by\nmeans of a microphone array. Signals acquired by the array are then processed\nusing various methods. One of the widely applied methods is direction of\narrival estimation. The conventional direction of arrival estimation methods\nassume that the array is fixed at a given position during the estimation.\nHowever, this is not necessarily true for an array installed on a moving\nhumanoid robot. The array motion, if not accounted for appropriately, can\nintroduce a significant error in the estimated direction of arrival. The\ncurrent paper presents a signal model that takes the motion into account. Based\non this model, two processing methods are proposed. The first one compensates\nfor the motion of the robot. The second method is applicable to periodic\nsignals and utilizes the motion in order to enhance the performance to a level\nbeyond that of a stationary array. Numerical simulations and an experimental\nstudy are provided, demonstrating that the motion compensation method almost\neliminates the motion-related error. It is also demonstrated that by using the\nmotion-based enhancement method it is possible to improve the direction of\narrival estimation performance, as compared to that obtained when using a\nstationary array.\n","authors":["Vladimir Tourbabin","Boaz Rafaely"],"pdf_url":"https://arxiv.org/pdf/2401.02386v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02376v1","updated":"2024-01-04T17:37:09Z","published":"2024-01-04T17:37:09Z","title":"Machine Learning in Robotic Ultrasound Imaging: Challenges and\n  Perspectives","summary":"  This article reviews the recent advances in intelligent robotic ultrasound\n(US) imaging systems. We commence by presenting the commonly employed robotic\nmechanisms and control techniques in robotic US imaging, along with their\nclinical applications. Subsequently, we focus on the deployment of machine\nlearning techniques in the development of robotic sonographers, emphasizing\ncrucial developments aimed at enhancing the intelligence of these systems. The\nmethods for achieving autonomous action reasoning are categorized into two sets\nof approaches: those relying on implicit environmental data interpretation and\nthose using explicit interpretation. Throughout this exploration, we also\ndiscuss practical challenges, including those related to the scarcity of\nmedical data, the need for a deeper understanding of the physical aspects\ninvolved, and effective data representation approaches. Moreover, we conclude\nby highlighting the open problems in the field and analyzing different possible\nperspectives on how the community could move forward in this research area.\n","authors":["Yuan Bi","Zhongliang Jiang","Felix Duelmer","Dianye Huang","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2401.02376v1.pdf","comment":"Accepted by Annual Review of Control, Robotics, and Autonomous\n  Systems"},{"id":"http://arxiv.org/abs/2401.02343v1","updated":"2024-01-04T16:34:27Z","published":"2024-01-04T16:34:27Z","title":"AERIAL-CORE: AI-Powered Aerial Robots for Inspection and Maintenance of\n  Electrical Power Infrastructures","summary":"  Large-scale infrastructures are prone to deterioration due to age,\nenvironmental influences, and heavy usage. Ensuring their safety through\nregular inspections and maintenance is crucial to prevent incidents that can\nsignificantly affect public safety and the environment. This is especially\npertinent in the context of electrical power networks, which, while essential\nfor energy provision, can also be sources of forest fires. Intelligent drones\nhave the potential to revolutionize inspection and maintenance, eliminating the\nrisks for human operators, increasing productivity, reducing inspection time,\nand improving data collection quality. However, most of the current methods and\ntechnologies in aerial robotics have been trialed primarily in indoor testbeds\nor outdoor settings under strictly controlled conditions, always within the\nline of sight of human operators. Additionally, these methods and technologies\nhave typically been evaluated in isolation, lacking comprehensive integration.\nThis paper introduces the first autonomous system that combines various\ninnovative aerial robots. This system is designed for extended-range\ninspections beyond the visual line of sight, features aerial manipulators for\nmaintenance tasks, and includes support mechanisms for human operators working\nat elevated heights. The paper further discusses the successful validation of\nthis system on numerous electrical power lines, with aerial robots executing\nflights over 10 kilometers away from their ground control stations.\n","authors":["Anibal Ollero","Alejandro Suarez","Christos Papaioannidis","Ioannis Pitas","Juan M. Marredo","Viet Duong","Emad Ebeid","Vit Kratky","Martin Saska","Chloe Hanoune","Amr Afifi","Antonio Franchi","Charalampos Vourtsis","Dario Floreano","Goran Vasiljevic","Stjepan Bogdan","Alvaro Caballero","Fabio Ruggiero","Vincenzo Lippiello","Carlos Matilla","Giovanni Cioffi","Davide Scaramuzza","Jose R. Martinez-de-Dios","Begona C. Arrue","Carlos Martin","Krzysztof Zurad","Carlos Gaitan","Jacob Rodriguez","Antonio Munoz","Antidio Viguria"],"pdf_url":"https://arxiv.org/pdf/2401.02343v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02339v1","updated":"2024-01-04T16:26:21Z","published":"2024-01-04T16:26:21Z","title":"How Do Pedestrians' Perception Change toward Autonomous Vehicles during\n  Unmarked Midblock Multilane Crossings: Role of AV Operation and Signal\n  Indication","summary":"  One of the primary impediments hindering the widespread acceptance of\nautonomous vehicles (AVs) among pedestrians is their limited comprehension of\nAVs. This study employs virtual reality (VR) to provide pedestrians with an\nimmersive environment for engaging with and comprehending AVs during unmarked\nmidblock multilane crossings. Diverse AV driving behaviors were modeled to\nexhibit negotiation behavior with a yellow signal indication or non-yielding\nbehavior with a blue signal indication. This paper aims to investigate the\nimpact of various factors, such as AV behavior and signaling, pedestrian past\nbehavior, etc., on pedestrians' perception change of AVs. Before and after the\nVR experiment, participants completed surveys assessing their perception of\nAVs, focusing on two main aspects: \"Attitude\" and \"System Effectiveness.\" The\nWilcoxon signed-rank test results demonstrated that both pedestrians' overall\nattitude score toward AVs and trust in the effectiveness of AV systems\nsignificantly increased following the VR experiment. Notably, individuals who\nexhibited a greater trust in the yellow signals were more inclined to display a\nhigher attitude score toward AVs and to augment their trust in the\neffectiveness of AV systems. This indicates that the design of the yellow\nsignal instills pedestrians with greater confidence in their interactions with\nAVs. Further, pedestrians who exhibit more aggressive crossing behavior are\nless likely to change their perception towards AVs as compared to those\npedestrians with more positive crossing behaviors. It is concluded that\nintegrating this paper's devised AV behavior and signaling within an immersive\nVR setting facilitated pedestrian engagement with AVs, thereby changing their\nperception of AVs.\n","authors":["Fengjiao Zou","Jennifer Harper Ogle","Patrick Gerard","Weimin Jin"],"pdf_url":"https://arxiv.org/pdf/2401.02339v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.07709v2","updated":"2024-01-04T13:12:49Z","published":"2023-09-14T13:44:15Z","title":"Aerial Manipulator Force Control Using Control Barrier Functions","summary":"  This article studies the problem of applying normal forces on a surface,\nusing an underactuated aerial vehicle equipped with a dexterous robotic arm. A\nforce-motion high-level controller is designed based on a Lyapunov function\nencompassing alignment and exerted force errors. This controller is coupled\nwith a Control Barrier Function constraint under an optimization scheme using\nQuadratic Programming. This aims to enforce a prescribed relationship between\nthe approaching motion for the end-effector and its alignment with the surface,\nthus ensuring safe operation. An adaptive low-level controller is devised for\nthe aerial vehicle, capable of tracking velocity commands generated by the\nhigh-level controller. Simulations and experiments are presented to demonstrate\nthe force exertion stability and safety of the controller in cases of large\ndisturbances.\n","authors":["Dimitris Chaikalis","Vinicius Goncalves","Nikolaos Evangeliou","Anthony Tzes","Farshad Khorrami"],"pdf_url":"https://arxiv.org/pdf/2309.07709v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02227v1","updated":"2024-01-04T12:25:00Z","published":"2024-01-04T12:25:00Z","title":"Enabling Digitalization in Modular Robotic Systems Integration","summary":"  Integrating robot systems into manufacturing lines is a time-consuming\nprocess. In the era of digitalization, the research and development of new\ntechnologies is crucial for improving integration processes. Numerous\nchallenges, including the lack of standardization, as well as intricate\nstakeholder relationships, complicate the process of robotic systems\nintegration. This process typically consists of acquisition, integration, and\ndeployment of the robot systems. This thesis focuses on three areas that help\nautomate and simplify robotic systems integration. In the first area, related\nto acquisition, a constraint-based configurator is demonstrated that resolves\ncompatibility challenges between robot devices, and automates the configuration\nprocess. This reduces the risk of integrating incompatible devices and\ndecreases the need for experts during the configuration phase. In the second\narea, related to integration, the interoperable modeling format, Unified Robot\nDescription Format (URDF), is investigated, where a detailed analysis is\nperformed, revealing significant inconsistencies and critical improvements.\nThis format is widely used for kinematic modeling and 3D visualization of\nrobots, and its models can be reused across simulation tools. Improving this\nformat benefits a wide range of users, including robotics engineers,\nresearchers, and students. In the third area, related to deployment, Digital\nTwins (DTs) for robot systems are explored, as these improve efficiency and\nreduce downtime. A comprehensive literature review of DTs is conducted, and a\ncase study of modular robot systems is developed. This research can accelerate\nthe adoption of DTs in the robotics industry. These insights and approaches\nimprove the process of robotic systems integration, offering valuable\ncontributions that future research can build upon, ultimately driving\nefficiency, and reducing costs.\n","authors":["Daniella Tola"],"pdf_url":"https://arxiv.org/pdf/2401.02227v1.pdf","comment":"PhD thesis"},{"id":"http://arxiv.org/abs/2306.07537v2","updated":"2024-01-04T12:19:18Z","published":"2023-06-13T04:54:03Z","title":"Hybrid and Oriented Harmonic Potentials for Safe Task Execution in\n  Unknown Environment","summary":"  Harmonic potentials provide globally convergent potential fields that are\nprovably free of local minima. Due to its analytical format, it is particularly\nsuitable for generating safe and reliable robot navigation policies. However,\nfor complex environments that consist of a large number of overlapping\nnon-sphere obstacles, the computation of associated transformation functions\ncan be tedious. This becomes more apparent when: (i) the workspace is initially\nunknown and the underlying potential fields are updated constantly as the robot\nexplores it; (ii) the high-level mission consists of sequential navigation\ntasks among numerous regions, requiring the robot to switch between different\npotentials. Thus, this work proposes an efficient and automated scheme to\nconstruct harmonic potentials incrementally online as guided by the task\nautomaton. A novel two-layer harmonic tree (HT) structure is introduced that\nfacilitates the hybrid combination of oriented search algorithms for task\nplanning and harmonic-based navigation controllers for non-holonomic robots.\nBoth layers are adapted efficiently and jointly during online execution to\nreflect the actual feasibility and cost of navigation within the updated\nworkspace. Global safety and convergence are ensured both for the high-level\ntask plan and the low-level robot trajectory. Known issues such as oscillation\nor long-detours for purely potential-based methods and sharp-turns or high\ncomputation complexity for purely search-based methods are prevented. Extensive\nnumerical simulation and hardware experiments are conducted against several\nstrong baselines.\n","authors":["Shuaikang Wang","Meng Guo"],"pdf_url":"https://arxiv.org/pdf/2306.07537v2.pdf","comment":"16 pages, 13 figures"},{"id":"http://arxiv.org/abs/2401.02194v1","updated":"2024-01-04T10:55:58Z","published":"2024-01-04T10:55:58Z","title":"Inherently robust suboptimal MPC for autonomous racing with anytime\n  feasible SQP","summary":"  In recent years, the increasing need for high-performance controllers in\napplications like autonomous driving has motivated the development of\noptimization routines tailored to specific control problems. In this paper, we\npropose an efficient inexact model predictive control (MPC) strategy for\nautonomous miniature racing with inherent robustness properties. We rely on a\nfeasible sequential quadratic programming (SQP) algorithm capable of generating\nfeasible intermediate iterates such that the solver can be stopped after any\nnumber of iterations, without jeopardizing recursive feasibility. In this way,\nwe provide a strategy that computes suboptimal and yet feasible solutions with\na computational footprint that is much lower than state-of-the-art methods\nbased on the computation of locally optimal solutions. Under suitable\nassumptions on the terminal set and on the controllability properties of the\nsystem, we can state that, for any sufficiently small disturbance affecting the\nsystem's dynamics, recursive feasibility can be guaranteed. We validate the\neffectiveness of the proposed strategy in simulation and by deploying it onto a\nphysical experiment with autonomous miniature race cars. Both the simulation\nand experimental results demonstrate that, using the feasible SQP method, a\nfeasible solution can be obtained with moderate additional computational effort\ncompared to strategies that resort to early termination without providing a\nfeasible solution. At the same time, the proposed method is significantly\nfaster than the state-of-the-art solver Ipopt.\n","authors":["Logan Numerow","Andrea Zanelli","Andrea Carron","Melanie N. Zeilinger"],"pdf_url":"https://arxiv.org/pdf/2401.02194v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08409v2","updated":"2024-01-04T10:38:54Z","published":"2023-12-13T10:29:59Z","title":"Towards Safe and Collaborative Robotic Ultrasound Tissue Scanning in\n  Neurosurgery","summary":"  Intraoperative ultrasound imaging is used to facilitate safe brain tumour\nresection. However, due to challenges with image interpretation and the\nphysical scanning, this tool has yet to achieve widespread adoption in\nneurosurgery. In this paper, we introduce the components and workflow of a\nnovel, versatile robotic platform for intraoperative ultrasound tissue scanning\nin neurosurgery. An RGB-D camera attached to the robotic arm allows for\nautomatic object localisation with ArUco markers, and 3D surface reconstruction\nas a triangular mesh using the ImFusion Suite software solution. Impedance\ncontrolled guidance of the US probe along arbitrary surfaces, represented as a\nmesh, enables collaborative US scanning, i.e., autonomous, teleoperated and\nhands-on guided data acquisition. A preliminary experiment evaluates the\nsuitability of the conceptual workflow and system components for probe landing\non a custom-made soft-tissue phantom. Further assessment in future experiments\nwill be necessary to prove the effectiveness of the presented platform.\n","authors":["Michael Dyck","Alistair Weld","Julian Klodmann","Alexander Kirst","Luke Dixon","Giulio Anichini","Sophie Camp","Alin Albu-Schäffer","Stamatia Giannarou"],"pdf_url":"https://arxiv.org/pdf/2312.08409v2.pdf","comment":"4 pages, 7 figures, accepted (05 December 2023) for publication in\n  IEEE Transaction on Medical Robotics and Bionics"},{"id":"http://arxiv.org/abs/2011.03275v4","updated":"2024-01-04T10:25:03Z","published":"2020-11-06T10:42:41Z","title":"Sample-efficient Reinforcement Learning in Robotic Table Tennis","summary":"  Reinforcement learning (RL) has achieved some impressive recent successes in\nvarious computer games and simulations. Most of these successes are based on\nhaving large numbers of episodes from which the agent can learn. In typical\nrobotic applications, however, the number of feasible attempts is very limited.\nIn this paper we present a sample-efficient RL algorithm applied to the example\nof a table tennis robot. In table tennis every stroke is different, with\nvarying placement, speed and spin. An accurate return therefore has to be found\ndepending on a high-dimensional continuous state space. To make learning in few\ntrials possible the method is embedded into our robot system. In this way we\ncan use a one-step environment. The state space depends on the ball at hitting\ntime (position, velocity, spin) and the action is the racket state\n(orientation, velocity) at hitting. An actor-critic based deterministic policy\ngradient algorithm was developed for accelerated learning. Our approach\nperforms competitively both in a simulation and on the real robot in a number\nof challenging scenarios. Accurate results are obtained without pre-training in\nunder $200$ episodes of training. The video presenting our experiments is\navailable at https://youtu.be/uRAtdoL6Wpw.\n","authors":["Jonas Tebbe","Lukas Krauch","Yapeng Gao","Andreas Zell"],"pdf_url":"https://arxiv.org/pdf/2011.03275v4.pdf","comment":"accepted at ICRA 2021 (Xian, China)"},{"id":"http://arxiv.org/abs/2401.02152v1","updated":"2024-01-04T09:04:16Z","published":"2024-01-04T09:04:16Z","title":"Estimating continuous data of wrist joint angles using ultrasound images","summary":"  Ultrasound imaging has recently been introduced as a sensing interface for\njoint motion estimation. The use of ultrasound images as an estimation method\nis expected to improve the control performance of assistive devices and\nhuman--machine interfaces. This study aimed to estimate continuous wrist joint\nangles using ultrasound images. Specifically, in an experiment, joint angle\ninformation was obtained during extension--flexion movements, and ultrasound\nimages of the associated muscles were acquired. Using the features obtained\nfrom ultrasound images, a multivariate linear regression model was used to\nestimate the joint angles. The coordinates of the feature points obtained using\noptical flow from the ultrasound images were used as explanatory variables of\nthe multivariate linear regression model. The model was trained and tested for\neach trial by each participant to verify the estimation accuracy. The results\nshow that the mean and standard deviation of the estimation accuracy for all\ntrials were root mean square error (RMSE)=1.82 $\\pm$ 0.54 deg and coefficient\nof determination (R2)=0.985 $\\pm$ 0.009. Our method achieves a highly accurate\nestimation of joint angles compared with previous studies using other signals,\nsuch as surface electromyography, while the multivariate linear regression\nmodel is simple and both computational and model training costs are low.\n","authors":["Yo Kobayashi","Yoshihiro Katagi"],"pdf_url":"https://arxiv.org/pdf/2401.02152v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00917v2","updated":"2024-01-04T08:52:23Z","published":"2024-01-01T07:03:15Z","title":"Fast and Continual Learning for Hybrid Control Policies using\n  Generalized Benders Decomposition","summary":"  Hybrid model predictive control with both continuous and discrete variables\nis widely applicable to robotic control tasks, especially those involving\ncontact with the environment. Due to the combinatorial complexity, the solving\nspeed of hybrid MPC can be insufficient for real-time applications. In this\npaper, we proposed a hybrid MPC solver based on Generalized Benders\nDecomposition (GBD). The algorithm enumerates and stores cutting planes online\ninside a finite buffer. After a short cold-start phase, the stored cuts provide\nwarm-starts for the new problem instances to enhance the solving speed. Despite\nthe disturbance and randomly changing environment, the solving speed maintains.\nLeveraging on the sparsity of feasibility cuts, we also propose a fast\nalgorithm for Benders master problems. Our solver is validated through\ncontrolling a cart-pole system with randomly moving soft contact walls, and a\nfree-flying robot navigating around obstacles. The results show that with\nsignificantly less data than previous works, the solver reaches competitive\nspeeds to the off-the-shelf solver Gurobi despite the Python overhead.\n","authors":["Xuan Lin"],"pdf_url":"https://arxiv.org/pdf/2401.00917v2.pdf","comment":"A more complete version of the previous paper \"Generalized Benders\n  Decomposition with Continual Learning for Hybrid Model Predictive Control in\n  Dynamic Environment\". The updated version fixes some minor issues and typos.\n  arXiv admin note: substantial text overlap with arXiv:2310.03344"},{"id":"http://arxiv.org/abs/2109.01365v6","updated":"2024-01-04T08:01:34Z","published":"2021-09-03T08:24:13Z","title":"A Comparative Study of Nonlinear MPC and Differential-Flatness-Based\n  Control for Quadrotor Agile Flight","summary":"  Accurate trajectory tracking control for quadrotors is essential for safe\nnavigation in cluttered environments. However, this is challenging in agile\nflights due to nonlinear dynamics, complex aerodynamic effects, and actuation\nconstraints. In this article, we empirically compare two state-of-the-art\ncontrol frameworks: the nonlinear-model-predictive controller (NMPC) and the\ndifferential-flatness-based controller (DFBC), by tracking a wide variety of\nagile trajectories at speeds up to 20 m/s (i.e.,72 km/h). The comparisons are\nperformed in both simulation and real-world environments to systematically\nevaluate both methods from the aspect of tracking accuracy, robustness, and\ncomputational efficiency. We show the superiority of NMPC in tracking\ndynamically infeasible trajectories, at the cost of higher computation time and\nrisk of numerical convergence issues. For both methods, we also quantitatively\nstudy the effect of adding an inner-loop controller using the incremental\nnonlinear dynamic inversion (INDI) method, and the effect of adding an\naerodynamic drag model. Our real-world experiments, performed in one of the\nworld's largest motion capture systems, demonstrate more than 78% tracking\nerror reduction of both NMPC and DFBC, indicating the necessity of using an\ninner-loop controller and aerodynamic drag model for agile trajectory tracking.\n","authors":["Sihao Sun","Angel Romero","Philipp Foehn","Elia Kaufmann","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2109.01365v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02117v1","updated":"2024-01-04T07:55:53Z","published":"2024-01-04T07:55:53Z","title":"Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost\n  Whole-Body Teleoperation","summary":"  Imitation learning from human demonstrations has shown impressive performance\nin robotics. However, most results focus on table-top manipulation, lacking the\nmobility and dexterity necessary for generally useful tasks. In this work, we\ndevelop a system for imitating mobile manipulation tasks that are bimanual and\nrequire whole-body control. We first present Mobile ALOHA, a low-cost and\nwhole-body teleoperation system for data collection. It augments the ALOHA\nsystem with a mobile base, and a whole-body teleoperation interface. Using data\ncollected with Mobile ALOHA, we then perform supervised behavior cloning and\nfind that co-training with existing static ALOHA datasets boosts performance on\nmobile manipulation tasks. With 50 demonstrations for each task, co-training\ncan increase success rates by up to 90%, allowing Mobile ALOHA to autonomously\ncomplete complex mobile manipulation tasks such as sauteing and serving a piece\nof shrimp, opening a two-door wall cabinet to store heavy cooking pots, calling\nand entering an elevator, and lightly rinsing a used pan using a kitchen\nfaucet. Project website: https://mobile-aloha.github.io\n","authors":["Zipeng Fu","Tony Z. Zhao","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2401.02117v1.pdf","comment":"Project website: https://mobile-aloha.github.io (Zipeng Fu and Tony\n  Z. Zhao are project co-leads, Chelsea Finn is the advisor)"},{"id":"http://arxiv.org/abs/2401.02105v1","updated":"2024-01-04T07:26:52Z","published":"2024-01-04T07:26:52Z","title":"Perceptions of Humanoid Robots in Caregiving: A Study of Skilled Nursing\n  Home and Long Term Care Administrators","summary":"  As the aging population increases and the shortage of healthcare workers\nincreases, the need to examine other means for caring for the aging population\nincreases. One such means is the use of humanoid robots to care for social,\nemotional, and physical wellbeing of the people above 65. Understanding skilled\nand long term care nursing home administrators' perspectives on humanoid robots\nin caregiving is crucial as their insights shape the implementation of robots\nand their potential impact on resident well-being and quality of life. This\nauthors surveyed two hundred and sixty nine nursing homes executives to\nunderstand their perspectives on the use of humanoid robots in their nursing\nhome facilities. The data was coded and results revealed that the executives\nwere keen on exploring other avenues for care such as robotics that would\nenhance their nursing homes abilities to care for their residents. Qualitative\nanalysis reveals diverse perspectives on integrating humanoid robots in nursing\nhomes. While acknowledging benefits like improved engagement and staff support,\nconcerns persist about costs, impacts on human interaction, and doubts about\nrobot effectiveness. This highlights complex barriers financial, technical, and\nhuman and emphasizes the need for strategic implementation. It underscores the\nimportance of thorough training, role clarity, and showcasing technology\nbenefits to ensure efficiency and satisfaction among staff and residents.\n","authors":["Rana Imtiaz","Arshia Khan"],"pdf_url":"https://arxiv.org/pdf/2401.02105v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2308.12517v2","updated":"2024-01-04T05:22:36Z","published":"2023-08-24T03:06:20Z","title":"Not Only Rewards But Also Constraints: Applications on Legged Robot\n  Locomotion","summary":"  Several earlier studies have shown impressive control performance in complex\nrobotic systems by designing the controller using a neural network and training\nit with model-free reinforcement learning. However, these outstanding\ncontrollers with natural motion style and high task performance are developed\nthrough extensive reward engineering, which is a highly laborious and\ntime-consuming process of designing numerous reward terms and determining\nsuitable reward coefficients. In this work, we propose a novel reinforcement\nlearning framework for training neural network controllers for complex robotic\nsystems consisting of both rewards and constraints. To let the engineers\nappropriately reflect their intent to constraints and handle them with minimal\ncomputation overhead, two constraint types and an efficient policy optimization\nalgorithm are suggested. The learning framework is applied to train locomotion\ncontrollers for several legged robots with different morphology and physical\nattributes to traverse challenging terrains. Extensive simulation and\nreal-world experiments demonstrate that performant controllers can be trained\nwith significantly less reward engineering, by tuning only a single reward\ncoefficient. Furthermore, a more straightforward and intuitive engineering\nprocess can be utilized, thanks to the interpretability and generalizability of\nconstraints. The summary video is available at https://youtu.be/KAlm3yskhvM.\n","authors":["Yunho Kim","Hyunsik Oh","Jeonghyun Lee","Jinhyeok Choi","Gwanghyeon Ji","Moonkyu Jung","Donghoon Youm","Jemin Hwangbo"],"pdf_url":"https://arxiv.org/pdf/2308.12517v2.pdf","comment":"Submitted to Transactions on Robotics (T-RO)"},{"id":"http://arxiv.org/abs/2310.12411v2","updated":"2024-01-04T04:53:14Z","published":"2023-10-19T01:50:33Z","title":"Online Multi-IMU Calibration Using Visual-Inertial Odometry","summary":"  This work presents a centralized multi-IMU filter framework with online\nintrinsic and extrinsic calibration for unsynchronized inertial measurement\nunits that is robust against changes in calibration parameters. The novel\nEKF-based method estimates the positional and rotational offsets of the system\nof sensors as well as their intrinsic biases without the use of rigid body\ngeometric constraints. Additionally, the filter is flexible in the total number\nof sensors used while leveraging the commonly used MSCKF framework for camera\nmeasurements. The filter framework has been validated using Monte Carlo\nsimulation as well as experimentally. In both simulations and experiments,\nusing multiple IMU measurement streams within the proposed filter framework\noutperforms the use of a single IMU in a filter prediction step while also\nproducing consistent and accurate estimates of initial calibration errors.\nCompared to current state-of-the-art optimizers, the filter produces similar\nintrinsic and extrinsic calibration parameters for each sensor. Finally, an\nopen source repository has been provided at\nhttps://github.com/unmannedlab/ekf-cal containing both the online estimator and\nthe simulation used for testing and evaluation.\n","authors":["Jacob Hartzer","Srikanth Saripalli"],"pdf_url":"https://arxiv.org/pdf/2310.12411v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09056v2","updated":"2024-01-04T00:56:28Z","published":"2023-11-15T15:49:27Z","title":"Range-Visual-Inertial Sensor Fusion for Micro Aerial Vehicle\n  Localization and Navigation","summary":"  We propose a fixed-lag smoother-based sensor fusion architecture to leverage\nthe complementary benefits of range-based sensors and visual-inertial odometry\n(VIO) for localization. We use two fixed-lag smoothers (FLS) to decouple\naccurate state estimation and high-rate pose generation for closed-loop\ncontrol. The first FLS combines ultrawideband (UWB)-based range measurements\nand VIO to estimate the robot trajectory and any systematic biases that affect\nthe range measurements in cluttered environments. The second FLS estimates\nsmooth corrections to VIO to generate pose estimates at a high rate for online\ncontrol. The proposed method is lightweight and can run on a computationally\nconstrained micro-aerial vehicle (MAV). We validate our approach through\nclosed-loop flight tests involving dynamic trajectories in multiple real-world\ncluttered indoor environments. Our method achieves\ndecimeter-to-sub-decimeter-level positioning accuracy using off-the-shelf\nsensors and decimeter-level tracking accuracy with minimally-tuned open-source\ncontrollers.\n","authors":["Abhishek Goudar","Wenda Zhao","Angela P. Schoellig"],"pdf_url":"https://arxiv.org/pdf/2311.09056v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02541v1","updated":"2024-01-04T21:10:28Z","published":"2024-01-04T21:10:28Z","title":"Autonomous Multi-Rotor UAVs: A Holistic Approach to Design,\n  Optimization, and Fabrication","summary":"  Unmanned Aerial Vehicles (UAVs) have become pivotal in domains spanning\nmilitary, agriculture, surveillance, and logistics, revolutionizing data\ncollection and environmental interaction. With the advancement in drone\ntechnology, there is a compelling need to develop a holistic methodology for\ndesigning UAVs. This research focuses on establishing a procedure encompassing\nconceptual design, use of composite materials, weight optimization, stability\nanalysis, avionics integration, advanced manufacturing, and incorporation of\nautonomous payload delivery through object detection models tailored to satisfy\nspecific applications while maintaining cost efficiency. The study conducts a\ncomparative assessment of potential composite materials and various quadcopter\nframe configurations. The novel features include a payload-dropping mechanism,\na unibody arm fixture, and the utilization of carbon-fibre-balsa composites. A\nquadcopter is designed and analyzed using the proposed methodology, followed by\nits fabrication using additive manufacturing and vacuum bagging techniques. A\ncomputer vision-based deep learning model enables precise delivery of payloads\nby autonomously detecting targets.\n","authors":["Aniruth A","Chirag Satpathy","Jothika K","Nitteesh M","Gokulraj M","Venkatram K","Harshith G","Shristi S","Anushka Vani","Jonathan Spurgeon"],"pdf_url":"https://arxiv.org/pdf/2401.02541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02539v1","updated":"2024-01-04T21:02:39Z","published":"2024-01-04T21:02:39Z","title":"Robot-Assisted Deep Venous Thrombosis Ultrasound Examination using\n  Virtual Fixture","summary":"  Deep Venous Thrombosis (DVT) is a common vascular disease with blood clots\ninside deep veins, which may block blood flow or even cause a life-threatening\npulmonary embolism. A typical exam for DVT using ultrasound (US) imaging is by\npressing the target vein until its lumen is fully compressed. However, the\ncompression exam is highly operator-dependent. To alleviate intra- and\ninter-variations, we present a robotic US system with a novel hybrid force\nmotion control scheme ensuring position and force tracking accuracy, and soft\nlanding of the probe onto the target surface. In addition, a path-based virtual\nfixture is proposed to realize easy human-robot interaction for repeat\ncompression operation at the lesion location. To ensure the biometric\nmeasurements obtained in different examinations are comparable, the 6D scanning\npath is determined in a coarse-to-fine manner using both an external RGBD\ncamera and US images. The RGBD camera is first used to extract a rough scanning\npath on the object. Then, the segmented vascular lumen from US images are used\nto optimize the scanning path to ensure the visibility of the target object. To\ngenerate a continuous scan path for developing virtual fixtures, an arc-length\nbased path fitting model considering both position and orientation is proposed.\nFinally, the whole system is evaluated on a human-like arm phantom with an\nuneven surface.\n","authors":["Dianye Huang","Chenguang Yang","Mingchuan Zhou","Angelos Karlas","Nassir Navab","Zhongliang Jiang"],"pdf_url":"https://arxiv.org/pdf/2401.02539v1.pdf","comment":"Accepted Paper IEEE T-ASE"}],"Multimedia":[{"id":"http://arxiv.org/abs/2401.02309v1","updated":"2024-01-04T14:55:57Z","published":"2024-01-04T14:55:57Z","title":"TR-DETR: Task-Reciprocal Transformer for Joint Moment Retrieval and\n  Highlight Detection","summary":"  Video moment retrieval (MR) and highlight detection (HD) based on natural\nlanguage queries are two highly related tasks, which aim to obtain relevant\nmoments within videos and highlight scores of each video clip. Recently,\nseveral methods have been devoted to building DETR-based networks to solve both\nMR and HD jointly. These methods simply add two separate task heads after\nmulti-modal feature extraction and feature interaction, achieving good\nperformance. Nevertheless, these approaches underutilize the reciprocal\nrelationship between two tasks. In this paper, we propose a task-reciprocal\ntransformer based on DETR (TR-DETR) that focuses on exploring the inherent\nreciprocity between MR and HD. Specifically, a local-global multi-modal\nalignment module is first built to align features from diverse modalities into\na shared latent space. Subsequently, a visual feature refinement is designed to\neliminate query-irrelevant information from visual features for modal\ninteraction. Finally, a task cooperation module is constructed to refine the\nretrieval pipeline and the highlight score prediction process by utilizing the\nreciprocity between MR and HD. Comprehensive experiments on QVHighlights,\nCharades-STA and TVSum datasets demonstrate that TR-DETR outperforms existing\nstate-of-the-art methods. Codes are available at\n\\url{https://github.com/mingyao1120/TR-DETR}.\n","authors":["Hao Sun","Mingyao Zhou","Wenjing Chen","Wei Xie"],"pdf_url":"https://arxiv.org/pdf/2401.02309v1.pdf","comment":"Accepted by AAAI-24"},{"id":"http://arxiv.org/abs/2305.08372v2","updated":"2024-01-04T10:15:28Z","published":"2023-05-15T06:14:36Z","title":"Hierarchical Aligned Multimodal Learning for NER on Tweet Posts","summary":"  Mining structured knowledge from tweets using named entity recognition (NER)\ncan be beneficial for many down stream applications such as recommendation and\nintention understanding. With tweet posts tending to be multimodal, multimodal\nnamed entity recognition (MNER) has attracted more attention. In this paper, we\npropose a novel approach, which can dynamically align the image and text\nsequence and achieve the multi-level cross-modal learning to augment textual\nword representation for MNER improvement. To be specific, our framework can be\nsplit into three main stages: the first stage focuses on intra-modality\nrepresentation learning to derive the implicit global and local knowledge of\neach modality, the second evaluates the relevance between the text and its\naccompanying image and integrates different grained visual information based on\nthe relevance, the third enforces semantic refinement via iterative cross-modal\ninteractions and co-attention. We conduct experiments on two open datasets, and\nthe results and detailed analysis demonstrate the advantage of our model.\n","authors":["Peipei Liu","Hong Li","Yimo Ren","Jie Liu","Shuaizong Si","Hongsong Zhu","Limin Sun"],"pdf_url":"https://arxiv.org/pdf/2305.08372v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02566v1","updated":"2024-01-04T22:51:13Z","published":"2024-01-04T22:51:13Z","title":"Siamese Residual Neural Network for Musical Shape Evaluation in Piano\n  Performance Assessment","summary":"  Understanding and identifying musical shape plays an important role in music\neducation and performance assessment. To simplify the otherwise time- and\ncost-intensive musical shape evaluation, in this paper we explore how\nartificial intelligence (AI) driven models can be applied. Considering musical\nshape evaluation as a classification problem, a light-weight Siamese residual\nneural network (S-ResNN) is proposed to automatically identify musical shapes.\nTo assess the proposed approach in the context of piano musical shape\nevaluation, we have generated a new dataset, containing 4116 music pieces\nderived by 147 piano preparatory exercises and performed in 28 categories of\nmusical shapes. The experimental results show that the S-ResNN significantly\noutperforms a number of benchmark methods in terms of the precision, recall and\nF1 score.\n","authors":["Xiaoquan Li","Stephan Weiss","Yijun Yan","Yinhe Li","Jinchang Ren","John Soraghan","Ming Gong"],"pdf_url":"https://arxiv.org/pdf/2401.02566v1.pdf","comment":"X.Li, S.Weiss, Y.Yan, Y.Li, J.Ren, J.Soraghan, M.Gong,\"Siamese\n  residual neural network for musical shape evaluation in piano performance\n  assessment\" in Proc. of the 31st European Signal Processing Conference,\n  Helsinki, Finland"},{"id":"http://arxiv.org/abs/2310.03937v2","updated":"2024-01-04T21:39:25Z","published":"2023-10-05T23:00:27Z","title":"Diffusion Models as Masked Audio-Video Learners","summary":"  Over the past several years, the synchronization between audio and visual\nsignals has been leveraged to learn richer audio-visual representations. Aided\nby the large availability of unlabeled videos, many unsupervised training\nframeworks have demonstrated impressive results in various downstream audio and\nvideo tasks. Recently, Masked Audio-Video Learners (MAViL) has emerged as a\nstate-of-the-art audio-video pre-training framework. MAViL couples contrastive\nlearning with masked autoencoding to jointly reconstruct audio spectrograms and\nvideo frames by fusing information from both modalities. In this paper, we\nstudy the potential synergy between diffusion models and MAViL, seeking to\nderive mutual benefits from these two frameworks. The incorporation of\ndiffusion into MAViL, combined with various training efficiency methodologies\nthat include the utilization of a masking ratio curriculum and adaptive batch\nsizing, results in a notable 32% reduction in pre-training Floating-Point\nOperations (FLOPS) and an 18% decrease in pre-training wall clock time.\nCrucially, this enhanced efficiency does not compromise the model's performance\nin downstream audio-classification tasks when compared to MAViL's performance.\n","authors":["Elvis Nunez","Yanzi Jin","Mohammad Rastegari","Sachin Mehta","Maxwell Horton"],"pdf_url":"https://arxiv.org/pdf/2310.03937v2.pdf","comment":"Camera-ready version for the Machine Learning for Audio Workshop at\n  NeurIPS 2023"}]},"2024-01-05T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2401.02954v1","updated":"2024-01-05T18:59:13Z","published":"2024-01-05T18:59:13Z","title":"DeepSeek LLM: Scaling Open-Source Language Models with Longtermism","summary":"  The rapid development of open-source large language models (LLMs) has been\ntruly remarkable. However, the scaling law described in previous literature\npresents varying conclusions, which casts a dark cloud over scaling LLMs. We\ndelve into the study of scaling laws and present our distinctive findings that\nfacilitate scaling of large scale models in two commonly used open-source\nconfigurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek\nLLM, a project dedicated to advancing open-source language models with a\nlong-term perspective. To support the pre-training phase, we have developed a\ndataset that currently consists of 2 trillion tokens and is continuously\nexpanding. We further conduct supervised fine-tuning (SFT) and Direct\nPreference Optimization (DPO) on DeepSeek LLM Base models, resulting in the\ncreation of DeepSeek Chat models. Our evaluation results demonstrate that\nDeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in\nthe domains of code, mathematics, and reasoning. Furthermore, open-ended\nevaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance\ncompared to GPT-3.5.\n","authors":[" DeepSeek-AI"," :","Xiao Bi","Deli Chen","Guanting Chen","Shanhuang Chen","Damai Dai","Chengqi Deng","Honghui Ding","Kai Dong","Qiushi Du","Zhe Fu","Huazuo Gao","Kaige Gao","Wenjun Gao","Ruiqi Ge","Kang Guan","Daya Guo","Jianzhong Guo","Guangbo Hao","Zhewen Hao","Ying He","Wenjie Hu","Panpan Huang","Erhang Li","Guowei Li","Jiashi Li","Yao Li","Y. K. Li","Wenfeng Liang","Fangyun Lin","A. X. Liu","Bo Liu","Wen Liu","Xiaodong Liu","Xin Liu","Yiyuan Liu","Haoyu Lu","Shanghao Lu","Fuli Luo","Shirong Ma","Xiaotao Nie","Tian Pei","Yishi Piao","Junjie Qiu","Hui Qu","Tongzheng Ren","Zehui Ren","Chong Ruan","Zhangli Sha","Zhihong Shao","Junxiao Song","Xuecheng Su","Jingxiang Sun","Yaofeng Sun","Minghui Tang","Bingxuan Wang","Peiyi Wang","Shiyu Wang","Yaohui Wang","Yongji Wang","Tong Wu","Y. Wu","Xin Xie","Zhenda Xie","Ziwei Xie","Yiliang Xiong","Hanwei Xu","R. X. Xu","Yanhong Xu","Dejian Yang","Yuxiang You","Shuiping Yu","Xingkai Yu","B. Zhang","Haowei Zhang","Lecong Zhang","Liyue Zhang","Mingchuan Zhang","Minghua Zhang","Wentao Zhang","Yichao Zhang","Chenggang Zhao","Yao Zhao","Shangyan Zhou","Shunfeng Zhou","Qihao Zhu","Yuheng Zou"],"pdf_url":"https://arxiv.org/pdf/2401.02954v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02921v1","updated":"2024-01-05T17:58:10Z","published":"2024-01-05T17:58:10Z","title":"Towards ASR Robust Spoken Language Understanding Through In-Context\n  Learning With Word Confusion Networks","summary":"  In the realm of spoken language understanding (SLU), numerous natural\nlanguage understanding (NLU) methodologies have been adapted by supplying large\nlanguage models (LLMs) with transcribed speech instead of conventional written\ntext. In real-world scenarios, prior to input into an LLM, an automated speech\nrecognition (ASR) system generates an output transcript hypothesis, where\ninherent errors can degrade subsequent SLU tasks. Here we introduce a method\nthat utilizes the ASR system's lattice output instead of relying solely on the\ntop hypothesis, aiming to encapsulate speech ambiguities and enhance SLU\noutcomes. Our in-context learning experiments, covering spoken question\nanswering and intent classification, underline the LLM's resilience to noisy\nspeech transcripts with the help of word confusion networks from lattices,\nbridging the SLU performance gap between using the top ASR hypothesis and an\noracle upper bound. Additionally, we delve into the LLM's robustness to varying\nASR performance conditions and scrutinize the aspects of in-context learning\nwhich prove the most influential.\n","authors":["Kevin Everson","Yile Gu","Huck Yang","Prashanth Gurunath Shivakumar","Guan-Ting Lin","Jari Kolehmainen","Ivan Bulyko","Ankur Gandhe","Shalini Ghosh","Wael Hamza","Hung-yi Lee","Ariya Rastrow","Andreas Stolcke"],"pdf_url":"https://arxiv.org/pdf/2401.02921v1.pdf","comment":"Accepted to ICASSP 2024"},{"id":"http://arxiv.org/abs/2401.02909v1","updated":"2024-01-05T17:15:01Z","published":"2024-01-05T17:15:01Z","title":"Introducing Bode: A Fine-Tuned Large Language Model for Portuguese\n  Prompt-Based Task","summary":"  Large Language Models (LLMs) are increasingly bringing advances to Natural\nLanguage Processing. However, low-resource languages, those lacking extensive\nprominence in datasets for various NLP tasks, or where existing datasets are\nnot as substantial, such as Portuguese, already obtain several benefits from\nLLMs, but not to the same extent. LLMs trained on multilingual datasets\nnormally struggle to respond to prompts in Portuguese satisfactorily,\npresenting, for example, code switching in their responses. This work proposes\na fine-tuned LLaMA 2-based model for Portuguese prompts named Bode in two\nversions: 7B and 13B. We evaluate the performance of this model in\nclassification tasks using the zero-shot approach with in-context learning, and\ncompare it with other LLMs. Our main contribution is to bring an LLM with\nsatisfactory results in the Portuguese language, as well as to provide a model\nthat is free for research or commercial purposes.\n","authors":["Gabriel Lino Garcia","Pedro Henrique Paiola","Luis Henrique Morelli","Giovani Candido","Arnaldo Cândido Júnior","Danilo Samuel Jodas","Luis C. S. Afonso","Ivan Rizzo Guilherme","Bruno Elias Penteado","João Paulo Papa"],"pdf_url":"https://arxiv.org/pdf/2401.02909v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2401.02906v1","updated":"2024-01-05T17:05:42Z","published":"2024-01-05T17:05:42Z","title":"MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance","summary":"  The deployment of multimodal large language models (MLLMs) has brought forth\na unique vulnerability: susceptibility to malicious attacks through visual\ninputs. We delve into the novel challenge of defending MLLMs against such\nattacks. We discovered that images act as a \"foreign language\" that is not\nconsidered during alignment, which can make MLLMs prone to producing harmful\nresponses. Unfortunately, unlike the discrete tokens considered in text-based\nLLMs, the continuous nature of image signals presents significant alignment\nchallenges, which poses difficulty to thoroughly cover the possible scenarios.\nThis vulnerability is exacerbated by the fact that open-source MLLMs are\npredominantly fine-tuned on limited image-text pairs that is much less than the\nextensive text-based pretraining corpus, which makes the MLLMs more prone to\ncatastrophic forgetting of their original abilities during explicit alignment\ntuning. To tackle these challenges, we introduce MLLM-Protector, a\nplug-and-play strategy combining a lightweight harm detector and a response\ndetoxifier. The harm detector's role is to identify potentially harmful outputs\nfrom the MLLM, while the detoxifier corrects these outputs to ensure the\nresponse stipulates to the safety standards. This approach effectively\nmitigates the risks posed by malicious visual inputs without compromising the\nmodel's overall performance. Our results demonstrate that MLLM-Protector offers\na robust solution to a previously unaddressed aspect of MLLM security.\n","authors":["Renjie Pi","Tianyang Han","Yueqi Xie","Rui Pan","Qing Lian","Hanze Dong","Jipeng Zhang","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.02906v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2203.17213v2","updated":"2024-01-05T16:10:15Z","published":"2022-03-31T17:41:03Z","title":"Analyzing Wrap-Up Effects through an Information-Theoretic Lens","summary":"  Numerous analyses of reading time (RT) data have been implemented -- all in\nan effort to better understand the cognitive processes driving reading\ncomprehension. However, data measured on words at the end of a sentence -- or\neven at the end of a clause -- is often omitted due to the confounding factors\nintroduced by so-called \"wrap-up effects,\" which manifests as a skewed\ndistribution of RTs for these words. Consequently, the understanding of the\ncognitive processes that might be involved in these wrap-up effects is limited.\nIn this work, we attempt to learn more about these processes by examining the\nrelationship between wrap-up effects and information-theoretic quantities, such\nas word and context surprisals. We find that the distribution of information in\nprior contexts is often predictive of sentence- and clause-final RTs (while not\nof sentence-medial RTs). This lends support to several prior hypotheses about\nthe processes involved in wrap-up effects.\n","authors":["Clara Meister","Tiago Pimentel","Thomas Hikaru Clark","Ryan Cotterell","Roger Levy"],"pdf_url":"https://arxiv.org/pdf/2203.17213v2.pdf","comment":"ACL 2022 (main conference)"},{"id":"http://arxiv.org/abs/2307.03749v2","updated":"2024-01-05T15:55:23Z","published":"2023-07-07T17:59:12Z","title":"On the Efficacy of Sampling Adapters","summary":"  Sampling is a common strategy for generating text from probabilistic models,\nyet standard ancestral sampling often results in text that is incoherent or\nungrammatical. To alleviate this issue, various modifications to a model's\nsampling distribution, such as nucleus or top-k sampling, have been introduced\nand are now ubiquitously used in language generation systems. We propose a\nunified framework for understanding these techniques, which we term sampling\nadapters. Sampling adapters often lead to qualitatively better text, which\nraises the question: From a formal perspective, how are they changing the\n(sub)word-level distributions of language generation models? And why do these\nlocal changes lead to higher-quality text? We argue that the shift they enforce\ncan be viewed as a trade-off between precision and recall: while the model\nloses its ability to produce certain strings, its precision rate on desirable\ntext increases. While this trade-off is not reflected in standard metrics of\ndistribution quality (such as perplexity), we find that several\nprecision-emphasizing measures indeed indicate that sampling adapters can lead\nto probability distributions more aligned with the true distribution. Further,\nthese measures correlate with higher sequence-level quality scores,\nspecifically, Mauve.\n","authors":["Clara Meister","Tiago Pimentel","Luca Malagutti","Ethan G. Wilcox","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2307.03749v2.pdf","comment":"ACL 2023 Main Conference Proceedings"},{"id":"http://arxiv.org/abs/2401.02870v1","updated":"2024-01-05T15:52:59Z","published":"2024-01-05T15:52:59Z","title":"AFSPP: Agent Framework for Shaping Preference and Personality with Large\n  Language Models","summary":"  The evolution of Large Language Models (LLMs) has introduced a new paradigm\nfor investigating human behavior emulation. Recent research has employed\nLLM-based Agents to create a sociological research environment, in which agents\nexhibit behavior based on the unfiltered characteristics of large language\nmodels. However, these studies overlook the iterative development within a\nhuman-like setting - Human preferences and personalities are complex, shaped by\nvarious factors and subject to ongoing change as a result of environmental and\nsubjective influences. In light of this observation, we propose Agent Framework\nfor Shaping Preference and Personality (AFSPP), exploring the multifaceted\nimpact of social networks and subjective consciousness on LLM-based Agents'\npreference and personality formation. With AFSPP, we have, for the first time,\nsuccessfully replicated several key findings from human personality\nexperiments. And other AFSPP-based experimental results indicate that plan\nmaking, sensory perceptions and social networking with subjective information,\nwield the most pronounced influence on preference shaping. AFSPP can\nsignificantly enhance the efficiency and scope of psychological experiments,\nwhile yielding valuable insights for Trustworthy Artificial Intelligence\nresearch for strategies to prevent undesirable preference and personality\ndevelopment.\n","authors":["Zihong He","Changwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.02870v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01698v2","updated":"2024-01-05T15:33:40Z","published":"2024-01-03T12:05:38Z","title":"Patterns of Persistence and Diffusibility across the World's Languages","summary":"  Language similarities can be caused by genetic relatedness, areal contact,\nuniversality, or chance. Colexification, i.e. a type of similarity where a\nsingle lexical form is used to convey multiple meanings, is underexplored. In\nour work, we shed light on the linguistic causes of cross-lingual similarity in\ncolexification and phonology, by exploring genealogical stability (persistence)\nand contact-induced change (diffusibility). We construct large-scale graphs\nincorporating semantic, genealogical, phonological and geographical data for\n1,966 languages. We then show the potential of this resource, by investigating\nseveral established hypotheses from previous work in linguistics, while\nproposing new ones. Our results strongly support a previously established\nhypothesis in the linguistic literature, while offering contradicting evidence\nto another. Our large scale resource opens for further research across\ndisciplines, e.g.~in multilingual NLP and comparative linguistics.\n","authors":["Yiyi Chen","Johannes Bjerva"],"pdf_url":"https://arxiv.org/pdf/2401.01698v2.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2401.02839v1","updated":"2024-01-05T14:47:20Z","published":"2024-01-05T14:47:20Z","title":"Pheme: Efficient and Conversational Speech Generation","summary":"  In recent years, speech generation has seen remarkable progress, now\nachieving one-shot generation capability that is often virtually\nindistinguishable from real human voice. Integrating such advancements in\nspeech generation with large language models might revolutionize a wide range\nof applications. However, certain applications, such as assistive\nconversational systems, require natural and conversational speech generation\ntools that also operate efficiently in real time. Current state-of-the-art\nmodels like VALL-E and SoundStorm, powered by hierarchical neural audio codecs,\nrequire large neural components and extensive training data to work well. In\ncontrast, MQTTS aims to build more compact conversational TTS models while\ncapitalizing on smaller-scale real-life conversational speech data. However,\nits autoregressive nature yields high inference latency and thus limits its\nreal-time usage. In order to mitigate the current limitations of the\nstate-of-the-art TTS models while capitalizing on their strengths, in this work\nwe introduce the Pheme model series that 1) offers compact yet high-performing\nmodels, 2) allows for parallel speech generation of 3) natural conversational\nspeech, and 4) it can be trained efficiently on smaller-scale conversational\ndata, cutting data demands by more than 10x but still matching the quality of\nthe autoregressive TTS models. We also show that through simple teacher-student\ndistillation we can meet significant improvements in voice quality for\nsingle-speaker setups on top of pretrained Pheme checkpoints, relying solely on\nsynthetic speech generated by much larger teacher models. Audio samples and\npretrained models are available online.\n","authors":["Paweł Budzianowski","Taras Sereda","Tomasz Cichy","Ivan Vulić"],"pdf_url":"https://arxiv.org/pdf/2401.02839v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07910v2","updated":"2024-01-05T14:45:00Z","published":"2023-12-13T05:58:34Z","title":"PromptBench: A Unified Library for Evaluation of Large Language Models","summary":"  The evaluation of large language models (LLMs) is crucial to assess their\nperformance and mitigate potential security risks. In this paper, we introduce\nPromptBench, a unified library to evaluate LLMs. It consists of several key\ncomponents that are easily used and extended by researchers: prompt\nconstruction, prompt engineering, dataset and model loading, adversarial prompt\nattack, dynamic evaluation protocols, and analysis tools. PromptBench is\ndesigned to be an open, general, and flexible codebase for research purposes\nthat can facilitate original study in creating new benchmarks, deploying\ndownstream applications, and designing new evaluation protocols. The code is\navailable at: https://github.com/microsoft/promptbench and will be continuously\nsupported.\n","authors":["Kaijie Zhu","Qinlin Zhao","Hao Chen","Jindong Wang","Xing Xie"],"pdf_url":"https://arxiv.org/pdf/2312.07910v2.pdf","comment":"An extension to PromptBench (arXiv:2306.04528) for unified evaluation\n  of LLMs using the same name; code: https://github.com/microsoft/promptbench"},{"id":"http://arxiv.org/abs/2311.14212v2","updated":"2024-01-05T14:18:35Z","published":"2023-11-23T21:54:22Z","title":"Annotation Sensitivity: Training Data Collection Methods Affect Model\n  Performance","summary":"  When training data are collected from human annotators, the design of the\nannotation instrument, the instructions given to annotators, the\ncharacteristics of the annotators, and their interactions can impact training\ndata. This study demonstrates that design choices made when creating an\nannotation instrument also impact the models trained on the resulting\nannotations. We introduce the term annotation sensitivity to refer to the\nimpact of annotation data collection methods on the annotations themselves and\non downstream model performance and predictions. We collect annotations of hate\nspeech and offensive language in five experimental conditions of an annotation\ninstrument, randomly assigning annotators to conditions. We then fine-tune BERT\nmodels on each of the five resulting datasets and evaluate model performance on\na holdout portion of each condition. We find considerable differences between\nthe conditions for 1) the share of hate speech/offensive language annotations,\n2) model performance, 3) model predictions, and 4) model learning curves. Our\nresults emphasize the crucial role played by the annotation instrument which\nhas received little attention in the machine learning literature. We call for\nadditional research into how and why the instrument impacts the annotations to\ninform the development of best practices in instrument design.\n","authors":["Christoph Kern","Stephanie Eckman","Jacob Beck","Rob Chew","Bolei Ma","Frauke Kreuter"],"pdf_url":"https://arxiv.org/pdf/2311.14212v2.pdf","comment":"EMNLP 2023 Findings:\n  https://aclanthology.org/2023.findings-emnlp.992/"},{"id":"http://arxiv.org/abs/2401.02823v1","updated":"2024-01-05T14:15:36Z","published":"2024-01-05T14:15:36Z","title":"DocGraphLM: Documental Graph Language Model for Information Extraction","summary":"  Advances in Visually Rich Document Understanding (VrDU) have enabled\ninformation extraction and question answering over documents with complex\nlayouts. Two tropes of architectures have emerged -- transformer-based models\ninspired by LLMs, and Graph Neural Networks. In this paper, we introduce\nDocGraphLM, a novel framework that combines pre-trained language models with\ngraph semantics. To achieve this, we propose 1) a joint encoder architecture to\nrepresent documents, and 2) a novel link prediction approach to reconstruct\ndocument graphs. DocGraphLM predicts both directions and distances between\nnodes using a convergent joint loss function that prioritizes neighborhood\nrestoration and downweighs distant node detection. Our experiments on three\nSotA datasets show consistent improvement on IE and QA tasks with the adoption\nof graph features. Moreover, we report that adopting the graph features\naccelerates convergence in the learning process during training, despite being\nsolely constructed through link prediction.\n","authors":["Dongsheng Wang","Zhiqiang Ma","Armineh Nourbakhsh","Kang Gu","Sameena Shah"],"pdf_url":"https://arxiv.org/pdf/2401.02823v1.pdf","comment":"Published at SIGIR'23 (repost for easier access)"},{"id":"http://arxiv.org/abs/2401.02797v1","updated":"2024-01-05T13:22:12Z","published":"2024-01-05T13:22:12Z","title":"PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language\n  Models for Medical Visual Question Answering","summary":"  Multimodal large language models (MLLMs) represent an evolutionary expansion\nin the capabilities of traditional large language models, enabling them to\ntackle challenges that surpass the scope of purely text-based applications. It\nleverages the knowledge previously encoded within these language models,\nthereby enhancing their applicability and functionality in the reign of\nmultimodal contexts. Recent works investigate the adaptation of MLLMs to\npredict free-form answers as a generative task to solve medical visual question\nanswering (Med-VQA) tasks. In this paper, we propose a parameter efficient\nframework for fine-tuning MLLM specifically tailored to Med-VQA applications,\nand empirically validate it on a public benchmark dataset. To accurately\nmeasure the performance, we employ human evaluation and the results reveal that\nour model achieves an overall accuracy of 81.9%, and outperforms the GPT-4v\nmodel by a significant margin of 26% absolute accuracy on closed-ended\nquestions. The code will be available here: https://github.com/jinlHe/PeFoMed.\n","authors":["Jinlong He","Pengfei Li","Gang Liu","Zixu Zhao","Shenjun Zhong"],"pdf_url":"https://arxiv.org/pdf/2401.02797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02789v1","updated":"2024-01-05T12:59:20Z","published":"2024-01-05T12:59:20Z","title":"Large Language Models in Plant Biology","summary":"  Large Language Models (LLMs), such as ChatGPT, have taken the world by storm\nand have passed certain forms of the Turing test. However, LLMs are not limited\nto human language and analyze sequential data, such as DNA, protein, and gene\nexpression. The resulting foundation models can be repurposed to identify the\ncomplex patterns within the data, resulting in powerful, multi-purpose\nprediction tools able to explain cellular systems. This review outlines the\ndifferent types of LLMs and showcases their recent uses in biology. Since LLMs\nhave not yet been embraced by the plant community, we also cover how these\nmodels can be deployed for the plant kingdom.\n","authors":["Hilbert Yuen In Lam","Xing Er Ong","Marek Mutwil"],"pdf_url":"https://arxiv.org/pdf/2401.02789v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.01282v4","updated":"2024-01-05T12:41:13Z","published":"2023-11-02T14:57:03Z","title":"FlashDecoding++: Faster Large Language Model Inference on GPUs","summary":"  As the Large Language Model (LLM) becomes increasingly important in various\ndomains. However, the following challenges still remain unsolved in\naccelerating LLM inference: (1) Synchronized partial softmax update. The\nsoftmax operation requires a synchronized update operation among each partial\nsoftmax result, leading to ~20% overheads for the attention computation in\nLLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices\nperforming GEMM in LLM inference is flat, leading to under-utilized computation\nand >50% performance loss after padding zeros in previous designs. (3)\nPerformance loss due to static dataflow. Kernel performance in LLM depends on\nvaried input data features, hardware configurations, etc. A single and static\ndataflow may lead to a 50.25% performance loss for GEMMs of different shapes in\nLLM inference.\n  We present FlashDecoding++, a fast LLM inference engine supporting mainstream\nLLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++\ncreatively proposes: (1) Asynchronized softmax with unified max value.\nFlashDecoding++ introduces a unified max value technique for different partial\nsoftmax computations to avoid synchronization. (2) Flat GEMM optimization with\ndouble buffering. FlashDecoding++ points out that flat GEMMs with different\nshapes face varied bottlenecks. Then, techniques like double buffering are\nintroduced. (3) Heuristic dataflow with hardware resource adaptation.\nFlashDecoding++ heuristically optimizes dataflow using different hardware\nresource considering input dynamics. Due to the versatility of optimizations in\nFlashDecoding++, FlashDecoding++ can achieve up to 4.86x and 2.18x speedup on\nboth NVIDIA and AMD GPUs compared to Hugging Face implementations.\nFlashDecoding++ also achieves an average speedup of 1.37x compared to\nstate-of-the-art LLM inference engines on mainstream LLMs.\n","authors":["Ke Hong","Guohao Dai","Jiaming Xu","Qiuli Mao","Xiuhong Li","Jun Liu","Kangdi Chen","Yuhan Dong","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2311.01282v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02777v1","updated":"2024-01-05T12:26:46Z","published":"2024-01-05T12:26:46Z","title":"From LLM to Conversational Agent: A Memory Enhanced Architecture with\n  Fine-Tuning of Large Language Models","summary":"  This paper introduces RAISE (Reasoning and Acting through Scratchpad and\nExamples), an advanced architecture enhancing the integration of Large Language\nModels (LLMs) like GPT-4 into conversational agents. RAISE, an enhancement of\nthe ReAct framework, incorporates a dual-component memory system, mirroring\nhuman short-term and long-term memory, to maintain context and continuity in\nconversations. It entails a comprehensive agent construction scenario,\nincluding phases like Conversation Selection, Scene Extraction, CoT Completion,\nand Scene Augmentation, leading to the LLMs Training phase. This approach\nappears to enhance agent controllability and adaptability in complex,\nmulti-turn dialogues. Our preliminary evaluations in a real estate sales\ncontext suggest that RAISE has some advantages over traditional agents,\nindicating its potential for broader applications. This work contributes to the\nAI field by providing a robust framework for developing more context-aware and\nversatile conversational agents.\n","authors":["Na Liu","Liangyu Chen","Xiaoyu Tian","Wei Zou","Kaijiang Chen","Ming Cui"],"pdf_url":"https://arxiv.org/pdf/2401.02777v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.10622v2","updated":"2024-01-05T12:13:55Z","published":"2022-12-20T19:52:41Z","title":"mFACE: Multilingual Summarization with Factual Consistency Evaluation","summary":"  Abstractive summarization has enjoyed renewed interest in recent years,\nthanks to pre-trained language models and the availability of large-scale\ndatasets. Despite promising results, current models still suffer from\ngenerating factually inconsistent summaries, reducing their utility for\nreal-world application. Several recent efforts attempt to address this by\ndevising models that automatically detect factual inconsistencies in machine\ngenerated summaries. However, they focus exclusively on English, a language\nwith abundant resources. In this work, we leverage factual consistency\nevaluation models to improve multilingual summarization. We explore two\nintuitive approaches to mitigate hallucinations based on the signal provided by\na multilingual NLI model, namely data filtering and controlled generation.\nExperimental results in the 45 languages from the XLSum dataset show gains over\nstrong baselines in both automatic and human evaluation.\n","authors":["Roee Aharoni","Shashi Narayan","Joshua Maynez","Jonathan Herzig","Elizabeth Clark","Mirella Lapata"],"pdf_url":"https://arxiv.org/pdf/2212.10622v2.pdf","comment":"28 pages with links to released data"},{"id":"http://arxiv.org/abs/2401.02772v1","updated":"2024-01-05T12:01:26Z","published":"2024-01-05T12:01:26Z","title":"Complex systems approach to natural language","summary":"  The review summarizes the main methodological concepts used in studying\nnatural language from the perspective of complexity science and documents their\napplicability in identifying both universal and system-specific features of\nlanguage in its written representation. Three main complexity-related research\ntrends in quantitative linguistics are covered. The first part addresses the\nissue of word frequencies in texts and demonstrates that taking punctuation\ninto consideration restores scaling whose violation in the Zipf's law is often\nobserved for the most frequent words. The second part introduces methods\ninspired by time series analysis, used in studying various kinds of\ncorrelations in written texts. The related time series are generated on the\nbasis of text partition into sentences or into phrases between consecutive\npunctuation marks. It turns out that these series develop features often found\nin signals generated by complex systems, like long-range correlations or\n(multi)fractal structures. Moreover, it appears that the distances between\npunctuation marks comply with the discrete variant of the Weibull distribution.\nIn the third part, the application of the network formalism to natural language\nis reviewed, particularly in the context of the so-called word-adjacency\nnetworks. Parameters characterizing topology of such networks can be used for\nclassification of texts, for example, from a stylometric perspective. Network\napproach can also be applied to represent the organization of word\nassociations. Structure of word-association networks turns out to be\nsignificantly different from that observed in random networks, revealing\ngenuine properties of language. Finally, punctuation seems to have a\nsignificant impact not only on the language's information-carrying ability but\nalso on its key statistical properties, hence it is recommended to consider\npunctuation marks on a par with words.\n","authors":["Tomasz Stanisz","Stanisław Drożdż","Jarosław Kwapień"],"pdf_url":"https://arxiv.org/pdf/2401.02772v1.pdf","comment":"113 pages, 49 figures"},{"id":"http://arxiv.org/abs/2401.02749v1","updated":"2024-01-05T11:02:08Z","published":"2024-01-05T11:02:08Z","title":"Hyperparameter-Free Approach for Faster Minimum Bayes Risk Decoding","summary":"  Minimum Bayes-Risk (MBR) decoding is shown to be a powerful alternative to\nbeam search decoding for a wide range of text generation tasks. However, MBR\nrequires a huge amount of time for inference to compute the MBR objective,\nwhich makes the method infeasible in many situations where response time is\ncritical. Confidence-based pruning (CBP) (Cheng and Vlachos, 2023) has recently\nbeen proposed to reduce the inference time in machine translation tasks.\nAlthough it is shown to significantly reduce the amount of computation, it\nrequires hyperparameter tuning using a development set to be effective. To this\nend, we propose Approximate Minimum Bayes-Risk (AMBR) decoding, a\nhyperparameter-free method to run MBR decoding approximately. AMBR is derived\nfrom the observation that the problem of computing the sample-based MBR\nobjective is the medoid identification problem. AMBR uses the Correlated\nSequential Halving (CSH) algorithm (Baharav and Tse, 2019), the best\napproximation algorithm to date for the medoid identification problem, to\ncompute the sample-based MBR objective. We evaluate AMBR on machine\ntranslation, text summarization, and image captioning tasks. The results show\nthat AMBR achieves on par with CBP, with CBP selecting hyperparameters through\nan Oracle for each given computation budget.\n","authors":["Yuu Jinnai","Kaito Ariu"],"pdf_url":"https://arxiv.org/pdf/2401.02749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02744v1","updated":"2024-01-05T10:41:55Z","published":"2024-01-05T10:41:55Z","title":"MAMI: Multi-Attentional Mutual-Information for Long Sequence Neuron\n  Captioning","summary":"  Neuron labeling is an approach to visualize the behaviour and respond of a\ncertain neuron to a certain pattern that activates the neuron. Neuron labeling\nextract information about the features captured by certain neurons in a deep\nneural network, one of which uses the encoder-decoder image captioning\napproach. The encoder used can be a pretrained CNN-based model and the decoder\nis an RNN-based model for text generation. Previous work, namely MILAN (Mutual\nInformation-guided Linguistic Annotation of Neuron), has tried to visualize the\nneuron behaviour using modified Show, Attend, and Tell (SAT) model in the\nencoder, and LSTM added with Bahdanau attention in the decoder. MILAN can show\ngreat result on short sequence neuron captioning, but it does not show great\nresult on long sequence neuron captioning, so in this work, we would like to\nimprove the performance of MILAN even more by utilizing different kind of\nattention mechanism and additionally adding several attention result into one,\nin order to combine all the advantages from several attention mechanism. Using\nour compound dataset, we obtained higher BLEU and F1-Score on our proposed\nmodel, achieving 17.742 and 0.4811 respectively. At some point where the model\nconverges at the peak, our model obtained BLEU of 21.2262 and BERTScore\nF1-Score of 0.4870.\n","authors":["Alfirsa Damasyifa Fauzulhaq","Wahyu Parwitayasa","Joseph Ananda Sugihdharma","M. Fadli Ridhani","Novanto Yudistira"],"pdf_url":"https://arxiv.org/pdf/2401.02744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.04695v2","updated":"2024-01-05T09:54:03Z","published":"2023-09-09T06:27:00Z","title":"Code-Style In-Context Learning for Knowledge-Based Question Answering","summary":"  Current methods for Knowledge-Based Question Answering (KBQA) usually rely on\ncomplex training techniques and model frameworks, leading to many limitations\nin practical applications. Recently, the emergence of In-Context Learning (ICL)\ncapabilities in Large Language Models (LLMs) provides a simple and\ntraining-free semantic parsing paradigm for KBQA: Given a small number of\nquestions and their labeled logical forms as demo examples, LLMs can understand\nthe task intent and generate the logic form for a new question. However,\ncurrent powerful LLMs have little exposure to logic forms during pre-training,\nresulting in a high format error rate. To solve this problem, we propose a\ncode-style in-context learning method for KBQA, which converts the generation\nprocess of unfamiliar logical form into the more familiar code generation\nprocess for LLMs. Experimental results on three mainstream datasets show that\nour method dramatically mitigated the formatting error problem in generating\nlogic forms while realizing a new SOTA on WebQSP, GrailQA, and GraphQ under the\nfew-shot setting. The code and supplementary files are released at\nhttps://github.com/Arthurizijar/KB-Coder .\n","authors":["Zhijie Nie","Richong Zhang","Zhongyuan Wang","Xudong Liu"],"pdf_url":"https://arxiv.org/pdf/2309.04695v2.pdf","comment":"AAAI2024 Camera Ready"},{"id":"http://arxiv.org/abs/2401.02709v1","updated":"2024-01-05T08:42:45Z","published":"2024-01-05T08:42:45Z","title":"German Text Embedding Clustering Benchmark","summary":"  This work introduces a benchmark assessing the performance of clustering\nGerman text embeddings in different domains. This benchmark is driven by the\nincreasing use of clustering neural text embeddings in tasks that require the\ngrouping of texts (such as topic modeling) and the need for German resources in\nexisting benchmarks. We provide an initial analysis for a range of pre-trained\nmono- and multilingual models evaluated on the outcome of different clustering\nalgorithms. Results include strong performing mono- and multilingual models.\nReducing the dimensions of embeddings can further improve clustering.\nAdditionally, we conduct experiments with continued pre-training for German\nBERT models to estimate the benefits of this additional training. Our\nexperiments suggest that significant performance improvements are possible for\nshort text. All code and datasets are publicly available.\n","authors":["Silvan Wehrli","Bert Arnrich","Christopher Irrgang"],"pdf_url":"https://arxiv.org/pdf/2401.02709v1.pdf","comment":"15 pages, 4 figures"},{"id":"http://arxiv.org/abs/2401.01623v2","updated":"2024-01-05T08:21:36Z","published":"2024-01-03T08:49:12Z","title":"Can AI Be as Creative as Humans?","summary":"  Creativity serves as a cornerstone for societal progress and innovation, but\nits assessment remains a complex and often subjective endeavor. With the rise\nof advanced generative AI models capable of tasks once reserved for human\ncreativity, the study of AI's creative potential becomes imperative for its\nresponsible development and application. This paper addresses the complexities\nin defining and evaluating creativity by introducing a new concept called\nRelative Creativity. Instead of trying to define creativity universally, we\nshift the focus to whether AI can match the creative abilities of a\nhypothetical human. This perspective draws inspiration from the Turing Test,\nexpanding upon it to address the challenges and subjectivities inherent in\nevaluating creativity. This methodological shift facilitates a statistically\nquantifiable evaluation of AI's creativity, which we term Statistical\nCreativity. This approach allows for direct comparisons of AI's creative\nabilities with those of specific human groups. Building on this foundation, we\ndiscuss the application of statistical creativity in contemporary\nprompt-conditioned autoregressive models. In addition to defining and analyzing\na measure of creativity, we introduce an actionable training guideline,\neffectively bridging the gap between theoretical quantification of creativity\nand practical model training. Through these multifaceted contributions, the\npaper establishes a cohesive, continuously evolving, and transformative\nframework for assessing and fostering statistical creativity in AI models.\n","authors":["Haonan Wang","James Zou","Michael Mozer","Anirudh Goyal","Alex Lamb","Linjun Zhang","Weijie J Su","Zhun Deng","Michael Qizhe Xie","Hannah Brown","Kenji Kawaguchi"],"pdf_url":"https://arxiv.org/pdf/2401.01623v2.pdf","comment":"The paper introduce the notion of \"Relative Creativity\", presents\n  measurable assessment, and provides AI training guidelines to foster AI's\n  creative capabilities Project Page: https://ai-relative-creativity.github.io/"},{"id":"http://arxiv.org/abs/2401.01916v2","updated":"2024-01-05T07:46:32Z","published":"2024-01-03T04:47:02Z","title":"AstroLLaMA-Chat: Scaling AstroLLaMA with Conversational and Diverse\n  Datasets","summary":"  We explore the potential of enhancing LLM performance in astronomy-focused\nquestion-answering through targeted, continual pre-training. By employing a\ncompact 7B-parameter LLaMA-2 model and focusing exclusively on a curated set of\nastronomy corpora -- comprising abstracts, introductions, and conclusions -- we\nachieve notable improvements in specialized topic comprehension. While general\nLLMs like GPT-4 excel in broader question-answering scenarios due to superior\nreasoning capabilities, our findings suggest that continual pre-training with\nlimited resources can still enhance model performance on specialized topics.\nAdditionally, we present an extension of AstroLLaMA: the fine-tuning of the 7B\nLLaMA model on a domain-specific conversational dataset, culminating in the\nrelease of the chat-enabled AstroLLaMA for community use. Comprehensive\nquantitative benchmarking is currently in progress and will be detailed in an\nupcoming full paper. The model, AstroLLaMA-Chat, is now available at\nhttps://huggingface.co/universeTBD, providing the first open-source\nconversational AI tool tailored for the astronomy community.\n","authors":["Ernest Perkowski","Rui Pan","Tuan Dung Nguyen","Yuan-Sen Ting","Sandor Kruk","Tong Zhang","Charlie O'Neill","Maja Jablonska","Zechang Sun","Michael J. Smith","Huiling Liu","Kevin Schawinski","Kartheik Iyer","Ioana Ciucă for UniverseTBD"],"pdf_url":"https://arxiv.org/pdf/2401.01916v2.pdf","comment":"4 pages, 1 figure, model is available at\n  https://huggingface.co/universeTBD, published in RNAAS"},{"id":"http://arxiv.org/abs/2306.11698v4","updated":"2024-01-05T07:01:05Z","published":"2023-06-20T17:24:23Z","title":"DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT\n  Models","summary":"  Generative Pre-trained Transformer (GPT) models have exhibited exciting\nprogress in their capabilities, capturing the interest of practitioners and the\npublic alike. Yet, while the literature on the trustworthiness of GPT models\nremains limited, practitioners have proposed employing capable GPT models for\nsensitive applications such as healthcare and finance -- where mistakes can be\ncostly. To this end, this work proposes a comprehensive trustworthiness\nevaluation for large language models with a focus on GPT-4 and GPT-3.5,\nconsidering diverse perspectives -- including toxicity, stereotype bias,\nadversarial robustness, out-of-distribution robustness, robustness on\nadversarial demonstrations, privacy, machine ethics, and fairness. Based on our\nevaluations, we discover previously unpublished vulnerabilities to\ntrustworthiness threats. For instance, we find that GPT models can be easily\nmisled to generate toxic and biased outputs and leak private information in\nboth training data and conversation history. We also find that although GPT-4\nis usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more\nvulnerable given jailbreaking system or user prompts, potentially because GPT-4\nfollows (misleading) instructions more precisely. Our work illustrates a\ncomprehensive trustworthiness evaluation of GPT models and sheds light on the\ntrustworthiness gaps. Our benchmark is publicly available at\nhttps://decodingtrust.github.io/; our dataset can be previewed at\nhttps://huggingface.co/datasets/AI-Secure/DecodingTrust; a concise version of\nthis work is at https://openreview.net/pdf?id=kaHpo8OZw2.\n","authors":["Boxin Wang","Weixin Chen","Hengzhi Pei","Chulin Xie","Mintong Kang","Chenhui Zhang","Chejian Xu","Zidi Xiong","Ritik Dutta","Rylan Schaeffer","Sang T. Truong","Simran Arora","Mantas Mazeika","Dan Hendrycks","Zinan Lin","Yu Cheng","Sanmi Koyejo","Dawn Song","Bo Li"],"pdf_url":"https://arxiv.org/pdf/2306.11698v4.pdf","comment":"NeurIPS 2023 Outstanding Paper (Datasets and Benchmarks Track)"},{"id":"http://arxiv.org/abs/2312.04889v2","updated":"2024-01-05T02:07:48Z","published":"2023-12-08T08:11:11Z","title":"KwaiAgents: Generalized Information-seeking Agent System with Large\n  Language Models","summary":"  Driven by curiosity, humans have continually sought to explore and understand\nthe world around them, leading to the invention of various tools to satiate\nthis inquisitiveness. Despite not having the capacity to process and memorize\nvast amounts of information in their brains, humans excel in critical thinking,\nplanning, reflection, and harnessing available tools to interact with and\ninterpret the world, enabling them to find answers efficiently. The recent\nadvancements in large language models (LLMs) suggest that machines might also\npossess the aforementioned human-like capabilities, allowing them to exhibit\npowerful abilities even with a constrained parameter count. In this paper, we\nintroduce KwaiAgents, a generalized information-seeking agent system based on\nLLMs. Within KwaiAgents, we propose an agent system that employs LLMs as its\ncognitive core, which is capable of understanding a user's query, behavior\nguidelines, and referencing external documents. The agent can also update and\nretrieve information from its internal memory, plan and execute actions using a\ntime-aware search-browse toolkit, and ultimately provide a comprehensive\nresponse. We further investigate the system's performance when powered by LLMs\nless advanced than GPT-4, and introduce the Meta-Agent Tuning (MAT) framework,\ndesigned to ensure even an open-sourced 7B or 13B model performs well among\nmany agent systems. We exploit both benchmark and human evaluations to\nsystematically validate these capabilities. Extensive experiments show the\nsuperiority of our agent system compared to other autonomous agents and\nhighlight the enhanced generalized agent-abilities of our fine-tuned LLMs.\n","authors":["Haojie Pan","Zepeng Zhai","Hao Yuan","Yaojia Lv","Ruiji Fu","Ming Liu","Zhongyuan Wang","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2312.04889v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02594v1","updated":"2024-01-05T01:31:14Z","published":"2024-01-05T01:31:14Z","title":"Unsupervised hard Negative Augmentation for contrastive learning","summary":"  We present Unsupervised hard Negative Augmentation (UNA), a method that\ngenerates synthetic negative instances based on the term frequency-inverse\ndocument frequency (TF-IDF) retrieval model. UNA uses TF-IDF scores to\nascertain the perceived importance of terms in a sentence and then produces\nnegative samples by replacing terms with respect to that. Our experiments\ndemonstrate that models trained with UNA improve the overall performance in\nsemantic textual similarity tasks. Additional performance gains are obtained\nwhen combining UNA with the paraphrasing augmentation. Further results show\nthat our method is compatible with different backbone models. Ablation studies\nalso support the choice of having a TF-IDF-driven control on negative\naugmentation.\n","authors":["Yuxuan Shu","Vasileios Lampos"],"pdf_url":"https://arxiv.org/pdf/2401.02594v1.pdf","comment":"The code and pre-trained models are available at\n  https://github.com/ClaudiaShu/UNA"},{"id":"http://arxiv.org/abs/2312.10997v4","updated":"2024-01-05T01:18:27Z","published":"2023-12-18T07:47:33Z","title":"Retrieval-Augmented Generation for Large Language Models: A Survey","summary":"  Large Language Models (LLMs) demonstrate significant capabilities but face\nchallenges such as hallucination, outdated knowledge, and non-transparent,\nuntraceable reasoning processes. Retrieval-Augmented Generation (RAG) has\nemerged as a promising solution by incorporating knowledge from external\ndatabases. This enhances the accuracy and credibility of the models,\nparticularly for knowledge-intensive tasks, and allows for continuous knowledge\nupdates and integration of domain-specific information. RAG synergistically\nmerges LLMs' intrinsic knowledge with the vast, dynamic repositories of\nexternal databases. This comprehensive review paper offers a detailed\nexamination of the progression of RAG paradigms, encompassing the Naive RAG,\nthe Advanced RAG, and the Modular RAG. It meticulously scrutinizes the\ntripartite foundation of RAG frameworks, which includes the retrieval , the\ngeneration and the augmentation techniques. The paper highlights the\nstate-of-the-art technologies embedded in each of these critical components,\nproviding a profound understanding of the advancements in RAG systems.\nFurthermore, this paper introduces the metrics and benchmarks for assessing RAG\nmodels, along with the most up-to-date evaluation framework. In conclusion, the\npaper delineates prospective avenues for research, including the identification\nof challenges, the expansion of multi-modalities, and the progression of the\nRAG infrastructure and its ecosystem.\n","authors":["Yunfan Gao","Yun Xiong","Xinyu Gao","Kangxiang Jia","Jinliu Pan","Yuxi Bi","Yi Dai","Jiawei Sun","Qianyu Guo","Meng Wang","Haofen Wang"],"pdf_url":"https://arxiv.org/pdf/2312.10997v4.pdf","comment":"Ongoing Work"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2401.02957v1","updated":"2024-01-05T18:59:52Z","published":"2024-01-05T18:59:52Z","title":"Denoising Vision Transformers","summary":"  We delve into a nuanced but significant challenge inherent to Vision\nTransformers (ViTs): feature maps of these models exhibit grid-like artifacts,\nwhich detrimentally hurt the performance of ViTs in downstream tasks. Our\ninvestigations trace this fundamental issue down to the positional embeddings\nat the input stage. To address this, we propose a novel noise model, which is\nuniversally applicable to all ViTs. Specifically, the noise model dissects ViT\noutputs into three components: a semantics term free from noise artifacts and\ntwo artifact-related terms that are conditioned on pixel locations. Such a\ndecomposition is achieved by enforcing cross-view feature consistency with\nneural fields in a per-image basis. This per-image optimization process\nextracts artifact-free features from raw ViT outputs, providing clean features\nfor offline applications. Expanding the scope of our solution to support online\nfunctionality, we introduce a learnable denoiser to predict artifact-free\nfeatures directly from unprocessed ViT outputs, which shows remarkable\ngeneralization capabilities to novel data without the need for per-image\noptimization. Our two-stage approach, termed Denoising Vision Transformers\n(DVT), does not require re-training existing pre-trained ViTs and is\nimmediately applicable to any Transformer-based architecture. We evaluate our\nmethod on a variety of representative ViTs (DINO, MAE, DeiT-III, EVA02, CLIP,\nDINOv2, DINOv2-reg). Extensive evaluations demonstrate that our DVT\nconsistently and significantly improves existing state-of-the-art\ngeneral-purpose models in semantic and geometric tasks across multiple datasets\n(e.g., +3.84 mIoU). We hope our study will encourage a re-evaluation of ViT\ndesign, especially regarding the naive use of positional embeddings.\n","authors":["Jiawei Yang","Katie Z Luo","Jiefeng Li","Kilian Q Weinberger","Yonglong Tian","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2401.02957v1.pdf","comment":"Project website: https://jiawei-yang.github.io/DenoisingViT/"},{"id":"http://arxiv.org/abs/2304.05292v4","updated":"2024-01-05T18:59:41Z","published":"2023-04-11T15:42:20Z","title":"MC-ViViT: Multi-branch Classifier-ViViT to detect Mild Cognitive\n  Impairment in older adults using facial videos","summary":"  Deep machine learning models including Convolutional Neural Networks (CNN)\nhave been successful in the detection of Mild Cognitive Impairment (MCI) using\nmedical images, questionnaires, and videos. This paper proposes a novel\nMulti-branch Classifier-Video Vision Transformer (MC-ViViT) model to\ndistinguish MCI from those with normal cognition by analyzing facial features.\nThe data comes from the I-CONECT, a behavioral intervention trial aimed at\nimproving cognitive function by providing frequent video chats. MC-ViViT\nextracts spatiotemporal features of videos in one branch and augments\nrepresentations by the MC module. The I-CONECT dataset is challenging as the\ndataset is imbalanced containing Hard-Easy and Positive-Negative samples, which\nimpedes the performance of MC-ViViT. We propose a loss function for Hard-Easy\nand Positive-Negative Samples (HP Loss) by combining Focal loss and AD-CORRE\nloss to address the imbalanced problem. Our experimental results on the\nI-CONECT dataset show the great potential of MC-ViViT in predicting MCI with a\nhigh accuracy of 90.63% accuracy on some of the interview videos.\n","authors":["Jian Sun","Hiroko H. Dodge","Mohammad H. Mahoor"],"pdf_url":"https://arxiv.org/pdf/2304.05292v4.pdf","comment":"13 pages, 7 tables, 7 figures, 9 equations"},{"id":"http://arxiv.org/abs/2401.02955v1","updated":"2024-01-05T18:59:22Z","published":"2024-01-05T18:59:22Z","title":"Open-Vocabulary SAM: Segment and Recognize Twenty-thousand Classes\n  Interactively","summary":"  The CLIP and Segment Anything Model (SAM) are remarkable vision foundation\nmodels (VFMs). SAM excels in segmentation tasks across diverse domains, while\nCLIP is renowned for its zero-shot recognition capabilities. This paper\npresents an in-depth exploration of integrating these two models into a unified\nframework. Specifically, we introduce the Open-Vocabulary SAM, a SAM-inspired\nmodel designed for simultaneous interactive segmentation and recognition,\nleveraging two unique knowledge transfer modules: SAM2CLIP and CLIP2SAM. The\nformer adapts SAM's knowledge into the CLIP via distillation and learnable\ntransformer adapters, while the latter transfers CLIP knowledge into SAM,\nenhancing its recognition capabilities. Extensive experiments on various\ndatasets and detectors show the effectiveness of Open-Vocabulary SAM in both\nsegmentation and recognition tasks, significantly outperforming the naive\nbaselines of simply combining SAM and CLIP. Furthermore, aided with image\nclassification data training, our method can segment and recognize\napproximately 22,000 classes.\n","authors":["Haobo Yuan","Xiangtai Li","Chong Zhou","Yining Li","Kai Chen","Chen Change Loy"],"pdf_url":"https://arxiv.org/pdf/2401.02955v1.pdf","comment":"Project page: https://www.mmlab-ntu.com/project/ovsam"},{"id":"http://arxiv.org/abs/2309.08471v2","updated":"2024-01-05T18:57:34Z","published":"2023-09-15T15:20:16Z","title":"TreeLearn: A Comprehensive Deep Learning Method for Segmenting\n  Individual Trees from Ground-Based LiDAR Forest Point Clouds","summary":"  Laser-scanned point clouds of forests make it possible to extract valuable\ninformation for forest management. To consider single trees, a forest point\ncloud needs to be segmented into individual tree point clouds. Existing\nsegmentation methods are usually based on hand-crafted algorithms, such as\nidentifying trunks and growing trees from them, and face difficulties in dense\nforests with overlapping tree crowns. In this study, we propose TreeLearn, a\ndeep learning-based approach for tree instance segmentation of forest point\nclouds. Unlike previous methods, TreeLearn is trained on already segmented\npoint clouds in a data-driven manner, making it less reliant on predefined\nfeatures and algorithms. Furthermore, TreeLearn is implemented as a fully\nautomatic pipeline and does not rely on extensive hyperparameter tuning, which\nmakes it easy to use. Additionally, we introduce a new manually segmented\nbenchmark forest dataset containing 156 full trees, and 79 partial trees, that\nhave been cleanly segmented by hand. The data is generated by mobile laser\nscanning and contributes to create a larger and more diverse data basis for\nmodel development and fine-grained instance segmentation evaluation. We trained\nTreeLearn on forest point clouds of 6665 trees, labeled using the Lidar360\nsoftware. An evaluation on the benchmark dataset shows that TreeLearn performs\nequally well or better than the algorithm used to generate its training data.\nFurthermore, the method's performance can be vastly improved by fine-tuning on\nthe cleanly labeled benchmark dataset. The TreeLearn code is available from\nhttps://github.com/ecker-lab/TreeLearn. The data as well as trained models can\nbe found at https://doi.org/10.25625/VPMPID.\n","authors":["Jonathan Henrich","Jan van Delden","Dominik Seidel","Thomas Kneib","Alexander Ecker"],"pdf_url":"https://arxiv.org/pdf/2309.08471v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02937v1","updated":"2024-01-05T18:28:51Z","published":"2024-01-05T18:28:51Z","title":"Locally Adaptive Neural 3D Morphable Models","summary":"  We present the Locally Adaptive Morphable Model (LAMM), a highly flexible\nAuto-Encoder (AE) framework for learning to generate and manipulate 3D meshes.\nWe train our architecture following a simple self-supervised training scheme in\nwhich input displacements over a set of sparse control vertices are used to\noverwrite the encoded geometry in order to transform one training sample into\nanother. During inference, our model produces a dense output that adheres\nlocally to the specified sparse geometry while maintaining the overall\nappearance of the encoded object. This approach results in state-of-the-art\nperformance in both disentangling manipulated geometry and 3D mesh\nreconstruction. To the best of our knowledge LAMM is the first end-to-end\nframework that enables direct local control of 3D vertex geometry in a single\nforward pass. A very efficient computational graph allows our network to train\nwith only a fraction of the memory required by previous methods and run faster\nduring inference, generating 12k vertex meshes at $>$60fps on a single CPU\nthread. We further leverage local geometry control as a primitive for higher\nlevel editing operations and present a set of derivative capabilities such as\nswapping and sampling object parts. Code and pretrained models can be found at\nhttps://github.com/michaeltrs/LAMM.\n","authors":["Michail Tarasiou","Rolandos Alexandros Potamias","Eimear O'Sullivan","Stylianos Ploumpis","Stefanos Zafeiriou"],"pdf_url":"https://arxiv.org/pdf/2401.02937v1.pdf","comment":"10 pages, 9 figures, 2 tables"},{"id":"http://arxiv.org/abs/2308.07688v4","updated":"2024-01-05T18:25:42Z","published":"2023-08-15T10:37:13Z","title":"Enhancing Network Initialization for Medical AI Models Using\n  Large-Scale, Unlabeled Natural Images","summary":"  Pre-training datasets, like ImageNet, have become the gold standard in\nmedical image analysis. However, the emergence of self-supervised learning\n(SSL), which leverages unlabeled data to learn robust features, presents an\nopportunity to bypass the intensive labeling process. In this study, we\nexplored if SSL for pre-training on non-medical images can be applied to chest\nradiographs and how it compares to supervised pre-training on non-medical\nimages and on medical images. We utilized a vision transformer and initialized\nits weights based on (i) SSL pre-training on natural images (DINOv2), (ii) SL\npre-training on natural images (ImageNet dataset), and (iii) SL pre-training on\nchest radiographs from the MIMIC-CXR database. We tested our approach on over\n800,000 chest radiographs from six large global datasets, diagnosing more than\n20 different imaging findings. Our SSL pre-training on curated images not only\noutperformed ImageNet-based pre-training (P<0.001 for all datasets) but, in\ncertain cases, also exceeded SL on the MIMIC-CXR dataset. Our findings suggest\nthat selecting the right pre-training strategy, especially with SSL, can be\npivotal for improving artificial intelligence (AI)'s diagnostic accuracy in\nmedical imaging. By demonstrating the promise of SSL in chest radiograph\nanalysis, we underline a transformative shift towards more efficient and\naccurate AI models in medical imaging.\n","authors":["Soroosh Tayebi Arasteh","Leo Misera","Jakob Nikolas Kather","Daniel Truhn","Sven Nebelung"],"pdf_url":"https://arxiv.org/pdf/2308.07688v4.pdf","comment":"Published in European Radiology Experimental"},{"id":"http://arxiv.org/abs/2401.02931v1","updated":"2024-01-05T18:15:26Z","published":"2024-01-05T18:15:26Z","title":"SPFormer: Enhancing Vision Transformer with Superpixel Representation","summary":"  In this work, we introduce SPFormer, a novel Vision Transformer enhanced by\nsuperpixel representation. Addressing the limitations of traditional Vision\nTransformers' fixed-size, non-adaptive patch partitioning, SPFormer employs\nsuperpixels that adapt to the image's content. This approach divides the image\ninto irregular, semantically coherent regions, effectively capturing intricate\ndetails and applicable at both initial and intermediate feature levels.\n  SPFormer, trainable end-to-end, exhibits superior performance across various\nbenchmarks. Notably, it exhibits significant improvements on the challenging\nImageNet benchmark, achieving a 1.4% increase over DeiT-T and 1.1% over DeiT-S\nrespectively. A standout feature of SPFormer is its inherent explainability.\nThe superpixel structure offers a window into the model's internal processes,\nproviding valuable insights that enhance the model's interpretability. This\nlevel of clarity significantly improves SPFormer's robustness, particularly in\nchallenging scenarios such as image rotations and occlusions, demonstrating its\nadaptability and resilience.\n","authors":["Jieru Mei","Liang-Chieh Chen","Alan Yuille","Cihang Xie"],"pdf_url":"https://arxiv.org/pdf/2401.02931v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.03538v2","updated":"2024-01-05T17:56:56Z","published":"2023-07-07T12:00:38Z","title":"Language-free Compositional Action Generation via Decoupling Refinement","summary":"  Composing simple elements into complex concepts is crucial yet challenging,\nespecially for 3D action generation. Existing methods largely rely on extensive\nneural language annotations to discern composable latent semantics, a process\nthat is often costly and labor-intensive. In this study, we introduce a novel\nframework to generate compositional actions without reliance on language\nauxiliaries. Our approach consists of three main components: Action Coupling,\nConditional Action Generation, and Decoupling Refinement. Action Coupling\nutilizes an energy model to extract the attention masks of each sub-action,\nsubsequently integrating two actions using these attentions to generate\npseudo-training examples. Then, we employ a conditional generative model, CVAE,\nto learn a latent space, facilitating the diverse generation. Finally, we\npropose Decoupling Refinement, which leverages a self-supervised pre-trained\nmodel MAE to ensure semantic consistency between the sub-actions and\ncompositional actions. This refinement process involves rendering generated 3D\nactions into 2D space, decoupling these images into two sub-segments, using the\nMAE model to restore the complete image from sub-segments, and constraining the\nrecovered images to match images rendered from raw sub-actions. Due to the lack\nof existing datasets containing both sub-actions and compositional actions, we\ncreated two new datasets, named HumanAct-C and UESTC-C, and present a\ncorresponding evaluation metric. Both qualitative and quantitative assessments\nare conducted to show our efficacy.\n","authors":["Xiao Liu","Guangyi Chen","Yansong Tang","Guangrun Wang","Ser-Nam Lim"],"pdf_url":"https://arxiv.org/pdf/2307.03538v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2401.02916v1","updated":"2024-01-05T17:39:52Z","published":"2024-01-05T17:39:52Z","title":"Uncovering the human motion pattern: Pattern Memory-based Diffusion\n  Model for Trajectory Prediction","summary":"  Human trajectory forecasting is a critical challenge in fields such as\nrobotics and autonomous driving. Due to the inherent uncertainty of human\nactions and intentions in real-world scenarios, various unexpected occurrences\nmay arise. To uncover latent motion patterns in human behavior, we introduce a\nnovel memory-based method, named Motion Pattern Priors Memory Network. Our\nmethod involves constructing a memory bank derived from clustered prior\nknowledge of motion patterns observed in the training set trajectories. We\nintroduce an addressing mechanism to retrieve the matched pattern and the\npotential target distributions for each prediction from the memory bank, which\nenables the identification and retrieval of natural motion patterns exhibited\nby agents, subsequently using the target priors memory token to guide the\ndiffusion model to generate predictions. Extensive experiments validate the\neffectiveness of our approach, achieving state-of-the-art trajectory prediction\naccuracy. The code will be made publicly available.\n","authors":["Yuxin Yang","Pengfei Zhu","Mengshi Qi","Huadong Ma"],"pdf_url":"https://arxiv.org/pdf/2401.02916v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.06683v5","updated":"2024-01-05T17:18:56Z","published":"2023-01-17T03:53:29Z","title":"Surgical Aggregation: Federated Class-Heterogeneous Learning","summary":"  The release of numerous chest x-ray datasets has spearheaded the development\nof deep learning models with expert-level performance. However, they have\nlimited interoperability due to class-heterogeneity -- a result of inconsistent\nlabeling schemes and partial annotations. Therefore, it is challenging to\nleverage these datasets in aggregate to train models with a complete\nrepresentation of abnormalities that may occur within the thorax. In this work,\nwe propose surgical aggregation, a federated learning framework for aggregating\nknowledge from class-heterogeneous datasets and learn a model that can\nsimultaneously predict the presence of all disease labels present across the\ndatasets. We evaluate our method using simulated and real-world\nclass-heterogeneous datasets across both independent and identically\ndistributed (iid) and non-iid settings. Our results show that surgical\naggregation outperforms current methods, has better generalizability, and is a\ncrucial first step towards tackling class-heterogeneity in federated learning\nto facilitate the development of clinically-useful models using previously\nnon-interoperable chest x-ray datasets.\n","authors":["Pranav Kulkarni","Adway Kanhere","Paul H. Yi","Vishwa S. Parekh"],"pdf_url":"https://arxiv.org/pdf/2301.06683v5.pdf","comment":"9 pages, 7 figures, 4 tables"},{"id":"http://arxiv.org/abs/2401.02906v1","updated":"2024-01-05T17:05:42Z","published":"2024-01-05T17:05:42Z","title":"MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance","summary":"  The deployment of multimodal large language models (MLLMs) has brought forth\na unique vulnerability: susceptibility to malicious attacks through visual\ninputs. We delve into the novel challenge of defending MLLMs against such\nattacks. We discovered that images act as a \"foreign language\" that is not\nconsidered during alignment, which can make MLLMs prone to producing harmful\nresponses. Unfortunately, unlike the discrete tokens considered in text-based\nLLMs, the continuous nature of image signals presents significant alignment\nchallenges, which poses difficulty to thoroughly cover the possible scenarios.\nThis vulnerability is exacerbated by the fact that open-source MLLMs are\npredominantly fine-tuned on limited image-text pairs that is much less than the\nextensive text-based pretraining corpus, which makes the MLLMs more prone to\ncatastrophic forgetting of their original abilities during explicit alignment\ntuning. To tackle these challenges, we introduce MLLM-Protector, a\nplug-and-play strategy combining a lightweight harm detector and a response\ndetoxifier. The harm detector's role is to identify potentially harmful outputs\nfrom the MLLM, while the detoxifier corrects these outputs to ensure the\nresponse stipulates to the safety standards. This approach effectively\nmitigates the risks posed by malicious visual inputs without compromising the\nmodel's overall performance. Our results demonstrate that MLLM-Protector offers\na robust solution to a previously unaddressed aspect of MLLM security.\n","authors":["Renjie Pi","Tianyang Han","Yueqi Xie","Rui Pan","Qing Lian","Hanze Dong","Jipeng Zhang","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.02906v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.13775v2","updated":"2024-01-05T16:13:31Z","published":"2023-04-26T18:46:26Z","title":"Advancing Ischemic Stroke Diagnosis: A Novel Two-Stage Approach for\n  Blood Clot Origin Identification","summary":"  An innovative two-stage methodology for categorizing blood clot origins is\npresented in this paper, which is important for the diagnosis and treatment of\nischemic stroke. First, a background classifier based on MobileNetV3 segments\nbig whole-slide digital pathology images into numerous tiles to detect the\npresence of cellular material. After that, different pre-trained image\nclassification algorithms are fine-tuned to determine the origin of blood\nclots. Due to complex blood flow dynamics and limitations in conventional\nimaging methods such as computed tomography (CT), magnetic resonance imaging\n(MRI), and ultrasound, identifying the sources of blood clots is a challenging\ntask. Although these techniques are useful for identifying blood clots, they\nare not very good at determining how they originated. To address these\nchallenges, our method makes use of robust computer vision models that have\nbeen refined using information from whole-slide digital pathology images. Out\nof all the models tested, the PoolFormer \\cite{yu2022metaformer} performs\nbetter than the others, with 93.4\\% accuracy, 93.4\\% precision, 93.4\\% recall,\nand 93.4\\% F1-score. Moreover, it achieves the good weighted multi-class\nlogarithmic loss (WMCLL) of 0.4361, which emphasizes how effective it is in\nthis particular application. These encouraging findings suggest that our\napproach can successfully identify the origin of blood clots in a variety of\nvascular locations, potentially advancing ischemic stroke diagnosis and\ntreatment approaches.\n","authors":["Koushik Sivarama Krishnan","P. J. Joe Nikesh","Swathi Gnanasekar","Karthik Sivarama Krishnan"],"pdf_url":"https://arxiv.org/pdf/2304.13775v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.16221v3","updated":"2024-01-05T15:43:12Z","published":"2023-10-24T22:24:44Z","title":"Hierarchical Randomized Smoothing","summary":"  Real-world data is complex and often consists of objects that can be\ndecomposed into multiple entities (e.g. images into pixels, graphs into\ninterconnected nodes). Randomized smoothing is a powerful framework for making\nmodels provably robust against small changes to their inputs - by guaranteeing\nrobustness of the majority vote when randomly adding noise before\nclassification. Yet, certifying robustness on such complex data via randomized\nsmoothing is challenging when adversaries do not arbitrarily perturb entire\nobjects (e.g. images) but only a subset of their entities (e.g. pixels). As a\nsolution, we introduce hierarchical randomized smoothing: We partially smooth\nobjects by adding random noise only on a randomly selected subset of their\nentities. By adding noise in a more targeted manner than existing methods we\nobtain stronger robustness guarantees while maintaining high accuracy. We\ninitialize hierarchical smoothing using different noising distributions,\nyielding novel robustness certificates for discrete and continuous domains. We\nexperimentally demonstrate the importance of hierarchical smoothing in image\nand node classification, where it yields superior robustness-accuracy\ntrade-offs. Overall, hierarchical smoothing is an important contribution\ntowards models that are both - certifiably robust to perturbations and\naccurate.\n","authors":["Yan Scholten","Jan Schuchardt","Aleksandar Bojchevski","Stephan Günnemann"],"pdf_url":"https://arxiv.org/pdf/2310.16221v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02861v1","updated":"2024-01-05T15:32:40Z","published":"2024-01-05T15:32:40Z","title":"Reversing the Irreversible: A Survey on Inverse Biometrics","summary":"  With the widespread use of biometric recognition, several issues related to\nthe privacy and security provided by this technology have been recently raised\nand analysed. As a result, the early common belief among the biometrics\ncommunity of templates irreversibility has been proven wrong. It is now an\naccepted fact that it is possible to reconstruct from an unprotected template a\nsynthetic sample that matches the bona fide one. This reverse engineering\nprocess, commonly referred to as \\textit{inverse biometrics}, constitutes a\nsevere threat for biometric systems from two different angles: on the one hand,\nsensitive personal data (i.e., biometric data) can be derived from compromised\nunprotected templates; on the other hand, other powerful attacks can be\nlaunched building upon these reconstructed samples. Given its important\nimplications, biometric stakeholders have produced over the last fifteen years\nnumerous works analysing the different aspects related to inverse biometrics:\ndevelopment of reconstruction algorithms for different characteristics;\nproposal of methodologies to assess the vulnerabilities of biometric systems to\nthe aforementioned algorithms; development of countermeasures to reduce the\npossible effects of attacks. The present article is an effort to condense all\nthis information in one comprehensive review of: the problem itself, the\nevaluation of the problem, and the mitigation of the problem. The present\narticle is an effort to condense all this information in one comprehensive\nreview of: the problem itself, the evaluation of the problem, and the\nmitigation of the problem.\n","authors":["Marta Gomez-Barrero","Javier Galbally"],"pdf_url":"https://arxiv.org/pdf/2401.02861v1.pdf","comment":"18 pages, journal, survey"},{"id":"http://arxiv.org/abs/2401.02847v1","updated":"2024-01-05T15:07:05Z","published":"2024-01-05T15:07:05Z","title":"Generating Non-Stationary Textures using Self-Rectification","summary":"  This paper addresses the challenge of example-based non-stationary texture\nsynthesis. We introduce a novel twostep approach wherein users first modify a\nreference texture using standard image editing tools, yielding an initial rough\ntarget for the synthesis. Subsequently, our proposed method, termed\n\"self-rectification\", automatically refines this target into a coherent,\nseamless texture, while faithfully preserving the distinct visual\ncharacteristics of the reference exemplar. Our method leverages a pre-trained\ndiffusion network, and uses self-attention mechanisms, to gradually align the\nsynthesized texture with the reference, ensuring the retention of the\nstructures in the provided target. Through experimental validation, our\napproach exhibits exceptional proficiency in handling non-stationary textures,\ndemonstrating significant advancements in texture synthesis when compared to\nexisting state-of-the-art techniques. Code is available at\nhttps://github.com/xiaorongjun000/Self-Rectification\n","authors":["Yang Zhou","Rongjun Xiao","Dani Lischinski","Daniel Cohen-Or","Hui Huang"],"pdf_url":"https://arxiv.org/pdf/2401.02847v1.pdf","comment":"Project page: https://github.com/xiaorongjun000/Self-Rectification"},{"id":"http://arxiv.org/abs/2401.02841v1","updated":"2024-01-05T14:48:19Z","published":"2024-01-05T14:48:19Z","title":"Multi-Stage Contrastive Regression for Action Quality Assessment","summary":"  In recent years, there has been growing interest in the video-based action\nquality assessment (AQA). Most existing methods typically solve AQA problem by\nconsidering the entire video yet overlooking the inherent stage-level\ncharacteristics of actions. To address this issue, we design a novel\nMulti-stage Contrastive Regression (MCoRe) framework for the AQA task. This\napproach allows us to efficiently extract spatial-temporal information, while\nsimultaneously reducing computational costs by segmenting the input video into\nmultiple stages or procedures. Inspired by the graph contrastive learning, we\npropose a new stage-wise contrastive learning loss function to enhance\nperformance. As a result, MCoRe demonstrates the state-of-the-art result so far\non the widely-adopted fine-grained AQA dataset.\n","authors":["Qi An","Mengshi Qi","Huadong Ma"],"pdf_url":"https://arxiv.org/pdf/2401.02841v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02838v1","updated":"2024-01-05T14:45:45Z","published":"2024-01-05T14:45:45Z","title":"CrisisViT: A Robust Vision Transformer for Crisis Image Classification","summary":"  In times of emergency, crisis response agencies need to quickly and\naccurately assess the situation on the ground in order to deploy relevant\nservices and resources. However, authorities often have to make decisions based\non limited information, as data on affected regions can be scarce until local\nresponse services can provide first-hand reports. Fortunately, the widespread\navailability of smartphones with high-quality cameras has made citizen\njournalism through social media a valuable source of information for crisis\nresponders. However, analyzing the large volume of images posted by citizens\nrequires more time and effort than is typically available. To address this\nissue, this paper proposes the use of state-of-the-art deep neural models for\nautomatic image classification/tagging, specifically by adapting\ntransformer-based architectures for crisis image classification (CrisisViT). We\nleverage the new Incidents1M crisis image dataset to develop a range of new\ntransformer-based image classification models. Through experimentation over the\nstandard Crisis image benchmark dataset, we demonstrate that the CrisisViT\nmodels significantly outperform previous approaches in emergency type, image\nrelevance, humanitarian category, and damage severity classification.\nAdditionally, we show that the new Incidents1M dataset can further augment the\nCrisisViT models resulting in an additional 1.25% absolute accuracy gain.\n","authors":["Zijun Long","Richard McCreadie","Muhammad Imran"],"pdf_url":"https://arxiv.org/pdf/2401.02838v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02831v1","updated":"2024-01-05T14:31:20Z","published":"2024-01-05T14:31:20Z","title":"Two-stage Progressive Residual Dense Attention Network for Image\n  Denoising","summary":"  Deep convolutional neural networks (CNNs) for image denoising can effectively\nexploit rich hierarchical features and have achieved great success. However,\nmany deep CNN-based denoising models equally utilize the hierarchical features\nof noisy images without paying attention to the more important and useful\nfeatures, leading to relatively low performance. To address the issue, we\ndesign a new Two-stage Progressive Residual Dense Attention Network\n(TSP-RDANet) for image denoising, which divides the whole process of denoising\ninto two sub-tasks to remove noise progressively. Two different attention\nmechanism-based denoising networks are designed for the two sequential\nsub-tasks: the residual dense attention module (RDAM) is designed for the first\nstage, and the hybrid dilated residual dense attention module (HDRDAM) is\nproposed for the second stage. The proposed attention modules are able to learn\nappropriate local features through dense connection between different\nconvolutional layers, and the irrelevant features can also be suppressed. The\ntwo sub-networks are then connected by a long skip connection to retain the\nshallow feature to enhance the denoising performance. The experiments on seven\nbenchmark datasets have verified that compared with many state-of-the-art\nmethods, the proposed TSP-RDANet can obtain favorable results both on synthetic\nand real noisy image denoising. The code of our TSP-RDANet is available at\nhttps://github.com/WenCongWu/TSP-RDANet.\n","authors":["Wencong Wu","An Ge","Guannan Lv","Yuelong Xia","Yungang Zhang","Wen Xiong"],"pdf_url":"https://arxiv.org/pdf/2401.02831v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02826v1","updated":"2024-01-05T14:20:22Z","published":"2024-01-05T14:20:22Z","title":"CRSOT: Cross-Resolution Object Tracking using Unaligned Frame and Event\n  Cameras","summary":"  Existing datasets for RGB-DVS tracking are collected with DVS346 camera and\ntheir resolution ($346 \\times 260$) is low for practical applications.\nActually, only visible cameras are deployed in many practical systems, and the\nnewly designed neuromorphic cameras may have different resolutions. The latest\nneuromorphic sensors can output high-definition event streams, but it is very\ndifficult to achieve strict alignment between events and frames on both spatial\nand temporal views. Therefore, how to achieve accurate tracking with unaligned\nneuromorphic and visible sensors is a valuable but unresearched problem. In\nthis work, we formally propose the task of object tracking using unaligned\nneuromorphic and visible cameras. We build the first unaligned frame-event\ndataset CRSOT collected with a specially built data acquisition system, which\ncontains 1,030 high-definition RGB-Event video pairs, 304,974 video frames. In\naddition, we propose a novel unaligned object tracking framework that can\nrealize robust tracking even using the loosely aligned RGB-Event data.\nSpecifically, we extract the template and search regions of RGB and Event data\nand feed them into a unified ViT backbone for feature embedding. Then, we\npropose uncertainty perception modules to encode the RGB and Event features,\nrespectively, then, we propose a modality uncertainty fusion module to\naggregate the two modalities. These three branches are jointly optimized in the\ntraining phase. Extensive experiments demonstrate that our tracker can\ncollaborate the dual modalities for high-performance tracking even without\nstrictly temporal and spatial alignment. The source code, dataset, and\npre-trained models will be released at\nhttps://github.com/Event-AHU/Cross_Resolution_SOT.\n","authors":["Yabin Zhu","Xiao Wang","Chenglong Li","Bo Jiang","Lin Zhu","Zhixiang Huang","Yonghong Tian","Jin Tang"],"pdf_url":"https://arxiv.org/pdf/2401.02826v1.pdf","comment":"In Peer Review"},{"id":"http://arxiv.org/abs/2401.02814v1","updated":"2024-01-05T13:54:45Z","published":"2024-01-05T13:54:45Z","title":"Object-Centric Instruction Augmentation for Robotic Manipulation","summary":"  Humans interpret scenes by recognizing both the identities and positions of\nobjects in their observations. For a robot to perform tasks such as\n\\enquote{pick and place}, understanding both what the objects are and where\nthey are located is crucial. While the former has been extensively discussed in\nthe literature that uses the large language model to enrich the text\ndescriptions, the latter remains underexplored. In this work, we introduce the\n\\textit{Object-Centric Instruction Augmentation (OCI)} framework to augment\nhighly semantic and information-dense language instruction with position cues.\nWe utilize a Multi-modal Large Language Model (MLLM) to weave knowledge of\nobject locations into natural language instruction, thus aiding the policy\nnetwork in mastering actions for versatile manipulation. Additionally, we\npresent a feature reuse mechanism to integrate the vision-language features\nfrom off-the-shelf pre-trained MLLM into policy networks. Through a series of\nsimulated and real-world robotic tasks, we demonstrate that robotic manipulator\nimitation policies trained with our enhanced instructions outperform those\nrelying solely on traditional language instructions.\n","authors":["Junjie Wen","Yichen Zhu","Minjie Zhu","Jinming Li","Zhiyuan Xu","Zhengping Che","Chaomin Shen","Yaxin Peng","Dong Liu","Feifei Feng","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2401.02814v1.pdf","comment":"Submitted to ICRA2024"},{"id":"http://arxiv.org/abs/2401.02804v1","updated":"2024-01-05T13:36:19Z","published":"2024-01-05T13:36:19Z","title":"Diffbody: Diffusion-based Pose and Shape Editing of Human Images","summary":"  Pose and body shape editing in a human image has received increasing\nattention. However, current methods often struggle with dataset biases and\ndeteriorate realism and the person's identity when users make large edits. We\npropose a one-shot approach that enables large edits with identity\npreservation. To enable large edits, we fit a 3D body model, project the input\nimage onto the 3D model, and change the body's pose and shape. Because this\ninitial textured body model has artifacts due to occlusion and the inaccurate\nbody shape, the rendered image undergoes a diffusion-based refinement, in which\nstrong noise destroys body structure and identity whereas insufficient noise\ndoes not help. We thus propose an iterative refinement with weak noise, applied\nfirst for the whole body and then for the face. We further enhance the realism\nby fine-tuning text embeddings via self-supervised learning. Our quantitative\nand qualitative evaluations demonstrate that our method outperforms other\nexisting methods across various datasets.\n","authors":["Yuta Okuyama","Yuki Endo","Yoshihiro Kanamori"],"pdf_url":"https://arxiv.org/pdf/2401.02804v1.pdf","comment":"Accepted to WACV 2024, project page:\n  https://www.cgg.cs.tsukuba.ac.jp/~okuyama/pub/diffbody/"},{"id":"http://arxiv.org/abs/2311.09215v2","updated":"2024-01-05T13:16:25Z","published":"2023-11-15T18:56:51Z","title":"ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy","summary":"  Modern computer vision offers a great variety of models to practitioners, and\nselecting a model from multiple options for specific applications can be\nchallenging. Conventionally, competing model architectures and training\nprotocols are compared by their classification accuracy on ImageNet. However,\nthis single metric does not fully capture performance nuances critical for\nspecialized tasks. In this work, we conduct an in-depth comparative analysis of\nmodel behaviors beyond ImageNet accuracy, for both ConvNet and Vision\nTransformer architectures, each across supervised and CLIP training paradigms.\nAlthough our selected models have similar ImageNet accuracies and compute\nrequirements, we find that they differ in many other aspects: types of\nmistakes, output calibration, transferability, and feature invariance, among\nothers. This diversity in model characteristics, not captured by traditional\nmetrics, highlights the need for more nuanced analysis when choosing among\ndifferent models. Our code is available at\nhttps://github.com/kirill-vish/Beyond-INet.\n","authors":["Kirill Vishniakov","Zhiqiang Shen","Zhuang Liu"],"pdf_url":"https://arxiv.org/pdf/2311.09215v2.pdf","comment":"Project page: https://kirill-vish.github.io/beyond-imagenet-accuracy/"},{"id":"http://arxiv.org/abs/2401.02794v1","updated":"2024-01-05T13:13:09Z","published":"2024-01-05T13:13:09Z","title":"Subjective and Objective Analysis of Indian Social Media Video Quality","summary":"  We conducted a large-scale subjective study of the perceptual quality of\nUser-Generated Mobile Video Content on a set of mobile-originated videos\nobtained from the Indian social media platform ShareChat. The content viewed by\nvolunteer human subjects under controlled laboratory conditions has the benefit\nof culturally diversifying the existing corpus of User-Generated Content (UGC)\nvideo quality datasets. There is a great need for large and diverse UGC-VQA\ndatasets, given the explosive global growth of the visual internet and social\nmedia platforms. This is particularly true in regard to videos obtained by\nsmartphones, especially in rapidly emerging economies like India. ShareChat\nprovides a safe and cultural community oriented space for users to generate and\nshare content in their preferred Indian languages and dialects. Our subjective\nquality study, which is based on this data, offers a boost of cultural, visual,\nand language diversification to the video quality research community. We expect\nthat this new data resource will also allow for the development of systems that\ncan predict the perceived visual quality of Indian social media videos, to\ncontrol scaling and compression protocols for streaming, provide better user\nrecommendations, and guide content analysis and processing. We demonstrate the\nvalue of the new data resource by conducting a study of leading blind video\nquality models on it, including a new model, called MoEVA, which deploys a\nmixture of experts to predict video quality. Both the new LIVE-ShareChat\ndataset and sample source code for MoEVA are being made freely available to the\nresearch community at https://github.com/sandeep-sm/LIVE-SC\n","authors":["Sandeep Mishra","Mukul Jha","Alan C. Bovik"],"pdf_url":"https://arxiv.org/pdf/2401.02794v1.pdf","comment":"Submitted to the IEEE Transactions on Image Processing"},{"id":"http://arxiv.org/abs/2401.02791v1","updated":"2024-01-05T13:05:02Z","published":"2024-01-05T13:05:02Z","title":"Weakly Semi-supervised Tool Detection in Minimally Invasive Surgery\n  Videos","summary":"  Surgical tool detection is essential for analyzing and evaluating minimally\ninvasive surgery videos. Current approaches are mostly based on supervised\nmethods that require large, fully instance-level labels (i.e., bounding boxes).\nHowever, large image datasets with instance-level labels are often limited\nbecause of the burden of annotation. Thus, surgical tool detection is important\nwhen providing image-level labels instead of instance-level labels since\nimage-level annotations are considerably more time-efficient than\ninstance-level annotations. In this work, we propose to strike a balance\nbetween the extremely costly annotation burden and detection performance. We\nfurther propose a co-occurrence loss, which considers a characteristic that\nsome tool pairs often co-occur together in an image to leverage image-level\nlabels. Encapsulating the knowledge of co-occurrence using the co-occurrence\nloss helps to overcome the difficulty in classification that originates from\nthe fact that some tools have similar shapes and textures. Extensive\nexperiments conducted on the Endovis2018 dataset in various data settings show\nthe effectiveness of our method.\n","authors":["Ryo Fujii","Ryo Hachiuma","Hideo Saito"],"pdf_url":"https://arxiv.org/pdf/2401.02791v1.pdf","comment":"ICASSP 2024"},{"id":"http://arxiv.org/abs/2306.02986v2","updated":"2024-01-05T12:48:31Z","published":"2023-06-05T15:56:30Z","title":"Brain tumor segmentation using synthetic MR images -- A comparison of\n  GANs and diffusion models","summary":"  Large annotated datasets are required for training deep learning models, but\nin medical imaging data sharing is often complicated due to ethics,\nanonymization and data protection legislation. Generative AI models, such as\ngenerative adversarial networks (GANs) and diffusion models, can today produce\nvery realistic synthetic images, and can potentially facilitate data sharing.\nHowever, in order to share synthetic medical images it must first be\ndemonstrated that they can be used for training different networks with\nacceptable performance. Here, we therefore comprehensively evaluate four GANs\n(progressive GAN, StyleGAN 1-3) and a diffusion model for the task of brain\ntumor segmentation (using two segmentation networks, U-Net and a Swin\ntransformer). Our results show that segmentation networks trained on synthetic\nimages reach Dice scores that are 80% - 90% of Dice scores when training with\nreal images, but that memorization of the training images can be a problem for\ndiffusion models if the original dataset is too small. Our conclusion is that\nsharing synthetic medical images is a viable option to sharing real images, but\nthat further work is required. The trained generative models and the generated\nsynthetic images are shared on AIDA data hub\n","authors":["Muhammad Usman Akbar","Måns Larsson","Anders Eklund"],"pdf_url":"https://arxiv.org/pdf/2306.02986v2.pdf","comment":"28 Pages. 5 figures"},{"id":"http://arxiv.org/abs/2311.05197v4","updated":"2024-01-05T12:09:38Z","published":"2023-11-09T08:23:44Z","title":"Deep learning in computed tomography pulmonary angiography imaging: a\n  dual-pronged approach for pulmonary embolism detection","summary":"  The increasing reliance on Computed Tomography Pulmonary Angiography (CTPA)\nfor Pulmonary Embolism (PE) diagnosis presents challenges and a pressing need\nfor improved diagnostic solutions. The primary objective of this study is to\nleverage deep learning techniques to enhance the Computer Assisted Diagnosis\n(CAD) of PE. With this aim, we propose a classifier-guided detection approach\nthat effectively leverages the classifier's probabilistic inference to direct\nthe detection predictions, marking a novel contribution in the domain of\nautomated PE diagnosis. Our classification system includes an Attention-Guided\nConvolutional Neural Network (AG-CNN) that uses local context by employing an\nattention mechanism. This approach emulates a human expert's attention by\nlooking at both global appearances and local lesion regions before making a\ndecision. The classifier demonstrates robust performance on the FUMPE dataset,\nachieving an AUROC of 0.927, sensitivity of 0.862, specificity of 0.879, and an\nF1-score of 0.805 with the Inception-v3 backbone architecture. Moreover, AG-CNN\noutperforms the baseline DenseNet-121 model, achieving an 8.1% AUROC gain.\nWhile previous research has mostly focused on finding PE in the main arteries,\nour use of cutting-edge object detection models and ensembling techniques\ngreatly improves the accuracy of detecting small embolisms in the peripheral\narteries. Finally, our proposed classifier-guided detection approach further\nrefines the detection metrics, contributing new state-of-the-art to the\ncommunity: mAP$_{50}$, sensitivity, and F1-score of 0.846, 0.901, and 0.779,\nrespectively, outperforming the former benchmark with a significant 3.7%\nimprovement in mAP$_{50}$. Our research aims to elevate PE patient care by\nintegrating AI solutions into clinical workflows, highlighting the potential of\nhuman-AI collaboration in medical diagnostics.\n","authors":["Fabiha Bushra","Muhammad E. H. Chowdhury","Rusab Sarmun","Saidul Kabir","Menatalla Said","Sohaib Bassam Zoghoul","Adam Mushtak","Israa Al-Hashimi","Abdulrahman Alqahtani","Anwarul Hasan"],"pdf_url":"https://arxiv.org/pdf/2311.05197v4.pdf","comment":"Published in Expert Systems With Applications"},{"id":"http://arxiv.org/abs/2305.12711v3","updated":"2024-01-05T12:03:37Z","published":"2023-05-22T04:40:30Z","title":"Unsupervised Visible-Infrared Person ReID by Collaborative Learning with\n  Neighbor-Guided Label Refinement","summary":"  Unsupervised learning visible-infrared person re-identification (USL-VI-ReID)\naims at learning modality-invariant features from unlabeled cross-modality\ndataset, which is crucial for practical applications in video surveillance\nsystems. The key to essentially address the USL-VI-ReID task is to solve the\ncross-modality data association problem for further heterogeneous joint\nlearning. To address this issue, we propose a Dual Optimal Transport Label\nAssignment (DOTLA) framework to simultaneously assign the generated labels from\none modality to its counterpart modality. The proposed DOTLA mechanism\nformulates a mutual reinforcement and efficient solution to cross-modality data\nassociation, which could effectively reduce the side-effects of some\ninsufficient and noisy label associations. Besides, we further propose a\ncross-modality neighbor consistency guided label refinement and regularization\nmodule, to eliminate the negative effects brought by the inaccurate supervised\nsignals, under the assumption that the prediction or label distribution of each\nexample should be similar to its nearest neighbors. Extensive experimental\nresults on the public SYSU-MM01 and RegDB datasets demonstrate the\neffectiveness of the proposed method, surpassing existing state-of-the-art\napproach by a large margin of 7.76% mAP on average, which even surpasses some\nsupervised VI-ReID methods.\n","authors":["De Cheng","Xiaojian Huang","Nannan Wang","Lingfeng He","Zhihui Li","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2305.12711v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.10907v3","updated":"2024-01-05T12:03:22Z","published":"2022-09-22T10:29:17Z","title":"DRKF: Distilled Rotated Kernel Fusion for Efficient Rotation Invariant\n  Descriptors in Local Feature Matching","summary":"  The performance of local feature descriptors degrades in the presence of\nlarge rotation variations. To address this issue, we present an efficient\napproach to learning rotation invariant descriptors. Specifically, we propose\nRotated Kernel Fusion (RKF) which imposes rotations on the convolution kernel\nto improve the inherent nature of CNN. Since RKF can be processed by the\nsubsequent re-parameterization, no extra computational costs will be introduced\nin the inference stage. Moreover, we present Multi-oriented Feature Aggregation\n(MOFA) which aggregates features extracted from multiple rotated versions of\nthe input image and can provide auxiliary knowledge for the training of RKF by\nleveraging the distillation strategy. We refer to the distilled RKF model as\nDRKF. Besides the evaluation on a rotation-augmented version of the public\ndataset HPatches, we also contribute a new dataset named DiverseBEV which is\ncollected during the drone's flight and consists of bird's eye view images with\nlarge viewpoint changes and camera rotations. Extensive experiments show that\nour method can outperform other state-of-the-art techniques when exposed to\nlarge rotation variations.\n","authors":["Ranran Huang","Jiancheng Cai","Chao Li","Zhuoyuan Wu","Xinmin Liu","Zhenhua Chai"],"pdf_url":"https://arxiv.org/pdf/2209.10907v3.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2312.08785v2","updated":"2024-01-05T11:42:22Z","published":"2023-12-14T10:08:12Z","title":"Managing the unknown: a survey on Open Set Recognition and tangential\n  areas","summary":"  In real-world scenarios classification models are often required to perform\nrobustly when predicting samples belonging to classes that have not appeared\nduring its training stage. Open Set Recognition addresses this issue by\ndevising models capable of detecting unknown classes from samples arriving\nduring the testing phase, while maintaining a good level of performance in the\nclassification of samples belonging to known classes. This review\ncomprehensively overviews the recent literature related to Open Set\nRecognition, identifying common practices, limitations, and connections of this\nfield with other machine learning research areas, such as continual learning,\nout-of-distribution detection, novelty detection, and uncertainty estimation.\nOur work also uncovers open problems and suggests several research directions\nthat may motivate and articulate future efforts towards more safe Artificial\nIntelligence methods.\n","authors":["Marcos Barcina-Blanco","Jesus L. Lobo","Pablo Garcia-Bringas","Javier Del Ser"],"pdf_url":"https://arxiv.org/pdf/2312.08785v2.pdf","comment":"35 pages, 1 figure, 1 table"},{"id":"http://arxiv.org/abs/2401.02764v1","updated":"2024-01-05T11:36:21Z","published":"2024-01-05T11:36:21Z","title":"Fus-MAE: A cross-attention-based data fusion approach for Masked\n  Autoencoders in remote sensing","summary":"  Self-supervised frameworks for representation learning have recently stirred\nup interest among the remote sensing community, given their potential to\nmitigate the high labeling costs associated with curating large satellite image\ndatasets. In the realm of multimodal data fusion, while the often used\ncontrastive learning methods can help bridging the domain gap between different\nsensor types, they rely on data augmentations techniques that require expertise\nand careful design, especially for multispectral remote sensing data. A\npossible but rather scarcely studied way to circumvent these limitations is to\nuse a masked image modelling based pretraining strategy. In this paper, we\nintroduce Fus-MAE, a self-supervised learning framework based on masked\nautoencoders that uses cross-attention to perform early and feature-level data\nfusion between synthetic aperture radar and multispectral optical data - two\nmodalities with a significant domain gap. Our empirical findings demonstrate\nthat Fus-MAE can effectively compete with contrastive learning strategies\ntailored for SAR-optical data fusion and outperforms other masked-autoencoders\nframeworks trained on a larger corpus.\n","authors":["Hugo Chan-To-Hing","Bharadwaj Veeravalli"],"pdf_url":"https://arxiv.org/pdf/2401.02764v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02759v1","updated":"2024-01-05T11:19:24Z","published":"2024-01-05T11:19:24Z","title":"Detection and Classification of Diabetic Retinopathy using Deep Learning\n  Algorithms for Segmentation to Facilitate Referral Recommendation for Test\n  and Treatment Prediction","summary":"  This research paper addresses the critical challenge of diabetic retinopathy\n(DR), a severe complication of diabetes leading to potential blindness. The\nproposed methodology leverages transfer learning with convolutional neural\nnetworks (CNNs) for automatic DR detection using a single fundus photograph,\ndemonstrating high effectiveness with a quadratic weighted kappa score of\n0.92546 in the APTOS 2019 Blindness Detection Competition. The paper reviews\nexisting literature on DR detection, spanning classical computer vision methods\nto deep learning approaches, particularly focusing on CNNs. It identifies gaps\nin the research, emphasizing the lack of exploration in integrating pretrained\nlarge language models with segmented image inputs for generating\nrecommendations and understanding dynamic interactions within a web application\ncontext.Objectives include developing a comprehensive DR detection methodology,\nexploring model integration, evaluating performance through competition\nranking, contributing significantly to DR detection methodologies, and\nidentifying research gaps.The methodology involves data preprocessing, data\naugmentation, and the use of a U-Net neural network architecture for\nsegmentation. The U-Net model efficiently segments retinal structures,\nincluding blood vessels, hard and soft exudates, haemorrhages, microaneurysms,\nand the optical disc. High evaluation scores in Jaccard, F1, recall, precision,\nand accuracy underscore the model's potential for enhancing diagnostic\ncapabilities in retinal pathology assessment.The outcomes of this research hold\npromise for improving patient outcomes through timely diagnosis and\nintervention in the fight against diabetic retinopathy, marking a significant\ncontribution to the field of medical image analysis.\n","authors":["Manoj S H","Arya A Bosale"],"pdf_url":"https://arxiv.org/pdf/2401.02759v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14404v2","updated":"2024-01-05T11:14:58Z","published":"2023-12-22T03:09:11Z","title":"Cross-Covariate Gait Recognition: A Benchmark","summary":"  Gait datasets are essential for gait research. However, this paper observes\nthat present benchmarks, whether conventional constrained or emerging\nreal-world datasets, fall short regarding covariate diversity. To bridge this\ngap, we undertake an arduous 20-month effort to collect a cross-covariate gait\nrecognition (CCGR) dataset. The CCGR dataset has 970 subjects and about 1.6\nmillion sequences; almost every subject has 33 views and 53 different\ncovariates. Compared to existing datasets, CCGR has both population and\nindividual-level diversity. In addition, the views and covariates are well\nlabeled, enabling the analysis of the effects of different factors. CCGR\nprovides multiple types of gait data, including RGB, parsing, silhouette, and\npose, offering researchers a comprehensive resource for exploration. In order\nto delve deeper into addressing cross-covariate gait recognition, we propose\nparsing-based gait recognition (ParsingGait) by utilizing the newly proposed\nparsing data. We have conducted extensive experiments. Our main results show:\n1) Cross-covariate emerges as a pivotal challenge for practical applications of\ngait recognition. 2) ParsingGait demonstrates remarkable potential for further\nadvancement. 3) Alarmingly, existing SOTA methods achieve less than 43%\naccuracy on the CCGR, highlighting the urgency of exploring cross-covariate\ngait recognition. Link: https://github.com/ShinanZou/CCGR.\n","authors":["Shinan Zou","Chao Fan","Jianbo Xiong","Chuanfu Shen","Shiqi Yu","Jin Tang"],"pdf_url":"https://arxiv.org/pdf/2312.14404v2.pdf","comment":"This paper has been accepted by AAAI2024"},{"id":"http://arxiv.org/abs/2401.02758v1","updated":"2024-01-05T11:14:07Z","published":"2024-01-05T11:14:07Z","title":"Systematic review of image segmentation using complex networks","summary":"  This review presents various image segmentation methods using complex\nnetworks.\n  Image segmentation is one of the important steps in image analysis as it\nhelps analyze and understand complex images. At first, it has been tried to\nclassify complex networks based on how it being used in image segmentation.\n  In computer vision and image processing applications, image segmentation is\nessential for analyzing complex images with irregular shapes, textures, or\noverlapping boundaries. Advanced algorithms make use of machine learning,\nclustering, edge detection, and region-growing techniques. Graph theory\nprinciples combined with community detection-based methods allow for more\nprecise analysis and interpretation of complex images. Hybrid approaches\ncombine multiple techniques for comprehensive, robust segmentation, improving\nresults in computer vision and image processing tasks.\n","authors":["Amin Rezaei","Fatemeh Asadi"],"pdf_url":"https://arxiv.org/pdf/2401.02758v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07180v4","updated":"2024-01-05T11:10:24Z","published":"2023-12-12T11:27:13Z","title":"Context-Aware Iteration Policy Network for Efficient Optical Flow\n  Estimation","summary":"  Existing recurrent optical flow estimation networks are computationally\nexpensive since they use a fixed large number of iterations to update the flow\nfield for each sample. An efficient network should skip iterations when the\nflow improvement is limited. In this paper, we develop a Context-Aware\nIteration Policy Network for efficient optical flow estimation, which\ndetermines the optimal number of iterations per sample. The policy network\nachieves this by learning contextual information to realize whether flow\nimprovement is bottlenecked or minimal. On the one hand, we use iteration\nembedding and historical hidden cell, which include previous iterations\ninformation, to convey how flow has changed from previous iterations. On the\nother hand, we use the incremental loss to make the policy network implicitly\nperceive the magnitude of optical flow improvement in the subsequent iteration.\nFurthermore, the computational complexity in our dynamic network is\ncontrollable, allowing us to satisfy various resource preferences with a single\ntrained model. Our policy network can be easily integrated into\nstate-of-the-art optical flow networks. Extensive experiments show that our\nmethod maintains performance while reducing FLOPs by about 40%/20% for the\nSintel/KITTI datasets.\n","authors":["Ri Cheng","Ruian He","Xuhao Jiang","Shili Zhou","Weimin Tan","Bo Yan"],"pdf_url":"https://arxiv.org/pdf/2312.07180v4.pdf","comment":"2024, Association for the Advancement of Artificial Intelligence"},{"id":"http://arxiv.org/abs/2401.02746v1","updated":"2024-01-05T10:47:42Z","published":"2024-01-05T10:47:42Z","title":"Reading Between the Frames: Multi-Modal Depression Detection in Videos\n  from Non-Verbal Cues","summary":"  Depression, a prominent contributor to global disability, affects a\nsubstantial portion of the population. Efforts to detect depression from social\nmedia texts have been prevalent, yet only a few works explored depression\ndetection from user-generated video content. In this work, we address this\nresearch gap by proposing a simple and flexible multi-modal temporal model\ncapable of discerning non-verbal depression cues from diverse modalities in\nnoisy, real-world videos. We show that, for in-the-wild videos, using\nadditional high-level non-verbal cues is crucial to achieving good performance,\nand we extracted and processed audio speech embeddings, face emotion\nembeddings, face, body and hand landmarks, and gaze and blinking information.\nThrough extensive experiments, we show that our model achieves state-of-the-art\nresults on three key benchmark datasets for depression detection from video by\na substantial margin. Our code is publicly available on GitHub.\n","authors":["David Gimeno-Gómez","Ana-Maria Bucur","Adrian Cosma","Carlos-David Martínez-Hinarejos","Paolo Rosso"],"pdf_url":"https://arxiv.org/pdf/2401.02746v1.pdf","comment":"Accepted at 46th European Conference on Information Retrieval (ECIR\n  2024)"},{"id":"http://arxiv.org/abs/2401.02744v1","updated":"2024-01-05T10:41:55Z","published":"2024-01-05T10:41:55Z","title":"MAMI: Multi-Attentional Mutual-Information for Long Sequence Neuron\n  Captioning","summary":"  Neuron labeling is an approach to visualize the behaviour and respond of a\ncertain neuron to a certain pattern that activates the neuron. Neuron labeling\nextract information about the features captured by certain neurons in a deep\nneural network, one of which uses the encoder-decoder image captioning\napproach. The encoder used can be a pretrained CNN-based model and the decoder\nis an RNN-based model for text generation. Previous work, namely MILAN (Mutual\nInformation-guided Linguistic Annotation of Neuron), has tried to visualize the\nneuron behaviour using modified Show, Attend, and Tell (SAT) model in the\nencoder, and LSTM added with Bahdanau attention in the decoder. MILAN can show\ngreat result on short sequence neuron captioning, but it does not show great\nresult on long sequence neuron captioning, so in this work, we would like to\nimprove the performance of MILAN even more by utilizing different kind of\nattention mechanism and additionally adding several attention result into one,\nin order to combine all the advantages from several attention mechanism. Using\nour compound dataset, we obtained higher BLEU and F1-Score on our proposed\nmodel, achieving 17.742 and 0.4811 respectively. At some point where the model\nconverges at the peak, our model obtained BLEU of 21.2262 and BERTScore\nF1-Score of 0.4870.\n","authors":["Alfirsa Damasyifa Fauzulhaq","Wahyu Parwitayasa","Joseph Ananda Sugihdharma","M. Fadli Ridhani","Novanto Yudistira"],"pdf_url":"https://arxiv.org/pdf/2401.02744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.19787v2","updated":"2024-01-05T10:29:59Z","published":"2023-05-31T12:27:58Z","title":"DeepMerge: Deep-Learning-Based Region-Merging for Image Segmentation","summary":"  Image segmentation aims to partition an image according to the objects in the\nscene and is a fundamental step in analysing very high spatial-resolution (VHR)\nremote sensing imagery. Current methods struggle to effectively consider land\nobjects with diverse shapes and sizes. Additionally, the determination of\nsegmentation scale parameters frequently adheres to a static and empirical\ndoctrine, posing limitations on the segmentation of large-scale remote sensing\nimages and yielding algorithms with limited interpretability. To address the\nabove challenges, we propose a deep-learning-based region merging method dubbed\nDeepMerge to handle the segmentation of complete objects in large VHR images by\nintegrating deep learning and region adjacency graph (RAG). This is the first\nmethod to use deep learning to learn the similarity and merge similar adjacent\nsuper-pixels in RAG. We propose a modified binary tree sampling method to\ngenerate shift-scale data, serving as inputs for transformer-based deep\nlearning networks, a shift-scale attention with 3-Dimension relative position\nembedding to learn features across scales, and an embedding to fuse learned\nfeatures with hand-crafted features. DeepMerge can achieve high segmentation\naccuracy in a supervised manner from large-scale remotely sensed images and\nprovides an interpretable optimal scale parameter, which is validated using a\nremote sensing image of 0.55 m resolution covering an area of 5,660 km^2. The\nexperimental results show that DeepMerge achieves the highest F value (0.9550)\nand the lowest total error TE (0.0895), correctly segmenting objects of\ndifferent sizes and outperforming all competing segmentation methods.\n","authors":["Xianwei Lv","Claudio Persello","Wangbin Li","Xiao Huang","Dongping Ming","Alfred Stein"],"pdf_url":"https://arxiv.org/pdf/2305.19787v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.17163v3","updated":"2024-01-05T10:01:14Z","published":"2023-12-28T17:52:09Z","title":"FENet: Focusing Enhanced Network for Lane Detection","summary":"  Inspired by human driving focus, this research pioneers networks augmented\nwith Focusing Sampling, Partial Field of View Evaluation, Enhanced FPN\narchitecture and Directional IoU Loss - targeted innovations addressing\nobstacles to precise lane detection for autonomous driving. Experiments\ndemonstrate our Focusing Sampling strategy, emphasizing vital distant details\nunlike uniform approaches, significantly boosts both benchmark and practical\ncurved/distant lane recognition accuracy essential for safety. While FENetV1\nachieves state-of-the-art conventional metric performance via enhancements\nisolating perspective-aware contexts mimicking driver vision, FENetV2 proves\nmost reliable on the proposed Partial Field analysis. Hence we specifically\nrecommend V2 for practical lane navigation despite fractional degradation on\nstandard entire-image measures. Future directions include collecting on-road\ndata and integrating complementary dual frameworks to further breakthroughs\nguided by human perception principles. The Code is available at\nhttps://github.com/HanyangZhong/FENet.\n","authors":["Liman Wang","Hanyang Zhong"],"pdf_url":"https://arxiv.org/pdf/2312.17163v3.pdf","comment":"12 pages including appendix. The Code is available at\n  https://github.com/HanyangZhong/FENet"},{"id":"http://arxiv.org/abs/2401.02727v1","updated":"2024-01-05T09:46:42Z","published":"2024-01-05T09:46:42Z","title":"Enhancing targeted transferability via feature space fine-tuning","summary":"  Adversarial examples (AEs) have been extensively studied due to their\npotential for privacy protection and inspiring robust neural networks. However,\nmaking a targeted AE transferable across unknown models remains challenging. In\nthis paper, to alleviate the overfitting dilemma common in an AE crafted by\nexisting simple iterative attacks, we propose fine-tuning it in the feature\nspace. Specifically, starting with an AE generated by a baseline attack, we\nencourage the features that contribute to the target class and discourage the\nfeatures that contribute to the original class in a middle layer of the source\nmodel. Extensive experiments demonstrate that only a few iterations of\nfine-tuning can boost existing attacks in terms of targeted transferability\nnontrivially and universally. Our results also verify that the simple iterative\nattacks can yield comparable or even better transferability than the\nresource-intensive methods, which rely on training target-specific classifiers\nor generators with additional data. The code is available at:\ngithub.com/zengh5/TA_feature_FT.\n","authors":["Hui Zeng","Biwei Chen","Anjie Peng"],"pdf_url":"https://arxiv.org/pdf/2401.02727v1.pdf","comment":"9 pages, 10 figures, accepted by 2024ICASSP"},{"id":"http://arxiv.org/abs/2401.02723v1","updated":"2024-01-05T09:36:42Z","published":"2024-01-05T09:36:42Z","title":"Predicting Traffic Flow with Federated Learning and Graph Neural with\n  Asynchronous Computations Network","summary":"  Real-time traffic flow prediction holds significant importance within the\ndomain of Intelligent Transportation Systems (ITS). The task of achieving a\nbalance between prediction precision and computational efficiency presents a\nsignificant challenge. In this article, we present a novel deep-learning method\ncalled Federated Learning and Asynchronous Graph Convolutional Network\n(FLAGCN). Our framework incorporates the principles of asynchronous graph\nconvolutional networks with federated learning to enhance the accuracy and\nefficiency of real-time traffic flow prediction. The FLAGCN model employs a\nspatial-temporal graph convolution technique to asynchronously address\nspatio-temporal dependencies within traffic data effectively. To efficiently\nhandle the computational requirements associated with this deep learning model,\nthis study used a graph federated learning technique known as GraphFL. This\napproach is designed to facilitate the training process. The experimental\nresults obtained from conducting tests on two distinct traffic datasets\ndemonstrate that the utilization of FLAGCN leads to the optimization of both\ntraining and inference durations while maintaining a high level of prediction\naccuracy. FLAGCN outperforms existing models with significant improvements by\nachieving up to approximately 6.85% reduction in RMSE, 20.45% reduction in\nMAPE, compared to the best-performing existing models.\n","authors":["Muhammad Yaqub","Shahzad Ahmad","Malik Abdul Manan","Imran Shabir Chuhan"],"pdf_url":"https://arxiv.org/pdf/2401.02723v1.pdf","comment":"15 pages, 7 Figures, 1 Table"},{"id":"http://arxiv.org/abs/2312.17290v2","updated":"2024-01-05T09:36:36Z","published":"2023-12-28T10:30:54Z","title":"Predicting Parkinson's disease evolution using deep learning","summary":"  Parkinson's disease is a neurological condition that occurs in nearly 1% of\nthe world's population. The disease is manifested by a drop in dopamine\nproduction, symptoms are cognitive and behavioural and include a wide range of\npersonality changes, depressive disorders, memory problems, and emotional\ndysregulation, which can occur as the disease progresses. Early diagnosis and\naccurate staging of the disease are essential to apply the appropriate\ntherapeutic approaches to slow cognitive and motor decline.\n  Currently, there is not a single blood test or biomarker available to\ndiagnose Parkinson's disease. Magnetic resonance imaging has been used for the\npast three decades to diagnose and distinguish between PD and other\nneurological conditions. However, in recent years new possibilities have\narisen: several AI algorithms have been developed to increase the precision and\naccuracy of differential diagnosis of PD at an early stage.\n  To our knowledge, no AI tools have been designed to identify the stage of\nprogression. This paper aims to fill this gap. Using the \"Parkinson's\nProgression Markers Initiative\" dataset, which reports the patient's MRI and an\nindication of the disease stage, we developed a model to identify the level of\nprogression. The images and the associated scores were used for training and\nassessing different deep-learning models. Our analysis distinguished four\ndistinct disease progression levels based on a standard scale (Hoehn and Yah\nscale). The final architecture consists of the cascading of a 3DCNN network,\nadopted to reduce and extract the spatial characteristics of the RMI for\nefficient training of the successive LSTM layers, aiming at modelling the\ntemporal dependencies among the data.\n  Our results show that the proposed 3DCNN + LSTM model achieves\nstate-of-the-art results by classifying the elements with 91.90\\% as macro\naveraged OVR AUC on four classes\n","authors":["Maria Frasca","Davide La Torre","Gabriella Pravettoni","Ilaria Cutica"],"pdf_url":"https://arxiv.org/pdf/2312.17290v2.pdf","comment":"27 pages, 11 figures"},{"id":"http://arxiv.org/abs/2401.02719v1","updated":"2024-01-05T09:26:35Z","published":"2024-01-05T09:26:35Z","title":"Learning Image Demoireing from Unpaired Real Data","summary":"  This paper focuses on addressing the issue of image demoireing. Unlike the\nlarge volume of existing studies that rely on learning from paired real data,\nwe attempt to learn a demoireing model from unpaired real data, i.e., moire\nimages associated with irrelevant clean images. The proposed method, referred\nto as Unpaired Demoireing (UnDeM), synthesizes pseudo moire images from\nunpaired datasets, generating pairs with clean images for training demoireing\nmodels. To achieve this, we divide real moire images into patches and group\nthem in compliance with their moire complexity. We introduce a novel moire\ngeneration framework to synthesize moire images with diverse moire features,\nresembling real moire patches, and details akin to real moire-free images.\nAdditionally, we introduce an adaptive denoise method to eliminate the\nlow-quality pseudo moire images that adversely impact the learning of\ndemoireing models. We conduct extensive experiments on the commonly-used FHDMi\nand UHDM datasets. Results manifest that our UnDeM performs better than\nexisting methods when using existing demoireing models such as MBCNN and\nESDNet-L. Code: https://github.com/zysxmu/UnDeM\n","authors":["Yunshan Zhong","Yuyao Zhou","Yuxin Zhang","Fei Chao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2401.02719v1.pdf","comment":"AAAI2024"},{"id":"http://arxiv.org/abs/2401.02717v1","updated":"2024-01-05T09:21:45Z","published":"2024-01-05T09:21:45Z","title":"Complementary Information Mutual Learning for Multimodality Medical\n  Image Segmentation","summary":"  Radiologists must utilize multiple modal images for tumor segmentation and\ndiagnosis due to the limitations of medical imaging and the diversity of tumor\nsignals. This leads to the development of multimodal learning in segmentation.\nHowever, the redundancy among modalities creates challenges for existing\nsubtraction-based joint learning methods, such as misjudging the importance of\nmodalities, ignoring specific modal information, and increasing cognitive load.\nThese thorny issues ultimately decrease segmentation accuracy and increase the\nrisk of overfitting. This paper presents the complementary information mutual\nlearning (CIML) framework, which can mathematically model and address the\nnegative impact of inter-modal redundant information. CIML adopts the idea of\naddition and removes inter-modal redundant information through inductive\nbias-driven task decomposition and message passing-based redundancy filtering.\nCIML first decomposes the multimodal segmentation task into multiple subtasks\nbased on expert prior knowledge, minimizing the information dependence between\nmodalities. Furthermore, CIML introduces a scheme in which each modality can\nextract information from other modalities additively through message passing.\nTo achieve non-redundancy of extracted information, the redundant filtering is\ntransformed into complementary information learning inspired by the variational\ninformation bottleneck. The complementary information learning procedure can be\nefficiently solved by variational inference and cross-modal spatial attention.\nNumerical results from the verification task and standard benchmarks indicate\nthat CIML efficiently removes redundant information between modalities,\noutperforming SOTA methods regarding validation accuracy and segmentation\neffect.\n","authors":["Chuyun Shen","Wenhao Li","Haoqing Chen","Xiaoling Wang","Fengping Zhu","Yuxin Li","Xiangfeng Wang","Bo Jin"],"pdf_url":"https://arxiv.org/pdf/2401.02717v1.pdf","comment":"35 pages, 18 figures"},{"id":"http://arxiv.org/abs/2312.11153v2","updated":"2024-01-05T08:41:06Z","published":"2023-12-18T12:46:35Z","title":"Research on Multilingual Natural Scene Text Detection Algorithm","summary":"  Natural scene text detection is a significant challenge in computer vision,\nwith tremendous potential applications in multilingual, diverse, and complex\ntext scenarios. We propose a multilingual text detection model to address the\nissues of low accuracy and high difficulty in detecting multilingual text in\nnatural scenes. In response to the challenges posed by multilingual text images\nwith multiple character sets and various font styles, we introduce the SFM Swin\nTransformer feature extraction network to enhance the model's robustness in\ndetecting characters and fonts across different languages. Dealing with the\nconsiderable variation in text scales and complex arrangements in natural scene\ntext images, we present the AS-HRFPN feature fusion network by incorporating an\nAdaptive Spatial Feature Fusion module and a Spatial Pyramid Pooling module.\nThe feature fusion network improvements enhance the model's ability to detect\ntext sizes and orientations. Addressing diverse backgrounds and font variations\nin multilingual scene text images is a challenge for existing methods. Limited\nlocal receptive fields hinder detection performance. To overcome this, we\npropose a Global Semantic Segmentation Branch, extracting and preserving global\nfeatures for more effective text detection, aligning with the need for\ncomprehensive information. In this study, we collected and built a real-world\nmultilingual natural scene text image dataset and conducted comprehensive\nexperiments and analyses. The experimental results demonstrate that the\nproposed algorithm achieves an F-measure of 85.02\\%, which is 4.71\\% higher\nthan the baseline model. We also conducted extensive cross-dataset validation\non MSRA-TD500, ICDAR2017MLT, and ICDAR2015 datasets to verify the generality of\nour approach. The code and dataset can be found at\nhttps://github.com/wangmelon/CEMLT.\n","authors":["Tao Wang"],"pdf_url":"https://arxiv.org/pdf/2312.11153v2.pdf","comment":"Sorry, we discovered certain mistake and asked that the current\n  version be removed in order to perform a thorough reanalysis"},{"id":"http://arxiv.org/abs/2401.02702v1","updated":"2024-01-05T08:10:49Z","published":"2024-01-05T08:10:49Z","title":"VoxelNextFusion: A Simple, Unified and Effective Voxel Fusion Framework\n  for Multi-Modal 3D Object Detection","summary":"  LiDAR-camera fusion can enhance the performance of 3D object detection by\nutilizing complementary information between depth-aware LiDAR points and\nsemantically rich images. Existing voxel-based methods face significant\nchallenges when fusing sparse voxel features with dense image features in a\none-to-one manner, resulting in the loss of the advantages of images, including\nsemantic and continuity information, leading to sub-optimal detection\nperformance, especially at long distances. In this paper, we present\nVoxelNextFusion, a multi-modal 3D object detection framework specifically\ndesigned for voxel-based methods, which effectively bridges the gap between\nsparse point clouds and dense images. In particular, we propose a voxel-based\nimage pipeline that involves projecting point clouds onto images to obtain both\npixel- and patch-level features. These features are then fused using a\nself-attention to obtain a combined representation. Moreover, to address the\nissue of background features present in patches, we propose a feature\nimportance module that effectively distinguishes between foreground and\nbackground features, thus minimizing the impact of the background features.\nExtensive experiments were conducted on the widely used KITTI and nuScenes 3D\nobject detection benchmarks. Notably, our VoxelNextFusion achieved around\n+3.20% in AP@0.7 improvement for car detection in hard level compared to the\nVoxel R-CNN baseline on the KITTI test dataset\n","authors":["Ziying Song","Guoxin Zhang","Jun Xie","Lin Liu","Caiyan Jia","Shaoqing Xu","Zhepeng Wang"],"pdf_url":"https://arxiv.org/pdf/2401.02702v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02695v1","updated":"2024-01-05T08:05:07Z","published":"2024-01-05T08:05:07Z","title":"VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language\n  Model","summary":"  In the realm of household robotics, the Zero-Shot Object Navigation (ZSON)\ntask empowers agents to adeptly traverse unfamiliar environments and locate\nobjects from novel categories without prior explicit training. This paper\nintroduces VoroNav, a novel semantic exploration framework that proposes the\nReduced Voronoi Graph to extract exploratory paths and planning nodes from a\nsemantic map constructed in real time. By harnessing topological and semantic\ninformation, VoroNav designs text-based descriptions of paths and images that\nare readily interpretable by a large language model (LLM). Our approach\npresents a synergy of path and farsight descriptions to represent the\nenvironmental context, enabling the LLM to apply commonsense reasoning to\nascertain the optimal waypoints for navigation. Extensive evaluation on the\nHM3D and HSSD datasets validates that VoroNav surpasses existing ZSON\nbenchmarks in both success rates and exploration efficiency (+2.8% Success and\n+3.7% SPL on HM3D, +2.6% Success and +3.8% SPL on HSSD). Additionally\nintroduced metrics that evaluate obstacle avoidance proficiency and perceptual\nefficiency further corroborate the enhancements achieved by our method in ZSON\nplanning.\n","authors":["Pengying Wu","Yao Mu","Bingxian Wu","Yi Hou","Ji Ma","Shanghang Zhang","Chang Liu"],"pdf_url":"https://arxiv.org/pdf/2401.02695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02687v1","updated":"2024-01-05T07:37:51Z","published":"2024-01-05T07:37:51Z","title":"PAHD: Perception-Action based Human Decision Making using Explainable\n  Graph Neural Networks on SAR Images","summary":"  Synthetic Aperture Radar (SAR) images are commonly utilized in military\napplications for automatic target recognition (ATR). Machine learning (ML)\nmethods, such as Convolutional Neural Networks (CNN) and Graph Neural Networks\n(GNN), are frequently used to identify ground-based objects, including battle\ntanks, personnel carriers, and missile launchers. Determining the vehicle\nclass, such as the BRDM2 tank, BMP2 tank, BTR60 tank, and BTR70 tank, is\ncrucial, as it can help determine whether the target object is an ally or an\nenemy. While the ML algorithm provides feedback on the recognized target, the\nfinal decision is left to the commanding officers. Therefore, providing\ndetailed information alongside the identified target can significantly impact\ntheir actions. This detailed information includes the SAR image features that\ncontributed to the classification, the classification confidence, and the\nprobability of the identified object being classified as a different object\ntype or class. We propose a GNN-based ATR framework that provides the final\nclassified class and outputs the detailed information mentioned above. This is\nthe first study to provide a detailed analysis of the classification class,\nmaking final decisions more straightforward. Moreover, our GNN framework\nachieves an overall accuracy of 99.2\\% when evaluated on the MSTAR dataset,\nimproving over previous state-of-the-art GNN methods.\n","authors":["Sasindu Wijeratne","Bingyi Zhang","Rajgopal Kannan","Viktor Prasanna","Carl Busart"],"pdf_url":"https://arxiv.org/pdf/2401.02687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02677v1","updated":"2024-01-05T07:21:46Z","published":"2024-01-05T07:21:46Z","title":"Progressive Knowledge Distillation Of Stable Diffusion XL Using Layer\n  Level Loss","summary":"  Stable Diffusion XL (SDXL) has become the best open source text-to-image\nmodel (T2I) for its versatility and top-notch image quality. Efficiently\naddressing the computational demands of SDXL models is crucial for wider reach\nand applicability. In this work, we introduce two scaled-down variants, Segmind\nStable Diffusion (SSD-1B) and Segmind-Vega, with 1.3B and 0.74B parameter\nUNets, respectively, achieved through progressive removal using layer-level\nlosses focusing on reducing the model size while preserving generative quality.\nWe release these models weights at https://hf.co/Segmind. Our methodology\ninvolves the elimination of residual networks and transformer blocks from the\nU-Net structure of SDXL, resulting in significant reductions in parameters, and\nlatency. Our compact models effectively emulate the original SDXL by\ncapitalizing on transferred knowledge, achieving competitive results against\nlarger multi-billion parameter SDXL. Our work underscores the efficacy of\nknowledge distillation coupled with layer-level losses in reducing model size\nwhile preserving the high-quality generative capabilities of SDXL, thus\nfacilitating more accessible deployment in resource-constrained environments.\n","authors":["Yatharth Gupta","Vishnu V. Jaddipal","Harish Prabhala","Sayak Paul","Patrick Von Platen"],"pdf_url":"https://arxiv.org/pdf/2401.02677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01386v2","updated":"2024-01-05T07:12:41Z","published":"2024-01-01T19:58:36Z","title":"Tissue Artifact Segmentation and Severity Analysis for Automated\n  Diagnosis Using Whole Slide Images","summary":"  Traditionally, pathological analysis and diagnosis are performed by manually\neyeballing glass slide specimens under a microscope by an expert. The whole\nslide image is the digital specimen produced from the glass slide. Whole slide\nimage enabled specimens to be observed on a computer screen and led to\ncomputational pathology where computer vision and artificial intelligence are\nutilized for automated analysis and diagnosis. With the current computational\nadvancement, the entire whole slide image can be analyzed autonomously without\nhuman supervision. However, the analysis could fail or lead to wrong diagnosis\nif the whole slide image is affected by tissue artifacts such as tissue fold or\nair bubbles depending on the severity. Existing artifact detection methods rely\non experts for severity assessment to eliminate artifact affected regions from\nthe analysis. This process is time consuming, exhausting and undermines the\ngoal of automated analysis or removal of artifacts without evaluating their\nseverity, which could result in the loss of diagnostically important data.\nTherefore, it is necessary to detect artifacts and then assess their severity\nautomatically. In this paper, we propose a system that incorporates severity\nevaluation with artifact detection utilizing convolutional neural networks. The\nproposed system uses DoubleUNet to segment artifacts and an ensemble network of\nsix fine tuned convolutional neural network models to determine severity. This\nmethod outperformed current state of the art in accuracy by 9 percent for\nartifact segmentation and achieved a strong correlation of 97 percent with the\nevaluation of pathologists for severity assessment. The robustness of the\nsystem was demonstrated using our proposed heterogeneous dataset and practical\nusability was ensured by integrating it with an automated analysis system.\n","authors":["Galib Muhammad Shahriar Himel"],"pdf_url":"https://arxiv.org/pdf/2401.01386v2.pdf","comment":"60 pages, 21 figures, 16 tables"},{"id":"http://arxiv.org/abs/2303.04940v4","updated":"2024-01-05T07:04:09Z","published":"2023-03-08T23:23:44Z","title":"Non-aligned supervision for Real Image Dehazing","summary":"  Removing haze from real-world images is challenging due to unpredictable\nweather conditions, resulting in the misalignment of hazy and clear image\npairs. In this paper, we propose an innovative dehazing framework that operates\nunder non-aligned supervision. This framework is grounded in the atmospheric\nscattering model, and consists of three interconnected networks: dehazing,\nairlight, and transmission networks. In particular, we explore a non-alignment\nscenario that a clear reference image, unaligned with the input hazy image, is\nutilized to supervise the dehazing network. To implement this, we present a\nmulti-scale reference loss that compares the feature representations between\nthe referred image and the dehazed output. Our scenario makes it easier to\ncollect hazy/clear image pairs in real-world environments, even under\nconditions of misalignment and shift views. To showcase the effectiveness of\nour scenario, we have collected a new hazy dataset including 415 image pairs\ncaptured by mobile Phone in both rural and urban areas, called \"Phone-Hazy\".\nFurthermore, we introduce a self-attention network based on mean and variance\nfor modeling real infinite airlight, using the dark channel prior as positional\nguidance. Additionally, a channel attention network is employed to estimate the\nthree-channel transmission. Experimental results demonstrate the superior\nperformance of our framework over existing state-of-the-art techniques in the\nreal-world image dehazing task. Phone-Hazy and code will be available at\nhttps://fanjunkai1.github.io/projectpage/NSDNet/index.html.\n","authors":["Junkai Fan","Fei Guo","Jianjun Qian","Xiang Li","Jun Li","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2303.04940v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.04512v2","updated":"2024-01-05T06:51:15Z","published":"2023-06-07T15:25:27Z","title":"Cross-attention learning enables real-time nonuniform rotational\n  distortion correction in OCT","summary":"  Nonuniform rotational distortion (NURD) correction is vital for endoscopic\noptical coherence tomography (OCT) imaging and its functional extensions, such\nas angiography and elastography. Current NURD correction methods require\ntime-consuming feature tracking or cross-correlation calculations and thus\nsacrifice temporal resolution. Here we propose a cross-attention learning\nmethod for the NURD correction in OCT. Our method is inspired by the recent\nsuccess of the self-attention mechanism in natural language processing and\ncomputer vision. By leveraging its ability to model long-range dependencies, we\ncan directly obtain the correlation between OCT A-lines at any distance, thus\naccelerating the NURD correction. We develop an end-to-end stacked\ncross-attention network and design three types of optimization constraints. We\ncompare our method with two traditional feature-based methods and a CNN-based\nmethod, on two publicly-available endoscopic OCT datasets and a private dataset\ncollected on our home-built endoscopic OCT system. Our method achieved a\n$\\sim3\\times$ speedup to real time ($26\\pm 3$ fps), and superior correction\nperformance.\n","authors":["Haoran Zhang","Jianlong Yang","Jingqian Zhang","Shiqing Zhao","Aili Zhang"],"pdf_url":"https://arxiv.org/pdf/2306.04512v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.16742v2","updated":"2024-01-05T06:47:45Z","published":"2023-08-31T14:00:47Z","title":"Unsupervised CT Metal Artifact Reduction by Plugging Diffusion Priors in\n  Dual Domains","summary":"  During the process of computed tomography (CT), metallic implants often cause\ndisruptive artifacts in the reconstructed images, impeding accurate diagnosis.\nSeveral supervised deep learning-based approaches have been proposed for\nreducing metal artifacts (MAR). However, these methods heavily rely on training\nwith simulated data, as obtaining paired metal artifact CT and clean CT data in\nclinical settings is challenging. This limitation can lead to decreased\nperformance when applying these methods in clinical practice. Existing\nunsupervised MAR methods, whether based on learning or not, typically operate\nwithin a single domain, either in the image domain or the sinogram domain. In\nthis paper, we propose an unsupervised MAR method based on the diffusion model,\na generative model with a high capacity to represent data distributions.\nSpecifically, we first train a diffusion model using CT images without metal\nartifacts. Subsequently, we iteratively utilize the priors embedded within the\npre-trained diffusion model in both the sinogram and image domains to restore\nthe degraded portions caused by metal artifacts. This dual-domain processing\nempowers our approach to outperform existing unsupervised MAR methods,\nincluding another MAR method based on the diffusion model, which we have\nqualitatively and quantitatively validated using synthetic datasets. Moreover,\nour method demonstrates superior visual results compared to both supervised and\nunsupervised methods on clinical datasets.\n","authors":["Xuan Liu","Yaoqin Xie","Songhui Diao","Shan Tan","Xiaokun Liang"],"pdf_url":"https://arxiv.org/pdf/2308.16742v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02656v1","updated":"2024-01-05T06:24:41Z","published":"2024-01-05T06:24:41Z","title":"GTA: Guided Transfer of Spatial Attention from Object-Centric\n  Representations","summary":"  Utilizing well-trained representations in transfer learning often results in\nsuperior performance and faster convergence compared to training from scratch.\nHowever, even if such good representations are transferred, a model can easily\noverfit the limited training dataset and lose the valuable properties of the\ntransferred representations. This phenomenon is more severe in ViT due to its\nlow inductive bias. Through experimental analysis using attention maps in ViT,\nwe observe that the rich representations deteriorate when trained on a small\ndataset. Motivated by this finding, we propose a novel and simple\nregularization method for ViT called Guided Transfer of spatial Attention\n(GTA). Our proposed method regularizes the self-attention maps between the\nsource and target models. A target model can fully exploit the knowledge\nrelated to object localization properties through this explicit regularization.\nOur experimental results show that the proposed GTA consistently improves the\naccuracy across five benchmark datasets especially when the number of training\ndata is small.\n","authors":["SeokHyun Seo","Jinwoo Hong","JungWoo Chae","Kyungyul Kim","Sangheum Hwang"],"pdf_url":"https://arxiv.org/pdf/2401.02656v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.07728v3","updated":"2024-01-05T06:22:32Z","published":"2023-08-15T12:08:43Z","title":"Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability","summary":"  Fine-tuning pre-trained neural network models has become a widely adopted\napproach across various domains. However, it can lead to the distortion of\npre-trained feature extractors that already possess strong generalization\ncapabilities. Mitigating feature distortion during adaptation to new target\ndomains is crucial. Recent studies have shown promising results in handling\nfeature distortion by aligning the head layer on in-distribution datasets\nbefore performing fine-tuning. Nonetheless, a significant limitation arises\nfrom the treatment of batch normalization layers during fine-tuning, leading to\nsuboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning\n(DAFT), a novel approach that incorporates batch normalization conversion and\nthe integration of linear probing and fine-tuning. Our batch normalization\nconversion method effectively mitigates feature distortion by reducing\nmodifications to the neural network during fine-tuning. Additionally, we\nintroduce the integration of linear probing and fine-tuning to optimize the\nhead layer with gradual adaptation of the feature extractor. By leveraging\nbatch normalization layers and integrating linear probing and fine-tuning, our\nDAFT significantly mitigates feature distortion and achieves improved model\nperformance on both in-distribution and out-of-distribution datasets. Extensive\nexperiments demonstrate that our method outperforms other baseline methods,\ndemonstrating its effectiveness in not only improving performance but also\nmitigating feature distortion.\n","authors":["Seokhyeon Ha","Sunbeom Jung","Jungwoo Lee"],"pdf_url":"https://arxiv.org/pdf/2308.07728v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02361v2","updated":"2024-01-05T06:21:19Z","published":"2024-01-04T17:00:49Z","title":"An Open and Comprehensive Pipeline for Unified Object Grounding and\n  Detection","summary":"  Grounding-DINO is a state-of-the-art open-set detection model that tackles\nmultiple vision tasks including Open-Vocabulary Detection (OVD), Phrase\nGrounding (PG), and Referring Expression Comprehension (REC). Its effectiveness\nhas led to its widespread adoption as a mainstream architecture for various\ndownstream applications. However, despite its significance, the original\nGrounding-DINO model lacks comprehensive public technical details due to the\nunavailability of its training code. To bridge this gap, we present\nMM-Grounding-DINO, an open-source, comprehensive, and user-friendly baseline,\nwhich is built with the MMDetection toolbox. It adopts abundant vision datasets\nfor pre-training and various detection and grounding datasets for fine-tuning.\nWe give a comprehensive analysis of each reported result and detailed settings\nfor reproduction. The extensive experiments on the benchmarks mentioned\ndemonstrate that our MM-Grounding-DINO-Tiny outperforms the Grounding-DINO-Tiny\nbaseline. We release all our models to the research community. Codes and\ntrained models are released at\nhttps://github.com/open-mmlab/mmdetection/tree/main/configs/mm_grounding_dino.\n","authors":["Xiangyu Zhao","Yicheng Chen","Shilin Xu","Xiangtai Li","Xinjiang Wang","Yining Li","Haian Huang"],"pdf_url":"https://arxiv.org/pdf/2401.02361v2.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2311.11700v3","updated":"2024-01-05T06:18:04Z","published":"2023-11-20T12:08:23Z","title":"GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting","summary":"  In this paper, we introduce $\\textbf{GS-SLAM}$ that first utilizes 3D\nGaussian representation in the Simultaneous Localization and Mapping (SLAM)\nsystem. It facilitates a better balance between efficiency and accuracy.\nCompared to recent SLAM methods employing neural implicit representations, our\nmethod utilizes a real-time differentiable splatting rendering pipeline that\noffers significant speedup to map optimization and RGB-D re-rendering.\nSpecifically, we propose an adaptive expansion strategy that adds new or\ndeletes noisy 3D Gaussian in order to efficiently reconstruct new observed\nscene geometry and improve the mapping of previously observed areas. This\nstrategy is essential to extend 3D Gaussian representation to reconstruct the\nwhole scene rather than synthesize a static object in existing methods.\nMoreover, in the pose tracking process, an effective coarse-to-fine technique\nis designed to select reliable 3D Gaussian representations to optimize camera\npose, resulting in runtime reduction and robust estimation. Our method achieves\ncompetitive performance compared with existing state-of-the-art real-time\nmethods on the Replica, TUM-RGBD datasets. The source code will be released\nsoon.\n","authors":["Chi Yan","Delin Qu","Dong Wang","Dan Xu","Zhigang Wang","Bin Zhao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2311.11700v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02651v1","updated":"2024-01-05T05:58:22Z","published":"2024-01-05T05:58:22Z","title":"Benchmarking PathCLIP for Pathology Image Analysis","summary":"  Accurate image classification and retrieval are of importance for clinical\ndiagnosis and treatment decision-making. The recent contrastive language-image\npretraining (CLIP) model has shown remarkable proficiency in understanding\nnatural images. Drawing inspiration from CLIP, PathCLIP is specifically\ndesigned for pathology image analysis, utilizing over 200,000 image and text\npairs in training. While the performance the PathCLIP is impressive, its\nrobustness under a wide range of image corruptions remains unknown. Therefore,\nwe conduct an extensive evaluation to analyze the performance of PathCLIP on\nvarious corrupted images from the datasets of Osteosarcoma and WSSS4LUAD. In\nour experiments, we introduce seven corruption types including brightness,\ncontrast, Gaussian blur, resolution, saturation, hue, and markup at four\nseverity levels. Through experiments, we find that PathCLIP is relatively\nrobustness to image corruptions and surpasses OpenAI-CLIP and PLIP in zero-shot\nclassification. Among the seven corruptions, blur and resolution can cause\nserver performance degradation of the PathCLIP. This indicates that ensuring\nthe quality of images is crucial before conducting a clinical test.\nAdditionally, we assess the robustness of PathCLIP in the task of image-image\nretrieval, revealing that PathCLIP performs less effectively than PLIP on\nOsteosarcoma but performs better on WSSS4LUAD under diverse corruptions.\nOverall, PathCLIP presents impressive zero-shot classification and retrieval\nperformance for pathology images, but appropriate care needs to be taken when\nusing it. We hope this study provides a qualitative impression of PathCLIP and\nhelps understand its differences from other CLIP models.\n","authors":["Sunyi Zheng","Xiaonan Cui","Yuxuan Sun","Jingxiong Li","Honglin Li","Yunlong Zhang","Pingyi Chen","Xueping Jing","Zhaoxiang Ye","Lin Yang"],"pdf_url":"https://arxiv.org/pdf/2401.02651v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02649v1","updated":"2024-01-05T05:40:59Z","published":"2024-01-05T05:40:59Z","title":"Enhancing 3D-Air Signature by Pen Tip Tail Trajectory Awareness: Dataset\n  and Featuring by Novel Spatio-temporal CNN","summary":"  This work proposes a novel process of using pen tip and tail 3D trajectory\nfor air signature. To acquire the trajectories we developed a new pen tool and\na stereo camera was used. We proposed SliT-CNN, a novel 2D spatial-temporal\nconvolutional neural network (CNN) for better featuring of the air signature.\nIn addition, we also collected an air signature dataset from $45$ signers.\nSkilled forgery signatures per user are also collected. A detailed benchmarking\nof the proposed dataset using existing techniques and proposed CNN on existing\nand proposed dataset exhibit the effectiveness of our methodology.\n","authors":["Saurabh Atreya","Maheswar Bora","Aritra Mukherjee","Abhijit Das"],"pdf_url":"https://arxiv.org/pdf/2401.02649v1.pdf","comment":"Accepted and presented in IJCB 2023"},{"id":"http://arxiv.org/abs/2401.02646v1","updated":"2024-01-05T05:32:37Z","published":"2024-01-05T05:32:37Z","title":"Recent Advancement in 3D Biometrics using Monocular Camera","summary":"  Recent literature has witnessed significant interest towards 3D biometrics\nemploying monocular vision for robust authentication methods. Motivated by\nthis, in this work we seek to provide insight on recent development in the area\nof 3D biometrics employing monocular vision. We present the similarity and\ndissimilarity of 3D monocular biometrics and classical biometrics, listing the\nstrengths and challenges. Further, we provide an overview of recent techniques\nin 3D biometrics with monocular vision, as well as application systems adopted\nby the industry. Finally, we discuss open research problems in this area of\nresearch\n","authors":["Aritra Mukherjee","Abhijit Das"],"pdf_url":"https://arxiv.org/pdf/2401.02646v1.pdf","comment":"Accepted and presented in IJCB 2023"},{"id":"http://arxiv.org/abs/2312.01129v2","updated":"2024-01-05T05:03:27Z","published":"2023-12-02T13:04:54Z","title":"ControlDreamer: Stylized 3D Generation with Multi-View ControlNet","summary":"  Recent advancements in text-to-3D generation have significantly contributed\nto the automation and democratization of 3D content creation. Building upon\nthese developments, we aim to address the limitations of current methods in\ngenerating 3D models with creative geometry and styles. We introduce multi-view\nControlNet, a novel depth-aware multi-view diffusion model trained on generated\ndatasets from a carefully curated text corpus. Our multi-view ControlNet is\nthen integrated into our two-stage pipeline, ControlDreamer, enabling\ntext-guided generation of stylized 3D models. Additionally, we present a\ncomprehensive benchmark for 3D style editing, encompassing a broad range of\nsubjects, including objects, animals, and characters, to further facilitate\nresearch on diverse 3D generation. Our comparative analysis reveals that this\nnew pipeline outperforms existing text-to-3D methods as evidenced by human\nevaluations and CLIP score metrics.\n","authors":["Yeongtak Oh","Jooyoung Choi","Yongsung Kim","Minjun Park","Chaehun Shin","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2312.01129v2.pdf","comment":"Project page: https://controldreamer.github.io/"},{"id":"http://arxiv.org/abs/2401.02634v1","updated":"2024-01-05T04:53:33Z","published":"2024-01-05T04:53:33Z","title":"AG-ReID.v2: Bridging Aerial and Ground Views for Person\n  Re-identification","summary":"  Aerial-ground person re-identification (Re-ID) presents unique challenges in\ncomputer vision, stemming from the distinct differences in viewpoints, poses,\nand resolutions between high-altitude aerial and ground-based cameras. Existing\nresearch predominantly focuses on ground-to-ground matching, with aerial\nmatching less explored due to a dearth of comprehensive datasets. To address\nthis, we introduce AG-ReID.v2, a dataset specifically designed for person Re-ID\nin mixed aerial and ground scenarios. This dataset comprises 100,502 images of\n1,615 unique individuals, each annotated with matching IDs and 15 soft\nattribute labels. Data were collected from diverse perspectives using a UAV,\nstationary CCTV, and smart glasses-integrated camera, providing a rich variety\nof intra-identity variations. Additionally, we have developed an explainable\nattention network tailored for this dataset. This network features a\nthree-stream architecture that efficiently processes pairwise image distances,\nemphasizes key top-down features, and adapts to variations in appearance due to\naltitude differences. Comparative evaluations demonstrate the superiority of\nour approach over existing baselines. We plan to release the dataset and\nalgorithm source code publicly, aiming to advance research in this specialized\nfield of computer vision. For access, please visit\nhttps://github.com/huynguyen792/AG-ReID.v2.\n","authors":["Huy Nguyen","Kien Nguyen","Sridha Sridharan","Clinton Fookes"],"pdf_url":"https://arxiv.org/pdf/2401.02634v1.pdf","comment":"13 pages, Accepted by TIFS 2023"},{"id":"http://arxiv.org/abs/2401.02633v1","updated":"2024-01-05T04:43:14Z","published":"2024-01-05T04:43:14Z","title":"A Random Ensemble of Encrypted models for Enhancing Robustness against\n  Adversarial Examples","summary":"  Deep neural networks (DNNs) are well known to be vulnerable to adversarial\nexamples (AEs). In addition, AEs have adversarial transferability, which means\nAEs generated for a source model can fool another black-box model (target\nmodel) with a non-trivial probability. In previous studies, it was confirmed\nthat the vision transformer (ViT) is more robust against the property of\nadversarial transferability than convolutional neural network (CNN) models such\nas ConvMixer, and moreover encrypted ViT is more robust than ViT without any\nencryption. In this article, we propose a random ensemble of encrypted ViT\nmodels to achieve much more robust models. In experiments, the proposed scheme\nis verified to be more robust against not only black-box attacks but also\nwhite-box ones than convention methods.\n","authors":["Ryota Iijima","Sayaka Shiota","Hitoshi Kiya"],"pdf_url":"https://arxiv.org/pdf/2401.02633v1.pdf","comment":"4 pages"},{"id":"http://arxiv.org/abs/2312.10983v2","updated":"2024-01-05T04:36:43Z","published":"2023-12-18T07:11:45Z","title":"MatchDet: A Collaborative Framework for Image Matching and Object\n  Detection","summary":"  Image matching and object detection are two fundamental and challenging\ntasks, while many related applications consider them two individual tasks (i.e.\ntask-individual). In this paper, a collaborative framework called MatchDet\n(i.e. task-collaborative) is proposed for image matching and object detection\nto obtain mutual improvements. To achieve the collaborative learning of the two\ntasks, we propose three novel modules, including a Weighted Spatial Attention\nModule (WSAM) for Detector, and Weighted Attention Module (WAM) and Box Filter\nfor Matcher. Specifically, the WSAM highlights the foreground regions of target\nimage to benefit the subsequent detector, the WAM enhances the connection\nbetween the foreground regions of pair images to ensure high-quality matches,\nand Box Filter mitigates the impact of false matches. We evaluate the\napproaches on a new benchmark with two datasets called Warp-COCO and\nminiScanNet. Experimental results show our approaches are effective and achieve\ncompetitive improvements.\n","authors":["Jinxiang Lai","Wenlong Wu","Bin-Bin Gao","Jun Liu","Jiawei Zhan","Congchong Nie","Yi Zeng","Chengjie Wang"],"pdf_url":"https://arxiv.org/pdf/2312.10983v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02616v1","updated":"2024-01-05T03:23:38Z","published":"2024-01-05T03:23:38Z","title":"FED-NeRF: Achieve High 3D Consistency and Temporal Coherence for Face\n  Video Editing on Dynamic NeRF","summary":"  The success of the GAN-NeRF structure has enabled face editing on NeRF to\nmaintain 3D view consistency. However, achieving simultaneously multi-view\nconsistency and temporal coherence while editing video sequences remains a\nformidable challenge. This paper proposes a novel face video editing\narchitecture built upon the dynamic face GAN-NeRF structure, which effectively\nutilizes video sequences to restore the latent code and 3D face geometry. By\nediting the latent code, multi-view consistent editing on the face can be\nensured, as validated by multiview stereo reconstruction on the resulting\nedited images in our dynamic NeRF. As the estimation of face geometries occurs\non a frame-by-frame basis, this may introduce a jittering issue. We propose a\nstabilizer that maintains temporal coherence by preserving smooth changes of\nface expressions in consecutive frames. Quantitative and qualitative analyses\nreveal that our method, as the pioneering 4D face video editor, achieves\nstate-of-the-art performance in comparison to existing 2D or 3D-based\napproaches independently addressing identity and motion. Codes will be\nreleased.\n","authors":["Hao Zhang","Yu-Wing Tai","Chi-Keung Tang"],"pdf_url":"https://arxiv.org/pdf/2401.02616v1.pdf","comment":"Our code will be available at: https://github.com/ZHANG1023/FED-NeRF"},{"id":"http://arxiv.org/abs/2401.02614v1","updated":"2024-01-05T03:12:03Z","published":"2024-01-05T03:12:03Z","title":"Scaling and Masking: A New Paradigm of Data Sampling for Image and Video\n  Quality Assessment","summary":"  Quality assessment of images and videos emphasizes both local details and\nglobal semantics, whereas general data sampling methods (e.g., resizing,\ncropping or grid-based fragment) fail to catch them simultaneously. To address\nthe deficiency, current approaches have to adopt multi-branch models and take\nas input the multi-resolution data, which burdens the model complexity. In this\nwork, instead of stacking up models, a more elegant data sampling method (named\nas SAMA, scaling and masking) is explored, which compacts both the local and\nglobal content in a regular input size. The basic idea is to scale the data\ninto a pyramid first, and reduce the pyramid into a regular data dimension with\na masking strategy. Benefiting from the spatial and temporal redundancy in\nimages and videos, the processed data maintains the multi-scale characteristics\nwith a regular input size, thus can be processed by a single-branch model. We\nverify the sampling method in image and video quality assessment. Experiments\nshow that our sampling method can improve the performance of current\nsingle-branch models significantly, and achieves competitive performance to the\nmulti-branch models without extra model complexity. The source code will be\navailable at https://github.com/Sissuire/SAMA.\n","authors":["Yongxu Liu","Yinghui Quan","Guoyao Xiao","Aobo Li","Jinjian Wu"],"pdf_url":"https://arxiv.org/pdf/2401.02614v1.pdf","comment":"Accepted by AAAI2024. Code has been released at\n  https://github.com/Sissuire/SAMA"},{"id":"http://arxiv.org/abs/2401.02309v2","updated":"2024-01-05T03:11:28Z","published":"2024-01-04T14:55:57Z","title":"TR-DETR: Task-Reciprocal Transformer for Joint Moment Retrieval and\n  Highlight Detection","summary":"  Video moment retrieval (MR) and highlight detection (HD) based on natural\nlanguage queries are two highly related tasks, which aim to obtain relevant\nmoments within videos and highlight scores of each video clip. Recently,\nseveral methods have been devoted to building DETR-based networks to solve both\nMR and HD jointly. These methods simply add two separate task heads after\nmulti-modal feature extraction and feature interaction, achieving good\nperformance. Nevertheless, these approaches underutilize the reciprocal\nrelationship between two tasks. In this paper, we propose a task-reciprocal\ntransformer based on DETR (TR-DETR) that focuses on exploring the inherent\nreciprocity between MR and HD. Specifically, a local-global multi-modal\nalignment module is first built to align features from diverse modalities into\na shared latent space. Subsequently, a visual feature refinement is designed to\neliminate query-irrelevant information from visual features for modal\ninteraction. Finally, a task cooperation module is constructed to refine the\nretrieval pipeline and the highlight score prediction process by utilizing the\nreciprocity between MR and HD. Comprehensive experiments on QVHighlights,\nCharades-STA and TVSum datasets demonstrate that TR-DETR outperforms existing\nstate-of-the-art methods. Codes are available at\n\\url{https://github.com/mingyao1120/TR-DETR}.\n","authors":["Hao Sun","Mingyao Zhou","Wenjing Chen","Wei Xie"],"pdf_url":"https://arxiv.org/pdf/2401.02309v2.pdf","comment":"Accepted by AAAI-24"},{"id":"http://arxiv.org/abs/2401.02611v1","updated":"2024-01-05T02:57:58Z","published":"2024-01-05T02:57:58Z","title":"MOODv2: Masked Image Modeling for Out-of-Distribution Detection","summary":"  The crux of effective out-of-distribution (OOD) detection lies in acquiring a\nrobust in-distribution (ID) representation, distinct from OOD samples. While\nprevious methods predominantly leaned on recognition-based techniques for this\npurpose, they often resulted in shortcut learning, lacking comprehensive\nrepresentations. In our study, we conducted a comprehensive analysis, exploring\ndistinct pretraining tasks and employing various OOD score functions. The\nresults highlight that the feature representations pre-trained through\nreconstruction yield a notable enhancement and narrow the performance gap among\nvarious score functions. This suggests that even simple score functions can\nrival complex ones when leveraging reconstruction-based pretext tasks.\nReconstruction-based pretext tasks adapt well to various score functions. As\nsuch, it holds promising potential for further expansion. Our OOD detection\nframework, MOODv2, employs the masked image modeling pretext task. Without\nbells and whistles, MOODv2 impressively enhances 14.30% AUROC to 95.68% on\nImageNet and achieves 99.98% on CIFAR-10.\n","authors":["Jingyao Li","Pengguang Chen","Shaozuo Yu","Shu Liu","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2401.02611v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02610v1","updated":"2024-01-05T02:54:23Z","published":"2024-01-05T02:54:23Z","title":"DHGCN: Dynamic Hop Graph Convolution Network for Self-supervised Point\n  Cloud Learning","summary":"  Recent works attempt to extend Graph Convolution Networks (GCNs) to point\nclouds for classification and segmentation tasks. These works tend to sample\nand group points to create smaller point sets locally and mainly focus on\nextracting local features through GCNs, while ignoring the relationship between\npoint sets. In this paper, we propose the Dynamic Hop Graph Convolution Network\n(DHGCN) for explicitly learning the contextual relationships between the\nvoxelized point parts, which are treated as graph nodes. Motivated by the\nintuition that the contextual information between point parts lies in the\npairwise adjacent relationship, which can be depicted by the hop distance of\nthe graph quantitatively, we devise a novel self-supervised part-level hop\ndistance reconstruction task and design a novel loss function accordingly to\nfacilitate training. In addition, we propose the Hop Graph Attention (HGA),\nwhich takes the learned hop distance as input for producing attention weights\nto allow edge features to contribute distinctively in aggregation. Eventually,\nthe proposed DHGCN is a plug-and-play module that is compatible with\npoint-based backbone networks. Comprehensive experiments on different backbones\nand tasks demonstrate that our self-supervised method achieves state-of-the-art\nperformance. Our source code is available at: https://github.com/Jinec98/DHGCN.\n","authors":["Jincen Jiang","Lizhi Zhao","Xuequan Lu","Wei Hu","Imran Razzak","Meili Wang"],"pdf_url":"https://arxiv.org/pdf/2401.02610v1.pdf","comment":"Accepted to AAAI 2024"},{"id":"http://arxiv.org/abs/2309.16924v3","updated":"2024-01-05T02:49:43Z","published":"2023-09-29T01:51:04Z","title":"Incremental Rotation Averaging Revisited and More: A New Rotation\n  Averaging Benchmark","summary":"  In order to further advance the accuracy and robustness of the incremental\nparameter estimation-based rotation averaging methods, in this paper, a new\nmember of the Incremental Rotation Averaging (IRA) family is introduced, which\nis termed as IRAv4. As the most significant feature of the IRAv4, a\ntask-specific connected dominating set is extracted to serve as a more reliable\nand accurate reference for rotation global alignment. In addition, to further\naddress the limitations of the existing rotation averaging benchmark of relying\non the slightly outdated Bundler camera calibration results as ground truths\nand focusing solely on rotation estimation accuracy, this paper presents a new\nCOLMAP-based rotation averaging benchmark that incorporates a cross check\nbetween COLMAP and Bundler, and employ the accuracy of both rotation and\ndownstream location estimation as evaluation metrics, which is desired to\nprovide a more reliable and comprehensive evaluation tool for the rotation\naveraging research. Comprehensive comparisons between the proposed IRAv4 and\nother mainstream rotation averaging methods on this new benchmark demonstrate\nthe effectiveness of our proposed approach.\n","authors":["Xiang Gao","Hainan Cui","Shuhan Shen"],"pdf_url":"https://arxiv.org/pdf/2309.16924v3.pdf","comment":"Submitted to IEEE Transactions"},{"id":"http://arxiv.org/abs/2401.02607v1","updated":"2024-01-05T02:46:08Z","published":"2024-01-05T02:46:08Z","title":"Partition-based Nonrigid Registration for 3D Face Model","summary":"  This paper presents a partition-based surface registration for 3D morphable\nmodel(3DMM). In the 3DMM, it often requires to warp a handcrafted template\nmodel into different captured models. The proposed method first utilizes the\nlandmarks to partition the template model then scale each part and finally\nsmooth the boundaries. This method is especially effective when the disparity\nbetween the template model and the target model is huge. The experiment result\nshows the method perform well than the traditional warp method and robust to\nthe local minima.\n","authors":["Yuping Ye","Zhan Song","Juan Zhao"],"pdf_url":"https://arxiv.org/pdf/2401.02607v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02606v1","updated":"2024-01-05T02:34:53Z","published":"2024-01-05T02:34:53Z","title":"Exploiting Polarized Material Cues for Robust Car Detection","summary":"  Car detection is an important task that serves as a crucial prerequisite for\nmany automated driving functions. The large variations in lighting/weather\nconditions and vehicle densities of the scenes pose significant challenges to\nexisting car detection algorithms to meet the highly accurate perception demand\nfor safety, due to the unstable/limited color information, which impedes the\nextraction of meaningful/discriminative features of cars. In this work, we\npresent a novel learning-based car detection method that leverages trichromatic\nlinear polarization as an additional cue to disambiguate such challenging\ncases. A key observation is that polarization, characteristic of the light\nwave, can robustly describe intrinsic physical properties of the scene objects\nin various imaging conditions and is strongly linked to the nature of materials\nfor cars (e.g., metal and glass) and their surrounding environment (e.g., soil\nand trees), thereby providing reliable and discriminative features for robust\ncar detection in challenging scenes. To exploit polarization cues, we first\nconstruct a pixel-aligned RGB-Polarization car detection dataset, which we\nsubsequently employ to train a novel multimodal fusion network. Our car\ndetection network dynamically integrates RGB and polarization features in a\nrequest-and-complement manner and can explore the intrinsic material properties\nof cars across all learning samples. We extensively validate our method and\ndemonstrate that it outperforms state-of-the-art detection methods.\nExperimental results show that polarization is a powerful cue for car\ndetection.\n","authors":["Wen Dong","Haiyang Mei","Ziqi Wei","Ao Jin","Sen Qiu","Qiang Zhang","Xin Yang"],"pdf_url":"https://arxiv.org/pdf/2401.02606v1.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2211.12735v2","updated":"2024-01-05T02:05:52Z","published":"2022-11-23T06:56:12Z","title":"Fast-iTPN: Integrally Pre-Trained Transformer Pyramid Network with Token\n  Migration","summary":"  We propose integrally pre-trained transformer pyramid network (iTPN), towards\njointly optimizing the network backbone and the neck, so that transfer gap\nbetween representation models and downstream tasks is minimal. iTPN is born\nwith two elaborated designs: 1) The first pre-trained feature pyramid upon\nvision transformer (ViT). 2) Multi-stage supervision to the feature pyramid\nusing masked feature modeling (MFM). iTPN is updated to Fast-iTPN, reducing\ncomputational memory overhead and accelerating inference through two flexible\ndesigns. 1) Token migration: dropping redundant tokens of the backbone while\nreplenishing them in the feature pyramid without attention operations. 2) Token\ngathering: reducing computation cost caused by global attention by introducing\nfew gathering tokens. The base/large-level Fast-iTPN achieve 88.75%/89.5% top-1\naccuracy on ImageNet-1K. With 1x training schedule using DINO, the\nbase/large-level Fast-iTPN achieves 58.4%/58.8% box AP on COCO object\ndetection, and a 57.5%/58.7% mIoU on ADE20K semantic segmentation using\nMaskDINO. Fast-iTPN can accelerate the inference procedure by up to 70%, with\nnegligible performance loss, demonstrating the potential to be a powerful\nbackbone for downstream vision tasks. The code is available at:\ngithub.com/sunsmarterjie/iTPN.\n","authors":["Yunjie Tian","Lingxi Xie","Jihao Qiu","Jianbin Jiao","Yaowei Wang","Qi Tian","Qixiang Ye"],"pdf_url":"https://arxiv.org/pdf/2211.12735v2.pdf","comment":"The tiny/small/base-level models report new records on ImageNet-1K.\n  Code: github.com/sunsmarterjie/iTPN"},{"id":"http://arxiv.org/abs/2309.06023v3","updated":"2024-01-05T02:04:04Z","published":"2023-09-12T07:50:54Z","title":"Learning from History: Task-agnostic Model Contrastive Learning for\n  Image Restoration","summary":"  Contrastive learning has emerged as a prevailing paradigm for high-level\nvision tasks, which, by introducing properly negative samples, has also been\nexploited for low-level vision tasks to achieve a compact optimization space to\naccount for their ill-posed nature. However, existing methods rely on manually\npredefined and task-oriented negatives, which often exhibit pronounced\ntask-specific biases. To address this challenge, our paper introduces an\ninnovative method termed 'learning from history', which dynamically generates\nnegative samples from the target model itself. Our approach, named Model\nContrastive paradigm for Image Restoration (MCIR), rejuvenates latency models\nas negative models, making it compatible with diverse image restoration tasks.\nWe propose the Self-Prior guided Negative loss (SPN) to enable it. This\napproach significantly enhances existing models when retrained with the\nproposed model contrastive paradigm. The results show significant improvements\nin image restoration across various tasks and architectures. For example,\nmodels retrained with SPN outperform the original FFANet and DehazeFormer by\n3.41 dB and 0.57 dB on the RESIDE indoor dataset for image dehazing. Similarly,\nthey achieve notable improvements of 0.47 dB on SPA-Data over IDT for image\nderaining and 0.12 dB on Manga109 for a 4x scale super-resolution over\nlightweight SwinIR, respectively. Code and retrained models are available at\nhttps://github.com/Aitical/MCIR.\n","authors":["Gang Wu","Junjun Jiang","Kui Jiang","Xianming Liu"],"pdf_url":"https://arxiv.org/pdf/2309.06023v3.pdf","comment":"Camera Ready Version. Accepted to The 38th Annual AAAI Conference on\n  Artificial Intelligence (AAAI 2024)"},{"id":"http://arxiv.org/abs/2308.04074v3","updated":"2024-01-05T02:03:52Z","published":"2023-08-08T06:16:37Z","title":"Exploiting Spatial-Temporal Context for Interacting Hand Reconstruction\n  on Monocular RGB Video","summary":"  Reconstructing interacting hands from monocular RGB data is a challenging\ntask, as it involves many interfering factors, e.g. self- and mutual occlusion\nand similar textures. Previous works only leverage information from a single\nRGB image without modeling their physically plausible relation, which leads to\ninferior reconstruction results. In this work, we are dedicated to explicitly\nexploiting spatial-temporal information to achieve better interacting hand\nreconstruction. On one hand, we leverage temporal context to complement\ninsufficient information provided by the single frame, and design a novel\ntemporal framework with a temporal constraint for interacting hand motion\nsmoothness. On the other hand, we further propose an interpenetration detection\nmodule to produce kinetically plausible interacting hands without physical\ncollisions. Extensive experiments are performed to validate the effectiveness\nof our proposed framework, which achieves new state-of-the-art performance on\npublic benchmarks.\n","authors":["Weichao Zhao","Hezhen Hu","Wengang Zhou","Li li","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2308.04074v3.pdf","comment":"Accepted by TOMM 2023"},{"id":"http://arxiv.org/abs/2311.12144v7","updated":"2024-01-05T01:58:58Z","published":"2023-11-20T19:45:27Z","title":"Applications of Large Scale Foundation Models for Autonomous Driving","summary":"  Since DARPA Grand Challenges (rural) in 2004/05 and Urban Challenges in 2007,\nautonomous driving has been the most active field of AI applications. Recently\npowered by large language models (LLMs), chat systems, such as chatGPT and\nPaLM, emerge and rapidly become a promising direction to achieve artificial\ngeneral intelligence (AGI) in natural language processing (NLP). There comes a\nnatural thinking that we could employ these abilities to reformulate autonomous\ndriving. By combining LLM with foundation models, it is possible to utilize the\nhuman knowledge, commonsense and reasoning to rebuild autonomous driving\nsystems from the current long-tailed AI dilemma. In this paper, we investigate\nthe techniques of foundation models and LLMs applied for autonomous driving,\ncategorized as simulation, world model, data annotation and planning or E2E\nsolutions etc.\n","authors":["Yu Huang","Yue Chen","Zhu Li"],"pdf_url":"https://arxiv.org/pdf/2311.12144v7.pdf","comment":"23 pages. A survey paper"},{"id":"http://arxiv.org/abs/2401.02600v1","updated":"2024-01-05T01:52:13Z","published":"2024-01-05T01:52:13Z","title":"Object-oriented backdoor attack against image captioning","summary":"  Backdoor attack against image classification task has been widely studied and\nproven to be successful, while there exist little research on the backdoor\nattack against vision-language models. In this paper, we explore backdoor\nattack towards image captioning models by poisoning training data. Assuming the\nattacker has total access to the training dataset, and cannot intervene in\nmodel construction or training process. Specifically, a portion of benign\ntraining samples is randomly selected to be poisoned. Afterwards, considering\nthat the captions are usually unfolded around objects in an image, we design an\nobject-oriented method to craft poisons, which aims to modify pixel values by a\nslight range with the modification number proportional to the scale of the\ncurrent detected object region. After training with the poisoned data, the\nattacked model behaves normally on benign images, but for poisoned images, the\nmodel will generate some sentences irrelevant to the given image. The attack\ncontrols the model behavior on specific test images without sacrificing the\ngeneration performance on benign test images. Our method proves the weakness of\nimage captioning models to backdoor attack and we hope this work can raise the\nawareness of defending against backdoor attack in the image captioning field.\n","authors":["Meiling Li","Nan Zhong","Xinpeng Zhang","Zhenxing Qian","Sheng Li"],"pdf_url":"https://arxiv.org/pdf/2401.02600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13018v3","updated":"2024-01-05T01:50:52Z","published":"2023-11-21T21:48:51Z","title":"GeoLocator: a location-integrated large multimodal model for inferring\n  geo-privacy","summary":"  Geographic privacy or geo-privacy refers to the keeping private of one's\ngeographic location, especially the restriction of geographical data maintained\nby personal electronic devices. Geo-privacy is a crucial aspect of personal\nsecurity; however, it often goes unnoticed in daily activities. With the surge\nin the use of Large Multimodal Models (LMMs), such as GPT-4, for Open Source\nIntelligence (OSINT), the potential risks associated with geo-privacy breaches\nhave intensified. This study develops a location-integrated GPT-4 based model\nnamed GeoLocator and designs four-dimensional experiments to demonstrate its\ncapability in inferring the locational information of input imageries and/or\nsocial media contents. Our experiments reveal that GeoLocator generates\nspecific geographic details with high accuracy and consequently embeds the risk\nof the model users exposing geospatial information to the public\nunintentionally, highlighting the thread of online data sharing, information\ngathering technologies and LLMs on geo-privacy. We conclude with the broader\nimplications of GeoLocator and our findings for individuals and the community\nat large, by emphasizing the urgency for enhanced awareness and protective\nmeasures against geo-privacy leakage in the era of advanced AI and widespread\nsocial media usage.\n","authors":["Yifan Yang","Siqin Wang","Daoyang Li","Yixian Zhang","Shuju Sun","Junzhou He"],"pdf_url":"https://arxiv.org/pdf/2311.13018v3.pdf","comment":"16pages, 2 figures"},{"id":"http://arxiv.org/abs/2308.15752v3","updated":"2024-01-05T01:50:38Z","published":"2023-08-30T04:29:48Z","title":"Large-scale data extraction from the UNOS organ donor documents","summary":"  In this paper we focus on three major task: 1) discussing our methods: Our\nmethod captures a portion of the data in DCD flowsheets, kidney perfusion data,\nand Flowsheet data captured peri-organ recovery surgery. 2) demonstrating the\nresult: We built a comprehensive, analyzable database from 2022 OPTN data. This\ndataset is by far larger than any previously available even in this preliminary\nphase; and 3) proving that our methods can be extended to all the past OPTN\ndata and future data.\n  The scope of our study is all Organ Procurement and Transplantation Network\n(OPTN) data of the USA organ donors since 2008. The data was not analyzable in\na large scale in the past because it was captured in PDF documents known as\n``Attachments'', whereby every donor's information was recorded into dozens of\nPDF documents in heterogeneous formats. To make the data analyzable, one needs\nto convert the content inside these PDFs to an analyzable data format, such as\na standard SQL database. In this paper we will focus on 2022 OPTN data, which\nconsists of $\\approx 400,000$ PDF documents spanning millions of pages. The\nentire OPTN data covers 15 years (2008--20022). This paper assumes that readers\nare familiar with the content of the OPTN data.\n","authors":["Marek Rychlik","Bekir Tanriover","Yan Han"],"pdf_url":"https://arxiv.org/pdf/2308.15752v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02588v1","updated":"2024-01-05T00:49:56Z","published":"2024-01-05T00:49:56Z","title":"Characterizing Satellite Geometry via Accelerated 3D Gaussian Splatting","summary":"  The accelerating deployment of spacecraft in orbit have generated interest in\non-orbit servicing (OOS), inspection of spacecraft, and active debris removal\n(ADR). Such missions require precise rendezvous and proximity operations in the\nvicinity of non-cooperative, possible unknown, resident space objects. Safety\nconcerns with manned missions and lag times with ground-based control\nnecessitate complete autonomy. This requires robust characterization of the\ntarget's geometry. In this article, we present an approach for mapping\ngeometries of satellites on orbit based on 3D Gaussian Splatting that can run\non computing resources available on current spaceflight hardware. We\ndemonstrate model training and 3D rendering performance on a\nhardware-in-the-loop satellite mock-up under several realistic lighting and\nmotion conditions. Our model is shown to be capable of training on-board and\nrendering higher quality novel views of an unknown satellite nearly 2 orders of\nmagnitude faster than previous NeRF-based algorithms. Such on-board\ncapabilities are critical to enable downstream machine intelligence tasks\nnecessary for autonomous guidance, navigation, and control tasks.\n","authors":["Van Minh Nguyen","Emma Sandidge","Trupti Mahendrakar","Ryan T. White"],"pdf_url":"https://arxiv.org/pdf/2401.02588v1.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2303.15065v3","updated":"2024-01-05T00:48:20Z","published":"2023-03-27T10:18:42Z","title":"Single-subject Multi-contrast MRI Super-resolution via Implicit Neural\n  Representations","summary":"  Clinical routine and retrospective cohorts commonly include multi-parametric\nMagnetic Resonance Imaging; however, they are mostly acquired in different\nanisotropic 2D views due to signal-to-noise-ratio and scan-time constraints.\nThus acquired views suffer from poor out-of-plane resolution and affect\ndownstream volumetric image analysis that typically requires isotropic 3D\nscans. Combining different views of multi-contrast scans into high-resolution\nisotropic 3D scans is challenging due to the lack of a large training cohort,\nwhich calls for a subject-specific framework. This work proposes a novel\nsolution to this problem leveraging Implicit Neural Representations (INR). Our\nproposed INR jointly learns two different contrasts of complementary views in a\ncontinuous spatial function and benefits from exchanging anatomical information\nbetween them. Trained within minutes on a single commodity GPU, our model\nprovides realistic super-resolution across different pairs of contrasts in our\nexperiments with three datasets. Using Mutual Information (MI) as a metric, we\nfind that our model converges to an optimum MI amongst sequences, achieving\nanatomically faithful reconstruction. Code is available at:\nhttps://github.com/jqmcginnis/multi_contrast_inr/\n","authors":["Julian McGinnis","Suprosanna Shit","Hongwei Bran Li","Vasiliki Sideri-Lampretsa","Robert Graf","Maik Dannecker","Jiazhen Pan","Nil Stolt Ansó","Mark Mühlau","Jan S. Kirschke","Daniel Rueckert","Benedikt Wiestler"],"pdf_url":"https://arxiv.org/pdf/2303.15065v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12910v2","updated":"2024-01-05T00:44:21Z","published":"2023-08-24T16:35:35Z","title":"SCoRD: Subject-Conditional Relation Detection with Text-Augmented Data","summary":"  We propose Subject-Conditional Relation Detection SCoRD, where conditioned on\nan input subject, the goal is to predict all its relations to other objects in\na scene along with their locations. Based on the Open Images dataset, we\npropose a challenging OIv6-SCoRD benchmark such that the training and testing\nsplits have a distribution shift in terms of the occurrence statistics of\n$\\langle$subject, relation, object$\\rangle$ triplets. To solve this problem, we\npropose an auto-regressive model that given a subject, it predicts its\nrelations, objects, and object locations by casting this output as a sequence\nof tokens. First, we show that previous scene-graph prediction methods fail to\nproduce as exhaustive an enumeration of relation-object pairs when conditioned\non a subject on this benchmark. Particularly, we obtain a recall@3 of 83.8% for\nour relation-object predictions compared to the 49.75% obtained by a recent\nscene graph detector. Then, we show improved generalization on both\nrelation-object and object-box predictions by leveraging during training\nrelation-object pairs obtained automatically from textual captions and for\nwhich no object-box annotations are available. Particularly, for\n$\\langle$subject, relation, object$\\rangle$ triplets for which no object\nlocations are available during training, we are able to obtain a recall@3 of\n33.80% for relation-object pairs and 26.75% for their box locations.\n","authors":["Ziyan Yang","Kushal Kafle","Zhe Lin","Scott Cohen","Zhihong Ding","Vicente Ordonez"],"pdf_url":"https://arxiv.org/pdf/2308.12910v2.pdf","comment":"WACV 2024"},{"id":"http://arxiv.org/abs/2401.02582v1","updated":"2024-01-05T00:26:07Z","published":"2024-01-05T00:26:07Z","title":"CoCoT: Contrastive Chain-of-Thought Prompting for Large Multimodal\n  Models with Multiple Image Inputs","summary":"  When exploring the development of Artificial General Intelligence (AGI), a\ncritical task for these models involves interpreting and processing information\nfrom multiple image inputs. However, Large Multimodal Models (LMMs) encounter\ntwo issues in such scenarios: (1) a lack of fine-grained perception, and (2) a\ntendency to blend information across multiple images. We first extensively\ninvestigate the capability of LMMs to perceive fine-grained visual details when\ndealing with multiple input images. The research focuses on two aspects: first,\nimage-to-image matching (to evaluate whether LMMs can effectively reason and\npair relevant images), and second, multi-image-to-text matching (to assess\nwhether LMMs can accurately capture and summarize detailed image information).\nWe conduct evaluations on a range of both open-source and closed-source large\nmodels, including GPT-4V, Gemini, OpenFlamingo, and MMICL. To enhance model\nperformance, we further develop a Contrastive Chain-of-Thought (CoCoT)\nprompting approach based on multi-input multimodal models. This method requires\nLMMs to compare the similarities and differences among multiple image inputs,\nand then guide the models to answer detailed questions about multi-image inputs\nbased on the identified similarities and differences. Our experimental results\nshowcase CoCoT's proficiency in enhancing the multi-image comprehension\ncapabilities of large multimodal models.\n","authors":["Daoan Zhang","Junming Yang","Hanjia Lyu","Zijian Jin","Yuan Yao","Mingkai Chen","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2401.02582v1.pdf","comment":null}],"Robotics":[{"id":"http://arxiv.org/abs/2401.02903v1","updated":"2024-01-05T17:04:43Z","published":"2024-01-05T17:04:43Z","title":"Deep Reinforcement Learning for Local Path Following of an Autonomous\n  Formula SAE Vehicle","summary":"  With the continued introduction of driverless events to Formula:Society of\nAutomotive Engineers (F:SAE) competitions around the world, teams are\ninvestigating all aspects of the autonomous vehicle stack. This paper presents\nthe use of Deep Reinforcement Learning (DRL) and Inverse Reinforcement Learning\n(IRL) to map locally-observed cone positions to a desired steering angle for\nrace track following. Two state-of-the-art algorithms not previously tested in\nthis context: soft actor critic (SAC) and adversarial inverse reinforcement\nlearning (AIRL), are used to train models in a representative simulation. Three\nnovel reward functions for use by RL algorithms in an autonomous racing context\nare also discussed. Tests performed in simulation and the real world suggest\nthat both algorithms can successfully train models for local path following.\nSuggestions for future work are presented to allow these models to scale to a\nfull F:SAE vehicle.\n","authors":["Harvey Merton","Thomas Delamore","Karl Stol","Henry Williams"],"pdf_url":"https://arxiv.org/pdf/2401.02903v1.pdf","comment":"As presented at the Australasian Conference on Robotics and\n  Automation (ACRA 2023)"},{"id":"http://arxiv.org/abs/2401.02899v1","updated":"2024-01-05T17:00:50Z","published":"2024-01-05T17:00:50Z","title":"Model predictive altitude and velocity control in ergodic potential\n  field directed multi-UAV search","summary":"  This research addresses the challenge of executing multi-UAV survey missions\nover diverse terrains characterized by varying elevations. The approach\nintegrates advanced two-dimensional ergodic search technique with model\npredictive control of UAV altitude and velocity. Optimization of altitude and\nvelocity is performed along anticipated UAV ground routes, considering multiple\nobjectives and constraints. This yields a flight regimen tailored to the\nterrain, as well as the motion and sensing characteristics of the UAVs. The\nproposed UAV motion control strategy is assessed through simulations of\nrealistic search missions and actual terrain models. Results demonstrate the\nsuccessful integration of model predictive altitude and velocity control with a\ntwo-dimensional potential field-guided ergodic search. Adjusting UAV altitudes\nto near-ideal levels facilitates the utilization of sensing ranges, thereby\nenhancing the effectiveness of the search. Furthermore, the control algorithm\nis capable of real-time computation, encouraging its practical application in\nreal-world scenarios.\n","authors":["Luka Lanča","Karlo Jakac","Stefan Ivić"],"pdf_url":"https://arxiv.org/pdf/2401.02899v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02883v1","updated":"2024-01-05T16:25:49Z","published":"2024-01-05T16:25:49Z","title":"iPolicy: Incremental Policy Algorithms for Feedback Motion Planning","summary":"  This paper presents policy-based motion planning for robotic systems. The\nmotion planning literature has been mostly focused on open-loop trajectory\nplanning which is followed by tracking online. In contrast, we solve the\nproblem of path planning and controller synthesis simultaneously by solving the\nrelated feedback control problem. We present a novel incremental policy\n(iPolicy) algorithm for motion planning, which integrates sampling-based\nmethods and set-valued optimal control methods to compute feedback controllers\nfor the robotic system. In particular, we use sampling to incrementally\nconstruct the state space of the system. Asynchronous value iterations are\nperformed on the sampled state space to synthesize the incremental policy\nfeedback controller. We show the convergence of the estimates to the optimal\nvalue function in continuous state space. Numerical results with various\ndifferent dynamical systems (including nonholonomic systems) verify the\noptimality and effectiveness of iPolicy.\n","authors":["Guoxiang Zhao","Devesh K. Jha","Yebin Wang","Minghui Zhu"],"pdf_url":"https://arxiv.org/pdf/2401.02883v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02848v1","updated":"2024-01-05T15:07:23Z","published":"2024-01-05T15:07:23Z","title":"Omnidirectional Multi-Rotor Aerial Vehicle Pose Optimization: A Novel\n  Approach to Physical Layer Security","summary":"  The integration of Multi-Rotor Aerial Vehicles (MRAVs) into 5G and 6G\nnetworks enhances coverage, connectivity, and congestion management. This\nfosters communication-aware robotics, exploring the interplay between robotics\nand communications, but also makes the MRAVs susceptible to malicious attacks,\nsuch as jamming. One traditional approach to counter these attacks is the use\nof beamforming on the MRAVs to apply physical layer security techniques. In\nthis paper, we explore pose optimization as an alternative approach to\ncountering jamming attacks on MRAVs. This technique is intended for\nomnidirectional MRAVs, which are drones capable of independently controlling\nboth their position and orientation, as opposed to the more common\nunderactuated MRAVs whose orientation cannot be controlled independently of\ntheir position. In this paper, we consider an omnidirectional MRAV serving as a\nBase Station (BS) for legitimate ground nodes, under attack by a malicious\njammer. We optimize the MRAV pose (i.e., position and orientation) to maximize\nthe minimum Signal-to-Interference-plus-Noise Ratio (SINR) over all legitimate\nnodes.\n","authors":["Daniel Bonilla Licea","Giuseppe Silano","Mounir Ghogho","Martin Saska"],"pdf_url":"https://arxiv.org/pdf/2401.02848v1.pdf","comment":"5 pages, 2 figures, Accepted for presentation to the 2024 IEEE\n  International Conference on Acoustics, Speech, and Signal Processing (ICASSP\n  2024), Seoul, Korea. Copyright may be transferred without notice, after which\n  this version may no longer be accessible"},{"id":"http://arxiv.org/abs/2401.02833v1","updated":"2024-01-05T14:35:02Z","published":"2024-01-05T14:35:02Z","title":"Integrating Flow Theory and Adaptive Robot Roles: A Conceptual Model of\n  Dynamic Robot Role Adaptation for the Enhanced Flow Experience in Long-term\n  Multi-person Human-Robot Interactions","summary":"  In this paper, we introduce a novel conceptual model for a robot's behavioral\nadaptation in its long-term interaction with humans, integrating dynamic robot\nrole adaptation with principles of flow experience from psychology. This\nconceptualization introduces a hierarchical interaction objective grounded in\nthe flow experience, serving as the overarching adaptation goal for the robot.\nThis objective intertwines both cognitive and affective sub-objectives and\nincorporates individual and group-level human factors. The dynamic role\nadaptation approach is a cornerstone of our model, highlighting the robot's\nability to fluidly adapt its support roles - from leader to follower - with the\naim of maintaining equilibrium between activity challenge and user skill,\nthereby fostering the user's optimal flow experiences. Moreover, this work\ndelves into a comprehensive exploration of the limitations and potential\napplications of our proposed conceptualization. Our model places a particular\nemphasis on the multi-person HRI paradigm, a dimension of HRI that is both\nunder-explored and challenging. In doing so, we aspire to extend the\napplicability and relevance of our conceptualization within the HRI field,\ncontributing to the future development of adaptive social robots capable of\nsustaining long-term interactions with humans.\n","authors":["Huili Chen","Sharifa Alghowinem","Cynthia Breazeal","Hae Won Park"],"pdf_url":"https://arxiv.org/pdf/2401.02833v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02816v1","updated":"2024-01-05T14:01:39Z","published":"2024-01-05T14:01:39Z","title":"Comparative Evaluation of RGB-D SLAM Methods for Humanoid Robot\n  Localization and Mapping","summary":"  In this paper, we conducted a comparative evaluation of three RGB-D SLAM\n(Simultaneous Localization and Mapping) algorithms: RTAB-Map, ORB-SLAM3, and\nOpenVSLAM for SURENA-V humanoid robot localization and mapping. Our test\ninvolves the robot to follow a full circular pattern, with an Intel RealSense\nD435 RGB-D camera installed on its head. In assessing localization accuracy,\nORB-SLAM3 outperformed the others with an ATE of 0.1073, followed by RTAB-Map\nat 0.1641 and OpenVSLAM at 0.1847. However, it should be noted that both\nORB-SLAM3 and OpenVSLAM faced challenges in maintaining accurate odometry when\nthe robot encountered a wall with limited feature points. Nevertheless,\nOpenVSLAM demonstrated the ability to detect loop closures and successfully\nrelocalize itself within the map when the robot approached its initial\nlocation. The investigation also extended to mapping capabilities, where\nRTAB-Map excelled by offering diverse mapping outputs, including dense,\nOctoMap, and occupancy grid maps. In contrast, both ORB-SLAM3 and OpenVSLAM\nprovided only sparse maps.\n","authors":["Amirhosein Vedadi","Aghil Yousefi-Koma","Parsa Yazdankhah","Amin Mozayyan"],"pdf_url":"https://arxiv.org/pdf/2401.02816v1.pdf","comment":"6 pages, 11th RSI International Conference on Robotics and\n  Mechatronics (ICRoM 2023)"},{"id":"http://arxiv.org/abs/2401.02814v1","updated":"2024-01-05T13:54:45Z","published":"2024-01-05T13:54:45Z","title":"Object-Centric Instruction Augmentation for Robotic Manipulation","summary":"  Humans interpret scenes by recognizing both the identities and positions of\nobjects in their observations. For a robot to perform tasks such as\n\\enquote{pick and place}, understanding both what the objects are and where\nthey are located is crucial. While the former has been extensively discussed in\nthe literature that uses the large language model to enrich the text\ndescriptions, the latter remains underexplored. In this work, we introduce the\n\\textit{Object-Centric Instruction Augmentation (OCI)} framework to augment\nhighly semantic and information-dense language instruction with position cues.\nWe utilize a Multi-modal Large Language Model (MLLM) to weave knowledge of\nobject locations into natural language instruction, thus aiding the policy\nnetwork in mastering actions for versatile manipulation. Additionally, we\npresent a feature reuse mechanism to integrate the vision-language features\nfrom off-the-shelf pre-trained MLLM into policy networks. Through a series of\nsimulated and real-world robotic tasks, we demonstrate that robotic manipulator\nimitation policies trained with our enhanced instructions outperform those\nrelying solely on traditional language instructions.\n","authors":["Junjie Wen","Yichen Zhu","Minjie Zhu","Jinming Li","Zhiyuan Xu","Zhengping Che","Chaomin Shen","Yaxin Peng","Dong Liu","Feifei Feng","Jian Tang"],"pdf_url":"https://arxiv.org/pdf/2401.02814v1.pdf","comment":"Submitted to ICRA2024"},{"id":"http://arxiv.org/abs/2401.02786v1","updated":"2024-01-05T12:51:17Z","published":"2024-01-05T12:51:17Z","title":"Kinematic Base State Estimation for Humanoid using Invariant Extended\n  Kalman Filter","summary":"  This paper presents the design and implementation of a Right Invariant\nExtended Kalman Filter (RIEKF) for estimating the states of the kinematic base\nof the Surena V humanoid robot. The state representation of the robot is\ndefined on the Lie group $SE_4(3)$, encompassing the position, velocity, and\norientation of the base, as well as the position of the left and right feet. In\naddition, we incorporated IMU biases as concatenated states within the filter.\n  The prediction step of the RIEKF utilizes IMU equations, while the update\nstep incorporates forward kinematics. To evaluate the performance of the RIEKF,\nwe conducted experiments using the Choreonoid dynamic simulation framework and\ncompared it against a Quaternion-based Extended Kalman Filter (QEKF). The\nresults of the analysis demonstrate that the RIEKF exhibits reduced drift in\nlocalization and achieves estimation convergence in a shorter time compared to\nthe QEKF. These findings highlight the effectiveness of the proposed RIEKF for\naccurate state estimation of the kinematic base in humanoid robotics.\n","authors":["Amirhosein Vedadi","Aghil Yousefi-Koma","Masoud Shariat-Panahi","Mahdi Nozari"],"pdf_url":"https://arxiv.org/pdf/2401.02786v1.pdf","comment":"7 pages, 11th RSI International Conference on Robotics and\n  Mechatronics (ICRoM 2023)"},{"id":"http://arxiv.org/abs/2212.00344v3","updated":"2024-01-05T12:45:02Z","published":"2022-12-01T08:14:59Z","title":"Bayesian Heuristics for Robust Spatial Perception","summary":"  Spatial perception is a key task in several machine intelligence applications\nsuch as robotics and computer vision. In general, it involves the nonlinear\nestimation of hidden variables that represent the system's state. However, in\nthe presence of measurement outliers, the standard nonlinear least squared\nformulation results in poor estimates. Several methods have been considered in\nthe literature to improve the reliability of the estimation process. Most\nmethods are based on heuristics since guaranteed global robust estimation is\nnot generally practical due to high computational costs. Recently general\npurpose robust estimation heuristics have been proposed that leverage existing\nnon-minimal solvers available for the outlier-free formulations without the\nneed for an initial guess. In this work, we propose three Bayesian heuristics\nthat have similar structures. We evaluate these heuristics in practical\nscenarios to demonstrate their merits in different applications including 3D\npoint cloud registration, mesh registration and pose graph optimization. The\ngeneral computational advantages our proposals offer make them attractive\ncandidates for spatial perception tasks.\n","authors":["Aamir Hussain Chughtai","Muhammad Tahir","Momin Uppal"],"pdf_url":"https://arxiv.org/pdf/2212.00344v3.pdf","comment":"10 pages, 8 figures"},{"id":"http://arxiv.org/abs/2401.01081v2","updated":"2024-01-05T12:01:37Z","published":"2024-01-02T07:54:40Z","title":"PLE-SLAM: A Visual-Inertial SLAM Based on Point-Line Features and\n  Efficient IMU Initialization","summary":"  Visual-inertial SLAM is crucial in various fields, such as aerial vehicles,\nindustrial robots, and autonomous driving. The fusion of camera and inertial\nmeasurement unit (IMU) makes up for the shortcomings of a signal sensor, which\nsignificantly improves the accuracy and robustness of localization in\nchallenging environments. This article presents PLE-SLAM, an accurate and\nreal-time visual-inertial SLAM algorithm based on point-line features and\nefficient IMU initialization. First, we use parallel computing methods to\nextract features and compute descriptors to ensure real-time performance.\nAdjacent short line segments are merged into long line segments, and isolated\nshort line segments are directly deleted. Second, a\nrotation-translation-decoupled initialization method is extended to use both\npoints and lines. Gyroscope bias is optimized by tightly coupling IMU\nmeasurements and image observations. Accelerometer bias and gravity direction\nare solved by an analytical method for efficiency. To improve the system's\nintelligence in handling complex environments, a scheme of leveraging semantic\ninformation and geometric constraints to eliminate dynamic features and A\nsolution for loop detection and closed-loop frame pose estimation using CNN and\nGNN are integrated into the system. All networks are accelerated to ensure\nreal-time performance. The experiment results on public datasets illustrate\nthat PLE-SLAM is one of the state-of-the-art visual-inertial SLAM systems.\n","authors":["Jiaming He","Mingrui Li","Yangyang Wang","Hongyu Wang"],"pdf_url":"https://arxiv.org/pdf/2401.01081v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02730v1","updated":"2024-01-05T09:54:33Z","published":"2024-01-05T09:54:33Z","title":"Design Optimization of Wire Arrangement with Variable Relay Points in\n  Numerical Simulation for Tendon-driven Robots","summary":"  One of the most important features of tendon-driven robots is the ease of\nwire arrangement and the degree of freedom it affords, enabling the\nconstruction of a body that satisfies the desired characteristics by modifying\nthe wire arrangement. Various wire arrangement optimization methods have been\nproposed, but they have simplified the configuration by assuming that the\nmoment arm of wires to joints are constant, or by disregarding wire\narrangements that span multiple joints and include relay points. In this study,\nwe formulate a more flexible wire arrangement optimization problem in which\neach wire is represented by a start point, multiple relay points, and an end\npoint, and achieve the desired physical performance based on black-box\noptimization. We consider a multi-objective optimization which simultaneously\ntakes into account both the feasible operational force space and velocity\nspace, and discuss the optimization results obtained from various\nconfigurations.\n","authors":["Kento Kawaharazuka","Shunnosuke Yoshimura","Temma Suzuki","Kei Okada","Masayuki Inaba"],"pdf_url":"https://arxiv.org/pdf/2401.02730v1.pdf","comment":"accepted at IEEE Robotics and Automation Letters (RA-L), website -\n  https://haraduka.github.io/muscle-arrange-optimization/"},{"id":"http://arxiv.org/abs/2401.02695v1","updated":"2024-01-05T08:05:07Z","published":"2024-01-05T08:05:07Z","title":"VoroNav: Voronoi-based Zero-shot Object Navigation with Large Language\n  Model","summary":"  In the realm of household robotics, the Zero-Shot Object Navigation (ZSON)\ntask empowers agents to adeptly traverse unfamiliar environments and locate\nobjects from novel categories without prior explicit training. This paper\nintroduces VoroNav, a novel semantic exploration framework that proposes the\nReduced Voronoi Graph to extract exploratory paths and planning nodes from a\nsemantic map constructed in real time. By harnessing topological and semantic\ninformation, VoroNav designs text-based descriptions of paths and images that\nare readily interpretable by a large language model (LLM). Our approach\npresents a synergy of path and farsight descriptions to represent the\nenvironmental context, enabling the LLM to apply commonsense reasoning to\nascertain the optimal waypoints for navigation. Extensive evaluation on the\nHM3D and HSSD datasets validates that VoroNav surpasses existing ZSON\nbenchmarks in both success rates and exploration efficiency (+2.8% Success and\n+3.7% SPL on HM3D, +2.6% Success and +3.8% SPL on HSSD). Additionally\nintroduced metrics that evaluate obstacle avoidance proficiency and perceptual\nefficiency further corroborate the enhancements achieved by our method in ZSON\nplanning.\n","authors":["Pengying Wu","Yao Mu","Bingxian Wu","Yi Hou","Ji Ma","Shanghang Zhang","Chang Liu"],"pdf_url":"https://arxiv.org/pdf/2401.02695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02684v1","updated":"2024-01-05T07:35:29Z","published":"2024-01-05T07:35:29Z","title":"Robot Vulnerability and the Elicitation of User Empathy","summary":"  This paper describes a between-subjects Amazon Mechanical Turk study (n =\n220) that investigated how a robot's affective narrative influences its ability\nto elicit empathy in human observers. We first conducted a pilot study to\ndevelop and validate the robot's affective narratives. Then, in the full study,\nthe robot used one of three different affective narrative strategies (funny,\nsad, neutral) while becoming less functional at its shopping task over the\ncourse of the interaction. As the functionality of the robot degraded,\nparticipants were repeatedly asked if they were willing to help the robot. The\nresults showed that conveying a sad narrative significantly influenced the\nparticipants' willingness to help the robot throughout the interaction and\ndetermined whether participants felt empathetic toward the robot throughout the\ninteraction. Furthermore, a higher amount of past experience with robots also\nincreased the participants' willingness to help the robot. This work suggests\nthat affective narratives can be useful in short-term interactions that benefit\nfrom emotional connections between humans and robots.\n","authors":["Morten Roed Frederiksen","Katrin Fischer","Maja Matarić"],"pdf_url":"https://arxiv.org/pdf/2401.02684v1.pdf","comment":"Published by and copyright protected by IEEE, 8 pages, 4 figures,\n  31st IEEE International Conference on Robot & Human Interactive Communication\n  (RO-MAN 2022)"},{"id":"http://arxiv.org/abs/2401.03088v1","updated":"2024-01-05T23:28:40Z","published":"2024-01-05T23:28:40Z","title":"The RoSiD Tool: Empowering Users to Design Multimodal Signals for\n  Human-Robot Collaboration","summary":"  Robots that cooperate with humans must be effective at communicating with\nthem. However, people have varied preferences for communication based on many\ncontextual factors, such as culture, environment, and past experience. To\ncommunicate effectively, robots must take those factors into consideration. In\nthis work, we present the Robot Signal Design (RoSiD) tool to empower people to\neasily self-specify communicative preferences for collaborative robots. We show\nthrough a participatory design study that the RoSiD tool enables users to\ncreate signals that align with their communicative preferences, and we\nilluminate how this tool can be further improved.\n","authors":["Nathaniel Dennler","David Delgado","Daniel Zeng","Stefanos Nikolaidis","Maja Matarić"],"pdf_url":"https://arxiv.org/pdf/2401.03088v1.pdf","comment":"Accepted to ISER 2023. 8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2401.03079v1","updated":"2024-01-05T22:37:52Z","published":"2024-01-05T22:37:52Z","title":"Integrating Open-World Shared Control in Immersive Avatars","summary":"  Teleoperated avatar robots allow people to transport their manipulation\nskills to environments that may be difficult or dangerous to work in. Current\nsystems are able to give operators direct control of many components of the\nrobot to immerse them in the remote environment, but operators still struggle\nto complete tasks as competently as they could in person. We present a\nframework for incorporating open-world shared control into avatar robots to\ncombine the benefits of direct and shared control. This framework preserves the\nfluency of our avatar interface by minimizing obstructions to the operator's\nview and using the same interface for direct, shared, and fully autonomous\ncontrol. In a human subjects study (N=19), we find that operators using this\nframework complete a range of tasks significantly more quickly and reliably\nthan those that do not.\n","authors":["Patrick Naughton","James Seungbum Nam","Andrew Stratton","Kris Hauser"],"pdf_url":"https://arxiv.org/pdf/2401.03079v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03071v1","updated":"2024-01-05T21:34:31Z","published":"2024-01-05T21:34:31Z","title":"Software Implementation of Digital Filtering via Tustin's Bilinear\n  Transform","summary":"  The purpose of this work is to provide some notes on a software\nimplementation for digital filtering via Tustins Bilinear Transform. The first\nsection discusses how to solve for the input and output coefficients by hand\nusing a generalized approach called Horners method. The second section presents\nsome results of this generalized digital filtering approach using the IHMC Open\nRobotics Software stack and Simulation Construction Set 2. This generalized\napproach can solve for the digital coefficients for any causal transfer\nfunction.\n","authors":["Connor W. Herron"],"pdf_url":"https://arxiv.org/pdf/2401.03071v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2310.13831v3","updated":"2024-01-05T19:10:56Z","published":"2023-10-20T22:10:00Z","title":"Transformers for Trajectory Optimization with Application to Spacecraft\n  Rendezvous","summary":"  Reliable and efficient trajectory optimization methods are a fundamental need\nfor autonomous dynamical systems, effectively enabling applications including\nrocket landing, hypersonic reentry, spacecraft rendezvous, and docking. Within\nsuch safety-critical application areas, the complexity of the emerging\ntrajectory optimization problems has motivated the application of AI-based\ntechniques to enhance the performance of traditional approaches. However,\ncurrent AI-based methods either attempt to fully replace traditional control\nalgorithms, thus lacking constraint satisfaction guarantees and incurring in\nexpensive simulation, or aim to solely imitate the behavior of traditional\nmethods via supervised learning. To address these limitations, this paper\nproposes the Autonomous Rendezvous Transformer (ART) and assesses the\ncapability of modern generative models to solve complex trajectory optimization\nproblems, both from a forecasting and control standpoint. Specifically, this\nwork assesses the capabilities of Transformers to (i) learn near-optimal\npolicies from previously collected data, and (ii) warm-start a sequential\noptimizer for the solution of non-convex optimal control problems, thus\nguaranteeing hard constraint satisfaction. From a forecasting perspective,\nresults highlight how ART outperforms other learning-based architectures at\npredicting known fuel-optimal trajectories. From a control perspective,\nempirical analyses show how policies learned through Transformers are able to\ngenerate near-optimal warm-starts, achieving trajectories that are (i) more\nfuel-efficient, (ii) obtained in fewer sequential optimizer iterations, and\n(iii) computed with an overall runtime comparable to benchmarks based on convex\noptimization.\n","authors":["Tommaso Guffanti","Daniele Gammelli","Simone D'Amico","Marco Pavone"],"pdf_url":"https://arxiv.org/pdf/2310.13831v3.pdf","comment":"Presented in 2024 IEEE Aerospace Conference"}],"Multimedia":[{"id":"http://arxiv.org/abs/2401.02838v1","updated":"2024-01-05T14:45:45Z","published":"2024-01-05T14:45:45Z","title":"CrisisViT: A Robust Vision Transformer for Crisis Image Classification","summary":"  In times of emergency, crisis response agencies need to quickly and\naccurately assess the situation on the ground in order to deploy relevant\nservices and resources. However, authorities often have to make decisions based\non limited information, as data on affected regions can be scarce until local\nresponse services can provide first-hand reports. Fortunately, the widespread\navailability of smartphones with high-quality cameras has made citizen\njournalism through social media a valuable source of information for crisis\nresponders. However, analyzing the large volume of images posted by citizens\nrequires more time and effort than is typically available. To address this\nissue, this paper proposes the use of state-of-the-art deep neural models for\nautomatic image classification/tagging, specifically by adapting\ntransformer-based architectures for crisis image classification (CrisisViT). We\nleverage the new Incidents1M crisis image dataset to develop a range of new\ntransformer-based image classification models. Through experimentation over the\nstandard Crisis image benchmark dataset, we demonstrate that the CrisisViT\nmodels significantly outperform previous approaches in emergency type, image\nrelevance, humanitarian category, and damage severity classification.\nAdditionally, we show that the new Incidents1M dataset can further augment the\nCrisisViT models resulting in an additional 1.25% absolute accuracy gain.\n","authors":["Zijun Long","Richard McCreadie","Muhammad Imran"],"pdf_url":"https://arxiv.org/pdf/2401.02838v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.08051v2","updated":"2024-01-05T14:10:51Z","published":"2023-09-14T22:35:39Z","title":"Retrieval-Augmented Text-to-Audio Generation","summary":"  Despite recent progress in text-to-audio (TTA) generation, we show that the\nstate-of-the-art models, such as AudioLDM, trained on datasets with an\nimbalanced class distribution, such as AudioCaps, are biased in their\ngeneration performance. Specifically, they excel in generating common audio\nclasses while underperforming in the rare ones, thus degrading the overall\ngeneration performance. We refer to this problem as long-tailed text-to-audio\ngeneration. To address this issue, we propose a simple retrieval-augmented\napproach for TTA models. Specifically, given an input text prompt, we first\nleverage a Contrastive Language Audio Pretraining (CLAP) model to retrieve\nrelevant text-audio pairs. The features of the retrieved audio-text data are\nthen used as additional conditions to guide the learning of TTA models. We\nenhance AudioLDM with our proposed approach and denote the resulting augmented\nsystem as Re-AudioLDM. On the AudioCaps dataset, Re-AudioLDM achieves a\nstate-of-the-art Frechet Audio Distance (FAD) of 1.37, outperforming the\nexisting approaches by a large margin. Furthermore, we show that Re-AudioLDM\ncan generate realistic audio for complex scenes, rare audio classes, and even\nunseen audio types, indicating its potential in TTA tasks.\n","authors":["Yi Yuan","Haohe Liu","Xubo Liu","Qiushi Huang","Mark D. Plumbley","Wenwu Wang"],"pdf_url":"https://arxiv.org/pdf/2309.08051v2.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2401.02678v1","updated":"2024-01-05T07:24:07Z","published":"2024-01-05T07:24:07Z","title":"MusicAOG: an Energy-Based Model for Learning and Sampling a Hierarchical\n  Representation of Symbolic Music","summary":"  In addressing the challenge of interpretability and generalizability of\nartificial music intelligence, this paper introduces a novel symbolic\nrepresentation that amalgamates both explicit and implicit musical information\nacross diverse traditions and granularities. Utilizing a hierarchical and-or\ngraph representation, the model employs nodes and edges to encapsulate a broad\nspectrum of musical elements, including structures, textures, rhythms, and\nharmonies. This hierarchical approach expands the representability across\nvarious scales of music. This representation serves as the foundation for an\nenergy-based model, uniquely tailored to learn musical concepts through a\nflexible algorithm framework relying on the minimax entropy principle.\nUtilizing an adapted Metropolis-Hastings sampling technique, the model enables\nfine-grained control over music generation. A comprehensive empirical\nevaluation, contrasting this novel approach with existing methodologies,\nmanifests considerable advancements in interpretability and controllability.\nThis study marks a substantial contribution to the fields of music analysis,\ncomposition, and computational musicology.\n","authors":["Yikai Qian","Tianle Wang","Xinyi Tong","Xin Jin","Duo Xu","Bo Zheng","Tiezheng Ge","Feng Yu","Song-Chun Zhu"],"pdf_url":"https://arxiv.org/pdf/2401.02678v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02614v1","updated":"2024-01-05T03:12:03Z","published":"2024-01-05T03:12:03Z","title":"Scaling and Masking: A New Paradigm of Data Sampling for Image and Video\n  Quality Assessment","summary":"  Quality assessment of images and videos emphasizes both local details and\nglobal semantics, whereas general data sampling methods (e.g., resizing,\ncropping or grid-based fragment) fail to catch them simultaneously. To address\nthe deficiency, current approaches have to adopt multi-branch models and take\nas input the multi-resolution data, which burdens the model complexity. In this\nwork, instead of stacking up models, a more elegant data sampling method (named\nas SAMA, scaling and masking) is explored, which compacts both the local and\nglobal content in a regular input size. The basic idea is to scale the data\ninto a pyramid first, and reduce the pyramid into a regular data dimension with\na masking strategy. Benefiting from the spatial and temporal redundancy in\nimages and videos, the processed data maintains the multi-scale characteristics\nwith a regular input size, thus can be processed by a single-branch model. We\nverify the sampling method in image and video quality assessment. Experiments\nshow that our sampling method can improve the performance of current\nsingle-branch models significantly, and achieves competitive performance to the\nmulti-branch models without extra model complexity. The source code will be\navailable at https://github.com/Sissuire/SAMA.\n","authors":["Yongxu Liu","Yinghui Quan","Guoyao Xiao","Aobo Li","Jinjian Wu"],"pdf_url":"https://arxiv.org/pdf/2401.02614v1.pdf","comment":"Accepted by AAAI2024. Code has been released at\n  https://github.com/Sissuire/SAMA"},{"id":"http://arxiv.org/abs/2401.02309v2","updated":"2024-01-05T03:11:28Z","published":"2024-01-04T14:55:57Z","title":"TR-DETR: Task-Reciprocal Transformer for Joint Moment Retrieval and\n  Highlight Detection","summary":"  Video moment retrieval (MR) and highlight detection (HD) based on natural\nlanguage queries are two highly related tasks, which aim to obtain relevant\nmoments within videos and highlight scores of each video clip. Recently,\nseveral methods have been devoted to building DETR-based networks to solve both\nMR and HD jointly. These methods simply add two separate task heads after\nmulti-modal feature extraction and feature interaction, achieving good\nperformance. Nevertheless, these approaches underutilize the reciprocal\nrelationship between two tasks. In this paper, we propose a task-reciprocal\ntransformer based on DETR (TR-DETR) that focuses on exploring the inherent\nreciprocity between MR and HD. Specifically, a local-global multi-modal\nalignment module is first built to align features from diverse modalities into\na shared latent space. Subsequently, a visual feature refinement is designed to\neliminate query-irrelevant information from visual features for modal\ninteraction. Finally, a task cooperation module is constructed to refine the\nretrieval pipeline and the highlight score prediction process by utilizing the\nreciprocity between MR and HD. Comprehensive experiments on QVHighlights,\nCharades-STA and TVSum datasets demonstrate that TR-DETR outperforms existing\nstate-of-the-art methods. Codes are available at\n\\url{https://github.com/mingyao1120/TR-DETR}.\n","authors":["Hao Sun","Mingyao Zhou","Wenjing Chen","Wei Xie"],"pdf_url":"https://arxiv.org/pdf/2401.02309v2.pdf","comment":"Accepted by AAAI-24"},{"id":"http://arxiv.org/abs/2311.11255v3","updated":"2024-01-05T02:17:34Z","published":"2023-11-19T06:50:52Z","title":"M$^{2}$UGen: Multi-modal Music Understanding and Generation with the\n  Power of Large Language Models","summary":"  The current landscape of research leveraging large language models (LLMs) is\nexperiencing a surge. Many works harness the powerful reasoning capabilities of\nthese models to comprehend various modalities, such as text, speech, images,\nvideos, etc. They also utilize LLMs to understand human intention and generate\ndesired outputs like images, videos, and music. However, research that combines\nboth understanding and generation using LLMs is still limited and in its\nnascent stage. To address this gap, we introduce a Multi-modal Music\nUnderstanding and Generation (M$^{2}$UGen) framework that integrates LLM's\nabilities to comprehend and generate music for different modalities. The\nM$^{2}$UGen framework is purpose-built to unlock creative potential from\ndiverse sources of inspiration, encompassing music, image, and video through\nthe use of pretrained MERT, ViT, and ViViT models, respectively. To enable\nmusic generation, we explore the use of AudioLDM 2 and MusicGen. Bridging\nmulti-modal understanding and music generation is accomplished through the\nintegration of the LLaMA 2 model. Furthermore, we make use of the MU-LLaMA\nmodel to generate extensive datasets that support text/image/video-to-music\ngeneration, facilitating the training of our M$^{2}$UGen framework. We conduct\na thorough evaluation of our proposed framework. The experimental results\ndemonstrate that our model achieves or surpasses the performance of the current\nstate-of-the-art models.\n","authors":["Atin Sakkeer Hussain","Shansong Liu","Chenshuo Sun","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2311.11255v3.pdf","comment":null}]},"2024-01-08T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2401.04088v1","updated":"2024-01-08T18:47:34Z","published":"2024-01-08T18:47:34Z","title":"Mixtral of Experts","summary":"  We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model.\nMixtral has the same architecture as Mistral 7B, with the difference that each\nlayer is composed of 8 feedforward blocks (i.e. experts). For every token, at\neach layer, a router network selects two experts to process the current state\nand combine their outputs. Even though each token only sees two experts, the\nselected experts can be different at each timestep. As a result, each token has\naccess to 47B parameters, but only uses 13B active parameters during inference.\nMixtral was trained with a context size of 32k tokens and it outperforms or\nmatches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular,\nMixtral vastly outperforms Llama 2 70B on mathematics, code generation, and\nmultilingual benchmarks. We also provide a model fine-tuned to follow\ninstructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo,\nClaude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both\nthe base and instruct models are released under the Apache 2.0 license.\n","authors":["Albert Q. Jiang","Alexandre Sablayrolles","Antoine Roux","Arthur Mensch","Blanche Savary","Chris Bamford","Devendra Singh Chaplot","Diego de las Casas","Emma Bou Hanna","Florian Bressand","Gianna Lengyel","Guillaume Bour","Guillaume Lample","Lélio Renard Lavaud","Lucile Saulnier","Marie-Anne Lachaux","Pierre Stock","Sandeep Subramanian","Sophia Yang","Szymon Antoniak","Teven Le Scao","Théophile Gervet","Thibaut Lavril","Thomas Wang","Timothée Lacroix","William El Sayed"],"pdf_url":"https://arxiv.org/pdf/2401.04088v1.pdf","comment":"See more details at https://mistral.ai/news/mixtral-of-experts/"},{"id":"http://arxiv.org/abs/2401.04081v1","updated":"2024-01-08T18:35:07Z","published":"2024-01-08T18:35:07Z","title":"MoE-Mamba: Efficient Selective State Space Models with Mixture of\n  Experts","summary":"  State Space Models (SSMs) have become serious contenders in the field of\nsequential modeling, challenging the dominance of Transformers. At the same\ntime, Mixture of Experts (MoE) has significantly improved Transformer-based\nLLMs, including recent state-of-the-art open-source models. We propose that to\nunlock the potential of SSMs for scaling, they should be combined with MoE. We\nshowcase this on Mamba, a recent SSM-based model that achieves remarkable,\nTransformer-like performance. Our model, MoE-Mamba, outperforms both Mamba and\nTransformer-MoE. In particular, MoE-Mamba reaches the same performance as Mamba\nin 2.2x less training steps while preserving the inference performance gains of\nMamba against the Transformer.\n","authors":["Maciej Pióro","Kamil Ciebiera","Krystian Król","Jan Ludziejewski","Sebastian Jaszczur"],"pdf_url":"https://arxiv.org/pdf/2401.04081v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11509v2","updated":"2024-01-08T17:57:53Z","published":"2023-12-12T04:58:11Z","title":"Toward A Reinforcement-Learning-Based System for Adjusting Medication to\n  Minimize Speech Disfluency","summary":"  We propose a Reinforcement-Learning-based system that would automatically\nprescribe a hypothetical patient medication that may help the patient with\ntheir mental-health-related speech disfluency, and adjust the medication and\nthe dosages in response to zero-cost frequent measurement of the fluency of the\npatient. We demonstrate the components of the system: a module that detects and\nevaluates speech disfluency on a large dataset we built, and a Reinforcement\nLearning algorithm that automatically finds good combinations of medications.\nTo support the two modules, we collect data on the effect of psychiatric\nmedications for speech disfluency from the literature, and build a plausible\npatient simulation system. We demonstrate that the Reinforcement Learning\nsystem is, under some circumstances, able to converge to a good medication\nregime. We collect and label a dataset of people with possible speech\ndisfluency and demonstrate our methods using that dataset. Our work is a proof\nof concept: we show that there is promise in the idea of using automatic data\ncollection to address disfluency.\n","authors":["Pavlos Constas","Vikram Rawal","Matthew Honorio Oliveira","Andreas Constas","Aditya Khan","Kaison Cheung","Najma Sultani","Carrie Chen","Micol Altomare","Michael Akzam","Jiacheng Chen","Vhea He","Lauren Altomare","Heraa Murqi","Asad Khan","Nimit Amikumar Bhanshali","Youssef Rachad","Michael Guerzhoy"],"pdf_url":"https://arxiv.org/pdf/2312.11509v2.pdf","comment":"In Proc. Machine Learning for Cognitive and Mental Health Workshop\n  (ML4CMH) at AAAI 2024"},{"id":"http://arxiv.org/abs/2401.04051v1","updated":"2024-01-08T17:44:43Z","published":"2024-01-08T17:44:43Z","title":"Empirical Analysis of Efficient Fine-Tuning Methods for Large\n  Pre-Trained Language Models","summary":"  Fine-tuning large pre-trained language models for downstream tasks remains a\ncritical challenge in natural language processing. This paper presents an\nempirical analysis comparing two efficient fine-tuning methods - BitFit and\nadapter modules - to standard full model fine-tuning. Experiments conducted on\nGLUE benchmark datasets (MRPC, COLA, STS-B) reveal several key insights. The\nBitFit approach, which trains only bias terms and task heads, matches full\nfine-tuning performance across varying amounts of training data and time\nconstraints. It demonstrates remarkable stability even with only 30\\% of data,\noutperforming full fine-tuning at intermediate data levels. Adapter modules\nexhibit high variability, with inconsistent gains over default models. The\nfindings indicate BitFit offers an attractive balance between performance and\nparameter efficiency. Our work provides valuable perspectives on model tuning,\nemphasizing robustness and highlighting BitFit as a promising alternative for\nresource-constrained or streaming task settings. The analysis offers actionable\nguidelines for efficient adaptation of large pre-trained models, while\nillustrating open challenges in stabilizing techniques like adapter modules.\n","authors":["Nigel Doering","Cyril Gorlla","Trevor Tuttle","Adhvaith Vijay"],"pdf_url":"https://arxiv.org/pdf/2401.04051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.04044v1","updated":"2024-01-08T17:29:16Z","published":"2024-01-08T17:29:16Z","title":"FFSplit: Split Feed-Forward Network For Optimizing Accuracy-Efficiency\n  Trade-off in Language Model Inference","summary":"  The large number of parameters in Pretrained Language Models enhance their\nperformance, but also make them resource-intensive, making it challenging to\ndeploy them on commodity hardware like a single GPU. Due to the memory and\npower limitations of these devices, model compression techniques are often used\nto decrease both the model's size and its inference latency. This usually\nresults in a trade-off between model accuracy and efficiency. Therefore,\noptimizing this balance is essential for effectively deploying LLMs on\ncommodity hardware. A significant portion of the efficiency challenge is the\nFeed-forward network (FFN) component, which accounts for roughly $\\frac{2}{3}$\ntotal parameters and inference latency. In this paper, we first observe that\nonly a few neurons of FFN module have large output norm for any input tokens,\na.k.a. heavy hitters, while the others are sparsely triggered by different\ntokens. Based on this observation, we explicitly split the FFN into two parts\naccording to the heavy hitters. We improve the efficiency-accuracy trade-off of\nexisting compression methods by allocating more resource to FFN parts with\nheavy hitters. In practice, our method can reduce model size by 43.1\\% and\nbring $1.25\\sim1.56\\times$ wall clock time speedup on different hardware with\nnegligible accuracy drop.\n","authors":["Zirui Liu","Qingquan Song","Qiang Charles Xiao","Sathiya Keerthi Selvaraj","Rahul Mazumder","Aman Gupta","Xia Hu"],"pdf_url":"https://arxiv.org/pdf/2401.04044v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.04025v1","updated":"2024-01-08T17:07:37Z","published":"2024-01-08T17:07:37Z","title":"IDoFew: Intermediate Training Using Dual-Clustering in Language Models\n  for Few Labels Text Classification","summary":"  Language models such as Bidirectional Encoder Representations from\nTransformers (BERT) have been very effective in various Natural Language\nProcessing (NLP) and text mining tasks including text classification. However,\nsome tasks still pose challenges for these models, including text\nclassification with limited labels. This can result in a cold-start problem.\nAlthough some approaches have attempted to address this problem through\nsingle-stage clustering as an intermediate training step coupled with a\npre-trained language model, which generates pseudo-labels to improve\nclassification, these methods are often error-prone due to the limitations of\nthe clustering algorithms. To overcome this, we have developed a novel\ntwo-stage intermediate clustering with subsequent fine-tuning that models the\npseudo-labels reliably, resulting in reduced prediction errors. The key novelty\nin our model, IDoFew, is that the two-stage clustering coupled with two\ndifferent clustering algorithms helps exploit the advantages of the\ncomplementary algorithms that reduce the errors in generating reliable\npseudo-labels for fine-tuning. Our approach has shown significant improvements\ncompared to strong comparative models.\n","authors":["Abdullah Alsuhaibani","Hamad Zogan","Imran Razzak","Shoaib Jameel","Guandong Xu"],"pdf_url":"https://arxiv.org/pdf/2401.04025v1.pdf","comment":"Published in The 17th ACM International Conference on Web Search and\n  Data Mining"},{"id":"http://arxiv.org/abs/2401.01286v2","updated":"2024-01-08T16:25:04Z","published":"2024-01-02T16:54:58Z","title":"A Comprehensive Study of Knowledge Editing for Large Language Models","summary":"  Large Language Models (LLMs) have shown extraordinary capabilities in\nunderstanding and generating text that closely mirrors human communication.\nHowever, a primary limitation lies in the significant computational demands\nduring training, arising from their extensive parameterization. This challenge\nis further intensified by the dynamic nature of the world, necessitating\nfrequent updates to LLMs to correct outdated information or integrate new\nknowledge, thereby ensuring their continued relevance. Note that many\napplications demand continual model adjustments post-training to address\ndeficiencies or undesirable behaviors. There is an increasing interest in\nefficient, lightweight methods for on-the-fly model modifications. To this end,\nrecent years have seen a burgeoning in the techniques of knowledge editing for\nLLMs, which aim to efficiently modify LLMs' behaviors within specific domains\nwhile preserving overall performance across various inputs. In this paper, we\nfirst define the knowledge editing problem and then provide a comprehensive\nreview of cutting-edge approaches. Drawing inspiration from educational and\ncognitive research theories, we propose a unified categorization criterion that\nclassifies knowledge editing methods into three groups: resorting to external\nknowledge, merging knowledge into the model, and editing intrinsic knowledge.\nFurthermore, we introduce a new benchmark, KnowEdit, for a comprehensive\nempirical evaluation of representative knowledge editing approaches.\nAdditionally, we provide an in-depth analysis of knowledge location, which can\nprovide a deeper understanding of the knowledge structures inherent within\nLLMs. Finally, we discuss several potential applications of knowledge editing,\noutlining its broad and impactful implications.\n","authors":["Ningyu Zhang","Yunzhi Yao","Bozhong Tian","Peng Wang","Shumin Deng","Mengru Wang","Zekun Xi","Shengyu Mao","Jintian Zhang","Yuansheng Ni","Siyuan Cheng","Ziwen Xu","Xin Xu","Jia-Chen Gu","Yong Jiang","Pengjun Xie","Fei Huang","Lei Liang","Zhiqiang Zhang","Xiaowei Zhu","Jun Zhou","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2401.01286v2.pdf","comment":"Ongoing work; 52 pages, 280 citations; benchmark is available at\n  https://huggingface.co/datasets/zjunlp/KnowEdit code is available at\n  https://github.com/zjunlp/EasyEdit paper list is available at\n  https://github.com/zjunlp/KnowledgeEditingPapers"},{"id":"http://arxiv.org/abs/2401.00812v2","updated":"2024-01-08T16:22:42Z","published":"2024-01-01T16:51:20Z","title":"If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code\n  Empowers Large Language Models to Serve as Intelligent Agents","summary":"  The prominent large language models (LLMs) of today differ from past language\nmodels not only in size, but also in the fact that they are trained on a\ncombination of natural language and formal language (code). As a medium between\nhumans and computers, code translates high-level goals into executable steps,\nfeaturing standard syntax, logical consistency, abstraction, and modularity. In\nthis survey, we present an overview of the various benefits of integrating code\ninto LLMs' training data. Specifically, beyond enhancing LLMs in code\ngeneration, we observe that these unique properties of code help (i) unlock the\nreasoning ability of LLMs, enabling their applications to a range of more\ncomplex natural language tasks; (ii) steer LLMs to produce structured and\nprecise intermediate steps, which can then be connected to external execution\nends through function calls; and (iii) take advantage of code compilation and\nexecution environment, which also provides diverse feedback for model\nimprovement. In addition, we trace how these profound capabilities of LLMs,\nbrought by code, have led to their emergence as intelligent agents (IAs) in\nsituations where the ability to understand instructions, decompose goals, plan\nand execute actions, and refine from feedback are crucial to their success on\ndownstream tasks. Finally, we present several key challenges and future\ndirections of empowering LLMs with code.\n","authors":["Ke Yang","Jiateng Liu","John Wu","Chaoqi Yang","Yi R. Fung","Sha Li","Zixuan Huang","Xu Cao","Xingyao Wang","Yiquan Wang","Heng Ji","Chengxiang Zhai"],"pdf_url":"https://arxiv.org/pdf/2401.00812v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01313v3","updated":"2024-01-08T16:19:17Z","published":"2024-01-02T17:56:30Z","title":"A Comprehensive Survey of Hallucination Mitigation Techniques in Large\n  Language Models","summary":"  As Large Language Models (LLMs) continue to advance in their ability to write\nhuman-like text, a key challenge remains around their tendency to hallucinate\ngenerating content that appears factual but is ungrounded. This issue of\nhallucination is arguably the biggest hindrance to safely deploying these\npowerful LLMs into real-world production systems that impact people's lives.\nThe journey toward widespread adoption of LLMs in practical settings heavily\nrelies on addressing and mitigating hallucinations. Unlike traditional AI\nsystems focused on limited tasks, LLMs have been exposed to vast amounts of\nonline text data during training. While this allows them to display impressive\nlanguage fluency, it also means they are capable of extrapolating information\nfrom the biases in training data, misinterpreting ambiguous prompts, or\nmodifying the information to align superficially with the input. This becomes\nhugely alarming when we rely on language generation capabilities for sensitive\napplications, such as summarizing medical records, financial analysis reports,\netc. This paper presents a comprehensive survey of over 32 techniques developed\nto mitigate hallucination in LLMs. Notable among these are Retrieval Augmented\nGeneration (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023),\nCoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we\nintroduce a detailed taxonomy categorizing these methods based on various\nparameters, such as dataset utilization, common tasks, feedback mechanisms, and\nretriever types. This classification helps distinguish the diverse approaches\nspecifically designed to tackle hallucination issues in LLMs. Additionally, we\nanalyze the challenges and limitations inherent in these techniques, providing\na solid foundation for future research in addressing hallucinations and related\nphenomena within the realm of LLMs.\n","authors":["S. M Towhidul Islam Tonmoy","S M Mehedi Zaman","Vinija Jain","Anku Rani","Vipula Rawte","Aman Chadha","Amitava Das"],"pdf_url":"https://arxiv.org/pdf/2401.01313v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03991v1","updated":"2024-01-08T16:13:08Z","published":"2024-01-08T16:13:08Z","title":"Advancing Spatial Reasoning in Large Language Models: An In-Depth\n  Evaluation and Enhancement Using the StepGame Benchmark","summary":"  Artificial intelligence (AI) has made remarkable progress across various\ndomains, with large language models like ChatGPT gaining substantial attention\nfor their human-like text-generation capabilities. Despite these achievements,\nspatial reasoning remains a significant challenge for these models. Benchmarks\nlike StepGame evaluate AI spatial reasoning, where ChatGPT has shown\nunsatisfactory performance. However, the presence of template errors in the\nbenchmark has an impact on the evaluation results. Thus there is potential for\nChatGPT to perform better if these template errors are addressed, leading to\nmore accurate assessments of its spatial reasoning capabilities. In this study,\nwe refine the StepGame benchmark, providing a more accurate dataset for model\nevaluation. We analyze GPT's spatial reasoning performance on the rectified\nbenchmark, identifying proficiency in mapping natural language text to spatial\nrelations but limitations in multi-hop reasoning. We provide a flawless\nsolution to the benchmark by combining template-to-relation mapping with\nlogic-based reasoning. This combination demonstrates proficiency in performing\nqualitative reasoning on StepGame without encountering any errors. We then\naddress the limitations of GPT models in spatial reasoning. We deploy\nChain-of-thought and Tree-of-thoughts prompting strategies, offering insights\ninto GPT's ``cognitive process\", and achieving remarkable improvements in\naccuracy. Our investigation not only sheds light on model deficiencies but also\nproposes enhancements, contributing to the advancement of AI with more robust\nspatial reasoning capabilities.\n","authors":["Fangjun Li","David C. Hogg","Anthony G. Cohn"],"pdf_url":"https://arxiv.org/pdf/2401.03991v1.pdf","comment":"Camera-Ready version for AAAI 2024"},{"id":"http://arxiv.org/abs/2304.08242v2","updated":"2024-01-08T15:14:29Z","published":"2023-04-14T07:01:57Z","title":"The Deep Latent Position Topic Model for Clustering and Representation\n  of Networks with Textual Edges","summary":"  Numerical interactions leading to users sharing textual content published by\nothers are naturally represented by a network where the individuals are\nassociated with the nodes and the exchanged texts with the edges. To understand\nthose heterogeneous and complex data structures, clustering nodes into\nhomogeneous groups as well as rendering a comprehensible visualisation of the\ndata is mandatory. To address both issues, we introduce Deep-LPTM, a\nmodel-based clustering strategy relying on a variational graph auto-encoder\napproach as well as a probabilistic model to characterise the topics of\ndiscussion. Deep-LPTM allows to build a joint representation of the nodes and\nof the edges in two embeddings spaces. The parameters are inferred using a\nvariational inference algorithm. We also introduce IC2L, a model selection\ncriterion specifically designed to choose models with relevant clustering and\nvisualisation properties. An extensive benchmark study on synthetic data is\nprovided. In particular, we find that Deep-LPTM better recovers the partitions\nof the nodes than the state-of-the art ETSBM and STBM. Eventually, the emails\nof the Enron company are analysed and visualisations of the results are\npresented, with meaningful highlights of the graph structure.\n","authors":["Rémi Boutin","Pierre Latouche","Charles Bouveyron"],"pdf_url":"https://arxiv.org/pdf/2304.08242v2.pdf","comment":"29 pages including the appendix, 13 figures, 6 tables, journal paper"},{"id":"http://arxiv.org/abs/2401.03946v1","updated":"2024-01-08T15:05:32Z","published":"2024-01-08T15:05:32Z","title":"TextMachina: Seamless Generation of Machine-Generated Text Datasets","summary":"  Recent advancements in Large Language Models (LLMs) have led to high-quality\nMachine-Generated Text (MGT), giving rise to countless new use cases and\napplications. However, easy access to LLMs is posing new challenges due to\nmisuse. To address malicious usage, researchers have released datasets to\neffectively train models on MGT-related tasks. Similar strategies are used to\ncompile these datasets, but no tool currently unifies them. In this scenario,\nwe introduce TextMachina, a modular and extensible Python framework, designed\nto aid in the creation of high-quality, unbiased datasets to build robust\nmodels for MGT-related tasks such as detection, attribution, or boundary\ndetection. It provides a user-friendly pipeline that abstracts away the\ninherent intricacies of building MGT datasets, such as LLM integrations, prompt\ntemplating, and bias mitigation. The quality of the datasets generated by\nTextMachina has been assessed in previous works, including shared tasks where\nmore than one hundred teams trained robust MGT detectors.\n","authors":["Areg Mikael Sarvazyan","José Ángel González","Marc Franco-Salvador"],"pdf_url":"https://arxiv.org/pdf/2401.03946v1.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2401.03945v1","updated":"2024-01-08T15:01:08Z","published":"2024-01-08T15:01:08Z","title":"SpeechAgents: Human-Communication Simulation with Multi-Modal\n  Multi-Agent Systems","summary":"  Human communication is a complex and diverse process that not only involves\nmultiple factors such as language, commonsense, and cultural backgrounds but\nalso requires the participation of multimodal information, such as speech.\nLarge Language Model (LLM)-based multi-agent systems have demonstrated\npromising performance in simulating human society. Can we leverage LLM-based\nmulti-agent systems to simulate human communication? However, current LLM-based\nmulti-agent systems mainly rely on text as the primary medium. In this paper,\nwe propose SpeechAgents, a multi-modal LLM based multi-agent system designed\nfor simulating human communication. SpeechAgents utilizes multi-modal LLM as\nthe control center for individual agent and employes multi-modal signals as the\nmedium for exchanged messages among agents. Additionally, we propose\nMulti-Agent Tuning to enhance the multi-agent capabilities of LLM without\ncompromising general abilities. To strengthen and evaluate the effectiveness of\nhuman communication simulation, we build the Human-Communication Simulation\nBenchmark. Experimental results demonstrate that SpeechAgents can simulate\nhuman communication dialogues with consistent content, authentic rhythm, and\nrich emotions and demonstrate excellent scalability even with up to 25 agents,\nwhich can apply to tasks such as drama creation and audio novels generation.\nCode and models will be open-sourced at https://github.\ncom/0nutation/SpeechAgents\n","authors":["Dong Zhang","Zhaowei Li","Pengyu Wang","Xin Zhang","Yaqian Zhou","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2401.03945v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2308.04306v4","updated":"2024-01-08T14:32:56Z","published":"2023-08-08T14:51:16Z","title":"Deep Learning-Based Knowledge Injection for Metaphor Detection: A\n  Comprehensive Review","summary":"  Metaphor as an advanced cognitive modality works by extracting familiar\nconcepts in the target domain in order to understand vague and abstract\nconcepts in the source domain. This helps humans to quickly understand and\nmaster new domains and thus adapt to changing environments. With the continuous\ndevelopment of metaphor research in the natural language community, many\nstudies using knowledge-assisted models to detect textual metaphors have\nemerged in recent years. Compared to not using knowledge, systems that\nintroduce various kinds of knowledge achieve greater performance gains and\nreach SOTA in a recent study. Based on this, the goal of this paper is to\nprovide a comprehensive review of research advances in the application of deep\nlearning for knowledge injection in metaphor detection tasks. We will first\nsystematically summarize and generalize the mainstream knowledge and knowledge\ninjection principles. Then, the datasets, evaluation metrics, and benchmark\nmodels used in metaphor detection tasks are examined. Finally, we explore the\ncurrent issues facing knowledge injection methods and provide an outlook on\nfuture research directions.\n","authors":["Cheng Yang","Zheng Li","Zhiyue Liu","Qingbao Huang"],"pdf_url":"https://arxiv.org/pdf/2308.04306v4.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2311.13273v2","updated":"2024-01-08T14:19:29Z","published":"2023-11-22T09:51:43Z","title":"Comparative Experimentation of Accuracy Metrics in Automated Medical\n  Reporting: The Case of Otitis Consultations","summary":"  Generative Artificial Intelligence (AI) can be used to automatically generate\nmedical reports based on transcripts of medical consultations. The aim is to\nreduce the administrative burden that healthcare professionals face. The\naccuracy of the generated reports needs to be established to ensure their\ncorrectness and usefulness. There are several metrics for measuring the\naccuracy of AI generated reports, but little work has been done towards the\napplication of these metrics in medical reporting. A comparative\nexperimentation of 10 accuracy metrics has been performed on AI generated\nmedical reports against their corresponding General Practitioner's (GP) medical\nreports concerning Otitis consultations. The number of missing, incorrect, and\nadditional statements of the generated reports have been correlated with the\nmetric scores. In addition, we introduce and define a Composite Accuracy Score\nwhich produces a single score for comparing the metrics within the field of\nautomated medical reporting. Findings show that based on the correlation study\nand the Composite Accuracy Score, the ROUGE-L and Word Mover's Distance metrics\nare the preferred metrics, which is not in line with previous work. These\nfindings help determine the accuracy of an AI generated medical report, which\naids the development of systems that generate medical reports for GPs to reduce\nthe administrative burden.\n","authors":["Wouter Faber","Renske Eline Bootsma","Tom Huibers","Sandra van Dulmen","Sjaak Brinkkemper"],"pdf_url":"https://arxiv.org/pdf/2311.13273v2.pdf","comment":"10 pages, 1 figure, to be presented at HEALTHINF 2024, Author\n  contributions: Wouter Faber and Renske Eline Bootsma performed research and\n  wrote paper, Tom Huibers provided needed software and research inspiration,\n  Sandra van Dulmen provided the data and feedback on paper, Sjaak Brinkkemper\n  supervised the project and provided continuous feedback"},{"id":"http://arxiv.org/abs/2401.03910v1","updated":"2024-01-08T14:12:31Z","published":"2024-01-08T14:12:31Z","title":"A Philosophical Introduction to Language Models -- Part I: Continuity\n  With Classic Debates","summary":"  Large language models like GPT-4 have achieved remarkable proficiency in a\nbroad spectrum of language-based tasks, some of which are traditionally\nassociated with hallmarks of human intelligence. This has prompted ongoing\ndisagreements about the extent to which we can meaningfully ascribe any kind of\nlinguistic or cognitive competence to language models. Such questions have deep\nphilosophical roots, echoing longstanding debates about the status of\nartificial neural networks as cognitive models. This article -- the first part\nof two companion papers -- serves both as a primer on language models for\nphilosophers, and as an opinionated survey of their significance in relation to\nclassic debates in the philosophy cognitive science, artificial intelligence,\nand linguistics. We cover topics such as compositionality, language\nacquisition, semantic competence, grounding, world models, and the transmission\nof cultural knowledge. We argue that the success of language models challenges\nseveral long-held assumptions about artificial neural networks. However, we\nalso highlight the need for further empirical investigation to better\nunderstand their internal mechanisms. This sets the stage for the companion\npaper (Part II), which turns to novel empirical methods for probing the inner\nworkings of language models, and new philosophical questions prompted by their\nlatest developments.\n","authors":["Raphaël Millière","Cameron Buckner"],"pdf_url":"https://arxiv.org/pdf/2401.03910v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03905v1","updated":"2024-01-08T14:08:33Z","published":"2024-01-08T14:08:33Z","title":"WEBDial, a Multi-domain, Multitask Statistical Dialogue Framework with\n  RDF","summary":"  Typically available dialogue frameworks have adopted a semantic\nrepresentation based on dialogue-acts and slot-value pairs. Despite its\nsimplicity, this representation has disadvantages such as the lack of\nexpressivity, scalability and explainability. We present WEBDial: a dialogue\nframework that relies on a graph formalism by using RDF triples instead of\nslot-value pairs. We describe its overall architecture and the graph-based\nsemantic representation. We show its applicability from simple to complex\napplications, by varying the complexity of domains and tasks: from single\ndomain and tasks to multiple domains and complex tasks.\n","authors":["Morgan Veyret","Jean-Baptiste Duchene","Kekeli Afonouvi","Quentin Brabant","Gwenole Lecorve","Lina M. Rojas-Barahona"],"pdf_url":"https://arxiv.org/pdf/2401.03905v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03901v1","updated":"2024-01-08T14:01:59Z","published":"2024-01-08T14:01:59Z","title":"STAIR: Spatial-Temporal Reasoning with Auditable Intermediate Results\n  for Video Question Answering","summary":"  Recently we have witnessed the rapid development of video question answering\nmodels. However, most models can only handle simple videos in terms of temporal\nreasoning, and their performance tends to drop when answering\ntemporal-reasoning questions on long and informative videos. To tackle this\nproblem we propose STAIR, a Spatial-Temporal Reasoning model with Auditable\nIntermediate Results for video question answering. STAIR is a neural module\nnetwork, which contains a program generator to decompose a given question into\na hierarchical combination of several sub-tasks, and a set of lightweight\nneural modules to complete each of these sub-tasks. Though neural module\nnetworks are already widely studied on image-text tasks, applying them to\nvideos is a non-trivial task, as reasoning on videos requires different\nabilities. In this paper, we define a set of basic video-text sub-tasks for\nvideo question answering and design a set of lightweight modules to complete\nthem. Different from most prior works, modules of STAIR return intermediate\noutputs specific to their intentions instead of always returning attention\nmaps, which makes it easier to interpret and collaborate with pre-trained\nmodels. We also introduce intermediate supervision to make these intermediate\noutputs more accurate. We conduct extensive experiments on several video\nquestion answering datasets under various settings to show STAIR's performance,\nexplainability, compatibility with pre-trained models, and applicability when\nprogram annotations are not available. Code:\nhttps://github.com/yellow-binary-tree/STAIR\n","authors":["Yueqian Wang","Yuxuan Wang","Kai Chen","Dongyan Zhao"],"pdf_url":"https://arxiv.org/pdf/2401.03901v1.pdf","comment":"To appear in AAAI 2024"},{"id":"http://arxiv.org/abs/2307.15504v2","updated":"2024-01-08T13:26:37Z","published":"2023-07-28T12:00:13Z","title":"Exploring Format Consistency for Instruction Tuning","summary":"  Instruction tuning has emerged as a promising approach to enhancing large\nlanguage models in following human instructions. It is shown that increasing\nthe diversity and number of instructions in the training data can consistently\nenhance generalization performance, which facilitates a recent endeavor to\ncollect various instructions and integrate existing instruction tuning datasets\ninto larger collections. However, different users have their unique ways of\nexpressing instructions, and there often exist variations across different\ndatasets in the instruction styles and formats, i.e., format inconsistency. In\nthis work, we propose a framework named Unified Instruction Tuning (UIT), which\ncalls OpenAI APIs for automatic format transfer among different instruction\ntuning datasets such as PromptSource, FLAN and CrossFit. With the framework, we\n(1) demonstrate the necessity of maintaining format consistency in instruction\ntuning; (2) improve the generalization performance on unseen instructions on\nT5-LM-xl; (3) provide a novel perplexity-based denoising method to reduce the\nnoise of automatic format transfer to make the UIT framework more practical and\na smaller offline model based on GPT-J that achieves comparable format transfer\ncapability to OpenAI APIs to reduce costs in practice. Further analysis\nregarding variations of targeted formats and other effects is intended.\n","authors":["Shihao Liang","Runchu Tian","Kunlun Zhu","Yujia Qin","Huadong Wang","Xin Cong","Zhiyuan Liu","Xiaojiang Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2307.15504v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01119v2","updated":"2024-01-08T13:09:24Z","published":"2023-10-02T11:49:05Z","title":"Synthetic Data Generation in Low-Resource Settings via Fine-Tuning of\n  Large Language Models","summary":"  The in-context learning ability of large language models (LLMs) enables them\nto generalize to novel downstream tasks with relatively few labeled examples.\nHowever, they require enormous computational resources to be deployed.\nAlternatively, smaller models can solve specific tasks if fine-tuned with\nenough labeled examples. These examples, however, are expensive to obtain. In\npursuit of the best of both worlds, we study synthetic data generation of\nfine-tuning training data via fine-tuned teacher LLMs to improve the downstream\nperformance of much smaller models. In four text classification and two text\ngeneration tasks, we find that both data generation and annotation dramatically\nimprove the respective downstream model's performance, occasionally\nnecessitating only a minor fraction of the original training dataset.\n","authors":["Jean Kaddour","Qi Liu"],"pdf_url":"https://arxiv.org/pdf/2310.01119v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.08637v4","updated":"2024-01-08T13:02:04Z","published":"2023-09-14T15:34:01Z","title":"TextBind: Multi-turn Interleaved Multimodal Instruction-following in the\n  Wild","summary":"  Large language models with instruction-following abilities have\nrevolutionized the field of artificial intelligence. These models show\nexceptional generalizability to tackle various real-world tasks through their\nnatural language interfaces. However, their performance heavily relies on\nhigh-quality exemplar data, which is often difficult to obtain. This challenge\nis further exacerbated when it comes to multimodal instruction following. We\nintroduce TextBind, an almost annotation-free framework for empowering larger\nlanguage models with the multi-turn interleaved multimodal\ninstruction-following capabilities. Our approach requires only image-caption\npairs and generates multi-turn multimodal instruction-response conversations\nfrom a language model. To accommodate interleaved image-text inputs and\noutputs, we devise MIM, a language model-centric architecture that seamlessly\nintegrates image encoder and decoder models. We release our dataset, model, and\ndemo to foster future research in the area of multimodal instruction following.\n","authors":["Huayang Li","Siheng Li","Deng Cai","Longyue Wang","Lemao Liu","Taro Watanabe","Yujiu Yang","Shuming Shi"],"pdf_url":"https://arxiv.org/pdf/2309.08637v4.pdf","comment":"https://textbind.github.io/"},{"id":"http://arxiv.org/abs/2308.11380v2","updated":"2024-01-08T12:54:47Z","published":"2023-08-22T12:09:30Z","title":"Convoifilter: A case study of doing cocktail party speech recognition","summary":"  This paper presents an end-to-end model designed to improve automatic speech\nrecognition (ASR) for a particular speaker in a crowded, noisy environment. The\nmodel utilizes a single-channel speech enhancement module that isolates the\nspeaker's voice from background noise (ConVoiFilter) and an ASR module. The\nmodel can decrease ASR's word error rate (WER) from 80% to 26.4% through this\napproach. Typically, these two components are adjusted independently due to\nvariations in data requirements. However, speech enhancement can create\nanomalies that decrease ASR efficiency. By implementing a joint fine-tuning\nstrategy, the model can reduce the WER from 26.4% in separate tuning to 14.5%\nin joint tuning. We openly share our pre-trained model to foster further\nresearch hf.co/nguyenvulebinh/voice-filter.\n","authors":["Thai-Binh Nguyen","Alexander Waibel"],"pdf_url":"https://arxiv.org/pdf/2308.11380v2.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2312.11945v2","updated":"2024-01-08T12:45:29Z","published":"2023-12-19T08:43:02Z","title":"Multi-Granularity Information Interaction Framework for Incomplete\n  Utterance Rewriting","summary":"  Recent approaches in Incomplete Utterance Rewriting (IUR) fail to capture the\nsource of important words, which is crucial to edit the incomplete utterance,\nand introduce words from irrelevant utterances. We propose a novel and\neffective multi-task information interaction framework including context\nselection, edit matrix construction, and relevance merging to capture the\nmulti-granularity of semantic information. Benefiting from fetching the\nrelevant utterance and figuring out the important words, our approach\noutperforms existing state-of-the-art models on two benchmark datasets\nRestoration-200K and CANAND in this field. Code will be provided on\n\\url{https://github.com/yanmenxue/QR}.\n","authors":["Haowei Du","Dinghao Zhang","Chen Li","Yang Li","Dongyan Zhao"],"pdf_url":"https://arxiv.org/pdf/2312.11945v2.pdf","comment":"Findings of EMNLP2023 (short)"},{"id":"http://arxiv.org/abs/2401.03855v1","updated":"2024-01-08T12:36:43Z","published":"2024-01-08T12:36:43Z","title":"Boldly Going Where No Benchmark Has Gone Before: Exposing Bias and\n  Shortcomings in Code Generation Evaluation","summary":"  Motivated by the increasing popularity of code generation from human\ndescriptions using large language models (LLMs), several benchmarks have been\nproposed to assess the capabilities of existing and emerging models. This study\npresents a large-scale human evaluation of HumanEval and MBPP, two widely used\nbenchmarks for Python code generation, focusing on their diversity and\ndifficulty. Our findings reveal a significant bias towards a limited number of\nprogramming concepts, with negligible or no representation of most concepts.\nAdditionally, we identify a concerningly high proportion of easy programming\nquestions, potentially leading to an overestimation of model performance on\ncode generation tasks.\n","authors":["Ankit Yadav","Mayank Singh"],"pdf_url":"https://arxiv.org/pdf/2401.03855v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03831v1","updated":"2024-01-08T11:40:48Z","published":"2024-01-08T11:40:48Z","title":"We Need to Talk About Classification Evaluation Metrics in NLP","summary":"  In Natural Language Processing (NLP) classification tasks such as topic\ncategorisation and sentiment analysis, model generalizability is generally\nmeasured with standard metrics such as Accuracy, F-Measure, or AUC-ROC. The\ndiversity of metrics, and the arbitrariness of their application suggest that\nthere is no agreement within NLP on a single best metric to use. This lack\nsuggests there has not been sufficient examination of the underlying heuristics\nwhich each metric encodes. To address this we compare several standard\nclassification metrics with more 'exotic' metrics and demonstrate that a\nrandom-guess normalised Informedness metric is a parsimonious baseline for task\nperformance. To show how important the choice of metric is, we perform\nextensive experiments on a wide range of NLP tasks including a synthetic\nscenario, natural language understanding, question answering and machine\ntranslation. Across these tasks we use a superset of metrics to rank models and\nfind that Informedness best captures the ideal model characteristics. Finally,\nwe release a Python implementation of Informedness following the SciKitLearn\nclassifier format.\n","authors":["Peter Vickers","Loïc Barrault","Emilio Monti","Nikolaos Aletras"],"pdf_url":"https://arxiv.org/pdf/2401.03831v1.pdf","comment":"Appeared in AACL 2023"},{"id":"http://arxiv.org/abs/2401.03804v1","updated":"2024-01-08T10:43:19Z","published":"2024-01-08T10:43:19Z","title":"TeleChat Technical Report","summary":"  In this technical report, we present TeleChat, a collection of large language\nmodels (LLMs) with parameters of 3 billion, 7 billion and 12 billion. It\nincludes pretrained language models as well as fine-tuned chat models that is\naligned with human preferences. TeleChat is initially pretrained on an\nextensive corpus containing a diverse collection of texts from both English and\nChinese languages, including trillions of tokens. Subsequently, the model\nundergoes fine-tuning to align with human preferences, following a detailed\nmethodology that we describe. We evaluate the performance of TeleChat on\nvarious tasks, including language understanding, mathematics, reasoning, code\ngeneration, and knowledge-based question answering. Our findings indicate that\nTeleChat achieves comparable performance to other open-source models of similar\nsize across a wide range of public benchmarks. To support future research and\napplications utilizing LLMs, we release the fine-tuned model checkpoints of\nTeleChat's 7B and 12B variant, along with code and a portion of our pretraining\ndata, to the public community.\n","authors":["Zihan Wang","Xinzhang Liu","Shixuan Liu","Yitong Yao","Yuyao Huang","Zhongjiang He","Xuelong Li","Yongxiang Li","Zhonghao Che","Zhaoxi Zhang","Yan Wang","Xin Wang","Luwen Pu","Huihan Xu","Ruiyu Fang","Yu Zhao","Jie Zhang","Xiaomeng Huang","Zhilong Lu","Jiaxin Peng","Wenjun Zheng","Shiquan Wang","Bingkai Yang","Xuewei he","Zhuoru Jiang","Qiyi Xie","Yanhan Zhang","Zhongqiu Li","Lingling Shi","Weiwei Fu","Yin Zhang","Zilu Huang","Sishi Xiong","Yuxiang Zhang","Chao Wang","Shuangyong Song"],"pdf_url":"https://arxiv.org/pdf/2401.03804v1.pdf","comment":"28 pages, 2 figures"},{"id":"http://arxiv.org/abs/2401.03797v1","updated":"2024-01-08T10:27:25Z","published":"2024-01-08T10:27:25Z","title":"Anatomy of Neural Language Models","summary":"  Generative AI and transfer learning fields have experienced remarkable\nadvancements in recent years especially in the domain of Natural Language\nProcessing (NLP). Transformers were at the heart of these advancements where\nthe cutting-edge transformer-based Language Models (LMs) enabled new\nstate-of-the-art results in a wide spectrum of applications. While the number\nof research works involving neural LMs is exponentially increasing, their vast\nmajority are high-level and far from self-contained. Consequently, a deep\nunderstanding of the literature in this area is a tough task especially at the\nabsence of a unified mathematical framework explaining the main types of neural\nLMs. We address the aforementioned problem in this tutorial where the objective\nis to explain neural LMs in a detailed, simplified and unambiguous mathematical\nframework accompanied with clear graphical illustrations. Concrete examples on\nwidely used models like BERT and GPT2 are explored. Finally, since transformers\npretrained on language-modeling-like tasks have been widely adopted in computer\nvision and time series applications, we briefly explore some examples of such\nsolutions in order to enable readers understand how transformers work in the\naforementioned domains and compare this use with the original one in NLP.\n","authors":["Majd Saleh","Stéphane Paquelet"],"pdf_url":"https://arxiv.org/pdf/2401.03797v1.pdf","comment":"36 Pages; 25 Figures"},{"id":"http://arxiv.org/abs/2301.13126v3","updated":"2024-01-08T10:08:40Z","published":"2023-01-30T18:05:08Z","title":"LEXTREME: A Multi-Lingual and Multi-Task Benchmark for the Legal Domain","summary":"  Lately, propelled by the phenomenal advances around the transformer\narchitecture, the legal NLP field has enjoyed spectacular growth. To measure\nprogress, well curated and challenging benchmarks are crucial. However, most\nbenchmarks are English only and in legal NLP specifically there is no\nmultilingual benchmark available yet. Additionally, many benchmarks are\nsaturated, with the best models clearly outperforming the best humans and\nachieving near perfect scores. We survey the legal NLP literature and select 11\ndatasets covering 24 languages, creating LEXTREME. To provide a fair\ncomparison, we propose two aggregate scores, one based on the datasets and one\non the languages. The best baseline (XLM-R large) achieves both a dataset\naggregate score a language aggregate score of 61.3. This indicates that\nLEXTREME is still very challenging and leaves ample room for improvement. To\nmake it easy for researchers and practitioners to use, we release LEXTREME on\nhuggingface together with all the code required to evaluate models and a public\nWeights and Biases project with all the runs.\n","authors":["Joel Niklaus","Veton Matoshi","Pooja Rani","Andrea Galassi","Matthias Stürmer","Ilias Chalkidis"],"pdf_url":"https://arxiv.org/pdf/2301.13126v3.pdf","comment":"Published at EMNLP Findings 2023"},{"id":"http://arxiv.org/abs/2401.03741v1","updated":"2024-01-08T09:01:29Z","published":"2024-01-08T09:01:29Z","title":"Enhanced Automated Code Vulnerability Repair using Large Language Models","summary":"  This research addresses the complex challenge of automated repair of code\nvulnerabilities, vital for enhancing digital security in an increasingly\ntechnology-driven world. The study introduces a novel and efficient format for\nthe representation of code modification, using advanced Large Language Models\n(LLMs) such as Code Llama and Mistral. These models, fine-tuned on datasets\nfeaturing C code vulnerabilities, significantly improve the accuracy and\nadaptability of automated code repair techniques. A key finding is the enhanced\nrepair accuracy of these models when compared to previous methods such as\nVulRepair, which underscores their practical utility and efficiency. The\nresearch also offers a critical assessment of current evaluation metrics, such\nas perfect predictions, and their limitations in reflecting the true\ncapabilities of automated repair models in real-world scenarios. Following\nthis, it underscores the importance of using test datasets devoid of train\nsamples, emphasizing the need for dataset integrity to enhance the\neffectiveness of LLMs in code repair tasks. The significance of this work is\nits contribution to digital security, setting new standards for automated code\nvulnerability repair and paving the way for future advancements in the fields\nof cybersecurity and artificial intelligence. The study does not only highlight\nthe potential of LLMs in enhancing code security but also fosters further\nexploration and research in these crucial areas.\n","authors":["David de-Fitero-Dominguez","Eva Garcia-Lopez","Antonio Garcia-Cabot","Jose-Javier Martinez-Herraiz"],"pdf_url":"https://arxiv.org/pdf/2401.03741v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03737v1","updated":"2024-01-08T08:58:46Z","published":"2024-01-08T08:58:46Z","title":"Can Large Language Models Beat Wall Street? Unveiling the Potential of\n  AI in Stock Selection","summary":"  In the dynamic and data-driven landscape of financial markets, this paper\nintroduces MarketSenseAI, a novel AI-driven framework leveraging the advanced\nreasoning capabilities of GPT-4 for scalable stock selection. MarketSenseAI\nincorporates Chain of Thought and In-Context Learning methodologies to analyze\na wide array of data sources, including market price dynamics, financial news,\ncompany fundamentals, and macroeconomic reports emulating the decision making\nprocess of prominent financial investment teams. The development,\nimplementation, and empirical validation of MarketSenseAI are detailed, with a\nfocus on its ability to provide actionable investment signals (buy, hold, sell)\nbacked by cogent explanations. A notable aspect of this study is the use of\nGPT-4 not only as a predictive tool but also as an evaluator, revealing the\nsignificant impact of the AI-generated explanations on the reliability and\nacceptance of the suggested investment signals. In an extensive empirical\nevaluation with S&P 100 stocks, MarketSenseAI outperformed the benchmark index\nby 13%, achieving returns up to 40%, while maintaining a risk profile\ncomparable to the market. These results demonstrate the efficacy of Large\nLanguage Models in complex financial decision-making and mark a significant\nadvancement in the integration of AI into financial analysis and investment\nstrategies. This research contributes to the financial AI field, presenting an\ninnovative approach and underscoring the transformative potential of AI in\nrevolutionizing traditional financial analysis investment methodologies.\n","authors":["Georgios Fatouros","Konstantinos Metaxas","John Soldatos","Dimosthenis Kyriazis"],"pdf_url":"https://arxiv.org/pdf/2401.03737v1.pdf","comment":"15 pages, 12 figures, 12 tables"},{"id":"http://arxiv.org/abs/2401.03735v1","updated":"2024-01-08T08:54:22Z","published":"2024-01-08T08:54:22Z","title":"Language Models Understand Numbers, at Least Partially","summary":"  Large language models (LLMs) have exhibited impressive competency in various\ntext-related tasks. However, their opaque internal mechanisms become a\nhindrance to leveraging them in mathematical problems. In this paper, we study\na fundamental question: whether language models understand numbers, which play\na basic element in mathematical problems. We assume that to solve mathematical\nproblems, language models should be capable of understanding numbers and\ncompressing these numbers in their hidden states. We construct a synthetic\ndataset comprising addition problems and utilize linear probes to read out\ninput numbers from the hidden states of models. Experimental results\ndemonstrate evidence supporting the existence of compressed numbers in the\nLLaMA-2 model family from early layers. However, the compression process seems\nto be not lossless, presenting difficulty in precisely reconstructing the\noriginal numbers. Further experiments show that language models can utilize the\nencoded numbers to perform arithmetic computations, and the computational\nability scales up with the model size. Our preliminary research suggests that\nlanguage models exhibit a partial understanding of numbers, offering insights\ninto future investigations about the models' capability of solving mathematical\nproblems.\n","authors":["Fangwei Zhu","Damai Dai","Zhifang Sui"],"pdf_url":"https://arxiv.org/pdf/2401.03735v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2401.03729v1","updated":"2024-01-08T08:28:08Z","published":"2024-01-08T08:28:08Z","title":"The Butterfly Effect of Altering Prompts: How Small Changes and\n  Jailbreaks Affect Large Language Model Performance","summary":"  Large Language Models (LLMs) are regularly being used to label data across\nmany domains and for myriad tasks. By simply asking the LLM for an answer, or\n``prompting,'' practitioners are able to use LLMs to quickly get a response for\nan arbitrary task. This prompting is done through a series of decisions by the\npractitioner, from simple wording of the prompt, to requesting the output in a\ncertain data format, to jailbreaking in the case of prompts that address more\nsensitive topics. In this work, we ask: do variations in the way a prompt is\nconstructed change the ultimate decision of the LLM? We answer this using a\nseries of prompt variations across a variety of text classification tasks. We\nfind that even the smallest of perturbations, such as adding a space at the end\nof a prompt, can cause the LLM to change its answer. Further, we find that\nrequesting responses in XML and commonly used jailbreaks can have cataclysmic\neffects on the data labeled by LLMs.\n","authors":["Abel Salinas","Fred Morstatter"],"pdf_url":"https://arxiv.org/pdf/2401.03729v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01283v3","updated":"2024-01-08T08:04:00Z","published":"2024-01-02T16:51:17Z","title":"Quality and Quantity of Machine Translation References for Automated\n  Metrics","summary":"  Automatic machine translation metrics often use human translations to\ndetermine the quality of system translations. Common wisdom in the field\ndictates that the human references should be of very high quality. However,\nthere are no cost-benefit analyses that could be used to guide practitioners\nwho plan to collect references for machine translation evaluation. We find that\nhigher-quality references lead to better metric correlations with humans at the\nsegment-level. Having up to 7 references per segment and taking their average\nhelps all metrics. Interestingly, the references from vendors of different\nqualities can be mixed together and improve metric success. Higher quality\nreferences, however, cost more to create and we frame this as an optimization\nproblem: given a specific budget, what references should be collected to\nmaximize metric success. These findings can be used by evaluators of shared\ntasks when references need to be created under a certain budget.\n","authors":["Vilém Zouhar","Ondřej Bojar"],"pdf_url":"https://arxiv.org/pdf/2401.01283v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.09150v2","updated":"2024-01-08T07:49:23Z","published":"2023-09-17T04:18:39Z","title":"Can Large Language Models Understand Real-World Complex Instructions?","summary":"  Large language models (LLMs) can understand human instructions, showing their\npotential for pragmatic applications beyond traditional NLP tasks. However,\nthey still struggle with complex instructions, which can be either complex task\ndescriptions that require multiple tasks and constraints, or complex input that\ncontains long context, noise, heterogeneous information and multi-turn format.\nDue to these features, LLMs often ignore semantic constraints from task\ndescriptions, generate incorrect formats, violate length or sample count\nconstraints, and be unfaithful to the input text. Existing benchmarks are\ninsufficient to assess LLMs' ability to understand complex instructions, as\nthey are close-ended and simple. To bridge this gap, we propose CELLO, a\nbenchmark for evaluating LLMs' ability to follow complex instructions\nsystematically. We design eight features for complex instructions and construct\na comprehensive evaluation dataset from real-world scenarios. We also establish\nfour criteria and develop corresponding metrics, as current ones are\ninadequate, biased or too strict and coarse-grained. We compare the performance\nof representative Chinese-oriented and English-oriented models in following\ncomplex instructions through extensive experiments. Resources of CELLO are\npublicly available at https://github.com/Abbey4799/CELLO.\n","authors":["Qianyu He","Jie Zeng","Wenhao Huang","Lina Chen","Jin Xiao","Qianxi He","Xunzhe Zhou","Lida Chen","Xintao Wang","Yuncheng Huang","Haoning Ye","Zihan Li","Shisong Chen","Yikai Zhang","Zhouhong Gu","Jiaqing Liang","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2309.09150v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.16781v2","updated":"2024-01-08T07:48:09Z","published":"2023-10-25T17:15:55Z","title":"Kiki or Bouba? Sound Symbolism in Vision-and-Language Models","summary":"  Although the mapping between sound and meaning in human language is assumed\nto be largely arbitrary, research in cognitive science has shown that there are\nnon-trivial correlations between particular sounds and meanings across\nlanguages and demographic groups, a phenomenon known as sound symbolism. Among\nthe many dimensions of meaning, sound symbolism is particularly salient and\nwell-demonstrated with regards to cross-modal associations between language and\nthe visual domain. In this work, we address the question of whether sound\nsymbolism is reflected in vision-and-language models such as CLIP and Stable\nDiffusion. Using zero-shot knowledge probing to investigate the inherent\nknowledge of these models, we find strong evidence that they do show this\npattern, paralleling the well-known kiki-bouba effect in psycholinguistics. Our\nwork provides a novel method for demonstrating sound symbolism and\nunderstanding its nature using computational tools. Our code will be made\npublicly available.\n","authors":["Morris Alper","Hadar Averbuch-Elor"],"pdf_url":"https://arxiv.org/pdf/2310.16781v2.pdf","comment":"Accepted to NeurIPS 2023 (spotlight). Project webpage:\n  https://kiki-bouba.github.io/"},{"id":"http://arxiv.org/abs/2305.08677v2","updated":"2024-01-08T06:33:25Z","published":"2023-05-15T14:35:00Z","title":"Natural Language Decomposition and Interpretation of Complex Utterances","summary":"  Designing natural language interfaces has historically required collecting\nsupervised data to translate user requests into carefully designed intent\nrepresentations. This requires enumerating and labeling a long tail of user\nrequests, which is challenging. At the same time, large language models (LLMs)\nencode knowledge about goals and plans that can help conversational assistants\ninterpret user requests requiring numerous steps to complete. We introduce an\napproach to handle complex-intent-bearing utterances from a user via a process\nof hierarchical natural language decomposition and interpretation. Our approach\nuses a pre-trained language model to decompose a complex utterance into a\nsequence of simpler natural language steps and interprets each step using the\nlanguage-to-program model designed for the interface. To test our approach, we\ncollect and release DeCU -- a new NL-to-program benchmark to evaluate\nDecomposition of Complex Utterances. Experiments show that the proposed\napproach enables the interpretation of complex utterances with almost no\ncomplex training data, while outperforming standard few-shot prompting\napproaches.\n","authors":["Harsh Jhamtani","Hao Fang","Patrick Xia","Eran Levy","Jacob Andreas","Ben Van Durme"],"pdf_url":"https://arxiv.org/pdf/2305.08677v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03677v1","updated":"2024-01-08T05:54:26Z","published":"2024-01-08T05:54:26Z","title":"Overview of the 2023 ICON Shared Task on Gendered Abuse Detection in\n  Indic Languages","summary":"  This paper reports the findings of the ICON 2023 on Gendered Abuse Detection\nin Indic Languages. The shared task deals with the detection of gendered abuse\nin online text. The shared task was conducted as a part of ICON 2023, based on\na novel dataset in Hindi, Tamil and the Indian dialect of English. The\nparticipants were given three subtasks with the train dataset consisting of\napproximately 6500 posts sourced from Twitter. For the test set, approximately\n1200 posts were provided. The shared task received a total of 9 registrations.\nThe best F-1 scores are 0.616 for subtask 1, 0.572 for subtask 2 and, 0.616 and\n0.582 for subtask 3. The paper contains examples of hateful content owing to\nits topic.\n","authors":["Aatman Vaidya","Arnav Arora","Aditya Joshi","Tarunima Prabhakar"],"pdf_url":"https://arxiv.org/pdf/2401.03677v1.pdf","comment":"This paper has been accepted at 20th International Conference on\n  Natural Language Processing (ICON), it is of 5 pages"},{"id":"http://arxiv.org/abs/2305.14387v4","updated":"2024-01-08T04:46:56Z","published":"2023-05-22T17:55:50Z","title":"AlpacaFarm: A Simulation Framework for Methods that Learn from Human\n  Feedback","summary":"  Large language models (LLMs) such as ChatGPT have seen widespread adoption\ndue to their strong instruction-following abilities. Developing these LLMs\ninvolves a complex yet poorly understood workflow requiring training with human\nfeedback. Replicating and understanding this instruction-following requires\ntackling three major challenges: the high cost of data collection, the lack of\ntrustworthy evaluation, and the absence of reference method implementations. We\naddress these challenges with AlpacaFarm, a simulator that enables research and\ndevelopment for learning from feedback at a low cost. First, we design LLM\nprompts to simulate human feedback that are 50x cheaper than crowdworkers and\ndisplay high agreement with humans. Second, we propose an automatic evaluation\nand validate it against human instructions obtained on real-world interactions.\nThird, we contribute reference implementations for several methods (PPO, DPO,\nbest-of-n, expert iteration, and more) that learn from pairwise feedback.\nFinally, as an end-to-end validation of AlpacaFarm, we train and evaluate\neleven models on 10k pairs of real human feedback and show that rankings of\nmodels trained in AlpacaFarm match rankings of models trained on human data. As\na demonstration of the research possible in AlpacaFarm, we find that methods\nthat use a reward model can substantially improve over supervised fine-tuning\nand that our reference PPO implementation leads to a +10% improvement in\nwin-rate against Davinci003. We release all components of AlpacaFarm at\nhttps://github.com/tatsu-lab/alpaca_farm.\n","authors":["Yann Dubois","Xuechen Li","Rohan Taori","Tianyi Zhang","Ishaan Gulrajani","Jimmy Ba","Carlos Guestrin","Percy Liang","Tatsunori B. Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2305.14387v4.pdf","comment":"Spotlight at NeurIPS 2023"},{"id":"http://arxiv.org/abs/2401.03642v1","updated":"2024-01-08T03:14:24Z","published":"2024-01-08T03:14:24Z","title":"A Content-Based Novelty Measure for Scholarly Publications: A Proof of\n  Concept","summary":"  Novelty, akin to gene mutation in evolution, opens possibilities for\nscientific advancement. Despite peer review being the gold standard for\nevaluating novelty in scholarly communication and resource allocation, the vast\nvolume of submissions necessitates an automated measure of scientific novelty.\nAdopting a perspective that views novelty as the atypical combination of\nexisting knowledge, we introduce an information-theoretic measure of novelty in\nscholarly publications. This measure is quantified by the degree of `surprise'\nperceived by a language model that represents the distribution of scientific\ndiscourse. The proposed measure is accompanied by face and construct validity\nevidence; the former demonstrates correspondence to scientific common sense,\nand the latter is endorsed through alignments with novelty evaluations from a\nselect panel of domain experts. Additionally, characterized by its\ninterpretability, fine granularity, and accessibility, this measure addresses\ngaps prevalent in existing methods. We believe this measure holds great\npotential to benefit editors, stakeholders, and policymakers, and it provides a\nconfident lens for examining the relationship between novelty and scientific\ndynamics such as creativity, interdisciplinarity, scientific advances, and\nmore.\n","authors":["Haining Wang"],"pdf_url":"https://arxiv.org/pdf/2401.03642v1.pdf","comment":"Accepted for publication in the proceedings of iConference2024"},{"id":"http://arxiv.org/abs/2312.15548v2","updated":"2024-01-08T02:36:55Z","published":"2023-12-24T21:33:03Z","title":"YAYI-UIE: A Chat-Enhanced Instruction Tuning Framework for Universal\n  Information Extraction","summary":"  The difficulty of the information extraction task lies in dealing with the\ntask-specific label schemas and heterogeneous data structures. Recent work has\nproposed methods based on large language models to uniformly model different\ninformation extraction tasks. However, these existing methods are deficient in\ntheir information extraction capabilities for Chinese languages other than\nEnglish. In this paper, we propose an end-to-end chat-enhanced instruction\ntuning framework for universal information extraction (YAYI-UIE), which\nsupports both Chinese and English. Specifically, we utilize dialogue data and\ninformation extraction data to enhance the information extraction performance\njointly. Experimental results show that our proposed framework achieves\nstate-of-the-art performance on Chinese datasets while also achieving\ncomparable performance on English datasets under both supervised settings and\nzero-shot settings.\n","authors":["Xinglin Xiao","Yijie Wang","Nan Xu","Yuqi Wang","Hanxuan Yang","Minzheng Wang","Yin Luo","Lei Wang","Wenji Mao","Daniel Zeng"],"pdf_url":"https://arxiv.org/pdf/2312.15548v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03631v1","updated":"2024-01-08T02:23:17Z","published":"2024-01-08T02:23:17Z","title":"Bridging the Skills Gap: Evaluating an AI-Assisted Provider Platform to\n  Support Care Providers with Empathetic Delivery of Protocolized Therapy","summary":"  Despite the high prevalence and burden of mental health conditions, there is\na global shortage of mental health providers. Artificial Intelligence (AI)\nmethods have been proposed as a way to address this shortage, by supporting\nproviders with less extensive training as they deliver care. To this end, we\ndeveloped the AI-Assisted Provider Platform (A2P2), a text-based virtual\ntherapy interface that includes a response suggestion feature, which supports\nproviders in delivering protocolized therapies empathetically. We studied\nproviders with and without expertise in mental health treatment delivering a\ntherapy session using the platform with (intervention) and without (control)\nAI-assistance features. Upon evaluation, the AI-assisted system significantly\ndecreased response times by 29.34% (p=0.002), tripled empathic response\naccuracy (p=0.0001), and increased goal recommendation accuracy by 66.67%\n(p=0.001) across both user groups compared to the control. Both groups rated\nthe system as having excellent usability.\n","authors":["William R. Kearns","Jessica Bertram","Myra Divina","Lauren Kemp","Yinzhou Wang","Alex Marin","Trevor Cohen","Weichao Yuwen"],"pdf_url":"https://arxiv.org/pdf/2401.03631v1.pdf","comment":"Accepted: AMIA Annual Symposium 2023. To appear as: Kearns W, Bertram\n  J, Divina M, Kemp L, Wang Y, Marin A, Cohen T, Yuwen W. Bridging the Skills\n  Gap: Evaluating an AI-Assisted Provider Platform to Support Care Providers\n  with Empathetic Delivery of Protocolized Therapy. AMIA Annual Symposium\n  Proceedings 2023. American Medical Informatics Association"},{"id":"http://arxiv.org/abs/2401.03630v1","updated":"2024-01-08T02:22:04Z","published":"2024-01-08T02:22:04Z","title":"Why Solving Multi-agent Path Finding with Large Language Model has not\n  Succeeded Yet","summary":"  With the explosive influence caused by the success of large language models\n(LLM) like ChatGPT and GPT-4, there has been an extensive amount of recent work\nshowing that foundation models can be used to solve a large variety of tasks.\nHowever, there is very limited work that shares insights on multi-agent\nplanning. Multi-agent planning is different from other domains by combining the\ndifficulty of multi-agent coordination and planning, and making it hard to\nleverage external tools to facilitate the reasoning needed. In this paper, we\nfocus on the problem of multi-agent path finding (MAPF), which is also known as\nmulti-robot route planning, and study how to solve MAPF with LLMs. We first\nshow the motivating success on an empty room map without obstacles, then the\nfailure to plan on a slightly harder room map. We present our hypothesis of why\ndirectly solving MAPF with LLMs has not been successful yet, and we use various\nexperiments to support our hypothesis.\n","authors":["Weizhe Chen","Sven Koenig","Bistra Dilkina"],"pdf_url":"https://arxiv.org/pdf/2401.03630v1.pdf","comment":"10 pages, 8 figures"},{"id":"http://arxiv.org/abs/2307.09312v3","updated":"2024-01-08T00:12:42Z","published":"2023-07-18T14:57:12Z","title":"Multi-Modal Discussion Transformer: Integrating Text, Images and Graph\n  Transformers to Detect Hate Speech on Social Media","summary":"  We present the Multi-Modal Discussion Transformer (mDT), a novel methodfor\ndetecting hate speech in online social networks such as Reddit discussions. In\ncontrast to traditional comment-only methods, our approach to labelling a\ncomment as hate speech involves a holistic analysis of text and images grounded\nin the discussion context. This is done by leveraging graph transformers to\ncapture the contextual relationships in the discussion surrounding a comment\nand grounding the interwoven fusion layers that combine text and image\nembeddings instead of processing modalities separately. To evaluate our work,\nwe present a new dataset, HatefulDiscussions, comprising complete multi-modal\ndiscussions from multiple online communities on Reddit. We compare the\nperformance of our model to baselines that only process individual comments and\nconduct extensive ablation studies.\n","authors":["Liam Hebert","Gaurav Sahu","Yuxuan Guo","Nanda Kishore Sreenivas","Lukasz Golab","Robin Cohen"],"pdf_url":"https://arxiv.org/pdf/2307.09312v3.pdf","comment":"Accepted to AAAI 2024 (AI for Social Impact Track)"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2401.04105v1","updated":"2024-01-08T18:59:31Z","published":"2024-01-08T18:59:31Z","title":"Dr$^2$Net: Dynamic Reversible Dual-Residual Networks for\n  Memory-Efficient Finetuning","summary":"  Large pretrained models are increasingly crucial in modern computer vision\ntasks. These models are typically used in downstream tasks by end-to-end\nfinetuning, which is highly memory-intensive for tasks with high-resolution\ndata, e.g., video understanding, small object detection, and point cloud\nanalysis. In this paper, we propose Dynamic Reversible Dual-Residual Networks,\nor Dr$^2$Net, a novel family of network architectures that acts as a surrogate\nnetwork to finetune a pretrained model with substantially reduced memory\nconsumption. Dr$^2$Net contains two types of residual connections, one\nmaintaining the residual structure in the pretrained models, and the other\nmaking the network reversible. Due to its reversibility, intermediate\nactivations, which can be reconstructed from output, are cleared from memory\nduring training. We use two coefficients on either type of residual connections\nrespectively, and introduce a dynamic training strategy that seamlessly\ntransitions the pretrained model to a reversible network with much higher\nnumerical precision. We evaluate Dr$^2$Net on various pretrained models and\nvarious tasks, and show that it can reach comparable performance to\nconventional finetuning but with significantly less memory usage.\n","authors":["Chen Zhao","Shuming Liu","Karttikeya Mangalam","Guocheng Qian","Fatimah Zohra","Abdulmohsen Alghannam","Jitendra Malik","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2401.04105v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.04099v1","updated":"2024-01-08T18:56:33Z","published":"2024-01-08T18:56:33Z","title":"AGG: Amortized Generative 3D Gaussians for Single Image to 3D","summary":"  Given the growing need for automatic 3D content creation pipelines, various\n3D representations have been studied to generate 3D objects from a single\nimage. Due to its superior rendering efficiency, 3D Gaussian splatting-based\nmodels have recently excelled in both 3D reconstruction and generation. 3D\nGaussian splatting approaches for image to 3D generation are often\noptimization-based, requiring many computationally expensive score-distillation\nsteps. To overcome these challenges, we introduce an Amortized Generative 3D\nGaussian framework (AGG) that instantly produces 3D Gaussians from a single\nimage, eliminating the need for per-instance optimization. Utilizing an\nintermediate hybrid representation, AGG decomposes the generation of 3D\nGaussian locations and other appearance attributes for joint optimization.\nMoreover, we propose a cascaded pipeline that first generates a coarse\nrepresentation of the 3D data and later upsamples it with a 3D Gaussian\nsuper-resolution module. Our method is evaluated against existing\noptimization-based 3D Gaussian frameworks and sampling-based pipelines\nutilizing other 3D representations, where AGG showcases competitive generation\nabilities both qualitatively and quantitatively while being several orders of\nmagnitude faster. Project page: https://ir1d.github.io/AGG/\n","authors":["Dejia Xu","Ye Yuan","Morteza Mardani","Sifei Liu","Jiaming Song","Zhangyang Wang","Arash Vahdat"],"pdf_url":"https://arxiv.org/pdf/2401.04099v1.pdf","comment":"Project page: https://ir1d.github.io/AGG/"},{"id":"http://arxiv.org/abs/2401.04092v1","updated":"2024-01-08T18:52:09Z","published":"2024-01-08T18:52:09Z","title":"GPT-4V(ision) is a Human-Aligned Evaluator for Text-to-3D Generation","summary":"  Despite recent advances in text-to-3D generative methods, there is a notable\nabsence of reliable evaluation metrics. Existing metrics usually focus on a\nsingle criterion each, such as how well the asset aligned with the input text.\nThese metrics lack the flexibility to generalize to different evaluation\ncriteria and might not align well with human preferences. Conducting user\npreference studies is an alternative that offers both adaptability and\nhuman-aligned results. User studies, however, can be very expensive to scale.\nThis paper presents an automatic, versatile, and human-aligned evaluation\nmetric for text-to-3D generative models. To this end, we first develop a prompt\ngenerator using GPT-4V to generate evaluating prompts, which serve as input to\ncompare text-to-3D models. We further design a method instructing GPT-4V to\ncompare two 3D assets according to user-defined criteria. Finally, we use these\npairwise comparison results to assign these models Elo ratings. Experimental\nresults suggest our metric strongly align with human preference across\ndifferent evaluation criteria.\n","authors":["Tong Wu","Guandao Yang","Zhibing Li","Kai Zhang","Ziwei Liu","Leonidas Guibas","Dahua Lin","Gordon Wetzstein"],"pdf_url":"https://arxiv.org/pdf/2401.04092v1.pdf","comment":"Project page: https://gpteval3d.github.io/; Code:\n  https://github.com/3DTopia/GPTEval3D"},{"id":"http://arxiv.org/abs/2401.04079v1","updated":"2024-01-08T18:31:38Z","published":"2024-01-08T18:31:38Z","title":"RudolfV: A Foundation Model by Pathologists for Pathologists","summary":"  Histopathology plays a central role in clinical medicine and biomedical\nresearch. While artificial intelligence shows promising results on many\npathological tasks, generalization and dealing with rare diseases, where\ntraining data is scarce, remains a challenge. Distilling knowledge from\nunlabeled data into a foundation model before learning from, potentially\nlimited, labeled data provides a viable path to address these challenges. In\nthis work, we extend the state of the art of foundation models for digital\npathology whole slide images by semi-automated data curation and incorporating\npathologist domain knowledge. Specifically, we combine computational and\npathologist domain knowledge (1) to curate a diverse dataset of 103k slides\ncorresponding to 750 million image patches covering data from different\nfixation, staining, and scanning protocols as well as data from different\nindications and labs across the EU and US, (2) for grouping semantically\nsimilar slides and tissue patches, and (3) to augment the input images during\ntraining. We evaluate the resulting model on a set of public and internal\nbenchmarks and show that although our foundation model is trained with an order\nof magnitude less slides, it performs on par or better than competing models.\nWe expect that scaling our approach to more data and larger models will further\nincrease its performance and capacity to deal with increasingly complex real\nworld tasks in diagnostics and biomedical research.\n","authors":["Jonas Dippel","Barbara Feulner","Tobias Winterhoff","Simon Schallenberg","Gabriel Dernbach","Andreas Kunft","Stephan Tietz","Philipp Jurmeister","David Horst","Lukas Ruff","Klaus-Robert Müller","Frederick Klauschen","Maximilian Alber"],"pdf_url":"https://arxiv.org/pdf/2401.04079v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.04071v1","updated":"2024-01-08T18:18:02Z","published":"2024-01-08T18:18:02Z","title":"Fun with Flags: Robust Principal Directions via Flag Manifolds","summary":"  Principal component analysis (PCA), along with its extensions to manifolds\nand outlier contaminated data, have been indispensable in computer vision and\nmachine learning. In this work, we present a unifying formalism for PCA and its\nvariants, and introduce a framework based on the flags of linear subspaces, \\ie\na hierarchy of nested linear subspaces of increasing dimension, which not only\nallows for a common implementation but also yields novel variants, not explored\npreviously. We begin by generalizing traditional PCA methods that either\nmaximize variance or minimize reconstruction error. We expand these\ninterpretations to develop a wide array of new dimensionality reduction\nalgorithms by accounting for outliers and the data manifold. To devise a common\ncomputational approach, we recast robust and dual forms of PCA as optimization\nproblems on flag manifolds. We then integrate tangent space approximations of\nprincipal geodesic analysis (tangent-PCA) into this flag-based framework,\ncreating novel robust and dual geodesic PCA variations. The remarkable\nflexibility offered by the 'flagification' introduced here enables even more\nalgorithmic variants identified by specific flag types. Last but not least, we\npropose an effective convergent solver for these flag-formulations employing\nthe Stiefel manifold. Our empirical results on both real-world and synthetic\nscenarios, demonstrate the superiority of our novel algorithms, especially in\nterms of robustness to outliers on manifolds.\n","authors":["Nathan Mankovich","Gustau Camps-Valls","Tolga Birdal"],"pdf_url":"https://arxiv.org/pdf/2401.04071v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02565v2","updated":"2024-01-08T18:15:59Z","published":"2024-01-04T22:49:15Z","title":"Vulnerabilities Unveiled: Adversarially Attacking a Multimodal Vision\n  Language Model for Pathology Imaging","summary":"  In the dynamic landscape of medical artificial intelligence, this study\nexplores the vulnerabilities of the Pathology Language-Image Pretraining (PLIP)\nmodel, a Vision Language Foundation model, under targeted adversarial\nconditions. Leveraging the Kather Colon dataset with 7,180 H&E images across\nnine tissue types, our investigation employs Projected Gradient Descent (PGD)\nadversarial attacks to intentionally induce misclassifications. The outcomes\nreveal a 100% success rate in manipulating PLIP's predictions, underscoring its\nsusceptibility to adversarial perturbations. The qualitative analysis of\nadversarial examples delves into the interpretability challenges, shedding\nlight on nuanced changes in predictions induced by adversarial manipulations.\nThese findings contribute crucial insights into the interpretability, domain\nadaptation, and trustworthiness of Vision Language Models in medical imaging.\nThe study emphasizes the pressing need for robust defenses to ensure the\nreliability of AI models.\n","authors":["Jai Prakash Veerla","Poojitha Thota","Partha Sai Guttikonda","Shirin Nilizadeh","Jacob M. Luber"],"pdf_url":"https://arxiv.org/pdf/2401.02565v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01470v2","updated":"2024-01-08T17:03:15Z","published":"2024-01-03T00:10:33Z","title":"TPC-ViT: Token Propagation Controller for Efficient Vision Transformer","summary":"  Vision transformers (ViTs) have achieved promising results on a variety of\nComputer Vision tasks, however their quadratic complexity in the number of\ninput tokens has limited their application specially in resource-constrained\nsettings. Previous approaches that employ gradual token reduction to address\nthis challenge assume that token redundancy in one layer implies redundancy in\nall the following layers. We empirically demonstrate that this assumption is\noften not correct, i.e., tokens that are redundant in one layer can be useful\nin later layers. We employ this key insight to propose a novel token\npropagation controller (TPC) that incorporates two different\ntoken-distributions, i.e., pause probability and restart probability to control\nthe reduction and reuse of tokens respectively, which results in more efficient\ntoken utilization. To improve the estimates of token distributions, we propose\na smoothing mechanism that acts as a regularizer and helps remove noisy\noutliers. Furthermore, to improve the training-stability of our proposed TPC,\nwe introduce a model stabilizer that is able to implicitly encode local image\nstructures and minimize accuracy fluctuations during model training. We present\nextensive experimental results on the ImageNet-1K dataset using DeiT, LV-ViT\nand Swin models to demonstrate the effectiveness of our proposed method. For\nexample, compared to baseline models, our proposed method improves the\ninference speed of the DeiT-S by 250% while increasing the classification\naccuracy by 1.0%.\n","authors":["Wentao Zhu"],"pdf_url":"https://arxiv.org/pdf/2401.01470v2.pdf","comment":"Accepted by the main conference of WACV 2024; well-formatted PDF is\n  in\n  https://drive.google.com/file/d/1Id3oEdYv3OWing1qojQMyjvhZO-gG-Dm/view?usp=sharing\n  ; supplementary is in\n  https://drive.google.com/file/d/15LhYlBdCXtompA0_TLAp_ZJb4_sq2N5V/view?usp=sharing"},{"id":"http://arxiv.org/abs/2401.04023v1","updated":"2024-01-08T17:02:25Z","published":"2024-01-08T17:02:25Z","title":"Efficient Multiscale Multimodal Bottleneck Transformer for Audio-Video\n  Classification","summary":"  In recent years, researchers combine both audio and video signals to deal\nwith challenges where actions are not well represented or captured by visual\ncues. However, how to effectively leverage the two modalities is still under\ndevelopment. In this work, we develop a multiscale multimodal Transformer (MMT)\nthat leverages hierarchical representation learning. Particularly, MMT is\ncomposed of a novel multiscale audio Transformer (MAT) and a multiscale video\nTransformer [43]. To learn a discriminative cross-modality fusion, we further\ndesign multimodal supervised contrastive objectives called audio-video\ncontrastive loss (AVC) and intra-modal contrastive loss (IMC) that robustly\nalign the two modalities. MMT surpasses previous state-of-the-art approaches by\n7.3% and 2.1% on Kinetics-Sounds and VGGSound in terms of the top-1 accuracy\nwithout external training data. Moreover, the proposed MAT significantly\noutperforms AST [28] by 22.2%, 4.4% and 4.7% on three public benchmark\ndatasets, and is about 3% more efficient based on the number of FLOPs and 9.8%\nmore efficient based on GPU memory usage.\n","authors":["Wentao Zhu"],"pdf_url":"https://arxiv.org/pdf/2401.04023v1.pdf","comment":"Accepted by WACV 2024; well-formatted PDF is in\n  https://drive.google.com/file/d/10Zo_ydJZFAm7YsxHDgTjhyc4dEJbW_dk/view?usp=sharing"},{"id":"http://arxiv.org/abs/2401.01286v2","updated":"2024-01-08T16:25:04Z","published":"2024-01-02T16:54:58Z","title":"A Comprehensive Study of Knowledge Editing for Large Language Models","summary":"  Large Language Models (LLMs) have shown extraordinary capabilities in\nunderstanding and generating text that closely mirrors human communication.\nHowever, a primary limitation lies in the significant computational demands\nduring training, arising from their extensive parameterization. This challenge\nis further intensified by the dynamic nature of the world, necessitating\nfrequent updates to LLMs to correct outdated information or integrate new\nknowledge, thereby ensuring their continued relevance. Note that many\napplications demand continual model adjustments post-training to address\ndeficiencies or undesirable behaviors. There is an increasing interest in\nefficient, lightweight methods for on-the-fly model modifications. To this end,\nrecent years have seen a burgeoning in the techniques of knowledge editing for\nLLMs, which aim to efficiently modify LLMs' behaviors within specific domains\nwhile preserving overall performance across various inputs. In this paper, we\nfirst define the knowledge editing problem and then provide a comprehensive\nreview of cutting-edge approaches. Drawing inspiration from educational and\ncognitive research theories, we propose a unified categorization criterion that\nclassifies knowledge editing methods into three groups: resorting to external\nknowledge, merging knowledge into the model, and editing intrinsic knowledge.\nFurthermore, we introduce a new benchmark, KnowEdit, for a comprehensive\nempirical evaluation of representative knowledge editing approaches.\nAdditionally, we provide an in-depth analysis of knowledge location, which can\nprovide a deeper understanding of the knowledge structures inherent within\nLLMs. Finally, we discuss several potential applications of knowledge editing,\noutlining its broad and impactful implications.\n","authors":["Ningyu Zhang","Yunzhi Yao","Bozhong Tian","Peng Wang","Shumin Deng","Mengru Wang","Zekun Xi","Shengyu Mao","Jintian Zhang","Yuansheng Ni","Siyuan Cheng","Ziwen Xu","Xin Xu","Jia-Chen Gu","Yong Jiang","Pengjun Xie","Fei Huang","Lei Liang","Zhiqiang Zhang","Xiaowei Zhu","Jun Zhou","Huajun Chen"],"pdf_url":"https://arxiv.org/pdf/2401.01286v2.pdf","comment":"Ongoing work; 52 pages, 280 citations; benchmark is available at\n  https://huggingface.co/datasets/zjunlp/KnowEdit code is available at\n  https://github.com/zjunlp/EasyEdit paper list is available at\n  https://github.com/zjunlp/KnowledgeEditingPapers"},{"id":"http://arxiv.org/abs/2301.13128v2","updated":"2024-01-08T16:24:17Z","published":"2023-01-30T18:07:09Z","title":"Standardized CycleGAN training for unsupervised stain adaptation in\n  invasive carcinoma classification for breast histopathology","summary":"  Generalization is one of the main challenges of computational pathology.\nSlide preparation heterogeneity and the diversity of scanners lead to poor\nmodel performance when used on data from medical centers not seen during\ntraining. In order to achieve stain invariance in breast invasive carcinoma\npatch classification, we implement a stain translation strategy using cycleGANs\nfor unsupervised image-to-image translation. We compare three cycleGAN-based\napproaches to a baseline classification model obtained without any stain\ninvariance strategy. Two of the proposed approaches use cycleGAN's translations\nat inference or training in order to build stain-specific classification\nmodels. The last method uses them for stain data augmentation during training.\nThis constrains the classification model to learn stain-invariant features.\nBaseline metrics are set by training and testing the baseline classification\nmodel on a reference stain. We assessed performances using three medical\ncenters with H&E and H&E&S staining. Every approach tested in this study\nimproves baseline metrics without needing labels on target stains. The stain\naugmentation-based approach produced the best results on every stain. Each\nmethod's pros and cons are studied and discussed in this paper. However,\ntraining highly performing cycleGANs models in itself represents a challenge.\nIn this work, we introduce a systematical method for optimizing cycleGAN\ntraining by setting a novel stopping criterion. This method has the benefit of\nnot requiring any visual inspection of cycleGAN results and proves superiority\nto methods using a predefined number of training epochs. In addition, we also\nstudy the minimal amount of data required for cycleGAN training.\n","authors":["Nicolas Nerrienet","Rémy Peyret","Marie Sockeel","Stéphane Sockeel"],"pdf_url":"https://arxiv.org/pdf/2301.13128v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03993v1","updated":"2024-01-08T16:15:43Z","published":"2024-01-08T16:15:43Z","title":"Behavioural Cloning in VizDoom","summary":"  This paper describes methods for training autonomous agents to play the game\n\"Doom 2\" through Imitation Learning (IL) using only pixel data as input. We\nalso explore how Reinforcement Learning (RL) compares to IL for humanness by\ncomparing camera movement and trajectory data. Through behavioural cloning, we\nexamine the ability of individual models to learn varying behavioural traits.\nWe attempt to mimic the behaviour of real players with different play styles,\nand find we can train agents that behave aggressively, passively, or simply\nmore human-like than traditional AIs. We propose these methods of introducing\nmore depth and human-like behaviour to agents in video games. The trained IL\nagents perform on par with the average players in our dataset, whilst\noutperforming the worst players. While performance was not as strong as common\nRL approaches, it provides much stronger human-like behavioural traits to the\nagent.\n","authors":["Ryan Spick","Timothy Bradley","Ayush Raina","Pierluigi Vito Amadori","Guy Moss"],"pdf_url":"https://arxiv.org/pdf/2401.03993v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2401.03989v1","updated":"2024-01-08T16:08:53Z","published":"2024-01-08T16:08:53Z","title":"MS-DETR: Efficient DETR Training with Mixed Supervision","summary":"  DETR accomplishes end-to-end object detection through iteratively generating\nmultiple object candidates based on image features and promoting one candidate\nfor each ground-truth object. The traditional training procedure using\none-to-one supervision in the original DETR lacks direct supervision for the\nobject detection candidates.\n  We aim at improving the DETR training efficiency by explicitly supervising\nthe candidate generation procedure through mixing one-to-one supervision and\none-to-many supervision. Our approach, namely MS-DETR, is simple, and places\none-to-many supervision to the object queries of the primary decoder that is\nused for inference. In comparison to existing DETR variants with one-to-many\nsupervision, such as Group DETR and Hybrid DETR, our approach does not need\nadditional decoder branches or object queries. The object queries of the\nprimary decoder in our approach directly benefit from one-to-many supervision\nand thus are superior in object candidate prediction. Experimental results show\nthat our approach outperforms related DETR variants, such as DN-DETR, Hybrid\nDETR, and Group DETR, and the combination with related DETR variants further\nimproves the performance.\n","authors":["Chuyang Zhao","Yifan Sun","Wenhao Wang","Qiang Chen","Errui Ding","Yi Yang","Jingdong Wang"],"pdf_url":"https://arxiv.org/pdf/2401.03989v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.00314v2","updated":"2024-01-08T14:57:36Z","published":"2023-09-01T07:55:53Z","title":"ARFA: An Asymmetric Receptive Field Autoencoder Model for Spatiotemporal\n  Prediction","summary":"  Spatiotemporal prediction aims to generate future sequences by paradigms\nlearned from historical contexts. It is essential in numerous domains, such as\ntraffic flow prediction and weather forecasting. Recently, research in this\nfield has been predominantly driven by deep neural networks based on\nautoencoder architectures. However, existing methods commonly adopt autoencoder\narchitectures with identical receptive field sizes. To address this issue, we\npropose an Asymmetric Receptive Field Autoencoder (ARFA) model, which\nintroduces corresponding sizes of receptive field modules tailored to the\ndistinct functionalities of the encoder and decoder. In the encoder, we present\na large kernel module for global spatiotemporal feature extraction. In the\ndecoder, we develop a small kernel module for local spatiotemporal information\nreconstruction. Experimental results demonstrate that ARFA consistently\nachieves state-of-the-art performance on popular datasets. Additionally, we\nconstruct the RainBench, a large-scale radar echo dataset for precipitation\nprediction, to address the scarcity of meteorological data in the domain.\n","authors":["Wenxuan Zhang","Xuechao Zou","Li Wu","Xiaoying Wang","Jianqiang Huang","Junliang Xing"],"pdf_url":"https://arxiv.org/pdf/2309.00314v2.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2401.03939v1","updated":"2024-01-08T14:57:32Z","published":"2024-01-08T14:57:32Z","title":"Multi-scale attention-based instance segmentation for measuring crystals\n  with large size variation","summary":"  Quantitative measurement of crystals in high-resolution images allows for\nimportant insights into underlying material characteristics. Deep learning has\nshown great progress in vision-based automatic crystal size measurement, but\ncurrent instance segmentation methods reach their limits with images that have\nlarge variation in crystal size or hard to detect crystal boundaries. Even\nsmall image segmentation errors, such as incorrectly fused or separated\nsegments, can significantly lower the accuracy of the measured results. Instead\nof improving the existing pixel-wise boundary segmentation methods, we propose\nto use an instance-based segmentation method, which gives more robust\nsegmentation results to improve measurement accuracy. Our novel method enhances\nflow maps with a size-aware multi-scale attention module. The attention module\nadaptively fuses information from multiple scales and focuses on the most\nrelevant scale for each segmented image area. We demonstrate that our proposed\nattention fusion strategy outperforms state-of-the-art instance and boundary\nsegmentation methods, as well as simple average fusion of multi-scale\npredictions. We evaluate our method on a refractory raw material dataset of\nhigh-resolution images with large variation in crystal size and show that our\nmodel can be used to calculate the crystal size more accurately than existing\nmethods.\n","authors":["Theresa Neubauer","Astrid Berg","Maria Wimmer","Dimitrios Lenis","David Major","Philip Matthias Winter","Gaia Romana De Paolis","Johannes Novotny","Daniel Lüftner","Katja Reinharter","Katja Bühler"],"pdf_url":"https://arxiv.org/pdf/2401.03939v1.pdf","comment":"has been accepted for publication in IEEE Transactions on\n  Instrumentation and Measurement"},{"id":"http://arxiv.org/abs/2307.03538v3","updated":"2024-01-08T14:54:49Z","published":"2023-07-07T12:00:38Z","title":"Language-free Compositional Action Generation via Decoupling Refinement","summary":"  Composing simple elements into complex concepts is crucial yet challenging,\nespecially for 3D action generation. Existing methods largely rely on extensive\nneural language annotations to discern composable latent semantics, a process\nthat is often costly and labor-intensive. In this study, we introduce a novel\nframework to generate compositional actions without reliance on language\nauxiliaries. Our approach consists of three main components: Action Coupling,\nConditional Action Generation, and Decoupling Refinement. Action Coupling\nutilizes an energy model to extract the attention masks of each sub-action,\nsubsequently integrating two actions using these attentions to generate\npseudo-training examples. Then, we employ a conditional generative model, CVAE,\nto learn a latent space, facilitating the diverse generation. Finally, we\npropose Decoupling Refinement, which leverages a self-supervised pre-trained\nmodel MAE to ensure semantic consistency between the sub-actions and\ncompositional actions. This refinement process involves rendering generated 3D\nactions into 2D space, decoupling these images into two sub-segments, using the\nMAE model to restore the complete image from sub-segments, and constraining the\nrecovered images to match images rendered from raw sub-actions. Due to the lack\nof existing datasets containing both sub-actions and compositional actions, we\ncreated two new datasets, named HumanAct-C and UESTC-C, and present a\ncorresponding evaluation metric. Both qualitative and quantitative assessments\nare conducted to show our efficacy.\n","authors":["Xiao Liu","Guangyi Chen","Yansong Tang","Guangrun Wang","Xiao-Ping Zhang","Ser-Nam Lim"],"pdf_url":"https://arxiv.org/pdf/2307.03538v3.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2308.04397v2","updated":"2024-01-08T14:50:29Z","published":"2023-08-08T17:01:33Z","title":"LEFormer: A Hybrid CNN-Transformer Architecture for Accurate Lake\n  Extraction from Remote Sensing Imagery","summary":"  Lake extraction from remote sensing images is challenging due to the complex\nlake shapes and inherent data noises. Existing methods suffer from blurred\nsegmentation boundaries and poor foreground modeling. This paper proposes a\nhybrid CNN-Transformer architecture, called LEFormer, for accurate lake\nextraction. LEFormer contains three main modules: CNN encoder, Transformer\nencoder, and cross-encoder fusion. The CNN encoder effectively recovers local\nspatial information and improves fine-scale details. Simultaneously, the\nTransformer encoder captures long-range dependencies between sequences of any\nlength, allowing them to obtain global features and context information. The\ncross-encoder fusion module integrates the local and global features to improve\nmask prediction. Experimental results show that LEFormer consistently achieves\nstate-of-the-art performance and efficiency on the Surface Water and the\nQinghai-Tibet Plateau Lake datasets. Specifically, LEFormer achieves 90.86% and\n97.42% mIoU on two datasets with a parameter count of 3.61M, respectively,\nwhile being 20 minor than the previous best lake extraction method. The source\ncode is available at https://github.com/BastianChen/LEFormer.\n","authors":["Ben Chen","Xuechao Zou","Yu Zhang","Jiayu Li","Kai Li","Junliang Xing","Pin Tao"],"pdf_url":"https://arxiv.org/pdf/2308.04397v2.pdf","comment":"Accepted by ICASSP 2024"},{"id":"http://arxiv.org/abs/2401.03922v1","updated":"2024-01-08T14:33:57Z","published":"2024-01-08T14:33:57Z","title":"Structure-focused Neurodegeneration Convolutional Neural Network for\n  Modeling and Classification of Alzheimer's Disease","summary":"  Alzheimer's disease (AD), the predominant form of dementia, poses a growing\nglobal challenge and underscores the urgency of accurate and early diagnosis.\nThe clinical technique radiologists adopt for distinguishing between mild\ncognitive impairment (MCI) and AD using Machine Resonance Imaging (MRI)\nencounter hurdles because they are not consistent and reliable. Machine\nlearning has been shown to offer promise for early AD diagnosis. However,\nexisting models focused on focal fine-grain features without considerations to\nfocal structural features that give off information on neurodegeneration of the\nbrain cerebral cortex. Therefore, this paper proposes a machine learning (ML)\nframework that integrates Gamma correction, an image enhancement technique, and\nincludes a structure-focused neurodegeneration convolutional neural network\n(CNN) architecture called SNeurodCNN for discriminating between AD and MCI. The\nML framework leverages the mid-sagittal and para-sagittal brain image\nviewpoints of the structure-focused Alzheimer's Disease Neuroimaging Initiative\n(ADNI) dataset. Through experiments, our proposed machine learning framework\nshows exceptional performance. The parasagittal viewpoint set achieves 97.8%\naccuracy, with 97.0% specificity and 98.5% sensitivity. The midsagittal\nviewpoint is shown to present deeper insights into the structural brain changes\ngiven the increase in accuracy, specificity, and sensitivity, which are 98.1%\n97.2%, and 99.0%, respectively. Using GradCAM technique, we show that our\nproposed model is capable of capturing the structural dynamics of MCI and AD\nwhich exist about the frontal lobe, occipital lobe, cerebellum, and parietal\nlobe. Therefore, our model itself as a potential brain structural change\nDigi-Biomarker for early diagnosis of AD.\n","authors":["Simisola Odimayo","Chollette C. Olisah","Khadija Mohammed"],"pdf_url":"https://arxiv.org/pdf/2401.03922v1.pdf","comment":"22 Pages, 2 figures, 3 tables"},{"id":"http://arxiv.org/abs/2401.03914v1","updated":"2024-01-08T14:21:02Z","published":"2024-01-08T14:21:02Z","title":"D3PRefiner: A Diffusion-based Denoise Method for 3D Human Pose\n  Refinement","summary":"  Three-dimensional (3D) human pose estimation using a monocular camera has\ngained increasing attention due to its ease of implementation and the abundance\nof data available from daily life. However, owing to the inherent depth\nambiguity in images, the accuracy of existing monocular camera-based 3D pose\nestimation methods remains unsatisfactory, and the estimated 3D poses usually\ninclude much noise. By observing the histogram of this noise, we find each\ndimension of the noise follows a certain distribution, which indicates the\npossibility for a neural network to learn the mapping between noisy poses and\nground truth poses. In this work, in order to obtain more accurate 3D poses, a\nDiffusion-based 3D Pose Refiner (D3PRefiner) is proposed to refine the output\nof any existing 3D pose estimator. We first introduce a conditional\nmultivariate Gaussian distribution to model the distribution of noisy 3D poses,\nusing paired 2D poses and noisy 3D poses as conditions to achieve greater\naccuracy. Additionally, we leverage the architecture of current diffusion\nmodels to convert the distribution of noisy 3D poses into ground truth 3D\nposes. To evaluate the effectiveness of the proposed method, two\nstate-of-the-art sequence-to-sequence 3D pose estimators are used as basic 3D\npose estimation models, and the proposed method is evaluated on different types\nof 2D poses and different lengths of the input sequence. Experimental results\ndemonstrate the proposed architecture can significantly improve the performance\nof current sequence-to-sequence 3D pose estimators, with a reduction of at\nleast 10.3% in the mean per joint position error (MPJPE) and at least 11.0% in\nthe Procrustes MPJPE (P-MPJPE).\n","authors":["Danqi Yan","Qing Gao","Yuepeng Qian","Xinxing Chen","Chenglong Fu","Yuquan Leng"],"pdf_url":"https://arxiv.org/pdf/2401.03914v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03912v1","updated":"2024-01-08T14:16:54Z","published":"2024-01-08T14:16:54Z","title":"Attention-Guided Erasing: A Novel Augmentation Method for Enhancing\n  Downstream Breast Density Classification","summary":"  The assessment of breast density is crucial in the context of breast cancer\nscreening, especially in populations with a higher percentage of dense breast\ntissues. This study introduces a novel data augmentation technique termed\nAttention-Guided Erasing (AGE), devised to enhance the downstream\nclassification of four distinct breast density categories in mammography\nfollowing the BI-RADS recommendation in the Vietnamese cohort. The proposed\nmethod integrates supplementary information during transfer learning, utilizing\nvisual attention maps derived from a vision transformer backbone trained using\nthe self-supervised DINO method. These maps are utilized to erase background\nregions in the mammogram images, unveiling only the potential areas of dense\nbreast tissues to the network. Through the incorporation of AGE during transfer\nlearning with varying random probabilities, we consistently surpass\nclassification performance compared to scenarios without AGE and the\ntraditional random erasing transformation. We validate our methodology using\nthe publicly available VinDr-Mammo dataset. Specifically, we attain a mean\nF1-score of 0.5910, outperforming values of 0.5594 and 0.5691 corresponding to\nscenarios without AGE and with random erasing (RE), respectively. This\nsuperiority is further substantiated by t-tests, revealing a p-value of\np<0.0001, underscoring the statistical significance of our approach.\n","authors":["Adarsh Bhandary Panambur","Hui Yu","Sheethal Bhat","Prathmesh Madhu","Siming Bayer","Andreas Maier"],"pdf_url":"https://arxiv.org/pdf/2401.03912v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03907v1","updated":"2024-01-08T14:10:24Z","published":"2024-01-08T14:10:24Z","title":"RoboFusion: Towards Robust Multi-Modal 3D obiect Detection via SAM","summary":"  Multi-modal 3D object detectors are dedicated to exploring secure and\nreliable perception systems for autonomous driving (AD). However, while\nachieving state-of-the-art (SOTA) performance on clean benchmark datasets, they\ntend to overlook the complexity and harsh conditions of real-world\nenvironments. Meanwhile, with the emergence of visual foundation models (VFMs),\nopportunities and challenges are presented for improving the robustness and\ngeneralization of multi-modal 3D object detection in autonomous driving.\nTherefore, we propose RoboFusion, a robust framework that leverages VFMs like\nSAM to tackle out-of-distribution (OOD) noise scenarios. We first adapt the\noriginal SAM for autonomous driving scenarios named SAM-AD. To align SAM or\nSAM-AD with multi-modal methods, we then introduce AD-FPN for upsampling the\nimage features extracted by SAM. We employ wavelet decomposition to denoise the\ndepth-guided images for further noise reduction and weather interference.\nLastly, we employ self-attention mechanisms to adaptively reweight the fused\nfeatures, enhancing informative features while suppressing excess noise. In\nsummary, our RoboFusion gradually reduces noise by leveraging the\ngeneralization and robustness of VFMs, thereby enhancing the resilience of\nmulti-modal 3D object detection. Consequently, our RoboFusion achieves\nstate-of-the-art performance in noisy scenarios, as demonstrated by the KITTI-C\nand nuScenes-C benchmarks.\n","authors":["Ziying Song","Guoxing Zhang","Lin Liu","Lei Yang","Shaoqing Xu","Caiyan Jia","Feiyang Jia","Li Wang"],"pdf_url":"https://arxiv.org/pdf/2401.03907v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03901v1","updated":"2024-01-08T14:01:59Z","published":"2024-01-08T14:01:59Z","title":"STAIR: Spatial-Temporal Reasoning with Auditable Intermediate Results\n  for Video Question Answering","summary":"  Recently we have witnessed the rapid development of video question answering\nmodels. However, most models can only handle simple videos in terms of temporal\nreasoning, and their performance tends to drop when answering\ntemporal-reasoning questions on long and informative videos. To tackle this\nproblem we propose STAIR, a Spatial-Temporal Reasoning model with Auditable\nIntermediate Results for video question answering. STAIR is a neural module\nnetwork, which contains a program generator to decompose a given question into\na hierarchical combination of several sub-tasks, and a set of lightweight\nneural modules to complete each of these sub-tasks. Though neural module\nnetworks are already widely studied on image-text tasks, applying them to\nvideos is a non-trivial task, as reasoning on videos requires different\nabilities. In this paper, we define a set of basic video-text sub-tasks for\nvideo question answering and design a set of lightweight modules to complete\nthem. Different from most prior works, modules of STAIR return intermediate\noutputs specific to their intentions instead of always returning attention\nmaps, which makes it easier to interpret and collaborate with pre-trained\nmodels. We also introduce intermediate supervision to make these intermediate\noutputs more accurate. We conduct extensive experiments on several video\nquestion answering datasets under various settings to show STAIR's performance,\nexplainability, compatibility with pre-trained models, and applicability when\nprogram annotations are not available. Code:\nhttps://github.com/yellow-binary-tree/STAIR\n","authors":["Yueqian Wang","Yuxuan Wang","Kai Chen","Dongyan Zhao"],"pdf_url":"https://arxiv.org/pdf/2401.03901v1.pdf","comment":"To appear in AAAI 2024"},{"id":"http://arxiv.org/abs/2401.03890v1","updated":"2024-01-08T13:42:59Z","published":"2024-01-08T13:42:59Z","title":"A Survey on 3D Gaussian Splatting","summary":"  3D Gaussian splatting (3D GS) has recently emerged as a transformative\ntechnique in the explicit radiance field and computer graphics landscape. This\ninnovative approach, characterized by the utilization of millions of 3D\nGaussians, represents a significant departure from the neural radiance field\n(NeRF) methodologies, which predominantly use implicit, coordinate-based models\nto map spatial coordinates to pixel values. 3D GS, with its explicit scene\nrepresentations and differentiable rendering algorithms, not only promises\nreal-time rendering capabilities but also introduces unprecedented levels of\ncontrol and editability. This positions 3D GS as a potential game-changer for\nthe next generation of 3D reconstruction and representation. In the present\npaper, we provide the first systematic overview of the recent developments and\ncritical contributions in the domain of 3D GS. We begin with a detailed\nexploration of the underlying principles and the driving forces behind the\nadvent of 3D GS, setting the stage for understanding its significance. A focal\npoint of our discussion is the practical applicability of 3D GS. By\nfacilitating real-time performance, 3D GS opens up a plethora of applications,\nranging from virtual reality to interactive media and beyond. This is\ncomplemented by a comparative analysis of leading 3D GS models, evaluated\nacross various benchmark tasks to highlight their performance and practical\nutility. The survey concludes by identifying current challenges and suggesting\npotential avenues for future research in this domain. Through this survey, we\naim to provide a valuable resource for both newcomers and seasoned researchers,\nfostering further exploration and advancement in applicable and explicit\nradiance field representation.\n","authors":["Guikun Chen","Wenguan Wang"],"pdf_url":"https://arxiv.org/pdf/2401.03890v1.pdf","comment":"Ongoing project"},{"id":"http://arxiv.org/abs/2310.15725v2","updated":"2024-01-08T13:36:14Z","published":"2023-10-24T11:00:56Z","title":"Ranking-based Adaptive Query Generation for DETRs in Crowded Pedestrian\n  Detection","summary":"  DEtection TRansformer (DETR) and its variants (DETRs) have been successfully\napplied to crowded pedestrian detection, which achieved promising performance.\nHowever, we find that, in different degrees of crowded scenes, the number of\nDETRs' queries must be adjusted manually, otherwise, the performance would\ndegrade to varying degrees. In this paper, we first analyze the two current\nquery generation methods and summarize four guidelines for designing the\nadaptive query generation method. Then, we propose Rank-based Adaptive Query\nGeneration (RAQG) to alleviate the problem. Specifically, we design a rank\nprediction head that can predict the rank of the lowest confidence positive\ntraining sample produced by the encoder. Based on the predicted rank, we design\nan adaptive selection method that can adaptively select coarse detection\nresults produced by the encoder to generate queries. Moreover, to train the\nrank prediction head better, we propose Soft Gradient L1 Loss. The gradient of\nSoft Gradient L1 Loss is continuous, which can describe the relationship\nbetween the loss value and the updated value of model parameters granularly.\nOur method is simple and effective, which can be plugged into any DETRs to make\nit query-adaptive in theory. The experimental results on Crowdhuman dataset and\nCitypersons dataset show that our method can adaptively generate queries for\nDETRs and achieve competitive results. Especially, our method achieves\nstate-of-the-art 39.4% MR on Crowdhuman dataset.\n","authors":["Feng Gao","Jiaxu Leng","Ji Gan","Xinbo Gao"],"pdf_url":"https://arxiv.org/pdf/2310.15725v2.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2312.13314v2","updated":"2024-01-08T13:30:56Z","published":"2023-12-20T09:39:19Z","title":"Unlocking Pre-trained Image Backbones for Semantic Image Synthesis","summary":"  Semantic image synthesis, i.e., generating images from user-provided semantic\nlabel maps, is an important conditional image generation task as it allows to\ncontrol both the content as well as the spatial layout of generated images.\nAlthough diffusion models have pushed the state of the art in generative image\nmodeling, the iterative nature of their inference process makes them\ncomputationally demanding. Other approaches such as GANs are more efficient as\nthey only need a single feed-forward pass for generation, but the image quality\ntends to suffer on large and diverse datasets. In this work, we propose a new\nclass of GAN discriminators for semantic image synthesis that generates highly\nrealistic images by exploiting feature backbone networks pre-trained for tasks\nsuch as image classification. We also introduce a new generator architecture\nwith better context modeling and using cross-attention to inject noise into\nlatent variables, leading to more diverse generated images. Our model, which we\ndub DP-SIMS, achieves state-of-the-art results in terms of image quality and\nconsistency with the input label maps on ADE-20K, COCO-Stuff, and Cityscapes,\nsurpassing recent diffusion models while requiring two orders of magnitude less\ncompute for inference.\n","authors":["Tariq Berrada","Jakob Verbeek","Camille Couprie","Karteek Alahari"],"pdf_url":"https://arxiv.org/pdf/2312.13314v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.11335v3","updated":"2024-01-08T13:29:26Z","published":"2023-06-20T07:06:04Z","title":"DamWorld: Progressive Reasoning with World Models for Robotic\n  Manipulation","summary":"  The research on embodied AI has greatly promoted the development of robot\nmanipulation. However, it still faces significant challenges in various aspects\nsuch as benchmark construction, multi-modal perception and decision-making, and\nphysical execution. Previous robot manipulation simulators were primarily\ndesigned to enrich manipulation types and types of objects while neglecting the\nbalance between physical manipulation and language instruction complexity in\nmulti-modal environments. This paper proposes a new robot manipulation\nsimulator and builds a comprehensive and systematic robot manipulation\nbenchmark with progressive reasoning tasks called SeaWave (i.e., a progressive\nreasoning benchmark). It provides a standard test platform for embedded AI\nagents in a multi-modal environment, which can evaluate and execute four levels\nof human natural language instructions at the same time.\n  Previous world model-based robot manipulation work lacked research on the\nperception and decision-making of complex instructions in multi-modal\nenvironments. To this end, we propose a new world model tailored for\ncross-modal robot manipulation called DamWorld. Specifically, DamWorld takes\nthe current visual scene and predicted execution actions based on natural\nlanguage instructions as input, and uses the next action frame to supervise the\noutput of the world model to force the model to learn robot manipulation\nconsistent with world knowledge. Compared with the renowned baselines (e.g.,\nRT-1), our DamWorld improves the manipulation success rate by 5.6% on average\non four levels of progressive reasoning tasks. It is worth noting that on the\nmost challenging level 4 manipulation task, DamWorld still improved by 9.0%\ncompared to prior works.\n","authors":["Pengzhen Ren","Kaidong Zhang","Hetao Zheng","Zixuan Li","Yuhang Wen","Fengda Zhu","Mas Ma","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2306.11335v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.11010v2","updated":"2024-01-08T13:27:47Z","published":"2022-11-20T16:01:31Z","title":"Revisiting Color-Event based Tracking: A Unified Network, Dataset, and\n  Metric","summary":"  Combining the Color and Event cameras (also called Dynamic Vision Sensors,\nDVS) for robust object tracking is a newly emerging research topic in recent\nyears. Existing color-event tracking framework usually contains multiple\nscattered modules which may lead to low efficiency and high computational\ncomplexity, including feature extraction, fusion, matching, interactive\nlearning, etc. In this paper, we propose a single-stage backbone network for\nColor-Event Unified Tracking (CEUTrack), which achieves the above functions\nsimultaneously. Given the event points and RGB frames, we first transform the\npoints into voxels and crop the template and search regions for both\nmodalities, respectively. Then, these regions are projected into tokens and\nparallelly fed into the unified Transformer backbone network. The output\nfeatures will be fed into a tracking head for target object localization. Our\nproposed CEUTrack is simple, effective, and efficient, which achieves over 75\nFPS and new SOTA performance. To better validate the effectiveness of our model\nand address the data deficiency of this task, we also propose a generic and\nlarge-scale benchmark dataset for color-event tracking, termed COESOT, which\ncontains 90 categories and 1354 video sequences. Additionally, a new evaluation\nmetric named BOC is proposed in our evaluation toolkit to evaluate the\nprominence with respect to the baseline methods. We hope the newly proposed\nmethod, dataset, and evaluation metric provide a better platform for\ncolor-event-based tracking. The dataset, toolkit, and source code will be\nreleased on: \\url{https://github.com/Event-AHU/COESOT}.\n","authors":["Chuanming Tang","Xiao Wang","Ju Huang","Bo Jiang","Lin Zhu","Jianlin Zhang","Yaowei Wang","Yonghong Tian"],"pdf_url":"https://arxiv.org/pdf/2211.11010v2.pdf","comment":"In Peer Review"},{"id":"http://arxiv.org/abs/2401.03872v1","updated":"2024-01-08T13:04:28Z","published":"2024-01-08T13:04:28Z","title":"A New Dataset and a Distractor-Aware Architecture for Transparent Object\n  Tracking","summary":"  Performance of modern trackers degrades substantially on transparent objects\ncompared to opaque objects. This is largely due to two distinct reasons.\nTransparent objects are unique in that their appearance is directly affected by\nthe background. Furthermore, transparent object scenes often contain many\nvisually similar objects (distractors), which often lead to tracking failure.\nHowever, development of modern tracking architectures requires large training\nsets, which do not exist in transparent object tracking. We present two\ncontributions addressing the aforementioned issues. We propose the first\ntransparent object tracking training dataset Trans2k that consists of over 2k\nsequences with 104,343 images overall, annotated by bounding boxes and\nsegmentation masks. Standard trackers trained on this dataset consistently\nimprove by up to 16%. Our second contribution is a new distractor-aware\ntransparent object tracker (DiTra) that treats localization accuracy and target\nidentification as separate tasks and implements them by a novel architecture.\nDiTra sets a new state-of-the-art in transparent object tracking and\ngeneralizes well to opaque objects.\n","authors":["Alan Lukezic","Ziga Trojer","Jiri Matas","Matej Kristan"],"pdf_url":"https://arxiv.org/pdf/2401.03872v1.pdf","comment":"Under the review. arXiv admin note: substantial text overlap with\n  arXiv:2210.03436"},{"id":"http://arxiv.org/abs/2401.03870v1","updated":"2024-01-08T13:01:54Z","published":"2024-01-08T13:01:54Z","title":"Gramformer: Learning Crowd Counting via Graph-Modulated Transformer","summary":"  Transformer has been popular in recent crowd counting work since it breaks\nthe limited receptive field of traditional CNNs. However, since crowd images\nalways contain a large number of similar patches, the self-attention mechanism\nin Transformer tends to find a homogenized solution where the attention maps of\nalmost all patches are identical. In this paper, we address this problem by\nproposing Gramformer: a graph-modulated transformer to enhance the network by\nadjusting the attention and input node features respectively on the basis of\ntwo different types of graphs. Firstly, an attention graph is proposed to\ndiverse attention maps to attend to complementary information. The graph is\nbuilding upon the dissimilarities between patches, modulating the attention in\nan anti-similarity fashion. Secondly, a feature-based centrality encoding is\nproposed to discover the centrality positions or importance of nodes. We encode\nthem with a proposed centrality indices scheme to modulate the node features\nand similarity relationships. Extensive experiments on four challenging crowd\ncounting datasets have validated the competitiveness of the proposed method.\nCode is available at {https://github.com/LoraLinH/Gramformer}.\n","authors":["Hui Lin","Zhiheng Ma","Xiaopeng Hong","Qinnan Shangguan","Deyu Meng"],"pdf_url":"https://arxiv.org/pdf/2401.03870v1.pdf","comment":"This is the accepted version of the paper and supplemental material\n  to appear in AAAI 2024. Please cite the final published version. Code is\n  available at {https://github.com/LoraLinH/Gramformer}"},{"id":"http://arxiv.org/abs/2303.14806v2","updated":"2024-01-08T12:54:09Z","published":"2023-03-26T20:19:28Z","title":"A Contrastive Learning Scheme with Transformer Innate Patches","summary":"  This paper presents Contrastive Transformer, a contrastive learning scheme\nusing the Transformer innate patches. Contrastive Transformer enables existing\ncontrastive learning techniques, often used for image classification, to\nbenefit dense downstream prediction tasks such as semantic segmentation. The\nscheme performs supervised patch-level contrastive learning, selecting the\npatches based on the ground truth mask, subsequently used for hard-negative and\nhard-positive sampling. The scheme applies to all vision-transformer\narchitectures, is easy to implement, and introduces minimal additional memory\nfootprint. Additionally, the scheme removes the need for huge batch sizes, as\neach patch is treated as an image.\n  We apply and test Contrastive Transformer for the case of aerial image\nsegmentation, known for low-resolution data, large class imbalance, and similar\nsemantic classes. We perform extensive experiments to show the efficacy of the\nContrastive Transformer scheme on the ISPRS Potsdam aerial image segmentation\ndataset. Additionally, we show the generalizability of our scheme by applying\nit to multiple inherently different Transformer architectures. Ultimately, the\nresults show a consistent increase in mean IoU across all classes.\n","authors":["Sander Riisøen Jyhne","Per-Arne Andersen","Morten Goodwin"],"pdf_url":"https://arxiv.org/pdf/2303.14806v2.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2401.03854v1","updated":"2024-01-08T12:35:15Z","published":"2024-01-08T12:35:15Z","title":"TIER: Text and Image Encoder-based Regression for AIGC Image Quality\n  Assessment","summary":"  Recently, AIGC image quality assessment (AIGCIQA), which aims to assess the\nquality of AI-generated images from a human perception perspective, has emerged\nas a new topic in computer vision. Unlike common image quality assessment tasks\nwhere images are derived from original ones distorted by noise, blur, and\ncompression, in AIGCIQA tasks, images are typically generated by generative\nmodels using text prompts. Considerable efforts have been made in the past\nyears to advance AIGCIQA. However, most existing AIGCIQA methods regress\npredicted scores directly from individual generated images, overlooking the\ninformation contained in the text prompts of these images. This oversight\npartially limits the performance of these AIGCIQA methods. To address this\nissue, we propose a text and image encoder-based regression (TIER) framework.\nSpecifically, we process the generated images and their corresponding text\nprompts as inputs, utilizing a text encoder and an image encoder to extract\nfeatures from these text prompts and generated images, respectively. To\ndemonstrate the effectiveness of our proposed TIER method, we conduct extensive\nexperiments on several mainstream AIGCIQA databases, including AGIQA-1K,\nAGIQA-3K, and AIGCIQA2023. The experimental results indicate that our proposed\nTIER method generally demonstrates superior performance compared to baseline in\nmost cases.\n","authors":["Jiquan Yuan","Xinyan Cao","Jinming Che","Qinyuan Wang","Sen Liang","Wei Ren","Jinlong Lin","Xixin Cao"],"pdf_url":"https://arxiv.org/pdf/2401.03854v1.pdf","comment":"12 pages, 8 figures. arXiv admin note: text overlap with\n  arXiv:2312.05897"},{"id":"http://arxiv.org/abs/2401.03851v1","updated":"2024-01-08T12:30:23Z","published":"2024-01-08T12:30:23Z","title":"Aligned with LLM: a new multi-modal training paradigm for encoding fMRI\n  activity in visual cortex","summary":"  Recently, there has been a surge in the popularity of pre trained large\nlanguage models (LLMs) (such as GPT-4), sweeping across the entire Natural\nLanguage Processing (NLP) and Computer Vision (CV) communities. These LLMs have\ndemonstrated advanced multi-modal understanding capabilities and showcased\nstrong performance across various benchmarks. The LLM has started to embody\ntraits of artificial general intelligence, which holds vital guidance for\nenhancing brain-like characteristics within visual encoding models. Hence, This\npaper proposes a new multi-modal training paradigm, aligning with LLM, for\nencoding fMRI activity in visual cortex. Based on this paradigm, we trained an\nencoding model in fMRI data named the LLM-Visual Encoding Model (LLM-VEM).\nSpecifically, we utilize LLM (miniGPT4) to generate descriptive text for all\nstimulus images, forming a high-quality textual description set. Moreover, we\nuse the pre-trained text encoder (CLIP) to process these detailed descriptions,\nobtaining the text embedding features. Next, we use the contrast loss function\nto minimize the distance between the image embedding features and the text\nembedding features to complete the alignment operation of the stimulus image\nand text information. With the assistance of the pre-trained LLM, this\nalignment process facilitates better learning of the visual encoding model,\nresulting in higher precision. The final experimental results indicate that our\ntraining paradigm has significantly aided in enhancing the performance of the\nvisual encoding model.\n","authors":["Shuxiao Ma","Linyuan Wang","Senbao Hou","Bin Yan"],"pdf_url":"https://arxiv.org/pdf/2401.03851v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.04751v2","updated":"2024-01-08T12:28:19Z","published":"2023-03-08T17:34:15Z","title":"Multimodal Parameter-Efficient Few-Shot Class Incremental Learning","summary":"  Few-Shot Class Incremental Learning (FSCIL) is a challenging continual\nlearning task, where limited training examples are available during several\nlearning sessions. To succeed in this task, it is necessary to avoid\nover-fitting new classes caused by biased distributions in the few-shot\ntraining sets. The general approach to address this issue involves enhancing\nthe representational capability of a pre-defined backbone architecture by\nadding special modules for backward compatibility with older classes. However,\nthis approach has not yet solved the dilemma of ensuring high classification\naccuracy over time while reducing the gap between the performance obtained on\nlarger training sets and the smaller ones. In this work, we propose an\nalternative approach called Continual Parameter-Efficient CLIP (CPE-CLIP) to\nreduce the loss of information between different learning sessions. Instead of\nadapting additional modules to address information loss, we leverage the vast\nknowledge acquired by CLIP in large-scale pre-training and its effectiveness in\ngeneralizing to new concepts. Our approach is multimodal and\nparameter-efficient, relying on learnable prompts for both the language and\nvision encoders to enable transfer learning across sessions. We also introduce\nprompt regularization to improve performance and prevent forgetting. Our\nexperimental results demonstrate that CPE-CLIP significantly improves FSCIL\nperformance compared to state-of-the-art proposals while also drastically\nreducing the number of learnable parameters and training costs.\n","authors":["Marco D'Alessandro","Alberto Alonso","Enrique Calabrés","Mikel Galar"],"pdf_url":"https://arxiv.org/pdf/2303.04751v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03846v1","updated":"2024-01-08T12:16:06Z","published":"2024-01-08T12:16:06Z","title":"UFO: Unidentified Foreground Object Detection in 3D Point Cloud","summary":"  In this paper, we raise a new issue on Unidentified Foreground Object (UFO)\ndetection in 3D point clouds, which is a crucial technology in autonomous\ndriving in the wild. UFO detection is challenging in that existing 3D object\ndetectors encounter extremely hard challenges in both 3D localization and\nOut-of-Distribution (OOD) detection. To tackle these challenges, we suggest a\nnew UFO detection framework including three tasks: evaluation protocol,\nmethodology, and benchmark. The evaluation includes a new approach to measure\nthe performance on our goal, i.e. both localization and OOD detection of UFOs.\nThe methodology includes practical techniques to enhance the performance of our\ngoal. The benchmark is composed of the KITTI Misc benchmark and our additional\nsynthetic benchmark for modeling a more diverse range of UFOs. The proposed\nframework consistently enhances performance by a large margin across all four\nbaseline detectors: SECOND, PointPillars, PV-RCNN, and PartA2, giving insight\nfor future work on UFO detection in the wild.\n","authors":["Hyunjun Choi","Hawook Jeong","Jin Young Choi"],"pdf_url":"https://arxiv.org/pdf/2401.03846v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2401.03844v1","updated":"2024-01-08T12:14:15Z","published":"2024-01-08T12:14:15Z","title":"Fully Attentional Networks with Self-emerging Token Labeling","summary":"  Recent studies indicate that Vision Transformers (ViTs) are robust against\nout-of-distribution scenarios. In particular, the Fully Attentional Network\n(FAN) - a family of ViT backbones, has achieved state-of-the-art robustness. In\nthis paper, we revisit the FAN models and improve their pre-training with a\nself-emerging token labeling (STL) framework. Our method contains a two-stage\ntraining framework. Specifically, we first train a FAN token labeler (FAN-TL)\nto generate semantically meaningful patch token labels, followed by a FAN\nstudent model training stage that uses both the token labels and the original\nclass label. With the proposed STL framework, our best model based on\nFAN-L-Hybrid (77.3M parameters) achieves 84.8% Top-1 accuracy and 42.1% mCE on\nImageNet-1K and ImageNet-C, and sets a new state-of-the-art for ImageNet-A\n(46.1%) and ImageNet-R (56.6%) without using extra data, outperforming the\noriginal FAN counterpart by significant margins. The proposed framework also\ndemonstrates significantly enhanced performance on downstream tasks such as\nsemantic segmentation, with up to 1.7% improvement in robustness over the\ncounterpart model. Code is available at https://github.com/NVlabs/STL.\n","authors":["Bingyin Zhao","Zhiding Yu","Shiyi Lan","Yutao Cheng","Anima Anandkumar","Yingjie Lao","Jose M. Alvarez"],"pdf_url":"https://arxiv.org/pdf/2401.03844v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03836v1","updated":"2024-01-08T11:50:23Z","published":"2024-01-08T11:50:23Z","title":"WidthFormer: Toward Efficient Transformer-based BEV View Transformation","summary":"  In this work, we present WidthFormer, a novel transformer-based\nBird's-Eye-View (BEV) 3D detection method tailored for real-time\nautonomous-driving applications. WidthFormer is computationally efficient,\nrobust and does not require any special engineering effort to deploy. In this\nwork, we propose a novel 3D positional encoding mechanism capable of accurately\nencapsulating 3D geometric information, which enables our model to generate\nhigh-quality BEV representations with only a single transformer decoder layer.\nThis mechanism is also beneficial for existing sparse 3D object detectors.\nInspired by the recently-proposed works, we further improve our model's\nefficiency by vertically compressing the image features when serving as\nattention keys and values. We also introduce two modules to compensate for\npotential information loss due to feature compression. Experimental evaluation\non the widely-used nuScenes 3D object detection benchmark demonstrates that our\nmethod outperforms previous approaches across different 3D detection\narchitectures. More importantly, our model is highly efficient. For example,\nwhen using $256\\times 704$ input images, it achieves 1.5 ms latency on NVIDIA\n3090 GPU. Furthermore, WidthFormer also exhibits strong robustness to different\ndegrees of camera perturbations. Our study offers valuable insights into the\ndeployment of BEV transformation methods in real-world, complex road\nenvironments. Code is available at\nhttps://github.com/ChenhongyiYang/WidthFormer .\n","authors":["Chenhongyi Yang","Tianwei Lin","Lichao Huang","Elliot J. Crowley"],"pdf_url":"https://arxiv.org/pdf/2401.03836v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03835v1","updated":"2024-01-08T11:46:45Z","published":"2024-01-08T11:46:45Z","title":"Limitations of Data-Driven Spectral Reconstruction -- An Optics-Aware\n  Analysis","summary":"  Hyperspectral imaging empowers computer vision systems with the distinct\ncapability of identifying materials through recording their spectral\nsignatures. Recent efforts in data-driven spectral reconstruction aim at\nextracting spectral information from RGB images captured by cost-effective RGB\ncameras, instead of dedicated hardware.\n  In this paper we systematically analyze the performance of such methods,\nevaluating both the practical limitations with respect to current datasets and\noverfitting, as well as fundamental limits with respect to the nature of the\ninformation encoded in the RGB images, and the dependency of this information\non the optical system of the camera.\n  We find that the current models are not robust under slight variations, e.g.,\nin noise level or compression of the RGB file. Both the methods and the\ndatasets are also limited in their ability to cope with metameric colors. This\nissue can in part be overcome with metameric data augmentation. Moreover,\noptical lens aberrations can help to improve the encoding of the metameric\ninformation into the RGB image, which paves the road towards higher performing\nspectral imaging and reconstruction approaches.\n","authors":["Qiang Fu","Matheus Souza","Eunsue Choi","Suhyun Shin","Seung-Hwan Baek","Wolfgang Heidrich"],"pdf_url":"https://arxiv.org/pdf/2401.03835v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03830v1","updated":"2024-01-08T11:37:44Z","published":"2024-01-08T11:37:44Z","title":"A foundation for exact binarized morphological neural networks","summary":"  Training and running deep neural networks (NNs) often demands a lot of\ncomputation and energy-intensive specialized hardware (e.g. GPU, TPU...). One\nway to reduce the computation and power cost is to use binary weight NNs, but\nthese are hard to train because the sign function has a non-smooth gradient. We\npresent a model based on Mathematical Morphology (MM), which can binarize\nConvNets without losing performance under certain conditions, but these\nconditions may not be easy to satisfy in real-world scenarios. To solve this,\nwe propose two new approximation methods and develop a robust theoretical\nframework for ConvNets binarization using MM. We propose as well regularization\nlosses to improve the optimization. We empirically show that our model can\nlearn a complex morphological network, and explore its performance on a\nclassification task.\n","authors":["Theodore Aouad","Hugues Talbot"],"pdf_url":"https://arxiv.org/pdf/2401.03830v1.pdf","comment":"Accepted at conference ICCV 2023 Workshop LBQNN. Same work, different\n  format, accepted at conference NeurIPS 2023 Workshop WANT. 8 pages, 17 pages\n  appendix"},{"id":"http://arxiv.org/abs/2401.03828v1","updated":"2024-01-08T11:35:25Z","published":"2024-01-08T11:35:25Z","title":"A multimodal gesture recognition dataset for desktop human-computer\n  interaction","summary":"  Gesture recognition is an indispensable component of natural and efficient\nhuman-computer interaction technology, particularly in desktop-level\napplications, where it can significantly enhance people's productivity.\nHowever, the current gesture recognition community lacks a suitable\ndesktop-level (top-view perspective) dataset for lightweight gesture capture\ndevices. In this study, we have established a dataset named GR4DHCI. What\ndistinguishes this dataset is its inherent naturalness, intuitive\ncharacteristics, and diversity. Its primary purpose is to serve as a valuable\nresource for the development of desktop-level portable applications. GR4DHCI\ncomprises over 7,000 gesture samples and a total of 382,447 frames for both\nStereo IR and skeletal modalities. We also address the variances in hand\npositioning during desktop interactions by incorporating 27 different hand\npositions into the dataset. Building upon the GR4DHCI dataset, we conducted a\nseries of experimental studies, the results of which demonstrate that the\nfine-grained classification blocks proposed in this paper can enhance the\nmodel's recognition accuracy. Our dataset and experimental findings presented\nin this paper are anticipated to propel advancements in desktop-level gesture\nrecognition research.\n","authors":["Qi Wang","Fengchao Zhu","Guangming Zhu","Liang Zhang","Ning Li","Eryang Gao"],"pdf_url":"https://arxiv.org/pdf/2401.03828v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03814v1","updated":"2024-01-08T11:08:03Z","published":"2024-01-08T11:08:03Z","title":"Gnuastro: visualizing the full dynamic range in color images","summary":"  Color plays a crucial role in the visualization, interpretation, and analysis\nof multi-wavelength astronomical images. However, generating color images that\naccurately represent the full dynamic range of astronomical sources is\nchallenging. In response, Gnuastro v0.22 introduces the program\n'astscript-color-faint-gray', which is extensively documented in the Gnuastro\nmanual. It employs a non-linear transformation to assign an 8-bit RGB\n(Red-Green-Blue) value to brighter pixels, while the fainter ones are shown in\nan inverse grayscale. This approach enables the simultaneous visualization of\nlow surface brightness features within the same image. This research note is\nreproducible with Maneage, on the Git commit 48f5408.\n","authors":["Raúl Infante-Sainz","Mohammad Akhlaghi"],"pdf_url":"https://arxiv.org/pdf/2401.03814v1.pdf","comment":"Accepted RNAAS. Supplementary data on Zenodo\n  (https://doi.org/10.5281/zenodo.10058165), project source on Codeberg\n  (https://codeberg.org/gnuastro/papers/src/branch/color-faint-gray) and\n  archived on Software Heritage\n  (swh:1:dir:1064a48d4bb58d6684c3df33c6633a04d4141d2d)"},{"id":"http://arxiv.org/abs/2401.03806v1","updated":"2024-01-08T10:55:06Z","published":"2024-01-08T10:55:06Z","title":"FM-AE: Frequency-masked Multimodal Autoencoder for Zinc Electrolysis\n  Plate Contact Abnormality Detection","summary":"  Zinc electrolysis is one of the key processes in zinc smelting, and\nmaintaining stable operation of zinc electrolysis is an important factor in\nensuring production efficiency and product quality. However, poor contact\nbetween the zinc electrolysis cathode and the anode is a common problem that\nleads to reduced production efficiency and damage to the electrolysis cell.\nTherefore, online monitoring of the contact status of the plates is crucial for\nensuring production quality and efficiency. To address this issue, we propose\nan end-to-end network, the Frequency-masked Multimodal Autoencoder (FM-AE).\nThis method takes the cell voltage signal and infrared image information as\ninput, and through automatic encoding, fuses the two features together and\npredicts the poor contact status of the plates through a cascaded detector.\nExperimental results show that the proposed method maintains high accuracy\n(86.2%) while having good robustness and generalization ability, effectively\ndetecting poor contact status of the zinc electrolysis cell, providing strong\nsupport for production practice.\n","authors":["Canzong Zhou","Can Zhou","Hongqiu Zhu","Tianhao Liu"],"pdf_url":"https://arxiv.org/pdf/2401.03806v1.pdf","comment":"2023 The 34th Chinese Process Control Conference (CPCC 2023)"},{"id":"http://arxiv.org/abs/2401.03800v1","updated":"2024-01-08T10:41:45Z","published":"2024-01-08T10:41:45Z","title":"MvKSR: Multi-view Knowledge-guided Scene Recovery for Hazy and Rainy\n  Degradation","summary":"  High-quality imaging is crucial for ensuring safety supervision and\nintelligent deployment in fields like transportation and industry. It enables\nprecise and detailed monitoring of operations, facilitating timely detection of\npotential hazards and efficient management. However, adverse weather\nconditions, such as atmospheric haziness and precipitation, can have a\nsignificant impact on image quality. When the atmosphere contains dense haze or\nwater droplets, the incident light scatters, leading to degraded captured\nimages. This degradation is evident in the form of image blur and reduced\ncontrast, increasing the likelihood of incorrect assessments and\ninterpretations by intelligent imaging systems (IIS). To address the challenge\nof restoring degraded images in hazy and rainy conditions, this paper proposes\na novel multi-view knowledge-guided scene recovery network (termed MvKSR).\nSpecifically, guided filtering is performed on the degraded image to separate\nhigh/low-frequency components. Subsequently, an en-decoder-based multi-view\nfeature coarse extraction module (MCE) is used to coarsely extract features\nfrom different views of the degraded image. The multi-view feature fine fusion\nmodule (MFF) will learn and infer the restoration of degraded images through\nmixed supervision under different views. Additionally, we suggest an atrous\nresidual block to handle global restoration and local repair in\nhazy/rainy/mixed scenes. Extensive experimental results demonstrate that MvKSR\noutperforms other state-of-the-art methods in terms of efficiency and stability\nfor restoring degraded scenarios in IIS.\n","authors":["Dong Yang","Wenyu Xu","Yuxu Lu","Yuan Gao","Jingming Zhang","Yu Guo"],"pdf_url":"https://arxiv.org/pdf/2401.03800v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.01096v2","updated":"2024-01-08T10:40:58Z","published":"2022-11-02T13:15:11Z","title":"Recovering Sign Bits of DCT Coefficients in Digital Images as an\n  Optimization Problem","summary":"  Recovering unknown, missing, damaged, distorted, or lost information in DCT\ncoefficients is a common task in multiple applications of digital image\nprocessing, including image compression, selective image encryption, and image\ncommunication. This paper investigates the recovery of sign bits in DCT\ncoefficients of digital images, by proposing two different approximation\nmethods to solve a mixed integer linear programming (MILP) problem, which is\nNP-hard in general. One method is a relaxation of the MILP problem to a linear\nprogramming (LP) problem, and the other splits the original MILP problem into\nsome smaller MILP problems and an LP problem. We considered how the proposed\nmethods can be applied to JPEG-encoded images and conducted extensive\nexperiments to validate their performances. The experimental results showed\nthat the proposed methods outperformed other existing methods by a substantial\nmargin, both according to objective quality metrics and our subjective\nevaluation.\n","authors":["Ruiyuan Lin","Sheng Liu","Jun Jiang","Shujun Li","Chengqing Li","C. -C. Jay Kuo"],"pdf_url":"https://arxiv.org/pdf/2211.01096v2.pdf","comment":"22 pages, 8 figures"},{"id":"http://arxiv.org/abs/2401.03792v1","updated":"2024-01-08T10:20:34Z","published":"2024-01-08T10:20:34Z","title":"Monitoring water contaminants in coastal areas through ML algorithms\n  leveraging atmospherically corrected Sentinel-2 data","summary":"  Monitoring water contaminants is of paramount importance, ensuring public\nhealth and environmental well-being. Turbidity, a key parameter, poses a\nsignificant problem, affecting water quality. Its accurate assessment is\ncrucial for safeguarding ecosystems and human consumption, demanding meticulous\nattention and action. For this, our study pioneers a novel approach to monitor\nthe Turbidity contaminant, integrating CatBoost Machine Learning (ML) with\nhigh-resolution data from Sentinel-2 Level-2A. Traditional methods are\nlabor-intensive while CatBoost offers an efficient solution, excelling in\npredictive accuracy. Leveraging atmospherically corrected Sentinel-2 data\nthrough the Google Earth Engine (GEE), our study contributes to scalable and\nprecise Turbidity monitoring. A specific tabular dataset derived from Hong Kong\ncontaminants monitoring stations enriches our study, providing region-specific\ninsights. Results showcase the viability of this integrated approach, laying\nthe foundation for adopting advanced techniques in global water quality\nmanagement.\n","authors":["Francesca Razzano","Francesco Mauro","Pietro Di Stasio","Gabriele Meoni","Marco Esposito","Gilda Schirinzi","Silvia Liberata Ullo"],"pdf_url":"https://arxiv.org/pdf/2401.03792v1.pdf","comment":"4 pages, 3 figures, IGARSS2024"},{"id":"http://arxiv.org/abs/2401.03788v1","updated":"2024-01-08T10:08:48Z","published":"2024-01-08T10:08:48Z","title":"Low-light Image Enhancement via CLIP-Fourier Guided Wavelet Diffusion","summary":"  Low-light image enhancement techniques have significantly progressed, but\nunstable image quality recovery and unsatisfactory visual perception are still\nsignificant challenges. To solve these problems, we propose a novel and robust\nlow-light image enhancement method via CLIP-Fourier Guided Wavelet Diffusion,\nabbreviated as CFWD. Specifically, we design a guided network with a multiscale\nvisual language in the frequency domain based on the wavelet transform to\nachieve effective image enhancement iteratively. In addition, we combine the\nadvantages of Fourier transform in detail perception to construct a hybrid\nfrequency domain space with significant perceptual capabilities(HFDPM). This\noperation guides wavelet diffusion to recover the fine-grained structure of the\nimage and avoid diversity confusion. Extensive quantitative and qualitative\nexperiments on publicly available real-world benchmarks show that our method\noutperforms existing state-of-the-art methods and better reproduces images\nsimilar to normal images. Code is available at\nhttps://github.com/He-Jinhong/CFWD.\n","authors":["Minglong Xue","Jinhong He","Yanyi He","Zhipu Liu","Wenhai Wang","Mingliang Zhou"],"pdf_url":"https://arxiv.org/pdf/2401.03788v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2401.03785v1","updated":"2024-01-08T10:06:52Z","published":"2024-01-08T10:06:52Z","title":"Identifying Important Group of Pixels using Interactions","summary":"  To better understand the behavior of image classifiers, it is useful to\nvisualize the contribution of individual pixels to the model prediction. In\nthis study, we propose a method, MoXI~($\\textbf{Mo}$del e$\\textbf{X}$planation\nby $\\textbf{I}$nteractions), that efficiently and accurately identifies a group\nof pixels with high prediction confidence. The proposed method employs\ngame-theoretic concepts, Shapley values and interactions, taking into account\nthe effects of individual pixels and the cooperative influence of pixels on\nmodel confidence. Theoretical analysis and experiments demonstrate that our\nmethod better identifies the pixels that are highly contributing to the model\noutputs than widely-used visualization methods using Grad-CAM, Attention\nrollout, and Shapley value. While prior studies have suffered from the\nexponential computational cost in the computation of Shapley value and\ninteractions, we show that this can be reduced to linear cost for our task.\n","authors":["Kosuke Sumiyasu","Kazuhiko Kawamoto","Hiroshi Kera"],"pdf_url":"https://arxiv.org/pdf/2401.03785v1.pdf","comment":"16 pages, 15 figures"},{"id":"http://arxiv.org/abs/2401.03771v1","updated":"2024-01-08T09:50:54Z","published":"2024-01-08T09:50:54Z","title":"NeRFmentation: NeRF-based Augmentation for Monocular Depth Estimation","summary":"  The capabilities of monocular depth estimation (MDE) models are limited by\nthe availability of sufficient and diverse datasets. In the case of MDE models\nfor autonomous driving, this issue is exacerbated by the linearity of the\ncaptured data trajectories. We propose a NeRF-based data augmentation pipeline\nto introduce synthetic data with more diverse viewing directions into training\ndatasets and demonstrate the benefits of our approach to model performance and\nrobustness. Our data augmentation pipeline, which we call \"NeRFmentation\",\ntrains NeRFs on each scene in the dataset, filters out subpar NeRFs based on\nrelevant metrics, and uses them to generate synthetic RGB-D images captured\nfrom new viewing directions. In this work, we apply our technique in\nconjunction with three state-of-the-art MDE architectures on the popular\nautonomous driving dataset KITTI, augmenting its training set of the Eigen\nsplit. We evaluate the resulting performance gain on the original test set, a\nseparate popular driving set, and our own synthetic test set.\n","authors":["Casimir Feldmann","Niall Siegenheim","Nikolas Hars","Lovro Rabuzin","Mert Ertugrul","Luca Wolfart","Marc Pollefeys","Zuria Bauer","Martin R. Oswald"],"pdf_url":"https://arxiv.org/pdf/2401.03771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01383v2","updated":"2024-01-08T09:46:38Z","published":"2024-01-01T10:20:01Z","title":"Predicting Infant Brain Connectivity with Federated Multi-Trajectory\n  GNNs using Scarce Data","summary":"  The understanding of the convoluted evolution of infant brain networks during\nthe first postnatal year is pivotal for identifying the dynamics of early brain\nconnectivity development. Existing deep learning solutions suffer from three\nmajor limitations. First, they cannot generalize to multi-trajectory prediction\ntasks, where each graph trajectory corresponds to a particular imaging modality\nor connectivity type (e.g., T1-w MRI). Second, existing models require\nextensive training datasets to achieve satisfactory performance which are often\nchallenging to obtain. Third, they do not efficiently utilize incomplete time\nseries data. To address these limitations, we introduce FedGmTE-Net++, a\nfederated graph-based multi-trajectory evolution network. Using the power of\nfederation, we aggregate local learnings among diverse hospitals with limited\ndatasets. As a result, we enhance the performance of each hospital's local\ngenerative model, while preserving data privacy. The three key innovations of\nFedGmTE-Net++ are: (i) presenting the first federated learning framework\nspecifically designed for brain multi-trajectory evolution prediction in a\ndata-scarce environment, (ii) incorporating an auxiliary regularizer in the\nlocal objective function to exploit all the longitudinal brain connectivity\nwithin the evolution trajectory and maximize data utilization, (iii)\nintroducing a two-step imputation process, comprising a preliminary KNN-based\nprecompletion followed by an imputation refinement step that employs regressors\nto improve similarity scores and refine imputations. Our comprehensive\nexperimental results showed the outperformance of FedGmTE-Net++ in brain\nmulti-trajectory prediction from a single baseline graph in comparison with\nbenchmark methods.\n","authors":["Michalis Pistos","Gang Li","Weili Lin","Dinggang Shen","Islem Rekik"],"pdf_url":"https://arxiv.org/pdf/2401.01383v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03765v1","updated":"2024-01-08T09:41:22Z","published":"2024-01-08T09:41:22Z","title":"InvariantOODG: Learning Invariant Features of Point Clouds for\n  Out-of-Distribution Generalization","summary":"  The convenience of 3D sensors has led to an increase in the use of 3D point\nclouds in various applications. However, the differences in acquisition devices\nor scenarios lead to divergence in the data distribution of point clouds, which\nrequires good generalization of point cloud representation learning methods.\nWhile most previous methods rely on domain adaptation, which involves\nfine-tuning pre-trained models on target domain data, this may not always be\nfeasible in real-world scenarios where target domain data may be unavailable.\nTo address this issue, we propose InvariantOODG, which learns invariability\nbetween point clouds with different distributions using a two-branch network to\nextract local-to-global features from original and augmented point clouds.\nSpecifically, to enhance local feature learning of point clouds, we define a\nset of learnable anchor points that locate the most useful local regions and\ntwo types of transformations to augment the input point clouds. The\nexperimental results demonstrate the effectiveness of the proposed model on 3D\ndomain generalization benchmarks.\n","authors":["Zhimin Zhang","Xiang Gao","Wei Hu"],"pdf_url":"https://arxiv.org/pdf/2401.03765v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03764v1","updated":"2024-01-08T09:41:07Z","published":"2024-01-08T09:41:07Z","title":"3D-SSGAN: Lifting 2D Semantics for 3D-Aware Compositional Portrait\n  Synthesis","summary":"  Existing 3D-aware portrait synthesis methods can generate impressive\nhigh-quality images while preserving strong 3D consistency. However, most of\nthem cannot support the fine-grained part-level control over synthesized\nimages. Conversely, some GAN-based 2D portrait synthesis methods can achieve\nclear disentanglement of facial regions, but they cannot preserve view\nconsistency due to a lack of 3D modeling abilities. To address these issues, we\npropose 3D-SSGAN, a novel framework for 3D-aware compositional portrait image\nsynthesis. First, a simple yet effective depth-guided 2D-to-3D lifting module\nmaps the generated 2D part features and semantics to 3D. Then, a volume\nrenderer with a novel 3D-aware semantic mask renderer is utilized to produce\nthe composed face features and corresponding masks. The whole framework is\ntrained end-to-end by discriminating between real and synthesized 2D images and\ntheir semantic masks. Quantitative and qualitative evaluations demonstrate the\nsuperiority of 3D-SSGAN in controllable part-level synthesis while preserving\n3D view consistency.\n","authors":["Ruiqi Liu","Peng Zheng","Ye Wang","Rui Ma"],"pdf_url":"https://arxiv.org/pdf/2401.03764v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03753v1","updated":"2024-01-08T09:24:39Z","published":"2024-01-08T09:24:39Z","title":"Color-$S^{4}L$: Self-supervised Semi-supervised Learning with Image\n  Colorization","summary":"  This work addresses the problem of semi-supervised image classification tasks\nwith the integration of several effective self-supervised pretext tasks.\nDifferent from widely-used consistency regularization within semi-supervised\nlearning, we explored a novel self-supervised semi-supervised learning\nframework (Color-$S^{4}L$) especially with image colorization proxy task and\ndeeply evaluate performances of various network architectures in such special\npipeline. Also, we demonstrated its effectiveness and optimal performance on\nCIFAR-10, SVHN and CIFAR-100 datasets in comparison to previous supervised and\nsemi-supervised optimal methods.\n","authors":["Hanxiao Chen"],"pdf_url":"https://arxiv.org/pdf/2401.03753v1.pdf","comment":"This original work has been accepted and presented in the Poster\n  Session at ECCV 2020 WiCV Workshop.\n  (https://sites.google.com/view/wicvworkshop-eccv2020/program/presentations)"},{"id":"http://arxiv.org/abs/2401.03749v1","updated":"2024-01-08T09:20:46Z","published":"2024-01-08T09:20:46Z","title":"Flying Bird Object Detection Algorithm in Surveillance Video","summary":"  Aiming at the characteristics of the flying bird object in surveillance\nvideo, such as the single frame image feature is not obvious, the size is small\nin most cases, and asymmetric, this paper proposes a Flying Bird Object\nDetection method for Surveillance Video (FBOD-SV). Firstly, a new feature\naggregation module, the Correlation Attention Feature Aggregation\n(Co-Attention-FA) module, is designed to aggregate the features of the flying\nbird object according to the bird object's correlation on multiple consecutive\nframes of images. Secondly, a Flying Bird Object Detection Network (FBOD-Net)\nwith down-sampling and then up-sampling is designed, which uses a large feature\nlayer that fuses fine spatial information and large receptive field information\nto detect special multi-scale (mostly small-scale) bird objects. Finally, the\nSimOTA dynamic label allocation method is applied to One-Category object\ndetection, and the SimOTA-OC dynamic label strategy is proposed to solve the\ndifficult problem of label allocation caused by irregular flying bird objects.\nIn this paper, the algorithm's performance is verified by the experimental data\nset of the surveillance video of the flying bird object of the traction\nsubstation. The experimental results show that the surveillance video flying\nbird object detection method proposed in this paper effectively improves the\ndetection performance of flying bird objects.\n","authors":["Ziwei Sun","Zexi Hua","Hengchao Li","Yan Li"],"pdf_url":"https://arxiv.org/pdf/2401.03749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.15731v3","updated":"2024-01-08T09:12:30Z","published":"2023-12-25T14:03:38Z","title":"Adaptive FSS: A Novel Few-Shot Segmentation Framework via Prototype\n  Enhancement","summary":"  The Few-Shot Segmentation (FSS) aims to accomplish the novel class\nsegmentation task with a few annotated images. Current FSS research based on\nmeta-learning focus on designing a complex interaction mechanism between the\nquery and support feature. However, unlike humans who can rapidly learn new\nthings from limited samples, the existing approach relies solely on fixed\nfeature matching to tackle new tasks, lacking adaptability. In this paper, we\npropose a novel framework based on the adapter mechanism, namely Adaptive FSS,\nwhich can efficiently adapt the existing FSS model to the novel classes. In\ndetail, we design the Prototype Adaptive Module (PAM), which utilizes accurate\ncategory information provided by the support set to derive class prototypes,\nenhancing class-specific information in the multi-stage representation. In\naddition, our approach is compatible with in diverse FSS methods with different\nbackbones by simply inserting PAM between the layers of the encoder.\nExperiments demonstrate that our method effectively improves the performance of\nthe FSS models (e.g., MSANet, HDMNet, FPTrans, and DCAMA) and achieve new\nstate-of-the-art (SOTA) results (i.e., 72.4\\% and 79.1\\% mIoU on PASCAL-5$^i$\n1-shot and 5-shot settings, 52.7\\% and 60.0\\% mIoU on COCO-20$^i$ 1-shot and\n5-shot settings). Our code can be available at\nhttps://github.com/jingw193/AdaptiveFSS.\n","authors":["Jing Wang","Jinagyun Li","Chen Chen","Yisi Zhang","Haoran Shen","Tianxiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.15731v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03742v1","updated":"2024-01-08T09:05:20Z","published":"2024-01-08T09:05:20Z","title":"Flowmind2Digital: The First Comprehensive Flowmind Recognition and\n  Conversion Approach","summary":"  Flowcharts and mind maps, collectively known as flowmind, are vital in daily\nactivities, with hand-drawn versions facilitating real-time collaboration.\nHowever, there's a growing need to digitize them for efficient processing.\nAutomated conversion methods are essential to overcome manual conversion\nchallenges. Existing sketch recognition methods face limitations in practical\nsituations, being field-specific and lacking digital conversion steps. Our\npaper introduces the Flowmind2digital method and hdFlowmind dataset to address\nthese challenges. Flowmind2digital, utilizing neural networks and keypoint\ndetection, achieves a record 87.3% accuracy on our dataset, surpassing previous\nmethods by 11.9%. The hdFlowmind dataset, comprising 1,776 annotated flowminds\nacross 22 scenarios, outperforms existing datasets. Additionally, our\nexperiments emphasize the importance of simple graphics, enhancing accuracy by\n9.3%.\n","authors":["Huanyu Liu","Jianfeng Cai","Tingjia Zhang","Hongsheng Li","Siyuan Wang","Guangming Zhu","Syed Afaq Ali Shah","Mohammed Bennamoun","Liang Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.03742v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01918v2","updated":"2024-01-08T08:53:28Z","published":"2024-01-03T08:23:49Z","title":"Distilling Temporal Knowledge with Masked Feature Reconstruction for 3D\n  Object Detection","summary":"  Striking a balance between precision and efficiency presents a prominent\nchallenge in the bird's-eye-view (BEV) 3D object detection. Although previous\ncamera-based BEV methods achieved remarkable performance by incorporating\nlong-term temporal information, most of them still face the problem of low\nefficiency. One potential solution is knowledge distillation. Existing\ndistillation methods only focus on reconstructing spatial features, while\noverlooking temporal knowledge. To this end, we propose TempDistiller, a\nTemporal knowledge Distiller, to acquire long-term memory from a teacher\ndetector when provided with a limited number of frames. Specifically, a\nreconstruction target is formulated by integrating long-term temporal knowledge\nthrough self-attention operation applied to feature teachers. Subsequently,\nnovel features are generated for masked student features via a generator.\nUltimately, we utilize this reconstruction target to reconstruct the student\nfeatures. In addition, we also explore temporal relational knowledge when\ninputting full frames for the student model. We verify the effectiveness of the\nproposed method on the nuScenes benchmark. The experimental results show our\nmethod obtain an enhancement of +1.6 mAP and +1.1 NDS compared to the baseline,\na speed improvement of approximately 6 FPS after compressing temporal\nknowledge, and the most accurate velocity estimation.\n","authors":["Haowen Zheng","Dong Cao","Jintao Xu","Rui Ai","Weihao Gu","Yang Yang","Yanyan Liang"],"pdf_url":"https://arxiv.org/pdf/2401.01918v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.08994v2","updated":"2024-01-08T08:53:05Z","published":"2023-07-18T06:15:23Z","title":"Human Action Recognition in Still Images Using ConViT","summary":"  Understanding the relationship between different parts of an image is crucial\nin a variety of applications, including object recognition, scene\nunderstanding, and image classification. Despite the fact that Convolutional\nNeural Networks (CNNs) have demonstrated impressive results in classifying and\ndetecting objects, they lack the capability to extract the relationship between\ndifferent parts of an image, which is a crucial factor in Human Action\nRecognition (HAR). To address this problem, this paper proposes a new module\nthat functions like a convolutional layer that uses Vision Transformer (ViT).\nIn the proposed model, the Vision Transformer can complement a convolutional\nneural network in a variety of tasks by helping it to effectively extract the\nrelationship among various parts of an image. It is shown that the proposed\nmodel, compared to a simple CNN, can extract meaningful parts of an image and\nsuppress the misleading parts. The proposed model has been evaluated on the\nStanford40 and PASCAL VOC 2012 action datasets and has achieved 95.5% mean\nAverage Precision (mAP) and 91.5% mAP results, respectively, which are\npromising compared to other state-of-the-art methods.\n","authors":["Seyed Rohollah Hosseyni","Sanaz Seyedin","Hasan Taheri"],"pdf_url":"https://arxiv.org/pdf/2307.08994v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.02317v2","updated":"2024-01-08T08:39:34Z","published":"2024-01-04T15:34:44Z","title":"BA-SAM: Scalable Bias-Mode Attention Mask for Segment Anything Model","summary":"  In this paper, we address the challenge of image resolution variation for the\nSegment Anything Model (SAM). SAM, known for its zero-shot generalizability,\nexhibits a performance degradation when faced with datasets with varying image\nsizes. Previous approaches tend to resize the image to a fixed size or adopt\nstructure modifications, hindering the preservation of SAM's rich prior\nknowledge. Besides, such task-specific tuning necessitates a complete\nretraining of the model, which is cost-expensive and unacceptable for\ndeployment in the downstream tasks. In this paper, we reformulate this issue as\na length extrapolation problem, where token sequence length varies while\nmaintaining a consistent patch size for images of different sizes. To this end,\nwe propose Scalable Bias-Mode Attention Mask (BA-SAM) to enhance SAM's\nadaptability to varying image resolutions while eliminating the need for\nstructure modifications. Firstly, we introduce a new scaling factor to ensure\nconsistent magnitude in the attention layer's dot product values when the token\nsequence length changes. Secondly, we present a bias-mode attention mask that\nallows each token to prioritize neighboring information, mitigating the impact\nof untrained distant information. Our BA-SAM demonstrates efficacy in two\nscenarios: zero-shot and fine-tuning. Extensive evaluation on diverse datasets,\nincluding DIS5K, DUTS, ISIC, COD10K, and COCO, reveals its ability to\nsignificantly mitigate performance degradation in the zero-shot setting and\nachieve state-of-the-art performance with minimal fine-tuning. Furthermore, we\npropose a generalized model and benchmark, showcasing BA-SAM's generalizability\nacross all four datasets simultaneously.\n","authors":["Yiran Song","Qianyu Zhou","Xiangtai Li","Deng-Ping Fan","Xuequan Lu","Lizhuang Ma"],"pdf_url":"https://arxiv.org/pdf/2401.02317v2.pdf","comment":"Code:https://github.com/zongzi13545329/BA-SAM"},{"id":"http://arxiv.org/abs/2304.10771v2","updated":"2024-01-08T08:06:48Z","published":"2023-04-21T06:41:17Z","title":"A Revisit of the Normalized Eight-Point Algorithm and A Self-Supervised\n  Deep Solution","summary":"  The Normalized Eight-Point algorithm has been widely viewed as the\ncornerstone in two-view geometry computation, where the seminal Hartley's\nnormalization greatly improves the performance of the direct linear\ntransformation (DLT) algorithm. A natural question is, whether there exists and\nhow to find other normalization methods that may further improve the\nperformance as per each input sample. In this paper, we provide a novel\nperspective and make two contributions towards this fundamental problem: 1) We\nrevisit the normalized eight-point algorithm and make a theoretical\ncontribution by showing the existence of different and better normalization\nalgorithms; 2) We present a deep convolutional neural network with a\nself-supervised learning strategy to the normalization. Given eight pairs of\ncorrespondences, our network directly predicts the normalization matrices, thus\nlearning to normalize each input sample. Our learning-based normalization\nmodule could be integrated with both traditional (e.g., RANSAC) and deep\nlearning framework (affording good interpretability) with minimal efforts.\nExtensive experiments on both synthetic and real images show the effectiveness\nof our proposed approach.\n","authors":["Bin Fan","Yuchao Dai","Yongduek Seo","Mingyi He"],"pdf_url":"https://arxiv.org/pdf/2304.10771v2.pdf","comment":"Accepted by Visual Intelligence"},{"id":"http://arxiv.org/abs/2310.07236v2","updated":"2024-01-08T08:00:22Z","published":"2023-10-11T06:56:08Z","title":"AdaMesh: Personalized Facial Expressions and Head Poses for Adaptive\n  Speech-Driven 3D Facial Animation","summary":"  Speech-driven 3D facial animation aims at generating facial movements that\nare synchronized with the driving speech, which has been widely explored\nrecently. Existing works mostly neglect the person-specific talking style in\ngeneration, including facial expression and head pose styles. Several works\nintend to capture the personalities by fine-tuning modules. However, limited\ntraining data leads to the lack of vividness. In this work, we propose AdaMesh,\na novel adaptive speech-driven facial animation approach, which learns the\npersonalized talking style from a reference video of about 10 seconds and\ngenerates vivid facial expressions and head poses. Specifically, we propose\nmixture-of-low-rank adaptation (MoLoRA) to fine-tune the expression adapter,\nwhich efficiently captures the facial expression style. For the personalized\npose style, we propose a pose adapter by building a discrete pose prior and\nretrieving the appropriate style embedding with a semantic-aware pose style\nmatrix without fine-tuning. Extensive experimental results show that our\napproach outperforms state-of-the-art methods, preserves the talking style in\nthe reference video, and generates vivid facial animation. The supplementary\nvideo and code will be available at https://adamesh.github.io.\n","authors":["Liyang Chen","Weihong Bao","Shun Lei","Boshi Tang","Zhiyong Wu","Shiyin Kang","Haozhi Huang"],"pdf_url":"https://arxiv.org/pdf/2310.07236v2.pdf","comment":"Project Page: https://adamesh.github.io"},{"id":"http://arxiv.org/abs/2312.10578v2","updated":"2024-01-08T07:55:37Z","published":"2023-12-17T01:44:29Z","title":"SAME: Sample Reconstruction against Model Extraction Attacks","summary":"  While deep learning models have shown significant performance across various\ndomains, their deployment needs extensive resources and advanced computing\ninfrastructure. As a solution, Machine Learning as a Service (MLaaS) has\nemerged, lowering the barriers for users to release or productize their deep\nlearning models. However, previous studies have highlighted potential privacy\nand security concerns associated with MLaaS, and one primary threat is model\nextraction attacks. To address this, there are many defense solutions but they\nsuffer from unrealistic assumptions and generalization issues, making them less\npractical for reliable protection. Driven by these limitations, we introduce a\nnovel defense mechanism, SAME, based on the concept of sample reconstruction.\nThis strategy imposes minimal prerequisites on the defender's capabilities,\neliminating the need for auxiliary Out-of-Distribution (OOD) datasets, user\nquery history, white-box model access, and additional intervention during model\ntraining. It is compatible with existing active defense methods. Our extensive\nexperiments corroborate the superior efficacy of SAME over state-of-the-art\nsolutions. Our code is available at https://github.com/xythink/SAME.\n","authors":["Yi Xie","Jie Zhang","Shiqian Zhao","Tianwei Zhang","Xiaofeng Chen"],"pdf_url":"https://arxiv.org/pdf/2312.10578v2.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2310.16781v2","updated":"2024-01-08T07:48:09Z","published":"2023-10-25T17:15:55Z","title":"Kiki or Bouba? Sound Symbolism in Vision-and-Language Models","summary":"  Although the mapping between sound and meaning in human language is assumed\nto be largely arbitrary, research in cognitive science has shown that there are\nnon-trivial correlations between particular sounds and meanings across\nlanguages and demographic groups, a phenomenon known as sound symbolism. Among\nthe many dimensions of meaning, sound symbolism is particularly salient and\nwell-demonstrated with regards to cross-modal associations between language and\nthe visual domain. In this work, we address the question of whether sound\nsymbolism is reflected in vision-and-language models such as CLIP and Stable\nDiffusion. Using zero-shot knowledge probing to investigate the inherent\nknowledge of these models, we find strong evidence that they do show this\npattern, paralleling the well-known kiki-bouba effect in psycholinguistics. Our\nwork provides a novel method for demonstrating sound symbolism and\nunderstanding its nature using computational tools. Our code will be made\npublicly available.\n","authors":["Morris Alper","Hadar Averbuch-Elor"],"pdf_url":"https://arxiv.org/pdf/2310.16781v2.pdf","comment":"Accepted to NeurIPS 2023 (spotlight). Project webpage:\n  https://kiki-bouba.github.io/"},{"id":"http://arxiv.org/abs/2401.02916v2","updated":"2024-01-08T07:42:21Z","published":"2024-01-05T17:39:52Z","title":"Uncovering the human motion pattern: Pattern Memory-based Diffusion\n  Model for Trajectory Prediction","summary":"  Human trajectory forecasting is a critical challenge in fields such as\nrobotics and autonomous driving. Due to the inherent uncertainty of human\nactions and intentions in real-world scenarios, various unexpected occurrences\nmay arise. To uncover latent motion patterns in human behavior, we introduce a\nnovel memory-based method, named Motion Pattern Priors Memory Network. Our\nmethod involves constructing a memory bank derived from clustered prior\nknowledge of motion patterns observed in the training set trajectories. We\nintroduce an addressing mechanism to retrieve the matched pattern and the\npotential target distributions for each prediction from the memory bank, which\nenables the identification and retrieval of natural motion patterns exhibited\nby agents, subsequently using the target priors memory token to guide the\ndiffusion model to generate predictions. Extensive experiments validate the\neffectiveness of our approach, achieving state-of-the-art trajectory prediction\naccuracy. The code will be made publicly available.\n","authors":["Yuxin Yang","Pengfei Zhu","Mengshi Qi","Huadong Ma"],"pdf_url":"https://arxiv.org/pdf/2401.02916v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03707v1","updated":"2024-01-08T07:34:43Z","published":"2024-01-08T07:34:43Z","title":"FMA-Net: Flow-Guided Dynamic Filtering and Iterative Feature Refinement\n  with Multi-Attention for Joint Video Super-Resolution and Deblurring","summary":"  We present a joint learning scheme of video super-resolution and deblurring,\ncalled VSRDB, to restore clean high-resolution (HR) videos from blurry\nlow-resolution (LR) ones. This joint restoration problem has drawn much less\nattention compared to single restoration problems. In this paper, we propose a\nnovel flow-guided dynamic filtering (FGDF) and iterative feature refinement\nwith multi-attention (FRMA), which constitutes our VSRDB framework, denoted as\nFMA-Net. Specifically, our proposed FGDF enables precise estimation of both\nspatio-temporally-variant degradation and restoration kernels that are aware of\nmotion trajectories through sophisticated motion representation learning.\nCompared to conventional dynamic filtering, the FGDF enables the FMA-Net to\neffectively handle large motions into the VSRDB. Additionally, the stacked FRMA\nblocks trained with our novel temporal anchor (TA) loss, which temporally\nanchors and sharpens features, refine features in a course-to-fine manner\nthrough iterative updates. Extensive experiments demonstrate the superiority of\nthe proposed FMA-Net over state-of-the-art methods in terms of both\nquantitative and qualitative quality. Codes and pre-trained models are\navailable at: https://kaist-viclab.github.io/fmanet-site\n","authors":["Geunhyuk Youk","Jihyong Oh","Munchurl Kim"],"pdf_url":"https://arxiv.org/pdf/2401.03707v1.pdf","comment":"The last two authors advised equally to this work (equal advising).\n  Please visit our project page at https://kaist-viclab.github.io/fmanet-site"},{"id":"http://arxiv.org/abs/2311.14925v2","updated":"2024-01-08T07:30:22Z","published":"2023-11-25T04:23:23Z","title":"Coordinate-based Neural Network for Fourier Phase Retrieval","summary":"  Fourier phase retrieval is essential for high-definition imaging of nanoscale\nstructures across diverse fields, notably coherent diffraction imaging. This\nstudy presents the Single impliCit neurAl Network (SCAN), a tool built upon\ncoordinate neural networks meticulously designed for enhanced phase retrieval\nperformance. Remedying the drawbacks of conventional iterative methods which\nare easiliy trapped into local minimum solutions and sensitive to noise, SCAN\nadeptly connects object coordinates to their amplitude and phase within a\nunified network in an unsupervised manner. While many existing methods\nprimarily use Fourier magnitude in their loss function, our approach\nincorporates both the predicted magnitude and phase, enhancing retrieval\naccuracy. Comprehensive tests validate SCAN's superiority over traditional and\nother deep learning models regarding accuracy and noise robustness. We also\ndemonstrate that SCAN excels in the ptychography setting.\n","authors":["Tingyou Li","Zixin Xu","Yong S. Chu","Xiaojing Huang","Jizhou Li"],"pdf_url":"https://arxiv.org/pdf/2311.14925v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03704v1","updated":"2024-01-08T07:22:59Z","published":"2024-01-08T07:22:59Z","title":"Sur2f: A Hybrid Representation for High-Quality and Efficient Surface\n  Reconstruction from Multi-view Images","summary":"  Multi-view surface reconstruction is an ill-posed, inverse problem in 3D\nvision research. It involves modeling the geometry and appearance with\nappropriate surface representations. Most of the existing methods rely either\non explicit meshes, using surface rendering of meshes for reconstruction, or on\nimplicit field functions, using volume rendering of the fields for\nreconstruction. The two types of representations in fact have their respective\nmerits. In this work, we propose a new hybrid representation, termed Sur2f,\naiming to better benefit from both representations in a complementary manner.\nTechnically, we learn two parallel streams of an implicit signed distance field\nand an explicit surrogate surface Sur2f mesh, and unify volume rendering of the\nimplicit signed distance function (SDF) and surface rendering of the surrogate\nmesh with a shared, neural shader; the unified shading promotes their\nconvergence to the same, underlying surface. We synchronize learning of the\nsurrogate mesh by driving its deformation with functions induced from the\nimplicit SDF. In addition, the synchronized surrogate mesh enables\nsurface-guided volume sampling, which greatly improves the sampling efficiency\nper ray in volume rendering. We conduct thorough experiments showing that\nSur$^2$f outperforms existing reconstruction methods and surface\nrepresentations, including hybrid ones, in terms of both recovery quality and\nrecovery efficiency.\n","authors":["Zhangjin Huang","Zhihao Liang","Haojie Zhang","Yangkai Lin","Kui Jia"],"pdf_url":"https://arxiv.org/pdf/2401.03704v1.pdf","comment":"18 pages, 16 figures"},{"id":"http://arxiv.org/abs/2401.00608v2","updated":"2024-01-08T07:00:15Z","published":"2023-12-31T23:32:03Z","title":"Bringing Back the Context: Camera Trap Species Identification as Link\n  Prediction on Multimodal Knowledge Graphs","summary":"  Camera traps are valuable tools in animal ecology for biodiversity monitoring\nand conservation. However, challenges like poor generalization to deployment at\nnew unseen locations limit their practical application. Images are naturally\nassociated with heterogeneous forms of context possibly in different\nmodalities. In this work, we leverage the structured context associated with\nthe camera trap images to improve out-of-distribution generalization for the\ntask of species identification in camera traps. For example, a photo of a wild\nanimal may be associated with information about where and when it was taken, as\nwell as structured biology knowledge about the animal species. While typically\noverlooked by existing work, bringing back such context offers several\npotential benefits for better image understanding, such as addressing data\nscarcity and enhancing generalization. However, effectively integrating such\nheterogeneous context into the visual domain is a challenging problem. To\naddress this, we propose a novel framework that reformulates species\nclassification as link prediction in a multimodal knowledge graph (KG). This\nframework seamlessly integrates various forms of multimodal context for visual\nrecognition. We apply this framework for out-of-distribution species\nclassification on the iWildCam2020-WILDS and Snapshot Mountain Zebra datasets\nand achieve competitive performance with state-of-the-art approaches.\nFurthermore, our framework successfully incorporates biological taxonomy for\nimproved generalization and enhances sample efficiency for recognizing\nunder-represented species.\n","authors":["Vardaan Pahuja","Weidi Luo","Yu Gu","Cheng-Hao Tu","Hong-You Chen","Tanya Berger-Wolf","Charles Stewart","Song Gao","Wei-Lun Chao","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2401.00608v2.pdf","comment":"13 pages, 5 figures"},{"id":"http://arxiv.org/abs/2208.05401v2","updated":"2024-01-08T06:58:13Z","published":"2022-08-10T15:41:48Z","title":"Benchmarking Joint Face Spoofing and Forgery Detection with Visual and\n  Physiological Cues","summary":"  Face anti-spoofing (FAS) and face forgery detection play vital roles in\nsecuring face biometric systems from presentation attacks (PAs) and vicious\ndigital manipulation (e.g., deepfakes). Despite promising performance upon\nlarge-scale data and powerful deep models, the generalization problem of\nexisting approaches is still an open issue. Most of recent approaches focus on\n1) unimodal visual appearance or physiological (i.e., remote\nphotoplethysmography (rPPG)) cues; and 2) separated feature representation for\nFAS or face forgery detection. On one side, unimodal appearance and rPPG\nfeatures are respectively vulnerable to high-fidelity face 3D mask and video\nreplay attacks, inspiring us to design reliable multi-modal fusion mechanisms\nfor generalized face attack detection. On the other side, there are rich common\nfeatures across FAS and face forgery detection tasks (e.g., periodic rPPG\nrhythms and vanilla appearance for bonafides), providing solid evidence to\ndesign a joint FAS and face forgery detection system in a multi-task learning\nfashion. In this paper, we establish the first joint face spoofing and forgery\ndetection benchmark using both visual appearance and physiological rPPG cues.\nTo enhance the rPPG periodicity discrimination, we design a two-branch\nphysiological network using both facial spatio-temporal rPPG signal map and its\ncontinuous wavelet transformed counterpart as inputs. To mitigate the modality\nbias and improve the fusion efficacy, we conduct a weighted batch and layer\nnormalization for both appearance and rPPG features before multi-modal fusion.\nWe find that the generalization capacities of both unimodal (appearance or\nrPPG) and multi-modal (appearance+rPPG) models can be obviously improved via\njoint training on these two tasks. We hope this new benchmark will facilitate\nthe future research of both FAS and deepfake detection communities.\n","authors":["Zitong Yu","Rizhao Cai","Zhi Li","Wenhan Yang","Jingang Shi","Alex C. Kot"],"pdf_url":"https://arxiv.org/pdf/2208.05401v2.pdf","comment":"Accepted by IEEE Transactions on Dependable and Secure Computing\n  (TDSC). Corresponding authors: Zitong Yu and Wenhan Yang"},{"id":"http://arxiv.org/abs/2401.03695v1","updated":"2024-01-08T06:53:33Z","published":"2024-01-08T06:53:33Z","title":"A Large-scale Empirical Study on Improving the Fairness of Deep Learning\n  Models","summary":"  Fairness has been a critical issue that affects the adoption of deep learning\nmodels in real practice. To improve model fairness, many existing methods have\nbeen proposed and evaluated to be effective in their own contexts. However,\nthere is still no systematic evaluation among them for a comprehensive\ncomparison under the same context, which makes it hard to understand the\nperformance distinction among them, hindering the research progress and\npractical adoption of them. To fill this gap, this paper endeavours to conduct\nthe first large-scale empirical study to comprehensively compare the\nperformance of existing state-of-the-art fairness improving techniques.\nSpecifically, we target the widely-used application scenario of image\nclassification, and utilized three different datasets and five commonly-used\nperformance metrics to assess in total 13 methods from diverse categories. Our\nfindings reveal substantial variations in the performance of each method across\ndifferent datasets and sensitive attributes, indicating over-fitting on\nspecific datasets by many existing methods. Furthermore, different fairness\nevaluation metrics, due to their distinct focuses, yield significantly\ndifferent assessment results. Overall, we observe that pre-processing methods\nand in-processing methods outperform post-processing methods, with\npre-processing methods exhibiting the best performance. Our empirical study\noffers comprehensive recommendations for enhancing fairness in deep learning\nmodels. We approach the problem from multiple dimensions, aiming to provide a\nuniform evaluation platform and inspire researchers to explore more effective\nfairness solutions via a set of implications.\n","authors":["Junjie Yang","Jiajun Jiang","Zeyu Sun","Junjie Chen"],"pdf_url":"https://arxiv.org/pdf/2401.03695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03694v1","updated":"2024-01-08T06:52:16Z","published":"2024-01-08T06:52:16Z","title":"GloTSFormer: Global Video Text Spotting Transformer","summary":"  Video Text Spotting (VTS) is a fundamental visual task that aims to predict\nthe trajectories and content of texts in a video. Previous works usually\nconduct local associations and apply IoU-based distance and complex\npost-processing procedures to boost performance, ignoring the abundant temporal\ninformation and the morphological characteristics in VTS. In this paper, we\npropose a novel Global Video Text Spotting Transformer GloTSFormer to model the\ntracking problem as global associations and utilize the Gaussian Wasserstein\ndistance to guide the morphological correlation between frames. Our main\ncontributions can be summarized as three folds. 1). We propose a\nTransformer-based global tracking method GloTSFormer for VTS and associate\nmultiple frames simultaneously. 2). We introduce a Wasserstein distance-based\nmethod to conduct positional associations between frames. 3). We conduct\nextensive experiments on public datasets. On the ICDAR2015 video dataset,\nGloTSFormer achieves 56.0 MOTA with 4.6 absolute improvement compared with the\nprevious SOTA method and outperforms the previous Transformer-based method by a\nsignificant 8.3 MOTA.\n","authors":["Han Wang","Yanjie Wang","Yang Li","Can Huang"],"pdf_url":"https://arxiv.org/pdf/2401.03694v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.09196v2","updated":"2024-01-08T06:25:26Z","published":"2023-09-17T07:52:07Z","title":"Efficient Pyramid Channel Attention Network for Pathological Myopia\n  Recognition","summary":"  Pathological myopia (PM) is the leading ocular disease for impaired vision\nworldwide. Clinically, the characteristic of pathology distribution in PM is\nglobal-local on the fundus image, which plays a significant role in assisting\nclinicians in diagnosing PM. However, most existing deep neural networks\nfocused on designing complex architectures but rarely explored the pathology\ndistribution prior of PM. To tackle this issue, we propose an efficient pyramid\nchannel attention (EPCA) module, which fully leverages the potential of the\nclinical pathology prior of PM with pyramid pooling and multi-scale context\nfusion. Then, we construct EPCA-Net for automatic PM recognition based on\nfundus images by stacking a sequence of EPCA modules. Moreover, motivated by\nthe recent pretraining-and-finetuning paradigm, we attempt to adapt pre-trained\nnatural image models for PM recognition by freezing them and treating the EPCA\nand other attention modules as adapters. In addition, we construct a PM\nrecognition benchmark termed PM-fundus by collecting fundus images of PM from\npublicly available datasets. The comprehensive experiments demonstrate the\nsuperiority of our EPCA-Net over state-of-the-art methods in the PM recognition\ntask. The results also show that our method based on the\npretraining-and-finetuning paradigm achieves competitive performance through\ncomparisons to part of previous methods based on traditional fine-tuning\nparadigm with fewer tunable parameters, which has the potential to leverage\nmore natural image foundation models to address the PM recognition task in\nlimited medical data regime.\n","authors":["Xiaoqing Zhang","Jilu Zhao","Yan Li","Hao Wu","Xiangtian Zhou","Jiang Liu"],"pdf_url":"https://arxiv.org/pdf/2309.09196v2.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2312.06454v2","updated":"2024-01-08T06:07:45Z","published":"2023-12-11T15:41:05Z","title":"Point Transformer with Federated Learning for Predicting Breast Cancer\n  HER2 Status from Hematoxylin and Eosin-Stained Whole Slide Images","summary":"  Directly predicting human epidermal growth factor receptor 2 (HER2) status\nfrom widely available hematoxylin and eosin (HE)-stained whole slide images\n(WSIs) can reduce technical costs and expedite treatment selection. Accurately\npredicting HER2 requires large collections of multi-site WSIs. Federated\nlearning enables collaborative training of these WSIs without gigabyte-size\nWSIs transportation and data privacy concerns. However, federated learning\nencounters challenges in addressing label imbalance in multi-site WSIs from the\nreal world. Moreover, existing WSI classification methods cannot simultaneously\nexploit local context information and long-range dependencies in the site-end\nfeature representation of federated learning. To address these issues, we\npresent a point transformer with federated learning for multi-site HER2 status\nprediction from HE-stained WSIs. Our approach incorporates two novel designs.\nWe propose a dynamic label distribution strategy and an auxiliary classifier,\nwhich helps to establish a well-initialized model and mitigate label\ndistribution variations across sites. Additionally, we propose a farthest\ncosine sampling based on cosine distance. It can sample the most distinctive\nfeatures and capture the long-range dependencies. Extensive experiments and\nanalysis show that our method achieves state-of-the-art performance at four\nsites with a total of 2687 WSIs. Furthermore, we demonstrate that our model can\ngeneralize to two unseen sites with 229 WSIs.\n","authors":["Bao Li","Zhenyu Liu","Lizhi Shao","Bensheng Qiu","Hong Bu","Jie Tian"],"pdf_url":"https://arxiv.org/pdf/2312.06454v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.16818v4","updated":"2024-01-08T06:00:01Z","published":"2023-03-29T16:08:59Z","title":"SimDistill: Simulated Multi-modal Distillation for BEV 3D Object\n  Detection","summary":"  Multi-view camera-based 3D object detection has become popular due to its low\ncost, but accurately inferring 3D geometry solely from camera data remains\nchallenging and may lead to inferior performance. Although distilling precise\n3D geometry knowledge from LiDAR data could help tackle this challenge, the\nbenefits of LiDAR information could be greatly hindered by the significant\nmodality gap between different sensory modalities. To address this issue, we\npropose a Simulated multi-modal Distillation (SimDistill) method by carefully\ncrafting the model architecture and distillation strategy. Specifically, we\ndevise multi-modal architectures for both teacher and student models, including\na LiDAR-camera fusion-based teacher and a simulated fusion-based student. Owing\nto the ``identical'' architecture design, the student can mimic the teacher to\ngenerate multi-modal features with merely multi-view images as input, where a\ngeometry compensation module is introduced to bridge the modality gap.\nFurthermore, we propose a comprehensive multi-modal distillation scheme that\nsupports intra-modal, cross-modal, and multi-modal fusion distillation\nsimultaneously in the Bird's-eye-view space. Incorporating them together, our\nSimDistill can learn better feature representations for 3D object detection\nwhile maintaining a cost-effective camera-only deployment. Extensive\nexperiments validate the effectiveness and superiority of SimDistill over\nstate-of-the-art methods, achieving an improvement of 4.8\\% mAP and 4.1\\% NDS\nover the baseline detector. The source code will be released at\nhttps://github.com/ViTAE-Transformer/SimDistill.\n","authors":["Haimei Zhao","Qiming Zhang","Shanshan Zhao","Zhe Chen","Jing Zhang","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2303.16818v4.pdf","comment":"Accepted by AAAI 2024"},{"id":"http://arxiv.org/abs/2401.02804v2","updated":"2024-01-08T04:41:30Z","published":"2024-01-05T13:36:19Z","title":"DiffBody: Diffusion-based Pose and Shape Editing of Human Images","summary":"  Pose and body shape editing in a human image has received increasing\nattention. However, current methods often struggle with dataset biases and\ndeteriorate realism and the person's identity when users make large edits. We\npropose a one-shot approach that enables large edits with identity\npreservation. To enable large edits, we fit a 3D body model, project the input\nimage onto the 3D model, and change the body's pose and shape. Because this\ninitial textured body model has artifacts due to occlusion and the inaccurate\nbody shape, the rendered image undergoes a diffusion-based refinement, in which\nstrong noise destroys body structure and identity whereas insufficient noise\ndoes not help. We thus propose an iterative refinement with weak noise, applied\nfirst for the whole body and then for the face. We further enhance the realism\nby fine-tuning text embeddings via self-supervised learning. Our quantitative\nand qualitative evaluations demonstrate that our method outperforms other\nexisting methods across various datasets.\n","authors":["Yuta Okuyama","Yuki Endo","Yoshihiro Kanamori"],"pdf_url":"https://arxiv.org/pdf/2401.02804v2.pdf","comment":"Accepted to WACV 2024, project page:\n  https://www.cgg.cs.tsukuba.ac.jp/~okuyama/pub/diffbody/"},{"id":"http://arxiv.org/abs/2312.16451v2","updated":"2024-01-08T04:38:59Z","published":"2023-12-27T07:35:17Z","title":"Domain Generalization with Vital Phase Augmentation","summary":"  Deep neural networks have shown remarkable performance in image\nclassification. However, their performance significantly deteriorates with\ncorrupted input data. Domain generalization methods have been proposed to train\nrobust models against out-of-distribution data. Data augmentation in the\nfrequency domain is one of such approaches that enable a model to learn phase\nfeatures to establish domain-invariant representations. This approach changes\nthe amplitudes of the input data while preserving the phases. However, using\nfixed phases leads to susceptibility to phase fluctuations because amplitudes\nand phase fluctuations commonly occur in out-of-distribution. In this study, to\naddress this problem, we introduce an approach using finite variation of the\nphases of input data rather than maintaining fixed phases. Based on the\nassumption that the degree of domain-invariant features varies for each phase,\nwe propose a method to distinguish phases based on this degree. In addition, we\npropose a method called vital phase augmentation (VIPAug) that applies the\nvariation to the phases differently according to the degree of domain-invariant\nfeatures of given phases. The model depends more on the vital phases that\ncontain more domain-invariant features for attaining robustness to amplitude\nand phase fluctuations. We present experimental evaluations of our proposed\napproach, which exhibited improved performance for both clean and corrupted\ndata. VIPAug achieved SOTA performance on the benchmark CIFAR-10 and CIFAR-100\ndatasets, as well as near-SOTA performance on the ImageNet-100 and ImageNet\ndatasets. Our code is available at https://github.com/excitedkid/vipaug.\n","authors":["Ingyun Lee","Wooju Lee","Hyun Myung"],"pdf_url":"https://arxiv.org/pdf/2312.16451v2.pdf","comment":"Accepted by AAAI-24"},{"id":"http://arxiv.org/abs/2401.03665v1","updated":"2024-01-08T04:37:35Z","published":"2024-01-08T04:37:35Z","title":"Primitive Geometry Segment Pre-training for 3D Medical Image\n  Segmentation","summary":"  The construction of 3D medical image datasets presents several issues,\nincluding requiring significant financial costs in data collection and\nspecialized expertise for annotation, as well as strict privacy concerns for\npatient confidentiality compared to natural image datasets. Therefore, it has\nbecome a pressing issue in 3D medical image segmentation to enable\ndata-efficient learning with limited 3D medical data and supervision. A\npromising approach is pre-training, but improving its performance in 3D medical\nimage segmentation is difficult due to the small size of existing 3D medical\nimage datasets. We thus present the Primitive Geometry Segment Pre-training\n(PrimGeoSeg) method to enable the learning of 3D semantic features by\npre-training segmentation tasks using only primitive geometric objects for 3D\nmedical image segmentation. PrimGeoSeg performs more accurate and efficient 3D\nmedical image segmentation without manual data collection and annotation.\nFurther, experimental results show that PrimGeoSeg on SwinUNETR improves\nperformance over learning from scratch on BTCV, MSD (Task06), and BraTS\ndatasets by 3.7%, 4.4%, and 0.3%, respectively. Remarkably, the performance was\nequal to or better than state-of-the-art self-supervised learning despite the\nequal number of pre-training data. From experimental results, we conclude that\neffective pre-training can be achieved by looking at primitive geometric\nobjects only. Code and dataset are available at\nhttps://github.com/SUPER-TADORY/PrimGeoSeg.\n","authors":["Ryu Tadokoro","Ryosuke Yamada","Kodai Nakashima","Ryo Nakamura","Hirokatsu Kataoka"],"pdf_url":"https://arxiv.org/pdf/2401.03665v1.pdf","comment":"Accepted to BMVC2023 (Oral)"},{"id":"http://arxiv.org/abs/2401.03664v1","updated":"2024-01-08T04:37:18Z","published":"2024-01-08T04:37:18Z","title":"Dual-Channel Reliable Breast Ultrasound Image Classification Based on\n  Explainable Attribution and Uncertainty Quantification","summary":"  This paper focuses on the classification task of breast ultrasound images and\nresearches on the reliability measurement of classification results. We\nproposed a dual-channel evaluation framework based on the proposed inference\nreliability and predictive reliability scores. For the inference reliability\nevaluation, human-aligned and doctor-agreed inference rationales based on the\nimproved feature attribution algorithm SP-RISA are gracefully applied.\nUncertainty quantification is used to evaluate the predictive reliability via\nthe Test Time Enhancement. The effectiveness of this reliability evaluation\nframework has been verified on our breast ultrasound clinical dataset YBUS, and\nits robustness is verified on the public dataset BUSI. The expected calibration\nerrors on both datasets are significantly lower than traditional evaluation\nmethods, which proves the effectiveness of our proposed reliability\nmeasurement.\n","authors":["Shuge Lei","Haonan Hu","Dasheng Sun","Huabin Zhang","Kehong Yuan","Jian Dai","Jijun Tang","Yan Tong"],"pdf_url":"https://arxiv.org/pdf/2401.03664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.17205v3","updated":"2024-01-08T04:05:56Z","published":"2023-12-28T18:40:31Z","title":"EFHQ: Multi-purpose ExtremePose-Face-HQ dataset","summary":"  The existing facial datasets, while having plentiful images at near frontal\nviews, lack images with extreme head poses, leading to the downgraded\nperformance of deep learning models when dealing with profile or pitched faces.\nThis work aims to address this gap by introducing a novel dataset named Extreme\nPose Face High-Quality Dataset (EFHQ), which includes a maximum of 450k\nhigh-quality images of faces at extreme poses. To produce such a massive\ndataset, we utilize a novel and meticulous dataset processing pipeline to\ncurate two publicly available datasets, VFHQ and CelebV-HQ, which contain many\nhigh-resolution face videos captured in various settings. Our dataset can\ncomplement existing datasets on various facial-related tasks, such as facial\nsynthesis with 2D/3D-aware GAN, diffusion-based text-to-image face generation,\nand face reenactment. Specifically, training with EFHQ helps models generalize\nwell across diverse poses, significantly improving performance in scenarios\ninvolving extreme views, confirmed by extensive experiments. Additionally, we\nutilize EFHQ to define a challenging cross-view face verification benchmark, in\nwhich the performance of SOTA face recognition models drops 5-37% compared to\nfrontal-to-frontal scenarios, aiming to stimulate studies on face recognition\nunder severe pose conditions in the wild.\n","authors":["Trung Tuan Dao","Duc Hong Vu","Cuong Pham","Anh Tran"],"pdf_url":"https://arxiv.org/pdf/2312.17205v3.pdf","comment":"Project Page: https://bomcon123456.github.io/efhq/"},{"id":"http://arxiv.org/abs/2401.01651v2","updated":"2024-01-08T03:44:04Z","published":"2024-01-03T10:08:40Z","title":"AIGCBench: Comprehensive Evaluation of Image-to-Video Content Generated\n  by AI","summary":"  The burgeoning field of Artificial Intelligence Generated Content (AIGC) is\nwitnessing rapid advancements, particularly in video generation. This paper\nintroduces AIGCBench, a pioneering comprehensive and scalable benchmark\ndesigned to evaluate a variety of video generation tasks, with a primary focus\non Image-to-Video (I2V) generation. AIGCBench tackles the limitations of\nexisting benchmarks, which suffer from a lack of diverse datasets, by including\na varied and open-domain image-text dataset that evaluates different\nstate-of-the-art algorithms under equivalent conditions. We employ a novel text\ncombiner and GPT-4 to create rich text prompts, which are then used to generate\nimages via advanced Text-to-Image models. To establish a unified evaluation\nframework for video generation tasks, our benchmark includes 11 metrics\nspanning four dimensions to assess algorithm performance. These dimensions are\ncontrol-video alignment, motion effects, temporal consistency, and video\nquality. These metrics are both reference video-dependent and video-free,\nensuring a comprehensive evaluation strategy. The evaluation standard proposed\ncorrelates well with human judgment, providing insights into the strengths and\nweaknesses of current I2V algorithms. The findings from our extensive\nexperiments aim to stimulate further research and development in the I2V field.\nAIGCBench represents a significant step toward creating standardized benchmarks\nfor the broader AIGC landscape, proposing an adaptable and equitable framework\nfor future assessments of video generation tasks. We have open-sourced the\ndataset and evaluation code on the project website:\nhttps://www.benchcouncil.org/AIGCBench.\n","authors":["Fanda Fan","Chunjie Luo","Wanling Gao","Jianfeng Zhan"],"pdf_url":"https://arxiv.org/pdf/2401.01651v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.03035v2","updated":"2024-01-08T03:42:25Z","published":"2023-11-06T11:14:19Z","title":"GTP-ViT: Efficient Vision Transformers via Graph-based Token Propagation","summary":"  Vision Transformers (ViTs) have revolutionized the field of computer vision,\nyet their deployments on resource-constrained devices remain challenging due to\nhigh computational demands. To expedite pre-trained ViTs, token pruning and\ntoken merging approaches have been developed, which aim at reducing the number\nof tokens involved in the computation. However, these methods still have some\nlimitations, such as image information loss from pruned tokens and inefficiency\nin the token-matching process. In this paper, we introduce a novel Graph-based\nToken Propagation (GTP) method to resolve the challenge of balancing model\nefficiency and information preservation for efficient ViTs. Inspired by graph\nsummarization algorithms, GTP meticulously propagates less significant tokens'\ninformation to spatially and semantically connected tokens that are of greater\nimportance. Consequently, the remaining few tokens serve as a summarization of\nthe entire token graph, allowing the method to reduce computational complexity\nwhile preserving essential information of eliminated tokens. Combined with an\ninnovative token selection strategy, GTP can efficiently identify image tokens\nto be propagated. Extensive experiments have validated GTP's effectiveness,\ndemonstrating both efficiency and performance improvements. Specifically, GTP\ndecreases the computational complexity of both DeiT-S and DeiT-B by up to 26%\nwith only a minimal 0.3% accuracy drop on ImageNet-1K without finetuning, and\nremarkably surpasses the state-of-the-art token merging method on various\nbackbones at an even faster inference speed. The source code is available at\nhttps://github.com/Ackesnal/GTP-ViT.\n","authors":["Xuwei Xu","Sen Wang","Yudong Chen","Yanping Zheng","Zhewei Wei","Jiajun Liu"],"pdf_url":"https://arxiv.org/pdf/2311.03035v2.pdf","comment":"Accepted to WACV2024 (Oral)"},{"id":"http://arxiv.org/abs/2310.13378v2","updated":"2024-01-08T03:26:40Z","published":"2023-10-20T09:46:24Z","title":"ScalableMap: Scalable Map Learning for Online Long-Range Vectorized HD\n  Map Construction","summary":"  We propose a novel end-to-end pipeline for online long-range vectorized\nhigh-definition (HD) map construction using on-board camera sensors. The\nvectorized representation of HD maps, employing polylines and polygons to\nrepresent map elements, is widely used by downstream tasks. However, previous\nschemes designed with reference to dynamic object detection overlook the\nstructural constraints within linear map elements, resulting in performance\ndegradation in long-range scenarios. In this paper, we exploit the properties\nof map elements to improve the performance of map construction. We extract more\naccurate bird's eye view (BEV) features guided by their linear structure, and\nthen propose a hierarchical sparse map representation to further leverage the\nscalability of vectorized map elements and design a progressive decoding\nmechanism and a supervision strategy based on this representation. Our\napproach, ScalableMap, demonstrates superior performance on the nuScenes\ndataset, especially in long-range scenarios, surpassing previous\nstate-of-the-art model by 6.5 mAP while achieving 18.3 FPS. Code is available\nat https://github.com/jingy1yu/ScalableMap.\n","authors":["Jingyi Yu","Zizhao Zhang","Shengfu Xia","Jizhang Sang"],"pdf_url":"https://arxiv.org/pdf/2310.13378v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11598v2","updated":"2024-01-08T03:14:56Z","published":"2023-10-17T21:45:51Z","title":"Learning Neural Implicit through Volume Rendering with Attentive Depth\n  Fusion Priors","summary":"  Learning neural implicit representations has achieved remarkable performance\nin 3D reconstruction from multi-view images. Current methods use volume\nrendering to render implicit representations into either RGB or depth images\nthat are supervised by multi-view ground truth. However, rendering a view each\ntime suffers from incomplete depth at holes and unawareness of occluded\nstructures from the depth supervision, which severely affects the accuracy of\ngeometry inference via volume rendering. To resolve this issue, we propose to\nlearn neural implicit representations from multi-view RGBD images through\nvolume rendering with an attentive depth fusion prior. Our prior allows neural\nnetworks to perceive coarse 3D structures from the Truncated Signed Distance\nFunction (TSDF) fused from all depth images available for rendering. The TSDF\nenables accessing the missing depth at holes on one depth image and the\noccluded parts that are invisible from the current view. By introducing a novel\nattention mechanism, we allow neural networks to directly use the depth fusion\nprior with the inferred occupancy as the learned implicit function. Our\nattention mechanism works with either a one-time fused TSDF that represents a\nwhole scene or an incrementally fused TSDF that represents a partial scene in\nthe context of Simultaneous Localization and Mapping (SLAM). Our evaluations on\nwidely used benchmarks including synthetic and real-world scans show our\nsuperiority over the latest neural implicit methods. Project page:\nhttps://machineperceptionlab.github.io/Attentive_DF_Prior/\n","authors":["Pengchong Hu","Zhizhong Han"],"pdf_url":"https://arxiv.org/pdf/2310.11598v2.pdf","comment":"NeurIPS 2023"},{"id":"http://arxiv.org/abs/2401.03641v1","updated":"2024-01-08T03:06:02Z","published":"2024-01-08T03:06:02Z","title":"DME-Driver: Integrating Human Decision Logic and 3D Scene Perception in\n  Autonomous Driving","summary":"  In the field of autonomous driving, two important features of autonomous\ndriving car systems are the explainability of decision logic and the accuracy\nof environmental perception. This paper introduces DME-Driver, a new autonomous\ndriving system that enhances the performance and reliability of autonomous\ndriving system. DME-Driver utilizes a powerful vision language model as the\ndecision-maker and a planning-oriented perception model as the control signal\ngenerator. To ensure explainable and reliable driving decisions, the logical\ndecision-maker is constructed based on a large vision language model. This\nmodel follows the logic employed by experienced human drivers and makes\ndecisions in a similar manner. On the other hand, the generation of accurate\ncontrol signals relies on precise and detailed environmental perception, which\nis where 3D scene perception models excel. Therefore, a planning oriented\nperception model is employed as the signal generator. It translates the logical\ndecisions made by the decision-maker into accurate control signals for the\nself-driving cars. To effectively train the proposed model, a new dataset for\nautonomous driving was created. This dataset encompasses a diverse range of\nhuman driver behaviors and their underlying motivations. By leveraging this\ndataset, our model achieves high-precision planning accuracy through a logical\nthinking process.\n","authors":["Wencheng Han","Dongqian Guo","Cheng-Zhong Xu","Jianbing Shen"],"pdf_url":"https://arxiv.org/pdf/2401.03641v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03639v1","updated":"2024-01-08T02:53:22Z","published":"2024-01-08T02:53:22Z","title":"Deep Learning for Visual Neuroprosthesis","summary":"  The visual pathway involves complex networks of cells and regions which\ncontribute to the encoding and processing of visual information. While some\naspects of visual perception are understood, there are still many unanswered\nquestions regarding the exact mechanisms of visual encoding and the\norganization of visual information along the pathway. This chapter discusses\nthe importance of visual perception and the challenges associated with\nunderstanding how visual information is encoded and represented in the brain.\nFurthermore, this chapter introduces the concept of neuroprostheses: devices\ndesigned to enhance or replace bodily functions, and highlights the importance\nof constructing computational models of the visual pathway in the\nimplementation of such devices. A number of such models, employing the use of\ndeep learning models, are outlined, and their value to understanding visual\ncoding and natural vision is discussed.\n","authors":["Peter Beech","Shanshan Jia","Zhaofei Yu","Jian K. Liu"],"pdf_url":"https://arxiv.org/pdf/2401.03639v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03638v1","updated":"2024-01-08T02:49:16Z","published":"2024-01-08T02:49:16Z","title":"Unifying Graph Contrastive Learning via Graph Message Augmentation","summary":"  Graph contrastive learning is usually performed by first conducting Graph\nData Augmentation (GDA) and then employing a contrastive learning pipeline to\ntrain GNNs. As we know that GDA is an important issue for graph contrastive\nlearning. Various GDAs have been developed recently which mainly involve\ndropping or perturbing edges, nodes, node attributes and edge attributes.\nHowever, to our knowledge, it still lacks a universal and effective augmentor\nthat is suitable for different types of graph data. To address this issue, in\nthis paper, we first introduce the graph message representation of graph data.\nBased on it, we then propose a novel Graph Message Augmentation (GMA), a\nuniversal scheme for reformulating many existing GDAs. The proposed unified GMA\nnot only gives a new perspective to understand many existing GDAs but also\nprovides a universal and more effective graph data augmentation for graph\nself-supervised learning tasks. Moreover, GMA introduces an easy way to\nimplement the mixup augmentor which is natural for images but usually\nchallengeable for graphs. Based on the proposed GMA, we then propose a unified\ngraph contrastive learning, termed Graph Message Contrastive Learning (GMCL),\nthat employs attribution-guided universal GMA for graph contrastive learning.\nExperiments on many graph learning tasks demonstrate the effectiveness and\nbenefits of the proposed GMA and GMCL approaches.\n","authors":["Ziyan Zhang","Bo Jiang","Jin Tang","Bin Luo"],"pdf_url":"https://arxiv.org/pdf/2401.03638v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03637v1","updated":"2024-01-08T02:47:47Z","published":"2024-01-08T02:47:47Z","title":"Inverse-like Antagonistic Scene Text Spotting via Reading-Order\n  Estimation and Dynamic Sampling","summary":"  Scene text spotting is a challenging task, especially for inverse-like scene\ntext, which has complex layouts, e.g., mirrored, symmetrical, or retro-flexed.\nIn this paper, we propose a unified end-to-end trainable inverse-like\nantagonistic text spotting framework dubbed IATS, which can effectively spot\ninverse-like scene texts without sacrificing general ones. Specifically, we\npropose an innovative reading-order estimation module (REM) that extracts\nreading-order information from the initial text boundary generated by an\ninitial boundary module (IBM). To optimize and train REM, we propose a joint\nreading-order estimation loss consisting of a classification loss, an\northogonality loss, and a distribution loss. With the help of IBM, we can\ndivide the initial text boundary into two symmetric control points and\niteratively refine the new text boundary using a lightweight boundary\nrefinement module (BRM) for adapting to various shapes and scales. To alleviate\nthe incompatibility between text detection and recognition, we propose a\ndynamic sampling module (DSM) with a thin-plate spline that can dynamically\nsample appropriate features for recognition in the detected text region.\nWithout extra supervision, the DSM can proactively learn to sample appropriate\nfeatures for text recognition through the gradient returned by the recognition\nmodule. Extensive experiments on both challenging scene text and inverse-like\nscene text datasets demonstrate that our method achieves superior performance\nboth on irregular and inverse-like text spotting.\n","authors":["Shi-Xue Zhang","Chun Yang","Xiaobin Zhu","Hongyang Zhou","Hongfa Wang","Xu-Cheng Yin"],"pdf_url":"https://arxiv.org/pdf/2401.03637v1.pdf","comment":"14 pages, 16 figures, Accepted by TIP-2024"},{"id":"http://arxiv.org/abs/2401.00926v3","updated":"2024-01-08T02:33:06Z","published":"2024-01-01T16:28:30Z","title":"Accurate Leukocyte Detection Based on Deformable-DETR and Multi-Level\n  Feature Fusion for Aiding Diagnosis of Blood Diseases","summary":"  In standard hospital blood tests, the traditional process requires doctors to\nmanually isolate leukocytes from microscopic images of patients' blood using\nmicroscopes. These isolated leukocytes are then categorized via automatic\nleukocyte classifiers to determine the proportion and volume of different types\nof leukocytes present in the blood samples, aiding disease diagnosis. This\nmethodology is not only time-consuming and labor-intensive, but it also has a\nhigh propensity for errors due to factors such as image quality and\nenvironmental conditions, which could potentially lead to incorrect subsequent\nclassifications and misdiagnosis. To address these issues, this paper proposes\nan innovative method of leukocyte detection: the Multi-level Feature Fusion and\nDeformable Self-attention DETR (MFDS-DETR). To tackle the issue of leukocyte\nscale disparity, we designed the High-level Screening-feature Fusion Pyramid\n(HS-FPN), enabling multi-level fusion. This model uses high-level features as\nweights to filter low-level feature information via a channel attention module\nand then merges the screened information with the high-level features, thus\nenhancing the model's feature expression capability. Further, we address the\nissue of leukocyte feature scarcity by incorporating a multi-scale deformable\nself-attention module in the encoder and using the self-attention and\ncross-deformable attention mechanisms in the decoder, which aids in the\nextraction of the global features of the leukocyte feature maps. The\neffectiveness, superiority, and generalizability of the proposed MFDS-DETR\nmethod are confirmed through comparisons with other cutting-edge leukocyte\ndetection models using the private WBCDD, public LISC and BCCD datasets. Our\nsource code and private WBCCD dataset are available at\nhttps://github.com/JustlfC03/MFDS-DETR.\n","authors":["Yifei Chen","Chenyan Zhang","Ben Chen","Yiyu Huang","Yifei Sun","Changmiao Wang","Xianjun Fu","Yuxing Dai","Feiwei Qin","Yong Peng","Yu Gao"],"pdf_url":"https://arxiv.org/pdf/2401.00926v3.pdf","comment":"15 pages, 11 figures, accept Computers in Biology and Medicine 2024"},{"id":"http://arxiv.org/abs/2305.16727v3","updated":"2024-01-08T02:26:12Z","published":"2023-05-26T08:27:09Z","title":"A Novel real-time arrhythmia detection model using YOLOv8","summary":"  In a landscape characterized by heightened connectivity and mobility, coupled\nwith a surge in cardiovascular ailments, the imperative to curtail healthcare\nexpenses through remote monitoring of cardiovascular health has become more\npronounced. The accurate detection and classification of cardiac arrhythmias\nare pivotal for diagnosing individuals with heart irregularities. This study\nunderscores the feasibility of employing electrocardiograms (ECG) measurements\nin the home environment for real-time arrhythmia detection. Presenting a fresh\napplication for arrhythmia detection, this paper leverages the cutting-edge\nYou-Only-Look-Once (YOLO)v8 algorithm to categorize single-lead ECG signals. We\nintroduce a novel loss-modified YOLOv8 model, fine-tuned on the MIT-BIH\narrhythmia dataset, enabling real-time continuous monitoring. The obtained\nresults substantiate the efficacy of our approach, with the model attaining an\naverage accuracy of 99.5% and 0.992 mAP@50, and a rapid detection time of 0.002\nseconds on an NVIDIA Tesla V100. Our investigation exemplifies the potential of\nreal-time arrhythmia detection, enabling users to visually interpret the model\noutput within the comfort of their homes. Furthermore, this study lays the\ngroundwork for an extension into a real-time explainable AI (XAI) model capable\nof deployment in the healthcare sector, thereby significantly advancing the\nrealm of healthcare solutions.\n","authors":["Guang Jun Nicholas Ang","Aritejh Kr Goil","Henryk Chan","Jieyi Jeric Lew","Xin Chun Lee","Raihan Bin Ahmad Mustaffa","Timotius Jason","Ze Ting Woon","Bingquan Shen"],"pdf_url":"https://arxiv.org/pdf/2305.16727v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.17468v2","updated":"2024-01-08T02:20:49Z","published":"2023-10-26T15:15:11Z","title":"Cross-modal Active Complementary Learning with Self-refining\n  Correspondence","summary":"  Recently, image-text matching has attracted more and more attention from\nacademia and industry, which is fundamental to understanding the latent\ncorrespondence across visual and textual modalities. However, most existing\nmethods implicitly assume the training pairs are well-aligned while ignoring\nthe ubiquitous annotation noise, a.k.a noisy correspondence (NC), thereby\ninevitably leading to a performance drop. Although some methods attempt to\naddress such noise, they still face two challenging problems: excessive\nmemorizing/overfitting and unreliable correction for NC, especially under high\nnoise. To address the two problems, we propose a generalized Cross-modal Robust\nComplementary Learning framework (CRCL), which benefits from a novel Active\nComplementary Loss (ACL) and an efficient Self-refining Correspondence\nCorrection (SCC) to improve the robustness of existing methods. Specifically,\nACL exploits active and complementary learning losses to reduce the risk of\nproviding erroneous supervision, leading to theoretically and experimentally\ndemonstrated robustness against NC. SCC utilizes multiple self-refining\nprocesses with momentum correction to enlarge the receptive field for\ncorrecting correspondences, thereby alleviating error accumulation and\nachieving accurate and stable corrections. We carry out extensive experiments\non three image-text benchmarks, i.e., Flickr30K, MS-COCO, and CC152K, to verify\nthe superior robustness of our CRCL against synthetic and real-world noisy\ncorrespondences.\n","authors":["Yang Qin","Yuan Sun","Dezhong Peng","Joey Tianyi Zhou","Xi Peng","Peng Hu"],"pdf_url":"https://arxiv.org/pdf/2310.17468v2.pdf","comment":"This paper is accepted by NeurIPS 2023"},{"id":"http://arxiv.org/abs/2401.03621v1","updated":"2024-01-08T01:29:00Z","published":"2024-01-08T01:29:00Z","title":"Machine Learning Applications in Traumatic Brain Injury Diagnosis and\n  Prognosis: A Spotlight on Mild TBI and CT Imaging","summary":"  Traumatic Brain Injury (TBI) poses a significant global public health\nchallenge, contributing to high morbidity and mortality rates and placing a\nsubstantial economic burden on healthcare systems worldwide. The diagnosis and\nprognosis of TBI relies on a combination of clinical and imaging data often\nacquired using a Computed Tomography (CT) scanner. Addressing the multifaceted\nchallenges posed by TBI requires innovative, data-driven approaches, for this\ncomplex condition. As such, we provide a summary of the state-of-the-art\nMachine Learning (ML) and Deep Learning (DL) techniques applied to clinical and\nimages in TBI, with a particular focus on mild TBI (mTBI). We explore the rich\nspectrum of ML and DL techniques used and highlight their impact in TBI . We\ncategorize ML and DL methods by TBI severity and showcase their application in\nmTBI and moderate-to-severe TBI scenarios. Finally, we emphasize the role of ML\nand DL in mTBI diagnosis, where conventional methods often fall short, and\ncomment on the potential of CT-based ML applications in TBI. This review may\nserve as a source of inspiration for future research endeavours aimed at\nimproving the diagnosis and prognosis of TBI.\n","authors":["Hanem Ellethy","Shekhar S. Chandra","Viktor Vegh"],"pdf_url":"https://arxiv.org/pdf/2401.03621v1.pdf","comment":"The manuscript has 36 pages, 2 figures, and 3 tables"},{"id":"http://arxiv.org/abs/2401.03615v1","updated":"2024-01-08T00:26:21Z","published":"2024-01-08T00:26:21Z","title":"Automated Detection of Myopic Maculopathy in MMAC 2023: Achievements in\n  Classification, Segmentation, and Spherical Equivalent Prediction","summary":"  Myopic macular degeneration is the most common complication of myopia and the\nprimary cause of vision loss in individuals with pathological myopia. Early\ndetection and prompt treatment are crucial in preventing vision impairment due\nto myopic maculopathy. This was the focus of the Myopic Maculopathy Analysis\nChallenge (MMAC), in which we participated. In task 1, classification of myopic\nmaculopathy, we employed the contrastive learning framework, specifically\nSimCLR, to enhance classification accuracy by effectively capturing enriched\nfeatures from unlabeled data. This approach not only improved the intrinsic\nunderstanding of the data but also elevated the performance of our\nclassification model. For Task 2 (segmentation of myopic maculopathy plus\nlesions), we have developed independent segmentation models tailored for\ndifferent lesion segmentation tasks and implemented a test-time augmentation\nstrategy to further enhance the model's performance. As for Task 3 (prediction\nof spherical equivalent), we have designed a deep regression model based on the\ndata distribution of the dataset and employed an integration strategy to\nenhance the model's prediction accuracy. The results we obtained are promising\nand have allowed us to position ourselves in the Top 6 of the classification\ntask, the Top 2 of the segmentation task, and the Top 1 of the prediction task.\nThe code is available at\n\\url{https://github.com/liyihao76/MMAC_LaTIM_Solution}.\n","authors":["Yihao Li","Philippe Zhang","Yubo Tan","Jing Zhang","Zhihan Wang","Weili Jiang","Pierre-Henri Conze","Mathieu Lamard","Gwenolé Quellec","Mostafa El Habib Daho"],"pdf_url":"https://arxiv.org/pdf/2401.03615v1.pdf","comment":"18 pages"}],"Robotics":[{"id":"http://arxiv.org/abs/2401.04032v1","updated":"2024-01-08T17:17:08Z","published":"2024-01-08T17:17:08Z","title":"Digital Twin for Autonomous Surface Vessels for Safe Maritime Navigation","summary":"  Autonomous surface vessels (ASVs) play an increasingly important role in the\nsafety and sustainability of open sea operations. Since most maritime accidents\nare related to human failure, intelligent algorithms for autonomous collision\navoidance and path following can drastically reduce the risk in the maritime\nsector. A DT is a virtual representative of a real physical system and can\nenhance the situational awareness (SITAW) of such an ASV to generate optimal\ndecisions. This work builds on an existing DT framework for ASVs and\ndemonstrates foundations for enabling predictive, prescriptive, and autonomous\ncapabilities. In this context, sophisticated target tracking approaches are\ncrucial for estimating and predicting the position and motion of other dynamic\nobjects. The applied tracking method is enabled by real-time automatic\nidentification system (AIS) data and synthetic light detection and ranging\n(Lidar) measurements. To guarantee safety during autonomous operations, we\napplied a predictive safety filter, based on the concept of nonlinear model\npredictive control (NMPC). The approaches are implemented into a DT built with\nthe Unity game engine. As a result, this work demonstrates the potential of a\nDT capable of making predictions, playing through various what-if scenarios,\nand providing optimal control decisions according to its enhanced SITAW.\n","authors":["Daniel Menges","Andreas Von Brandis","Adil Rasheed"],"pdf_url":"https://arxiv.org/pdf/2401.04032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.04007v1","updated":"2024-01-08T16:37:55Z","published":"2024-01-08T16:37:55Z","title":"Task-Oriented Active Learning of Model Preconditions for Inaccurate\n  Dynamics Models","summary":"  When planning with an inaccurate dynamics model, a practical strategy is to\nrestrict planning to regions of state-action space where the model is accurate:\nalso known as a model precondition. Empirical real-world trajectory data is\nvaluable for defining data-driven model preconditions regardless of the model\nform (analytical, simulator, learned, etc...). However, real-world data is\noften expensive and dangerous to collect. In order to achieve data efficiency,\nthis paper presents an algorithm for actively selecting trajectories to learn a\nmodel precondition for an inaccurate pre-specified dynamics model. Our proposed\ntechniques address challenges arising from the sequential nature of\ntrajectories, and potential benefit of prioritizing task-relevant data. The\nexperimental analysis shows how algorithmic properties affect performance in\nthree planning scenarios: icy gridworld, simulated plant watering, and\nreal-world plant watering. Results demonstrate an improvement of approximately\n80% after only four real-world trajectories when using our proposed techniques.\n","authors":["Alex LaGrassa","Moonyoung Lee","Oliver Kroemer"],"pdf_url":"https://arxiv.org/pdf/2401.04007v1.pdf","comment":"Submitted to International Conference on Robotics and Automation"},{"id":"http://arxiv.org/abs/2401.04003v1","updated":"2024-01-08T16:35:13Z","published":"2024-01-08T16:35:13Z","title":"Simultaneous Task Allocation and Planning for Multi-Robots under\n  Hierarchical Temporal Logic Specifications","summary":"  Past research into robotic planning with temporal logic specifications,\nnotably Linear Temporal Logic (LTL), was largely based on singular formulas for\nindividual or groups of robots. But with increasing task complexity, LTL\nformulas unavoidably grow lengthy, complicating interpretation and\nspecification generation, and straining the computational capacities of the\nplanners. In order to maximize the potential of LTL specifications, we\ncapitalized on the intrinsic structure of tasks and introduced a hierarchical\nstructure to LTL specifications, and designed an algorithm to ascertain whether\nthey are satisfied given an input sequence. Second, we employ a search-based\napproach to synthesize plans for a multi-robot system, accomplishing\nsimultaneous task allocation and planning. The search space is approximated by\nloosely interconnected sub-spaces, with each sub-space corresponding to one LTL\nspecification. The search is predominantly confined to a single sub-space,\ntransitioning to another sub-space under certain conditions, determined by the\ndecomposition of automatons. Moreover, multiple heuristics are formulated to\nexpedite the search significantly. A theoretical analysis concerning\ncompleteness and optimality is conducted under mild assumptions. When compared\nwith existing methods on service tasks, our method outperforms in terms of\nexecution times with comparable solution quality. Finally, scalability is\nevaluated by testing a group of 30 robots and achieving reasonable runtimes.\n","authors":["Xusheng Luo","Changliu Liu"],"pdf_url":"https://arxiv.org/pdf/2401.04003v1.pdf","comment":"20 pages, 7 figures"},{"id":"http://arxiv.org/abs/2304.03057v2","updated":"2024-01-08T16:18:14Z","published":"2023-04-06T13:18:02Z","title":"Distributed formation-enforcing control for UAVs robust to observation\n  noise in relative pose measurements","summary":"  A technique that allows a formation-enforcing control (FEC) derived from\ngraph rigidity theory to interface with a realistic relative localization\nsystem onboard lightweight Unmanned Aerial Vehicles (UAVs) is proposed in this\npaper. The proposed methodology enables reliable real-world deployment of UAVs\nin tight formation using real relative localization systems burdened by\nnon-negligible sensory noise, which is typically not fully taken into account\nin FEC algorithms. The proposed solution is based on decomposition of the\ngradient descent-based FEC command into interpretable elements, and then\nmodifying these individually based on the estimated distribution of sensory\nnoise, such that the resulting action limits the probability of overshooting\nthe desired formation. The behavior of the system has been analyzed and the\npracticality of the proposed solution has been compared to pure\ngradient-descent in real-world experiments where it presented significantly\nbetter performance in terms of oscillations, deviation from the desired state\nand convergence time.\n","authors":["Viktor Walter","Matouš Vrba","Daniel Bonilla Licea","Matej Hilmer","Martin Saska"],"pdf_url":"https://arxiv.org/pdf/2304.03057v2.pdf","comment":"Submitted to IEEE Transactions on Robotics journal on December 19.\n  2023"},{"id":"http://arxiv.org/abs/2212.08742v2","updated":"2024-01-08T16:09:22Z","published":"2022-12-16T22:45:30Z","title":"Attentiveness Map Estimation for Haptic Teleoperation of Mobile Robot\n  Obstacle Avoidance and Approach","summary":"  Haptic feedback can improve safety of teleoperated robots when situational\nawareness is limited or operators are inattentive. Standard potential field\napproaches increase haptic resistance as an obstacle is approached, which is\ndesirable when the operator is unaware of the obstacle but undesirable when the\nmovement is intentional, such as when the operator wishes to inspect or\nmanipulate an object. This paper presents a novel haptic teleoperation\nframework that estimates the operator's attentiveness to obstacles and dampens\nhaptic feedback for intentional movement. A biologically-inspired attention\nmodel is developed based on computational working memory theories to integrate\nvisual saliency estimation with spatial mapping. The attentiveness map is\ngenerated in real-time, and our system renders lower haptic forces for\nobstacles that the operator is estimated to be aware of. Experimental results\nin simulation show that the proposed framework outperforms haptic teleoperation\nwithout attentiveness estimation in terms of task performance, robot safety,\nand user experience.\n","authors":["Ninghan Zhong","Kris Hauser"],"pdf_url":"https://arxiv.org/pdf/2212.08742v2.pdf","comment":"Accepted by IEEE RA-L"},{"id":"http://arxiv.org/abs/2401.03944v1","updated":"2024-01-08T15:00:46Z","published":"2024-01-08T15:00:46Z","title":"Diegetic Graphical User Interfaces and Intuitive Control of Assistive\n  Robots via Eye-gaze","summary":"  Individuals with tetraplegia and similar forms of paralysis suffer physically\nand emotionally due to a lack of autonomy. To help regain part of this\nautonomy, assistive robotic arms have been shown to increase living\nindependence. However, users with paralysis pose unique challenging conditions\nfor the control of these devices. In this article, we present the use of\nDiegetic Graphical User Interfaces, a novel, intuitive, and computationally\ninexpensive approach for gaze-controlled interfaces applied to robots. By using\nsymbols paired with fiducial markers, interactive buttons can be defined in the\nreal world which the user can trigger via gaze, and which can be embedded\neasily into the environment. We apply this system to pilot a\n3-degree-of-freedom robotic arm for precision pick-and-place tasks. The\ninterface is placed directly on the robot to allow intuitive and direct\ninteraction, eliminating the need for context-switching between external\nscreens, menus, and the robot. After calibration and a brief habituation\nperiod, twenty-one participants from multiple backgrounds, ages and eye-sight\nconditions completed the Yale-CMU-Berkeley (YCB) Block Pick and Place Protocol\nto benchmark the system, achieving a mean score of 13.71 out of the maximum\n16.00 points. Good usability and user experience were reported (System\nUsability Score of 75.36) while achieving a low task workload measure (NASA-TLX\nof 44.76). Results show that users can employ multiple interface elements to\nperform actions with minimal practice and with a small cognitive load. To our\nknowledge, this is the first easily reconfigurable screenless system that\nenables robot control entirely via gaze for Cartesian robot control without the\nneed for eye or face gestures.\n","authors":["Emanuel Nunez Sardinha","Marcela Munera","Nancy Zook","David Western","Virginia Ruiz Garate"],"pdf_url":"https://arxiv.org/pdf/2401.03944v1.pdf","comment":"10 pages, 11 figures, 5 tables Submitted to Transactions in Robotics.\n  Github repo at https://github.com/enunezs/D-GUI For the associated video, see\n  https://youtu.be/hrXuNYLDFds"},{"id":"http://arxiv.org/abs/2401.03938v1","updated":"2024-01-08T14:57:22Z","published":"2024-01-08T14:57:22Z","title":"Recovering the 3D UUV Position using UAV Imagery in Shallow-Water\n  Environments","summary":"  In this paper we propose a novel approach aimed at recovering the 3D position\nof an UUV from UAV imagery in shallow-water environments. Through combination\nof UAV and UUV measurements, we show that our method can be utilized as an\naccurate and cost-effective alternative when compared to acoustic sensing\nmethods, typically required to obtain ground truth information in underwater\nlocalization problems. Furthermore, our approach allows for a seamless\nconversion to geo-referenced coordinates which can be utilized for navigation\npurposes. To validate our method, we present the results with data collected\nthrough a simulation environment and field experiments, demonstrating the\nability to successfully recover the UUV position with sub-meter accuracy.\n","authors":["Antun Đuraš","Matija Sukno","Ivana Palunko"],"pdf_url":"https://arxiv.org/pdf/2401.03938v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03932v1","updated":"2024-01-08T14:45:15Z","published":"2024-01-08T14:45:15Z","title":"Using reinforcement learning to improve drone-based inference of\n  greenhouse gas fluxes","summary":"  Accurate mapping of greenhouse gas fluxes at the Earth's surface is essential\nfor the validation and calibration of climate models. In this study, we present\na framework for surface flux estimation with drones. Our approach uses data\nassimilation (DA) to infer fluxes from drone-based observations, and\nreinforcement learning (RL) to optimize the drone's sampling strategy. Herein,\nwe demonstrate that a RL-trained drone can quantify a CO2 hotspot more\naccurately than a drone sampling along a predefined flight path that traverses\nthe emission plume. We find that information-based reward functions can match\nthe performance of an error-based reward function that quantifies the\ndifference between the estimated surface flux and the true value. Reward\nfunctions based on information gain and information entropy can motivate\nactions that increase the drone's confidence in its updated belief, without\nrequiring knowledge of the true surface flux. These findings provide valuable\ninsights for further development of the framework for the mapping of more\ncomplex surface flux fields.\n","authors":["Alouette van Hove","Kristoffer Aalstad","Norbert Pirk"],"pdf_url":"https://arxiv.org/pdf/2401.03932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03904v1","updated":"2024-01-08T14:05:13Z","published":"2024-01-08T14:05:13Z","title":"Guided Time-optimal Model Predictive Control of a Multi-rotor","summary":"  Time-optimal control of a multi-rotor remains an open problem due to the\nunder-actuation and nonlinearity of its dynamics, which make it difficult to\nsolve this problem directly. In this paper, the time-optimal control problem of\nthe multi-rotor is studied. Firstly, a thrust limit optimal decomposition\nmethod is proposed, which can reasonably decompose the limited thrust into\nthree directions according to the current state and the target state. As a\nresult, the thrust limit constraint is decomposed as a linear constraint. With\nthe linear constraint and decoupled dynamics, a time-optimal guidance\ntrajectory can be obtained. Then, a cost function is defined based on the\ntime-optimal guidance trajectory, which has a quadratic form and can be used to\nevaluate the time-optimal performance of the system outputs. Finally, based on\nthe cost function, the time-optimal control problem is reformulated as an MPC\n(Model Predictive Control) problem. The experimental results demonstrate the\nfeasibility and validity of the proposed methods.\n","authors":["Guangyu Zhang","Yongjie Zheng","Yuqing He","Liying Yang","Hongyu Nie","Chaoxiong Huang","Yiwen Zhao"],"pdf_url":"https://arxiv.org/pdf/2401.03904v1.pdf","comment":"6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2401.03903v1","updated":"2024-01-08T14:04:19Z","published":"2024-01-08T14:04:19Z","title":"An Aerial Manipulator for Robot-to-robot Torch Relay Task: System Design\n  and Control Scheme","summary":"  Torch relay is an important tradition of the Olympics and heralds the start\nof the Games. Robots applied in the torch relay activity can not only\ndemonstrate the technological capability of humans to the world but also\nprovide a sight of human lives with robots in the future. This article presents\nan aerial manipulator designed for the robot-to-robot torch relay task of the\nBeijing 2022 Winter Olympics. This aerial manipulator system is composed of a\nquadrotor, a 3 DoF (Degree of Freedom) manipulator, and a monocular camera.\nThis article primarily describes the system design and system control scheme of\nthe aerial manipulator. The experimental results demonstrate that it can\ncomplete robot-to-robot torch relay task under the guidance of vision in the\nice and snow field.\n","authors":["Guangyu Zhang","Yuqing He","Liying Yang","Chaoxiong Huang","Yanchun Chang","Siliang Li"],"pdf_url":"https://arxiv.org/pdf/2401.03903v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.11335v3","updated":"2024-01-08T13:29:26Z","published":"2023-06-20T07:06:04Z","title":"DamWorld: Progressive Reasoning with World Models for Robotic\n  Manipulation","summary":"  The research on embodied AI has greatly promoted the development of robot\nmanipulation. However, it still faces significant challenges in various aspects\nsuch as benchmark construction, multi-modal perception and decision-making, and\nphysical execution. Previous robot manipulation simulators were primarily\ndesigned to enrich manipulation types and types of objects while neglecting the\nbalance between physical manipulation and language instruction complexity in\nmulti-modal environments. This paper proposes a new robot manipulation\nsimulator and builds a comprehensive and systematic robot manipulation\nbenchmark with progressive reasoning tasks called SeaWave (i.e., a progressive\nreasoning benchmark). It provides a standard test platform for embedded AI\nagents in a multi-modal environment, which can evaluate and execute four levels\nof human natural language instructions at the same time.\n  Previous world model-based robot manipulation work lacked research on the\nperception and decision-making of complex instructions in multi-modal\nenvironments. To this end, we propose a new world model tailored for\ncross-modal robot manipulation called DamWorld. Specifically, DamWorld takes\nthe current visual scene and predicted execution actions based on natural\nlanguage instructions as input, and uses the next action frame to supervise the\noutput of the world model to force the model to learn robot manipulation\nconsistent with world knowledge. Compared with the renowned baselines (e.g.,\nRT-1), our DamWorld improves the manipulation success rate by 5.6% on average\non four levels of progressive reasoning tasks. It is worth noting that on the\nmost challenging level 4 manipulation task, DamWorld still improved by 9.0%\ncompared to prior works.\n","authors":["Pengzhen Ren","Kaidong Zhang","Hetao Zheng","Zixuan Li","Yuhang Wen","Fengda Zhu","Mas Ma","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2306.11335v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03794v1","updated":"2024-01-08T10:23:10Z","published":"2024-01-08T10:23:10Z","title":"\"Oh, Sorry, I Think I Interrupted You'': Designing Repair Strategies for\n  Robotic Longitudinal Well-being Coaching","summary":"  Robotic well-being coaches have been shown to successfully promote people's\nmental well-being. To provide successful coaching, a robotic coach should have\nthe capability to repair the mistakes it makes. Past investigations of robot\nmistakes are limited to game or task-based, one-off and in-lab studies. This\npaper presents a 4-phase design process to design repair strategies for robotic\nlongitudinal well-being coaching with the involvement of real-world\nstakeholders: 1) designing repair strategies with a professional well-being\ncoach; 2) a longitudinal study with the involvement of experienced users (i.e.,\nwho had already interacted with a robotic coach) to investigate the repair\nstrategies defined in (1); 3) a design workshop with users from the study in\n(2) to gather their perspectives on the robotic coach's repair strategies; 4)\ndiscussing the results obtained in (2) and (3) with the mental well-being\nprofessional to reflect on how to design repair strategies for robotic\ncoaching. Our results show that users have different expectations for a robotic\ncoach than a human coach, which influences how repair strategies should be\ndesigned. We show that different repair strategies (e.g., apologizing,\nexplaining, or repairing empathically) are appropriate in different scenarios,\nand that preferences for repair strategies change during longitudinal\ninteractions with the robotic coach.\n","authors":["Minja Axelsson","Micol Spitale","Hatice Gunes"],"pdf_url":"https://arxiv.org/pdf/2401.03794v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03786v1","updated":"2024-01-08T10:07:31Z","published":"2024-01-08T10:07:31Z","title":"Long-term Safe Reinforcement Learning with Binary Feedback","summary":"  Safety is an indispensable requirement for applying reinforcement learning\n(RL) to real problems. Although there has been a surge of safe RL algorithms\nproposed in recent years, most existing work typically 1) relies on receiving\nnumeric safety feedback; 2) does not guarantee safety during the learning\nprocess; 3) limits the problem to a priori known, deterministic transition\ndynamics; and/or 4) assume the existence of a known safe policy for any states.\nAddressing the issues mentioned above, we thus propose Long-term Binaryfeedback\nSafe RL (LoBiSaRL), a safe RL algorithm for constrained Markov decision\nprocesses (CMDPs) with binary safety feedback and an unknown, stochastic state\ntransition function. LoBiSaRL optimizes a policy to maximize rewards while\nguaranteeing a long-term safety that an agent executes only safe state-action\npairs throughout each episode with high probability. Specifically, LoBiSaRL\nmodels the binary safety function via a generalized linear model (GLM) and\nconservatively takes only a safe action at every time step while inferring its\neffect on future safety under proper assumptions. Our theoretical results show\nthat LoBiSaRL guarantees the long-term safety constraint, with high\nprobability. Finally, our empirical results demonstrate that our algorithm is\nsafer than existing methods without significantly compromising performance in\nterms of reward.\n","authors":["Akifumi Wachi","Wataru Hashimoto","Kazumune Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2401.03786v1.pdf","comment":"Accepted to AAAI-24"},{"id":"http://arxiv.org/abs/2401.03701v1","updated":"2024-01-08T07:13:26Z","published":"2024-01-08T07:13:26Z","title":"ExTraCT -- Explainable Trajectory Corrections from language inputs using\n  Textual description of features","summary":"  Natural language provides an intuitive and expressive way of conveying human\nintent to robots. Prior works employed end-to-end methods for learning\ntrajectory deformations from language corrections. However, such methods do not\ngeneralize to new initial trajectories or object configurations. This work\npresents ExTraCT, a modular framework for trajectory corrections using natural\nlanguage that combines Large Language Models (LLMs) for natural language\nunderstanding and trajectory deformation functions. Given a scene, ExTraCT\ngenerates the trajectory modification features (scene-specific and\nscene-independent) and their corresponding natural language textual\ndescriptions for the objects in the scene online based on a template. We use\nLLMs for semantic matching of user utterances to the textual descriptions of\nfeatures. Based on the feature matched, a trajectory modification function is\napplied to the initial trajectory, allowing generalization to unseen\ntrajectories and object configurations. Through user studies conducted both in\nsimulation and with a physical robot arm, we demonstrate that trajectories\ndeformed using our method were more accurate and were preferred in about 80\\%\nof cases, outperforming the baseline. We also showcase the versatility of our\nsystem in a manipulation task and an assistive feeding task.\n","authors":["J-Anne Yow","Neha Priyadarshini Garg","Manoj Ramanathan","Wei Tech Ang"],"pdf_url":"https://arxiv.org/pdf/2401.03701v1.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2206.10981v5","updated":"2024-01-08T05:20:02Z","published":"2022-06-22T11:09:58Z","title":"Adaptive Sampling-based Particle Filter for Visual-inertial Gimbal in\n  the Wild","summary":"  In this paper, we present a Computer Vision (CV) based tracking and fusion\nalgorithm, dedicated to a 3D printed gimbal system on drones operating in\nnature. The whole gimbal system can stabilize the camera orientation robustly\nin a challenging nature scenario by using skyline and ground plane as\nreferences. Our main contributions are the following: a) a light-weight\nResnet-18 backbone network model was trained from scratch, and deployed onto\nthe Jetson Nano platform to segment the image into binary parts (ground and\nsky); b) our geometry assumption from nature cues delivers the potential for\nrobust visual tracking by using the skyline and ground plane as a reference; c)\na spherical surface-based adaptive particle sampling, can fuse orientation from\nmultiple sensor sources flexibly. The whole algorithm pipeline is tested on our\ncustomized gimbal module including Jetson and other hardware components. The\nexperiments were performed on top of a building in the real landscape.\n","authors":["Xueyang Kang","Ariel Herrera","Henry Lema","Esteban Valencia","Patrick Vandewalle"],"pdf_url":"https://arxiv.org/pdf/2206.10981v5.pdf","comment":"content in 6 pages, 9 figures, 2 pseudo codes, one table, accepted by\n  ICRA 2023"},{"id":"http://arxiv.org/abs/2401.03641v1","updated":"2024-01-08T03:06:02Z","published":"2024-01-08T03:06:02Z","title":"DME-Driver: Integrating Human Decision Logic and 3D Scene Perception in\n  Autonomous Driving","summary":"  In the field of autonomous driving, two important features of autonomous\ndriving car systems are the explainability of decision logic and the accuracy\nof environmental perception. This paper introduces DME-Driver, a new autonomous\ndriving system that enhances the performance and reliability of autonomous\ndriving system. DME-Driver utilizes a powerful vision language model as the\ndecision-maker and a planning-oriented perception model as the control signal\ngenerator. To ensure explainable and reliable driving decisions, the logical\ndecision-maker is constructed based on a large vision language model. This\nmodel follows the logic employed by experienced human drivers and makes\ndecisions in a similar manner. On the other hand, the generation of accurate\ncontrol signals relies on precise and detailed environmental perception, which\nis where 3D scene perception models excel. Therefore, a planning oriented\nperception model is employed as the signal generator. It translates the logical\ndecisions made by the decision-maker into accurate control signals for the\nself-driving cars. To effectively train the proposed model, a new dataset for\nautonomous driving was created. This dataset encompasses a diverse range of\nhuman driver behaviors and their underlying motivations. By leveraging this\ndataset, our model achieves high-precision planning accuracy through a logical\nthinking process.\n","authors":["Wencheng Han","Dongqian Guo","Cheng-Zhong Xu","Jianbing Shen"],"pdf_url":"https://arxiv.org/pdf/2401.03641v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2108.11092v2","updated":"2024-01-08T02:22:44Z","published":"2021-08-25T07:35:21Z","title":"INVIGORATE: Interactive Visual Grounding and Grasping in Clutter","summary":"  This paper presents INVIGORATE, a robot system that interacts with human\nthrough natural language and grasps a specified object in clutter. The objects\nmay occlude, obstruct, or even stack on top of one another. INVIGORATE embodies\nseveral challenges: (i) infer the target object among other occluding objects,\nfrom input language expressions and RGB images, (ii) infer object blocking\nrelationships (OBRs) from the images, and (iii) synthesize a multi-step plan to\nask questions that disambiguate the target object and to grasp it successfully.\nWe train separate neural networks for object detection, for visual grounding,\nfor question generation, and for OBR detection and grasping. They allow for\nunrestricted object categories and language expressions, subject to the\ntraining datasets. However, errors in visual perception and ambiguity in human\nlanguages are inevitable and negatively impact the robot's performance. To\novercome these uncertainties, we build a partially observable Markov decision\nprocess (POMDP) that integrates the learned neural network modules. Through\napproximate POMDP planning, the robot tracks the history of observations and\nasks disambiguation questions in order to achieve a near-optimal sequence of\nactions that identify and grasp the target object. INVIGORATE combines the\nbenefits of model-based POMDP planning and data-driven deep learning.\nPreliminary experiments with INVIGORATE on a Fetch robot show significant\nbenefits of this integrated approach to object grasping in clutter with natural\nlanguage interactions. A demonstration video is available at\nhttps://youtu.be/zYakh80SGcU.\n","authors":["Hanbo Zhang","Yunfan Lu","Cunjun Yu","David Hsu","Xuguang Lan","Nanning Zheng"],"pdf_url":"https://arxiv.org/pdf/2108.11092v2.pdf","comment":"12 pages, full version"},{"id":"http://arxiv.org/abs/2401.03629v1","updated":"2024-01-08T02:17:09Z","published":"2024-01-08T02:17:09Z","title":"DDM-Lag : A Diffusion-based Decision-making Model for Autonomous\n  Vehicles with Lagrangian Safety Enhancement","summary":"  Decision-making stands as a pivotal component in the realm of autonomous\nvehicles (AVs), playing a crucial role in navigating the intricacies of\nautonomous driving. Amidst the evolving landscape of data-driven methodologies,\nenhancing decision-making performance in complex scenarios has emerged as a\nprominent research focus. Despite considerable advancements, current\nlearning-based decision-making approaches exhibit potential for refinement,\nparticularly in aspects of policy articulation and safety assurance. To address\nthese challenges, we introduce DDM-Lag, a Diffusion Decision Model,augmented\nwith Lagrangian-based safety enhancements.In our approach, the autonomous\ndriving decision-making conundrum is conceptualized as a Constrained Markov\nDecision Process (CMDP). We have crafted an Actor-Critic framework, wherein the\ndiffusion model is employed as the actor,facilitating policy exploration and\nlearning. The integration of safety constraints in the CMDP and the adoption of\na Lagrangian relaxation-based policy optimization technique ensure enhanced\ndecision safety. A PID controller is employed for the stable updating of model\nparameters. The effectiveness of DDM-Lag is evaluated through different driving\ntasks, showcasing improvements in decision-making safety and overall\nperformance compared to baselines.\n","authors":["Jiaqi Liu","Peng Hang","Xiaocong Zhao","Jianqiang Wang","Jian Sun"],"pdf_url":"https://arxiv.org/pdf/2401.03629v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2401.04039v1","updated":"2024-01-08T17:24:16Z","published":"2024-01-08T17:24:16Z","title":"Bjøntegaard Delta (BD): A Tutorial Overview of the Metric, Evolution,\n  Challenges, and Recommendations","summary":"  The Bj{\\o}ntegaard Delta (BD) method proposed in 2001 has become a popular\ntool for comparing video codec compression efficiency. It was initially\nproposed to compute bitrate and quality differences between two Rate-Distortion\ncurves using PSNR as a distortion metric. Over the years, many works have\ncalculated and reported BD results using other objective quality metrics such\nas SSIM, VMAF and, in some cases, even subjective ratings (mean opinion\nscores). However, the lack of consolidated literature explaining the metric,\nits evolution over the years, and a systematic evaluation of the same under\ndifferent test conditions can result in a wrong interpretation of the BD\nresults thus obtained.\n  Towards this end, this paper presents a detailed tutorial describing the BD\nmethod and example cases where the metric might fail. We also provide a\ndetailed history of its evolution, including a discussion of various proposed\nimprovements and variations over the last 20 years. In addition, we evaluate\nthe various BD methods and their open-source implementations, considering\ndifferent objective quality metrics and subjective ratings taking into account\ndifferent RD characteristics. Based on our results, we present a set of\nrecommendations on using existing BD metrics and various insights for possible\nexploration towards developing more effective tools for codec compression\nefficiency evaluation and comparison.\n","authors":["Nabajeet Barman","Maria G. Martini","Yuriy Reznik"],"pdf_url":"https://arxiv.org/pdf/2401.04039v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01470v2","updated":"2024-01-08T17:03:15Z","published":"2024-01-03T00:10:33Z","title":"TPC-ViT: Token Propagation Controller for Efficient Vision Transformer","summary":"  Vision transformers (ViTs) have achieved promising results on a variety of\nComputer Vision tasks, however their quadratic complexity in the number of\ninput tokens has limited their application specially in resource-constrained\nsettings. Previous approaches that employ gradual token reduction to address\nthis challenge assume that token redundancy in one layer implies redundancy in\nall the following layers. We empirically demonstrate that this assumption is\noften not correct, i.e., tokens that are redundant in one layer can be useful\nin later layers. We employ this key insight to propose a novel token\npropagation controller (TPC) that incorporates two different\ntoken-distributions, i.e., pause probability and restart probability to control\nthe reduction and reuse of tokens respectively, which results in more efficient\ntoken utilization. To improve the estimates of token distributions, we propose\na smoothing mechanism that acts as a regularizer and helps remove noisy\noutliers. Furthermore, to improve the training-stability of our proposed TPC,\nwe introduce a model stabilizer that is able to implicitly encode local image\nstructures and minimize accuracy fluctuations during model training. We present\nextensive experimental results on the ImageNet-1K dataset using DeiT, LV-ViT\nand Swin models to demonstrate the effectiveness of our proposed method. For\nexample, compared to baseline models, our proposed method improves the\ninference speed of the DeiT-S by 250% while increasing the classification\naccuracy by 1.0%.\n","authors":["Wentao Zhu"],"pdf_url":"https://arxiv.org/pdf/2401.01470v2.pdf","comment":"Accepted by the main conference of WACV 2024; well-formatted PDF is\n  in\n  https://drive.google.com/file/d/1Id3oEdYv3OWing1qojQMyjvhZO-gG-Dm/view?usp=sharing\n  ; supplementary is in\n  https://drive.google.com/file/d/15LhYlBdCXtompA0_TLAp_ZJb4_sq2N5V/view?usp=sharing"},{"id":"http://arxiv.org/abs/2401.04023v1","updated":"2024-01-08T17:02:25Z","published":"2024-01-08T17:02:25Z","title":"Efficient Multiscale Multimodal Bottleneck Transformer for Audio-Video\n  Classification","summary":"  In recent years, researchers combine both audio and video signals to deal\nwith challenges where actions are not well represented or captured by visual\ncues. However, how to effectively leverage the two modalities is still under\ndevelopment. In this work, we develop a multiscale multimodal Transformer (MMT)\nthat leverages hierarchical representation learning. Particularly, MMT is\ncomposed of a novel multiscale audio Transformer (MAT) and a multiscale video\nTransformer [43]. To learn a discriminative cross-modality fusion, we further\ndesign multimodal supervised contrastive objectives called audio-video\ncontrastive loss (AVC) and intra-modal contrastive loss (IMC) that robustly\nalign the two modalities. MMT surpasses previous state-of-the-art approaches by\n7.3% and 2.1% on Kinetics-Sounds and VGGSound in terms of the top-1 accuracy\nwithout external training data. Moreover, the proposed MAT significantly\noutperforms AST [28] by 22.2%, 4.4% and 4.7% on three public benchmark\ndatasets, and is about 3% more efficient based on the number of FLOPs and 9.8%\nmore efficient based on GPU memory usage.\n","authors":["Wentao Zhu"],"pdf_url":"https://arxiv.org/pdf/2401.04023v1.pdf","comment":"Accepted by WACV 2024; well-formatted PDF is in\n  https://drive.google.com/file/d/10Zo_ydJZFAm7YsxHDgTjhyc4dEJbW_dk/view?usp=sharing"},{"id":"http://arxiv.org/abs/2401.03890v1","updated":"2024-01-08T13:42:59Z","published":"2024-01-08T13:42:59Z","title":"A Survey on 3D Gaussian Splatting","summary":"  3D Gaussian splatting (3D GS) has recently emerged as a transformative\ntechnique in the explicit radiance field and computer graphics landscape. This\ninnovative approach, characterized by the utilization of millions of 3D\nGaussians, represents a significant departure from the neural radiance field\n(NeRF) methodologies, which predominantly use implicit, coordinate-based models\nto map spatial coordinates to pixel values. 3D GS, with its explicit scene\nrepresentations and differentiable rendering algorithms, not only promises\nreal-time rendering capabilities but also introduces unprecedented levels of\ncontrol and editability. This positions 3D GS as a potential game-changer for\nthe next generation of 3D reconstruction and representation. In the present\npaper, we provide the first systematic overview of the recent developments and\ncritical contributions in the domain of 3D GS. We begin with a detailed\nexploration of the underlying principles and the driving forces behind the\nadvent of 3D GS, setting the stage for understanding its significance. A focal\npoint of our discussion is the practical applicability of 3D GS. By\nfacilitating real-time performance, 3D GS opens up a plethora of applications,\nranging from virtual reality to interactive media and beyond. This is\ncomplemented by a comparative analysis of leading 3D GS models, evaluated\nacross various benchmark tasks to highlight their performance and practical\nutility. The survey concludes by identifying current challenges and suggesting\npotential avenues for future research in this domain. Through this survey, we\naim to provide a valuable resource for both newcomers and seasoned researchers,\nfostering further exploration and advancement in applicable and explicit\nradiance field representation.\n","authors":["Guikun Chen","Wenguan Wang"],"pdf_url":"https://arxiv.org/pdf/2401.03890v1.pdf","comment":"Ongoing project"},{"id":"http://arxiv.org/abs/2211.01096v2","updated":"2024-01-08T10:40:58Z","published":"2022-11-02T13:15:11Z","title":"Recovering Sign Bits of DCT Coefficients in Digital Images as an\n  Optimization Problem","summary":"  Recovering unknown, missing, damaged, distorted, or lost information in DCT\ncoefficients is a common task in multiple applications of digital image\nprocessing, including image compression, selective image encryption, and image\ncommunication. This paper investigates the recovery of sign bits in DCT\ncoefficients of digital images, by proposing two different approximation\nmethods to solve a mixed integer linear programming (MILP) problem, which is\nNP-hard in general. One method is a relaxation of the MILP problem to a linear\nprogramming (LP) problem, and the other splits the original MILP problem into\nsome smaller MILP problems and an LP problem. We considered how the proposed\nmethods can be applied to JPEG-encoded images and conducted extensive\nexperiments to validate their performances. The experimental results showed\nthat the proposed methods outperformed other existing methods by a substantial\nmargin, both according to objective quality metrics and our subjective\nevaluation.\n","authors":["Ruiyuan Lin","Sheng Liu","Jun Jiang","Shujun Li","Chengqing Li","C. -C. Jay Kuo"],"pdf_url":"https://arxiv.org/pdf/2211.01096v2.pdf","comment":"22 pages, 8 figures"},{"id":"http://arxiv.org/abs/2305.02375v2","updated":"2024-01-08T08:26:33Z","published":"2023-05-03T18:28:14Z","title":"MaskSearch: Querying Image Masks at Scale","summary":"  Machine learning tasks over image databases often generate masks that\nannotate image content (e.g., saliency maps, segmentation maps, depth maps) and\nenable a variety of applications (e.g., determine if a model is learning\nspurious correlations or if an image was maliciously modified to mislead a\nmodel). While queries that retrieve examples based on mask properties are\nvaluable to practitioners, existing systems do not support them efficiently. In\nthis paper, we formalize the problem and propose MaskSearch, a system that\nfocuses on accelerating queries over databases of image masks while\nguaranteeing the correctness of query results. MaskSearch leverages a novel\nindexing technique and an efficient filter-verification query execution\nframework. Experiments with our prototype show that MaskSearch, using indexes\napproximately 5% of the compressed data size, accelerates individual queries by\nup to two orders of magnitude and consistently outperforms existing methods on\nvarious multi-query workloads that simulate dataset exploration and analysis\nprocesses.\n","authors":["Dong He","Jieyu Zhang","Maureen Daum","Alexander Ratner","Magdalena Balazinska"],"pdf_url":"https://arxiv.org/pdf/2305.02375v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07236v2","updated":"2024-01-08T08:00:22Z","published":"2023-10-11T06:56:08Z","title":"AdaMesh: Personalized Facial Expressions and Head Poses for Adaptive\n  Speech-Driven 3D Facial Animation","summary":"  Speech-driven 3D facial animation aims at generating facial movements that\nare synchronized with the driving speech, which has been widely explored\nrecently. Existing works mostly neglect the person-specific talking style in\ngeneration, including facial expression and head pose styles. Several works\nintend to capture the personalities by fine-tuning modules. However, limited\ntraining data leads to the lack of vividness. In this work, we propose AdaMesh,\na novel adaptive speech-driven facial animation approach, which learns the\npersonalized talking style from a reference video of about 10 seconds and\ngenerates vivid facial expressions and head poses. Specifically, we propose\nmixture-of-low-rank adaptation (MoLoRA) to fine-tune the expression adapter,\nwhich efficiently captures the facial expression style. For the personalized\npose style, we propose a pose adapter by building a discrete pose prior and\nretrieving the appropriate style embedding with a semantic-aware pose style\nmatrix without fine-tuning. Extensive experimental results show that our\napproach outperforms state-of-the-art methods, preserves the talking style in\nthe reference video, and generates vivid facial animation. The supplementary\nvideo and code will be available at https://adamesh.github.io.\n","authors":["Liyang Chen","Weihong Bao","Shun Lei","Boshi Tang","Zhiyong Wu","Shiyin Kang","Haozhi Huang"],"pdf_url":"https://arxiv.org/pdf/2310.07236v2.pdf","comment":"Project Page: https://adamesh.github.io"},{"id":"http://arxiv.org/abs/2312.16228v2","updated":"2024-01-08T03:52:24Z","published":"2023-12-24T18:27:22Z","title":"Deformable Audio Transformer for Audio Event Detection","summary":"  Transformers have achieved promising results on a variety of tasks. However,\nthe quadratic complexity in self-attention computation has limited the\napplications, especially in low-resource settings and mobile or edge devices.\nExisting works have proposed to exploit hand-crafted attention patterns to\nreduce computation complexity. However, such hand-crafted patterns are\ndata-agnostic and may not be optimal. Hence, it is likely that relevant keys or\nvalues are being reduced, while less important ones are still preserved. Based\non this key insight, we propose a novel deformable audio Transformer for audio\nrecognition, named DATAR, where a deformable attention equipping with a pyramid\ntransformer backbone is constructed and learnable. Such an architecture has\nbeen proven effective in prediction tasks,~\\textit{e.g.}, event classification.\nMoreover, we identify that the deformable attention map computation may\nover-simplify the input feature, which can be further enhanced. Hence, we\nintroduce a learnable input adaptor to alleviate this issue, and DATAR achieves\nstate-of-the-art performance.\n","authors":["Wentao Zhu"],"pdf_url":"https://arxiv.org/pdf/2312.16228v2.pdf","comment":"ICASSP 2024. arXiv admin note: substantial text overlap with\n  arXiv:2201.00520 by other authors"},{"id":"http://arxiv.org/abs/2307.09312v3","updated":"2024-01-08T00:12:42Z","published":"2023-07-18T14:57:12Z","title":"Multi-Modal Discussion Transformer: Integrating Text, Images and Graph\n  Transformers to Detect Hate Speech on Social Media","summary":"  We present the Multi-Modal Discussion Transformer (mDT), a novel methodfor\ndetecting hate speech in online social networks such as Reddit discussions. In\ncontrast to traditional comment-only methods, our approach to labelling a\ncomment as hate speech involves a holistic analysis of text and images grounded\nin the discussion context. This is done by leveraging graph transformers to\ncapture the contextual relationships in the discussion surrounding a comment\nand grounding the interwoven fusion layers that combine text and image\nembeddings instead of processing modalities separately. To evaluate our work,\nwe present a new dataset, HatefulDiscussions, comprising complete multi-modal\ndiscussions from multiple online communities on Reddit. We compare the\nperformance of our model to baselines that only process individual comments and\nconduct extensive ablation studies.\n","authors":["Liam Hebert","Gaurav Sahu","Yuxuan Guo","Nanda Kishore Sreenivas","Lukasz Golab","Robin Cohen"],"pdf_url":"https://arxiv.org/pdf/2307.09312v3.pdf","comment":"Accepted to AAAI 2024 (AI for Social Impact Track)"}]},"2024-01-07T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2401.03605v1","updated":"2024-01-07T23:17:42Z","published":"2024-01-07T23:17:42Z","title":"ChatGPT for Conversational Recommendation: Refining Recommendations by\n  Reprompting with Feedback","summary":"  Recommendation algorithms have been pivotal in handling the overwhelming\nvolume of online content. However, these algorithms seldom consider direct user\ninput, resulting in superficial interaction between them. Efforts have been\nmade to include the user directly in the recommendation process through\nconversation, but these systems too have had limited interactivity. Recently,\nLarge Language Models (LLMs) like ChatGPT have gained popularity due to their\nease of use and their ability to adapt dynamically to various tasks while\nresponding to feedback. In this paper, we investigate the effectiveness of\nChatGPT as a top-n conversational recommendation system. We build a rigorous\npipeline around ChatGPT to simulate how a user might realistically probe the\nmodel for recommendations: by first instructing and then reprompting with\nfeedback to refine a set of recommendations. We further explore the effect of\npopularity bias in ChatGPT's recommendations, and compare its performance to\nbaseline models. We find that reprompting ChatGPT with feedback is an effective\nstrategy to improve recommendation relevancy, and that popularity bias can be\nmitigated through prompt engineering.\n","authors":["Kyle Dylan Spurlock","Cagla Acun","Esin Saka","Olfa Nasraoui"],"pdf_url":"https://arxiv.org/pdf/2401.03605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03601v1","updated":"2024-01-07T23:01:56Z","published":"2024-01-07T23:01:56Z","title":"InFoBench: Evaluating Instruction Following Ability in Large Language\n  Models","summary":"  This paper introduces the Decomposed Requirements Following Ratio (DRFR), a\nnew metric for evaluating Large Language Models' (LLMs) ability to follow\ninstructions. Addressing a gap in current methodologies, DRFR breaks down\ncomplex instructions into simpler criteria, facilitating a detailed analysis of\nLLMs' compliance with various aspects of tasks. Alongside this metric, we\npresent InFoBench, a benchmark comprising 500 diverse instructions and 2,250\ndecomposed questions across multiple constraint categories. Our experiments\ncompare DRFR with traditional scoring methods and explore annotation sources,\nincluding human experts, crowd-sourced workers, and GPT-4. The findings\ndemonstrate DRFR's higher reliability and the effectiveness of using GPT-4 as a\ncost-efficient annotator. The evaluation of several advanced LLMs using this\nframework reveals their strengths and areas needing improvement, particularly\nin complex instruction-following. This study contributes a novel metric and\nbenchmark, offering insights for future LLM development and evaluation.\n","authors":["Yiwei Qin","Kaiqiang Song","Yebowen Hu","Wenlin Yao","Sangwoo Cho","Xiaoyang Wang","Xuansheng Wu","Fei Liu","Pengfei Liu","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2401.03601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03591v1","updated":"2024-01-07T22:20:55Z","published":"2024-01-07T22:20:55Z","title":"Text Classification Based on Knowledge Graphs and Improved Attention\n  Mechanism","summary":"  To resolve the semantic ambiguity in texts, we propose a model, which\ninnovatively combines a knowledge graph with an improved attention mechanism.\nAn existing knowledge base is utilized to enrich the text with relevant\ncontextual concepts. The model operates at both character and word levels to\ndeepen its understanding by integrating the concepts. We first adopt\ninformation gain to select import words. Then an encoder-decoder framework is\nused to encode the text along with the related concepts. The local attention\nmechanism adjusts the weight of each concept, reducing the influence of\nirrelevant or noisy concepts during classification. We improve the calculation\nformula for attention scores in the local self-attention mechanism, ensuring\nthat words with different frequencies of occurrence in the text receive higher\nattention scores. Finally, the model employs a Bi-directional Gated Recurrent\nUnit (Bi-GRU), which is effective in feature extraction from texts for improved\nclassification accuracy. Its performance is demonstrated on datasets such as\nAGNews, Ohsumed, and TagMyNews, achieving accuracy of 75.1%, 58.7%, and 68.5%\nrespectively, showing its effectiveness in classifying tasks.\n","authors":["Siyu Li","Lu Chen","Chenwei Song","Xinyi Liu"],"pdf_url":"https://arxiv.org/pdf/2401.03591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03590v1","updated":"2024-01-07T22:11:36Z","published":"2024-01-07T22:11:36Z","title":"Building Efficient and Effective OpenQA Systems for Low-Resource\n  Languages","summary":"  Question answering (QA) is the task of answering questions posed in natural\nlanguage with free-form natural language answers extracted from a given\npassage. In the OpenQA variant, only a question text is given, and the system\nmust retrieve relevant passages from an unstructured knowledge source and use\nthem to provide answers, which is the case in the mainstream QA systems on the\nWeb. QA systems currently are mostly limited to the English language due to the\nlack of large-scale labeled QA datasets in non-English languages. In this\npaper, we show that effective, low-cost OpenQA systems can be developed for\nlow-resource languages. The key ingredients are (1) weak supervision using\nmachine-translated labeled datasets and (2) a relevant unstructured knowledge\nsource in the target language. Furthermore, we show that only a few hundred\ngold assessment examples are needed to reliably evaluate these systems. We\napply our method to Turkish as a challenging case study, since English and\nTurkish are typologically very distinct. We present SQuAD-TR, a machine\ntranslation of SQuAD2.0, and we build our OpenQA system by adapting ColBERT-QA\nfor Turkish. We obtain a performance improvement of 9-34% in the EM score and\n13-33% in the F1 score compared to the BM25-based and DPR-based baseline QA\nreader models by using two versions of Wikipedia dumps spanning two years. Our\nresults show that SQuAD-TR makes OpenQA feasible for Turkish, which we hope\nencourages researchers to build OpenQA systems in other low-resource languages.\nWe make all the code, models, and the dataset publicly available.\n","authors":["Emrah Budur","Rıza Özçelik","Dilara Soylu","Omar Khattab","Tunga Güngör","Christopher Potts"],"pdf_url":"https://arxiv.org/pdf/2401.03590v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11571v2","updated":"2024-01-07T21:01:55Z","published":"2023-10-17T20:40:59Z","title":"Pragmatic Evaluation of Clarifying Questions with Fact-Level Masking","summary":"  The ability to derive useful information by asking clarifying questions (ACQ)\nis an important element of real life collaboration on reasoning tasks, such as\nquestion answering (QA). Existing natural language ACQ challenges, however,\nevaluate generations based on word overlap rather than the value of the\ninformation itself. Word overlap is often an inappropriate metric for question\ngeneration since many different questions could be useful in a given situation,\nand a single question can be phrased many different ways. Instead, we propose\nevaluating questions pragmatically based on the value of the information they\nretrieve. Here we present a definition and framework for natural language\npragmatic asking of clarifying questions (PACQ), the problem of generating\nquestions that result in answers useful for a reasoning task. We also present\nfact-level masking (FLM), a procedure for converting natural language datasets\ninto self-supervised PACQ datasets by omitting particular critical facts.\nFinally, we generate a PACQ dataset from the HotpotQA dataset using FLM and\nevaluate several zero-shot language models on it. Our experiments show that\ncurrent zero-shot models struggle to ask questions that retrieve useful\ninformation, as compared to human annotators. These results demonstrate an\nopportunity to use FLM datasets and the PACQ framework to objectively evaluate\nand improve question generation and other language models.\n","authors":["Matthew Toles","Yukun Huang","Zhou Yu","Luis Gravano"],"pdf_url":"https://arxiv.org/pdf/2310.11571v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10475v7","updated":"2024-01-07T20:25:09Z","published":"2023-03-18T19:17:47Z","title":"A Comprehensive Survey on Instruction Following","summary":"  Task semantics can be expressed by a set of input-output examples or a piece\nof textual instruction. Conventional machine learning approaches for natural\nlanguage processing (NLP) mainly rely on the availability of large-scale sets\nof task-specific examples. Two issues arise: first, collecting task-specific\nlabeled examples does not apply to scenarios where tasks may be too complicated\nor costly to annotate, or the system is required to handle a new task\nimmediately; second, this is not user-friendly since end-users are probably\nmore willing to provide task description rather than a set of examples before\nusing the system. Therefore, the community is paying increasing interest in a\nnew supervision-seeking paradigm for NLP: learning to follow task instructions,\ni.e., instruction following. Despite its impressive progress, there are some\ncommon issues that the community struggles with. This survey paper tries to\nsummarize and provide insights to the current research on instruction\nfollowing, particularly, by answering the following questions: (i) What is task\ninstruction, and what instruction types exist? (ii) How to model instructions?\n(iii) What are popular instruction following datasets and evaluation metrics?\n(iv) What factors influence and explain the instructions' performance? (v) What\nchallenges remain in instruction following? To our knowledge, this is the first\ncomprehensive survey about instruction following.\n","authors":["Renze Lou","Kai Zhang","Wenpeng Yin"],"pdf_url":"https://arxiv.org/pdf/2303.10475v7.pdf","comment":"Preprint. The paper list is available at\n  https://github.com/RenzeLou/awesome-instruction-learning"},{"id":"http://arxiv.org/abs/2401.03563v1","updated":"2024-01-07T18:12:20Z","published":"2024-01-07T18:12:20Z","title":"Data-CUBE: Data Curriculum for Instruction-based Sentence Representation\n  Learning","summary":"  Recently, multi-task instruction tuning has been applied into sentence\nrepresentation learning, which endows the capability of generating specific\nrepresentations with the guidance of task instruction, exhibiting strong\ngeneralization ability on new tasks. However, these methods mostly neglect the\npotential interference problems across different tasks and instances, which may\naffect the training and convergence of the model. To address it, we propose a\ndata curriculum method, namely Data-CUBE, that arranges the orders of all the\nmulti-task data for training, to minimize the interference risks from the two\nviews. In the task level, we aim to find the optimal task order to minimize the\ntotal cross-task interference risk, which is exactly the traveling salesman\nproblem, hence we utilize a simulated annealing algorithm to find its solution.\nIn the instance level, we measure the difficulty of all instances per task,\nthen divide them into the easy-to-difficult mini-batches for training.\nExperiments on MTEB sentence representation evaluation tasks show that our\napproach can boost the performance of state-of-the-art methods. Our code and\ndata are publicly available at the link:\n\\url{https://github.com/RUCAIBox/Data-CUBE}.\n","authors":["Yingqian Min","Kun Zhou","Dawei Gao","Wayne Xin Zhao","He Hu","Yaliang Li"],"pdf_url":"https://arxiv.org/pdf/2401.03563v1.pdf","comment":"14 pages, working in progress"},{"id":"http://arxiv.org/abs/2310.01132v2","updated":"2024-01-07T17:43:50Z","published":"2023-10-02T12:11:17Z","title":"Automated Evaluation of Classroom Instructional Support with LLMs and\n  BoWs: Connecting Global Predictions to Specific Feedback","summary":"  With the aim to provide teachers with more specific, frequent, and actionable\nfeedback about their teaching, we explore how Large Language Models (LLMs) can\nbe used to estimate ``Instructional Support'' domain scores of the CLassroom\nAssessment Scoring System (CLASS), a widely used observation protocol. We\ndesign a machine learning architecture that uses either zero-shot prompting of\nMeta's Llama2, and/or a classic Bag of Words (BoW) model, to classify\nindividual utterances of teachers' speech (transcribed automatically using\nOpenAI's Whisper) for the presence of Instructional Support. Then, these\nutterance-level judgments are aggregated over an entire 15-min observation\nsession to estimate a global CLASS score. Experiments on two CLASS-coded\ndatasets of toddler and pre-kindergarten classrooms indicate that (1) automatic\nCLASS Instructional Support estimation accuracy using the proposed method\n(Pearson $R$ up to $0.47$) approaches human inter-rater reliability (up to\n$R=0.55$); (2) LLMs yield slightly greater accuracy than BoW for this task,\nthough the best models often combined features extracted from both LLM and BoW;\nand (3) for classifying individual utterances, there is still room for\nimprovement of automated methods compared to human-level judgments. Finally,\n(4) we illustrate how the model's outputs can be visualized at the utterance\nlevel to provide teachers with explainable feedback on which utterances were\nmost positively or negatively correlated with specific CLASS dimensions.\n","authors":["Jacob Whitehill","Jennifer LoCasale-Crouch"],"pdf_url":"https://arxiv.org/pdf/2310.01132v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03551v1","updated":"2024-01-07T17:23:27Z","published":"2024-01-07T17:23:27Z","title":"CAPTAIN at COLIEE 2023: Efficient Methods for Legal Information\n  Retrieval and Entailment Tasks","summary":"  The Competition on Legal Information Extraction/Entailment (COLIEE) is held\nannually to encourage advancements in the automatic processing of legal texts.\nProcessing legal documents is challenging due to the intricate structure and\nmeaning of legal language. In this paper, we outline our strategies for\ntackling Task 2, Task 3, and Task 4 in the COLIEE 2023 competition. Our\napproach involved utilizing appropriate state-of-the-art deep learning methods,\ndesigning methods based on domain characteristics observation, and applying\nmeticulous engineering practices and methodologies to the competition. As a\nresult, our performance in these tasks has been outstanding, with first places\nin Task 2 and Task 3, and promising results in Task 4. Our source code is\navailable at https://github.com/Nguyen2015/CAPTAIN-COLIEE2023/tree/coliee2023.\n","authors":["Chau Nguyen","Phuong Nguyen","Thanh Tran","Dat Nguyen","An Trieu","Tin Pham","Anh Dang","Le-Minh Nguyen"],"pdf_url":"https://arxiv.org/pdf/2401.03551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.00446v2","updated":"2024-01-07T17:17:00Z","published":"2023-04-30T10:41:43Z","title":"Building a Non-native Speech Corpus Featuring Chinese-English Bilingual\n  Children: Compilation and Rationale","summary":"  This paper introduces a non-native speech corpus consisting of narratives\nfrom fifty 5- to 6-year-old Chinese-English children. Transcripts totaling 6.5\nhours of children taking a narrative comprehension test in English (L2) are\npresented, along with human-rated scores and annotations of grammatical and\npronunciation errors. The children also completed the parallel MAIN tests in\nChinese (L1) for reference purposes. For all tests we recorded audio and video\nwith our innovative self-developed remote collection methods. The video\nrecordings serve to mitigate the challenge of low intelligibility in L2\nnarratives produced by young children during the transcription process. This\ncorpus offers valuable resources for second language teaching and has the\npotential to enhance the overall performance of automatic speech recognition\n(ASR).\n","authors":["Hiuchung Hung","Andreas Maier","Thorsten Piske"],"pdf_url":"https://arxiv.org/pdf/2305.00446v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03545v1","updated":"2024-01-07T17:12:08Z","published":"2024-01-07T17:12:08Z","title":"Is there really a Citation Age Bias in NLP?","summary":"  Citations are a key ingredient of scientific research to relate a paper to\nothers published in the community. Recently, it has been noted that there is a\ncitation age bias in the Natural Language Processing (NLP) community, one of\nthe currently fastest growing AI subfields, in that the mean age of the\nbibliography of NLP papers has become ever younger in the last few years,\nleading to `citation amnesia' in which older knowledge is increasingly\nforgotten. In this work, we put such claims into perspective by analyzing the\nbibliography of $\\sim$300k papers across 15 different scientific fields\nsubmitted to the popular preprint server Arxiv in the time period from 2013 to\n2022. We find that all AI subfields (in particular: cs.AI, cs.CL, cs.CV, cs.LG)\nhave similar trends of citation amnesia, in which the age of the bibliography\nhas roughly halved in the last 10 years (from above 12 in 2013 to below 7 in\n2022), on average. Rather than diagnosing this as a citation age bias in the\nNLP community, we believe this pattern is an artefact of the dynamics of these\nresearch fields, in which new knowledge is produced in ever shorter time\nintervals.\n","authors":["Hoa Nguyen","Steffen Eger"],"pdf_url":"https://arxiv.org/pdf/2401.03545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03538v1","updated":"2024-01-07T16:39:34Z","published":"2024-01-07T16:39:34Z","title":"Transfer the linguistic representations from TTS to accent conversion\n  with non-parallel data","summary":"  Accent conversion aims to convert the accent of a source speech to a target\naccent, meanwhile preserving the speaker's identity. This paper introduces a\nnovel non-autoregressive framework for accent conversion that learns\naccent-agnostic linguistic representations and employs them to convert the\naccent in the source speech. Specifically, the proposed system aligns speech\nrepresentations with linguistic representations obtained from Text-to-Speech\n(TTS) systems, enabling training of the accent voice conversion model on\nnon-parallel data. Furthermore, we investigate the effectiveness of a\npretraining strategy on native data and different acoustic features within our\nproposed framework. We conduct a comprehensive evaluation using both subjective\nand objective metrics to assess the performance of our approach. The evaluation\nresults highlight the benefits of the pretraining strategy and the\nincorporation of richer semantic features, resulting in significantly enhanced\naudio quality and intelligibility.\n","authors":["Xi Chen","Jiakun Pei","Liumeng Xue","Mingyang Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.03538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03515v1","updated":"2024-01-07T15:13:24Z","published":"2024-01-07T15:13:24Z","title":"RoBERTurk: Adjusting RoBERTa for Turkish","summary":"  We pretrain RoBERTa on a Turkish corpora using BPE tokenizer. Our model\noutperforms BERTurk family models on the BOUN dataset for the POS task while\nresulting in underperformance on the IMST dataset for the same task and\nachieving competitive scores on the Turkish split of the XTREME dataset for the\nNER task - all while being pretrained on smaller data than its competitors. We\nrelease our pretrained model and tokenizer.\n","authors":["Nuri Tas"],"pdf_url":"https://arxiv.org/pdf/2401.03515v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03514v1","updated":"2024-01-07T15:05:26Z","published":"2024-01-07T15:05:26Z","title":"ROIC-DM: Robust Text Inference and Classification via Diffusion Model","summary":"  While language models have made many milestones in text inference and\nclassification tasks, they remain susceptible to adversarial attacks that can\nlead to unforeseen outcomes. Existing works alleviate this problem by equipping\nlanguage models with defense patches. However, these defense strategies often\nrely on impractical assumptions or entail substantial sacrifices in model\nperformance. Consequently, enhancing the resilience of the target model using\nsuch defense mechanisms is a formidable challenge. This paper introduces an\ninnovative model for robust text inference and classification, built upon\ndiffusion models (ROIC-DM). Benefiting from its training involving denoising\nstages, ROIC-DM inherently exhibits greater robustness compared to conventional\nlanguage models. Moreover, ROIC-DM can attain comparable, and in some cases,\nsuperior performance to language models, by effectively incorporating them as\nadvisory components. Extensive experiments conducted with several strong\ntextual adversarial attacks on three datasets demonstrate that (1) ROIC-DM\noutperforms traditional language models in robustness, even when the latter are\nfortified with advanced defense mechanisms; (2) ROIC-DM can achieve comparable\nand even better performance than traditional language models by using them as\nadvisors.\n","authors":["Shilong Yuan","Wei Yuan","Tieke HE"],"pdf_url":"https://arxiv.org/pdf/2401.03514v1.pdf","comment":"aaai2024"},{"id":"http://arxiv.org/abs/2401.03512v1","updated":"2024-01-07T15:00:36Z","published":"2024-01-07T15:00:36Z","title":"Token-free LLMs Can Generate Chinese Classical Poetry with More Accurate\n  Format","summary":"  Finetuned large language models (such as ChatGPT and Qwen-chat) can generate\nChinese classical poetry following human's instructions. LLMs perform well in\ncontent, but are usually lacking in format, with occasionally excess or\ninsufficient number of characters in each line. Since most SOTA LLMs are\ntoken-based, we assume that the format inaccuracy is due to the difficulty of\nthe \"token planning\" task, which means that the LLM need to know exactly how\nmuch characters are contained in each token and do length-control planning\nbased on that knowledge. In this paper, we first confirm our assumption by\nshowing that existing token-based large language models has limited knowledge\non token-character relationship. We use a spelling bee probing procedure, and\nfind that Qwen-chat failed in nearly 15% Chinese spelling test. We then show\nthat a token-based model can be easily tailored into a token-free model (in\nterms of Chinese), which can largely solve the format accuracy problem. Our\ntailoring procedure removes long-token from vocabulary and keeps only\ncharacter-level or byte-level tokens. As part of our contribution, we release\nthe finetuned token-free model (which is based on Qwen-chat-7B), which can\ngenerate chinese classical poetry following complex instructions like LLMs\n(such as story paraphrasing), and also perform well in format. On the test set,\nour token-free model achives an format accuracy of 0.96, compared to 0.84 for\ntoken-based counterparts and 0.38 for GPT-4.\n","authors":["Chengyue Yu","Lei Zang","Jiaotuan Wang","Chenyi Zhuang","Jinjie Gu"],"pdf_url":"https://arxiv.org/pdf/2401.03512v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.02045v2","updated":"2024-01-07T14:59:15Z","published":"2023-09-05T08:44:23Z","title":"Enhance Multi-domain Sentiment Analysis of Review Texts through\n  Prompting Strategies","summary":"  Large Language Models (LLMs) have made significant strides in both scientific\nresearch and practical applications. Existing studies have demonstrated the\nstate-of-the-art (SOTA) performance of LLMs in various natural language\nprocessing tasks. However, the question of how to further enhance LLMs'\nperformance in specific task using prompting strategies remains a pivotal\nconcern. This paper explores the enhancement of LLMs' performance in sentiment\nanalysis through the application of prompting strategies. We formulate the\nprocess of prompting for sentiment analysis tasks and introduce two novel\nstrategies tailored for sentiment analysis: RolePlaying (RP) prompting and\nChain-of-thought (CoT) prompting. Specifically, we also propose the RP-CoT\nprompting strategy which is a combination of RP prompting and CoT prompting. We\nconduct comparative experiments on three distinct domain datasets to evaluate\nthe effectiveness of the proposed sentiment analysis strategies. The results\ndemonstrate that the adoption of the proposed prompting strategies leads to a\nincreasing enhancement in sentiment analysis accuracy. Further, the CoT\nprompting strategy exhibits a notable impact on implicit sentiment analysis,\nwith the RP-CoT prompting strategy delivering the most superior performance\namong all strategies.\n","authors":["Yajing Wang","Zongwei Luo"],"pdf_url":"https://arxiv.org/pdf/2309.02045v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03497v1","updated":"2024-01-07T14:31:27Z","published":"2024-01-07T14:31:27Z","title":"EAT: Self-Supervised Pre-Training with Efficient Audio Transformer","summary":"  Audio self-supervised learning (SSL) pre-training, which aims to learn good\nrepresentations from unlabeled audio, has made remarkable progress. However,\nthe extensive computational demands during pre-training pose a significant\nbarrier to the potential application and optimization of audio SSL models. In\nthis paper, inspired by the success of data2vec 2.0 in image modality and\nAudio-MAE in audio modality, we introduce Efficient Audio Transformer (EAT) to\nfurther improve the effectiveness and efficiency in audio SSL. The proposed EAT\nadopts the bootstrap self-supervised training paradigm to the audio domain. A\nnovel Utterance-Frame Objective (UFO) is designed to enhance the modeling\ncapability of acoustic events. Furthermore, we reveal that the masking strategy\nis critical in audio SSL pre-training, and superior audio representations can\nbe obtained with large inverse block masks. Experiment results demonstrate that\nEAT achieves state-of-the-art (SOTA) performance on a range of audio-related\ntasks, including AudioSet (AS-2M, AS-20K), ESC-50, and SPC-2, along with a\nsignificant pre-training speedup up to ~15x compared to existing audio SSL\nmodels.\n","authors":["Wenxi Chen","Yuzhe Liang","Ziyang Ma","Zhisheng Zheng","Xie Chen"],"pdf_url":"https://arxiv.org/pdf/2401.03497v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03472v1","updated":"2024-01-07T12:48:07Z","published":"2024-01-07T12:48:07Z","title":"PEneo: Unifying Line Extraction, Line Grouping, and Entity Linking for\n  End-to-end Document Pair Extraction","summary":"  Document pair extraction aims to identify key and value entities as well as\ntheir relationships from visually-rich documents. Most existing methods divide\nit into two separate tasks: semantic entity recognition (SER) and relation\nextraction (RE). However, simply concatenating SER and RE serially can lead to\nsevere error propagation, and it fails to handle cases like multi-line entities\nin real scenarios. To address these issues, this paper introduces a novel\nframework, PEneo (Pair Extraction new decoder option), which performs document\npair extraction in a unified pipeline, incorporating three concurrent\nsub-tasks: line extraction, line grouping, and entity linking. This approach\nalleviates the error accumulation problem and can handle the case of multi-line\nentities. Furthermore, to better evaluate the model's performance and to\nfacilitate future research on pair extraction, we introduce RFUND, a\nre-annotated version of the commonly used FUNSD and XFUND datasets, to make\nthem more accurate and cover realistic situations. Experiments on various\nbenchmarks demonstrate PEneo's superiority over previous pipelines, boosting\nthe performance by a large margin (e.g., 19.89%-22.91% F1 score on RFUND-EN)\nwhen combined with various backbones like LiLT and LayoutLMv3, showing its\neffectiveness and generality. Codes and the new annotations will be open to the\npublic.\n","authors":["Zening Lin","Jiapeng Wang","Teng Li","Wenhui Liao","Dayi Huang","Longfei Xiong","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2401.03472v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03467v1","updated":"2024-01-07T12:27:14Z","published":"2024-01-07T12:27:14Z","title":"Maintaining Journalistic Integrity in the Digital Age: A Comprehensive\n  NLP Framework for Evaluating Online News Content","summary":"  The rapid growth of online news platforms has led to an increased need for\nreliable methods to evaluate the quality and credibility of news articles. This\npaper proposes a comprehensive framework to analyze online news texts using\nnatural language processing (NLP) techniques, particularly a language model\nspecifically trained for this purpose, alongside other well-established NLP\nmethods. The framework incorporates ten journalism standards-objectivity,\nbalance and fairness, readability and clarity, sensationalism and clickbait,\nethical considerations, public interest and value, source credibility,\nrelevance and timeliness, factual accuracy, and attribution and transparency-to\nassess the quality of news articles. By establishing these standards,\nresearchers, media organizations, and readers can better evaluate and\nunderstand the content they consume and produce. The proposed method has some\nlimitations, such as potential difficulty in detecting subtle biases and the\nneed for continuous updating of the language model to keep pace with evolving\nlanguage patterns.\n","authors":["Ljubisa Bojic","Nikola Prodanovic","Agariadne Dwinggo Samala"],"pdf_url":"https://arxiv.org/pdf/2401.03467v1.pdf","comment":"21 pages"},{"id":"http://arxiv.org/abs/2401.03462v1","updated":"2024-01-07T11:57:40Z","published":"2024-01-07T11:57:40Z","title":"Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon","summary":"  The utilization of long contexts poses a big challenge for large language\nmodels due to their limited context window length. Although the context window\ncan be extended through fine-tuning, it will result in a considerable cost at\nboth training and inference time, and exert an unfavorable impact to the LLM's\noriginal capabilities. In this work, we propose Activation Beacon, which\ncondenses LLM's raw activations into more compact forms such that it can\nperceive a much longer context with a limited context window. Activation Beacon\nis introduced as a plug-and-play module for the LLM. It fully preserves the\nLLM's original capability on short contexts while extending the new capability\non processing longer contexts. Besides, it works with short sliding windows to\nprocess the long context, which achieves a competitive memory and time\nefficiency in both training and inference. Activation Beacon is learned by the\nauto-regression task conditioned on a mixture of beacons with diversified\ncondensing ratios. Thanks to such a treatment, it can be efficiently trained\npurely with short-sequence data in just 10K steps, which consumes less than 9\nhours on a single 8xA800 GPU machine. The experimental studies show that\nActivation Beacon is able to extend Llama-2-7B's context length by $\\times100$\ntimes (from 4K to 400K), meanwhile achieving a superior result on both\nlong-context generation and understanding tasks. Our model and code will be\navailable at the BGE repository.\n","authors":["Peitian Zhang","Zheng Liu","Shitao Xiao","Ninglu Shao","Qiwei Ye","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2401.03462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01843v2","updated":"2024-01-07T11:51:33Z","published":"2024-01-03T17:22:48Z","title":"Investigating Semi-Supervised Learning Algorithms in Text Datasets","summary":"  Using large training datasets enhances the generalization capabilities of\nneural networks. Semi-supervised learning (SSL) is useful when there are few\nlabeled data and a lot of unlabeled data. SSL methods that use data\naugmentation are most successful for image datasets. In contrast, texts do not\nhave consistent augmentation methods as images. Consequently, methods that use\naugmentation are not as effective in text data as they are in image data. In\nthis study, we compared SSL algorithms that do not require augmentation; these\nare self-training, co-training, tri-training, and tri-training with\ndisagreement. In the experiments, we used 4 different text datasets for\ndifferent tasks. We examined the algorithms from a variety of perspectives by\nasking experiment questions and suggested several improvements. Among the\nalgorithms, tri-training with disagreement showed the closest performance to\nthe Oracle; however, performance gap shows that new semi-supervised algorithms\nor improvements in existing methods are needed.\n","authors":["Himmet Toprak Kesgin","Mehmet Fatih Amasyali"],"pdf_url":"https://arxiv.org/pdf/2401.01843v2.pdf","comment":"Innovations in Intelligent Systems and Applications Conference (ASYU)"},{"id":"http://arxiv.org/abs/2312.11193v6","updated":"2024-01-07T11:12:59Z","published":"2023-12-18T13:40:16Z","title":"\"Paraphrasing The Original Text\" Makes High Accuracy Long-Context QA","summary":"  Most open-source generative language models currently have a context window\nof no more than 4k, limiting their ability when facing long text. Even models\nwith longer context windows cannot guarantee satisfactory accuracy on\nlong-context problems. To tackle this issue, we explore from the perspective of\ntraining data and theoretically demonstrate that improving the capability to\nhandle long contexts requires \"effective\" rather than simply \"long\" data. Based\non this insight, we propose using the \"original text paraphrasing\" task and\nsuccessfully extend the context window of existing models to 32k through a\nlow-cost and effective method. Our fine-tuned model achieves state-of-the-art\naccuracy in multi-document-QA among models of comparable scale. The model and\ntraining data have been made available on\nHuggingFace(https://huggingface.co/yuyijiong/Qwen-14b-chat-yarn-32k) and\nWiseModel(https://wisemodel.cn/models/yuyijiong/Qwen-14b-chat-yarn-32k).\n","authors":["Yijiong Yu"],"pdf_url":"https://arxiv.org/pdf/2312.11193v6.pdf","comment":"Chinese version of this paper can be downloaded from\n  (https://cloud.tsinghua.edu.cn/d/5894ec4442e54a6aac96/)"},{"id":"http://arxiv.org/abs/2401.03426v1","updated":"2024-01-07T09:06:58Z","published":"2024-01-07T09:06:58Z","title":"On Leveraging Large Language Models for Enhancing Entity Resolution","summary":"  Entity resolution, the task of identifying and consolidating records that\npertain to the same real-world entity, plays a pivotal role in various sectors\nsuch as e-commerce, healthcare, and law enforcement. The emergence of Large\nLanguage Models (LLMs) like GPT-4 has introduced a new dimension to this task,\nleveraging their advanced linguistic capabilities. This paper explores the\npotential of LLMs in the entity resolution process, shedding light on both\ntheir advantages and the computational complexities associated with large-scale\nmatching. We introduce strategies for the efficient utilization of LLMs,\nincluding the selection of an optimal set of matching questions, namely MQsSP,\nwhich is proved to be a NP-hard problem. Our approach optimally chooses the\nmost effective matching questions while keep consumption limited to your budget\n. Additionally, we propose a method to adjust the distribution of possible\npartitions after receiving responses from LLMs, with the goal of reducing the\nuncertainty of entity resolution. We evaluate the effectiveness of our approach\nusing entropy as a metric, and our experimental results demonstrate the\nefficiency and effectiveness of our proposed methods, offering promising\nprospects for real-world applications.\n","authors":["Huahang Li","Longyu Feng","Shuangyin Li","Fei Hao","Chen Jason Zhang","Yuanfeng Song","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2401.03426v1.pdf","comment":"12 pages,6 figures, ICDE 2024"},{"id":"http://arxiv.org/abs/2312.09785v3","updated":"2024-01-07T08:58:02Z","published":"2023-12-15T13:40:25Z","title":"RJUA-QA: A Comprehensive QA Dataset for Urology","summary":"  We introduce RJUA-QA, a novel medical dataset for question answering (QA) and\nreasoning with clinical evidence, contributing to bridge the gap between\ngeneral large language models (LLMs) and medical-specific LLM applications.\nRJUA-QA is derived from realistic clinical scenarios and aims to facilitate\nLLMs in generating reliable diagnostic and advice. The dataset contains 2,132\ncurated Question-Context-Answer pairs, corresponding about 25,000 diagnostic\nrecords and clinical cases. The dataset covers 67 common urological disease\ncategories, where the disease coverage exceeds 97.6\\% of the population seeking\nmedical services in urology. Each data instance in RJUA-QA comprises: (1) a\nquestion mirroring real patient to inquiry about clinical symptoms and medical\nconditions, (2) a context including comprehensive expert knowledge, serving as\na reference for medical examination and diagnosis, (3) a doctor response\noffering the diagnostic conclusion and suggested examination guidance, (4) a\ndiagnosed clinical disease as the recommended diagnostic outcome, and (5)\nclinical advice providing recommendations for medical examination. RJUA-QA is\nthe first medical QA dataset for clinical reasoning over the patient inquiries,\nwhere expert-level knowledge and experience are required for yielding\ndiagnostic conclusions and medical examination advice. A comprehensive\nevaluation is conducted to evaluate the performance of both medical-specific\nand general LLMs on the RJUA-QA dataset. Our data is are publicly available at\n\\url{https://github.com/alipay/RJU_Ant_QA}.\n","authors":["Shiwei Lyu","Chenfei Chi","Hongbo Cai","Lei Shi","Xiaoyan Yang","Lei Liu","Xiang Chen","Deng Zhao","Zhiqiang Zhang","Xianguo Lyu","Ming Zhang","Fangzhou Li","Xiaowei Ma","Yue Shen","Jinjie Gu","Wei Xue","Yiran Huang"],"pdf_url":"https://arxiv.org/pdf/2312.09785v3.pdf","comment":"An initial version"},{"id":"http://arxiv.org/abs/2312.01040v3","updated":"2024-01-07T08:52:24Z","published":"2023-12-02T05:54:06Z","title":"From Beginner to Expert: Modeling Medical Knowledge into General LLMs","summary":"  Recently, large language model (LLM) based artificial intelligence (AI)\nsystems have demonstrated remarkable capabilities in natural language\nunderstanding and generation. However, these models face a significant\nchallenge when it comes to sensitive applications, such as reasoning over\nmedical knowledge and answering medical questions in a physician-like manner.\nPrior studies attempted to overcome this challenge by increasing the model size\n(>100B) to learn more general medical knowledge, while there is still room for\nimprovement in LLMs with smaller-scale model sizes (<100B). In this work, we\nstart from a pre-trained general LLM model (AntGLM-10B) and fine-tune it from a\nmedical beginner towards a medical expert (called AntGLM-Med-10B), which\nleverages a 3-stage optimization procedure, i.e., general medical knowledge\ninjection, medical domain instruction tuning, and specific medical task\nadaptation. Our contributions are threefold: (1) We specifically investigate\nhow to adapt a pre-trained general LLM in medical domain, especially for a\nspecific medical task. (2) We collect and construct large-scale medical\ndatasets for each stage of the optimization process. These datasets encompass\nvarious data types and tasks, such as question-answering, medical reasoning,\nmulti-choice questions, and medical conversations. (3) Specifically for\nmulti-choice questions in the medical domain, we propose a novel\nVerification-of-Choice approach for prompting engineering, which significantly\nenhances the reasoning ability of LLMs. Remarkably, by combining the above\napproaches, our AntGLM-Med-10B model can outperform the most of LLMs on\nPubMedQA, including both general and medical LLMs, even when these LLMs have\nlarger model size.\n","authors":["Qiang Li","Xiaoyan Yang","Haowen Wang","Qin Wang","Lei Liu","Junjie Wang","Yang Zhang","Mingyuan Chu","Sen Hu","Yicheng Chen","Yue Shen","Cong Fan","Wangshu Zhang","Teng Xu","Jinjie Gu","Jing Zheng","Guannan Zhang Ant Group"],"pdf_url":"https://arxiv.org/pdf/2312.01040v3.pdf","comment":"Developed by Ant Group for PubMedQA leaderboard"},{"id":"http://arxiv.org/abs/2401.03411v1","updated":"2024-01-07T08:03:06Z","published":"2024-01-07T08:03:06Z","title":"GRAM: Global Reasoning for Multi-Page VQA","summary":"  The increasing use of transformer-based large language models brings forward\nthe challenge of processing long sequences. In document visual question\nanswering (DocVQA), leading methods focus on the single-page setting, while\ndocuments can span hundreds of pages. We present GRAM, a method that seamlessly\nextends pre-trained single-page models to the multi-page setting, without\nrequiring computationally-heavy pretraining. To do so, we leverage a\nsingle-page encoder for local page-level understanding, and enhance it with\ndocument-level designated layers and learnable tokens, facilitating the flow of\ninformation across pages for global reasoning. To enforce our model to utilize\nthe newly introduced document-level tokens, we propose a tailored bias\nadaptation method. For additional computational savings during decoding, we\nintroduce an optional compression stage using our C-Former model, which reduces\nthe encoded sequence length, thereby allowing a tradeoff between quality and\nlatency. Extensive experiments showcase GRAM's state-of-the-art performance on\nthe benchmarks for multi-page DocVQA, demonstrating the effectiveness of our\napproach.\n","authors":["Tsachi Blau","Sharon Fogel","Roi Ronen","Alona Golts","Roy Ganz","Elad Ben Avraham","Aviad Aberdam","Shahar Tsiper","Ron Litman"],"pdf_url":"https://arxiv.org/pdf/2401.03411v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2103.00676v3","updated":"2024-01-07T08:00:31Z","published":"2021-03-01T01:00:09Z","title":"Token-Modification Adversarial Attacks for Natural Language Processing:\n  A Survey","summary":"  Many adversarial attacks target natural language processing systems, most of\nwhich succeed through modifying the individual tokens of a document. Despite\nthe apparent uniqueness of each of these attacks, fundamentally they are simply\na distinct configuration of four components: a goal function, allowable\ntransformations, a search method, and constraints. In this survey, we\nsystematically present the different components used throughout the literature,\nusing an attack-independent framework which allows for easy comparison and\ncategorisation of components. Our work aims to serve as a comprehensive guide\nfor newcomers to the field and to spark targeted research into refining the\nindividual attack components.\n","authors":["Tom Roth","Yansong Gao","Alsharif Abuadbba","Surya Nepal","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2103.00676v3.pdf","comment":"Version 3: edited and expanded"},{"id":"http://arxiv.org/abs/2401.03408v1","updated":"2024-01-07T07:59:10Z","published":"2024-01-07T07:59:10Z","title":"Escalation Risks from Language Models in Military and Diplomatic\n  Decision-Making","summary":"  Governments are increasingly considering integrating autonomous AI agents in\nhigh-stakes military and foreign-policy decision-making, especially with the\nemergence of advanced generative AI models like GPT-4. Our work aims to\nscrutinize the behavior of multiple AI agents in simulated wargames,\nspecifically focusing on their predilection to take escalatory actions that may\nexacerbate multilateral conflicts. Drawing on political science and\ninternational relations literature about escalation dynamics, we design a novel\nwargame simulation and scoring framework to assess the escalation risks of\nactions taken by these agents in different scenarios. Contrary to prior\nstudies, our research provides both qualitative and quantitative insights and\nfocuses on large language models (LLMs). We find that all five studied\noff-the-shelf LLMs show forms of escalation and difficult-to-predict escalation\npatterns. We observe that models tend to develop arms-race dynamics, leading to\ngreater conflict, and in rare cases, even to the deployment of nuclear weapons.\nQualitatively, we also collect the models' reported reasonings for chosen\nactions and observe worrying justifications based on deterrence and\nfirst-strike tactics. Given the high stakes of military and foreign-policy\ncontexts, we recommend further examination and cautious consideration before\ndeploying autonomous language model agents for strategic military or diplomatic\ndecision-making.\n","authors":["Juan-Pablo Rivera","Gabriel Mukobi","Anka Reuel","Max Lamparth","Chandler Smith","Jacquelyn Schneider"],"pdf_url":"https://arxiv.org/pdf/2401.03408v1.pdf","comment":"10 pages body, 57 pages appendix, 46 figures, 11 tables"},{"id":"http://arxiv.org/abs/2401.01523v2","updated":"2024-01-07T07:23:06Z","published":"2024-01-03T03:28:55Z","title":"GOAT-Bench: Safety Insights to Large Multimodal Models through\n  Meme-Based Social Abuse","summary":"  The exponential growth of social media has profoundly transformed how\ninformation is created, disseminated, and absorbed, exceeding any precedent in\nthe digital age. Regrettably, this explosion has also spawned a significant\nincrease in the online abuse of memes. Evaluating the negative impact of memes\nis notably challenging, owing to their often subtle and implicit meanings,\nwhich are not directly conveyed through the overt text and imagery. In light of\nthis, large multimodal models (LMMs) have emerged as a focal point of interest\ndue to their remarkable capabilities in handling diverse multimodal tasks. In\nresponse to this development, our paper aims to thoroughly examine the capacity\nof various LMMs (e.g. GPT-4V) to discern and respond to the nuanced aspects of\nsocial abuse manifested in memes. We introduce the comprehensive meme\nbenchmark, GOAT-Bench, comprising over 6K varied memes encapsulating themes\nsuch as implicit hate speech, sexism, and cyberbullying, etc. Utilizing\nGOAT-Bench, we delve into the ability of LMMs to accurately assess hatefulness,\nmisogyny, offensiveness, sarcasm, and harmful content. Our extensive\nexperiments across a range of LMMs reveal that current models still exhibit a\ndeficiency in safety awareness, showing insensitivity to various forms of\nimplicit abuse. We posit that this shortfall represents a critical impediment\nto the realization of safe artificial intelligence. The GOAT-Bench and\naccompanying resources are publicly accessible at https://goatlmm.github.io/,\ncontributing to ongoing research in this vital field.\n","authors":["Hongzhan Lin","Ziyang Luo","Bo Wang","Ruichao Yang","Jing Ma"],"pdf_url":"https://arxiv.org/pdf/2401.01523v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2401.03401v1","updated":"2024-01-07T07:13:50Z","published":"2024-01-07T07:13:50Z","title":"Empirical Study of Large Language Models as Automated Essay Scoring\n  Tools in English Composition__Taking TOEFL Independent Writing Task for\n  Example","summary":"  Large language models have demonstrated exceptional capabilities in tasks\ninvolving natural language generation, reasoning, and comprehension. This study\naims to construct prompts and comments grounded in the diverse scoring criteria\ndelineated within the official TOEFL guide. The primary objective is to assess\nthe capabilities and constraints of ChatGPT, a prominent representative of\nlarge language models, within the context of automated essay scoring. The\nprevailing methodologies for automated essay scoring involve the utilization of\ndeep neural networks, statistical machine learning techniques, and fine-tuning\npre-trained models. However, these techniques face challenges when applied to\ndifferent contexts or subjects, primarily due to their substantial data\nrequirements and limited adaptability to small sample sizes. In contrast, this\nstudy employs ChatGPT to conduct an automated evaluation of English essays,\neven with a small sample size, employing an experimental approach. The\nempirical findings indicate that ChatGPT can provide operational functionality\nfor automated essay scoring, although the results exhibit a regression effect.\nIt is imperative to underscore that the effective design and implementation of\nChatGPT prompts necessitate a profound domain expertise and technical\nproficiency, as these prompts are subject to specific threshold criteria.\nKeywords: ChatGPT, Automated Essay Scoring, Prompt Learning, TOEFL Independent\nWriting Task\n","authors":["Wei Xia","Shaoguang Mao","Chanjing Zheng"],"pdf_url":"https://arxiv.org/pdf/2401.03401v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12770v3","updated":"2024-01-07T07:05:37Z","published":"2023-08-24T13:17:35Z","title":"WavMark: Watermarking for Audio Generation","summary":"  Recent breakthroughs in zero-shot voice synthesis have enabled imitating a\nspeaker's voice using just a few seconds of recording while maintaining a high\nlevel of realism. Alongside its potential benefits, this powerful technology\nintroduces notable risks, including voice fraud and speaker impersonation.\nUnlike the conventional approach of solely relying on passive methods for\ndetecting synthetic data, watermarking presents a proactive and robust defence\nmechanism against these looming risks. This paper introduces an innovative\naudio watermarking framework that encodes up to 32 bits of watermark within a\nmere 1-second audio snippet. The watermark is imperceptible to human senses and\nexhibits strong resilience against various attacks. It can serve as an\neffective identifier for synthesized voices and holds potential for broader\napplications in audio copyright protection. Moreover, this framework boasts\nhigh flexibility, allowing for the combination of multiple watermark segments\nto achieve heightened robustness and expanded capacity. Utilizing 10 to\n20-second audio as the host, our approach demonstrates an average Bit Error\nRate (BER) of 0.48\\% across ten common attacks, a remarkable reduction of over\n2800\\% in BER compared to the state-of-the-art watermarking tool. See\nhttps://aka.ms/wavmark for demos of our work.\n","authors":["Guangyu Chen","Yu Wu","Shujie Liu","Tao Liu","Xiaoyong Du","Furu Wei"],"pdf_url":"https://arxiv.org/pdf/2308.12770v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03388v1","updated":"2024-01-07T04:46:23Z","published":"2024-01-07T04:46:23Z","title":"LLMs for Robotic Object Disambiguation","summary":"  The advantages of pre-trained large language models (LLMs) are apparent in a\nvariety of language processing tasks. But can a language model's knowledge be\nfurther harnessed to effectively disambiguate objects and navigate\ndecision-making challenges within the realm of robotics? Our study reveals the\nLLM's aptitude for solving complex decision making challenges that are often\npreviously modeled by Partially Observable Markov Decision Processes (POMDPs).\nA pivotal focus of our research is the object disambiguation capability of\nLLMs. We detail the integration of an LLM into a tabletop environment\ndisambiguation task, a decision making problem where the robot's task is to\ndiscern and retrieve a user's desired object from an arbitrarily large and\ncomplex cluster of objects. Despite multiple query attempts with zero-shot\nprompt engineering (details can be found in the Appendix), the LLM struggled to\ninquire about features not explicitly provided in the scene description. In\nresponse, we have developed a few-shot prompt engineering system to improve the\nLLM's ability to pose disambiguating queries. The result is a model capable of\nboth using given features when they are available and inferring new relevant\nfeatures when necessary, to successfully generate and navigate down a precise\ndecision tree to the correct object--even when faced with identical options.\n","authors":["Connie Jiang","Yiqing Xu","David Hsu"],"pdf_url":"https://arxiv.org/pdf/2401.03388v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03385v1","updated":"2024-01-07T04:32:29Z","published":"2024-01-07T04:32:29Z","title":"Grimoire is All You Need for Enhancing Large Language Models","summary":"  In-context learning (ICL) is one of the key methods for enhancing the\nperformance of large language models on specific tasks by providing a set of\nfew-shot question and answer examples. However, the ICL capability of different\ntypes of models shows significant variation due to factors such as model\narchitecture, volume of learning data, and the size of parameters. Generally,\nthe larger the model's parameter size and the more extensive the learning data,\nthe stronger its ICL capability. In this paper, we propose a method SLEICL\n(Strong LLM Enhanced ICL) that involves learning from examples using strong\nlanguage models and then summarizing and transferring these learned skills to\nweak language models for inference and application. This ensures the stability\nand effectiveness of ICL. Compared to directly enabling weak language models to\nlearn from prompt examples, SLEICL reduces the difficulty of ICL for these\nmodels. Our experiments, conducted on up to eight datasets with five language\nmodels, demonstrate that weak language models achieve consistent improvement\nover their own zero-shot or few-shot capabilities using the SLEICL method. Some\nweak language models even surpass the performance of GPT4-1106-preview\n(zero-shot) with the aid of SLEICL.\n","authors":["Ding Chen","Shichao Song","Qingchen Yu","Zhiyu Li","Wenjin Wang","Feiyu Xiong","Bo Tang"],"pdf_url":"https://arxiv.org/pdf/2401.03385v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2401.03346v1","updated":"2024-01-07T00:39:33Z","published":"2024-01-07T00:39:33Z","title":"An Investigation of Large Language Models for Real-World Hate Speech\n  Detection","summary":"  Hate speech has emerged as a major problem plaguing our social spaces today.\nWhile there have been significant efforts to address this problem, existing\nmethods are still significantly limited in effectively detecting hate speech\nonline. A major limitation of existing methods is that hate speech detection is\na highly contextual problem, and these methods cannot fully capture the context\nof hate speech to make accurate predictions. Recently, large language models\n(LLMs) have demonstrated state-of-the-art performance in several natural\nlanguage tasks. LLMs have undergone extensive training using vast amounts of\nnatural language data, enabling them to grasp intricate contextual details.\nHence, they could be used as knowledge bases for context-aware hate speech\ndetection. However, a fundamental problem with using LLMs to detect hate speech\nis that there are no studies on effectively prompting LLMs for context-aware\nhate speech detection. In this study, we conduct a large-scale study of hate\nspeech detection, employing five established hate speech datasets. We discover\nthat LLMs not only match but often surpass the performance of current benchmark\nmachine learning models in identifying hate speech. By proposing four diverse\nprompting strategies that optimize the use of LLMs in detecting hate speech.\nOur study reveals that a meticulously crafted reasoning prompt can effectively\ncapture the context of hate speech by fully utilizing the knowledge base in\nLLMs, significantly outperforming existing techniques. Furthermore, although\nLLMs can provide a rich knowledge base for the contextual detection of hate\nspeech, suitable prompting strategies play a crucial role in effectively\nleveraging this knowledge base for efficient detection.\n","authors":["Keyan Guo","Alexander Hu","Jaden Mu","Ziheng Shi","Ziming Zhao","Nishant Vishwamitra","Hongxin Hu"],"pdf_url":"https://arxiv.org/pdf/2401.03346v1.pdf","comment":"Accepted for publication on 22nd International Conference of Machine\n  Learning and Applications, ICMLA 2023"},{"id":"http://arxiv.org/abs/2206.15462v4","updated":"2024-01-07T00:24:21Z","published":"2022-06-30T17:55:12Z","title":"Improving Visual Grounding by Encouraging Consistent Gradient-based\n  Explanations","summary":"  We propose a margin-based loss for tuning joint vision-language models so\nthat their gradient-based explanations are consistent with region-level\nannotations provided by humans for relatively smaller grounding datasets. We\nrefer to this objective as Attention Mask Consistency (AMC) and demonstrate\nthat it produces superior visual grounding results than previous methods that\nrely on using vision-language models to score the outputs of object detectors.\nParticularly, a model trained with AMC on top of standard vision-language\nmodeling objectives obtains a state-of-the-art accuracy of 86.49% in the\nFlickr30k visual grounding benchmark, an absolute improvement of 5.38% when\ncompared to the best previous model trained under the same level of\nsupervision. Our approach also performs exceedingly well on established\nbenchmarks for referring expression comprehension where it obtains 80.34%\naccuracy in the easy test of RefCOCO+, and 64.55% in the difficult split. AMC\nis effective, easy to implement, and is general as it can be adopted by any\nvision-language model, and can use any type of region annotations.\n","authors":["Ziyan Yang","Kushal Kafle","Franck Dernoncourt","Vicente Ordonez"],"pdf_url":"https://arxiv.org/pdf/2206.15462v4.pdf","comment":"CVPR 2023. Fix ReferIt results. Code:\n  https://github.com/uvavision/AMC-grounding Project Webpage:\n  https://vislang.ai/amc"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2401.03604v1","updated":"2024-01-07T23:13:51Z","published":"2024-01-07T23:13:51Z","title":"Amirkabir campus dataset: Real-world challenges and scenarios of Visual\n  Inertial Odometry (VIO) for visually impaired people","summary":"  Visual Inertial Odometry (VIO) algorithms estimate the accurate camera\ntrajectory by using camera and Inertial Measurement Unit (IMU) sensors. The\napplications of VIO span a diverse range, including augmented reality and\nindoor navigation. VIO algorithms hold the potential to facilitate navigation\nfor visually impaired individuals in both indoor and outdoor settings.\nNevertheless, state-of-the-art VIO algorithms encounter substantial challenges\nin dynamic environments, particularly in densely populated corridors. Existing\nVIO datasets, e.g., ADVIO, typically fail to effectively exploit these\nchallenges. In this paper, we introduce the Amirkabir campus dataset (AUT-VI)\nto address the mentioned problem and improve the navigation systems. AUT-VI is\na novel and super-challenging dataset with 126 diverse sequences in 17\ndifferent locations. This dataset contains dynamic objects, challenging\nloop-closure/map-reuse, different lighting conditions, reflections, and sudden\ncamera movements to cover all extreme navigation scenarios. Moreover, in\nsupport of ongoing development efforts, we have released the Android\napplication for data capture to the public. This allows fellow researchers to\neasily capture their customized VIO dataset variations. In addition, we\nevaluate state-of-the-art Visual Inertial Odometry (VIO) and Visual Odometry\n(VO) methods on our dataset, emphasizing the essential need for this\nchallenging dataset.\n","authors":["Ali Samadzadeh","Mohammad Hassan Mojab","Heydar Soudani","Seyed Hesamoddin Mireshghollah","Ahmad Nickabadi"],"pdf_url":"https://arxiv.org/pdf/2401.03604v1.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2401.03587v1","updated":"2024-01-07T21:50:24Z","published":"2024-01-07T21:50:24Z","title":"Big Data and Deep Learning in Smart Cities: A Comprehensive Dataset for\n  AI-Driven Traffic Accident Detection and Computer Vision Systems","summary":"  In the dynamic urban landscape, where the interplay of vehicles and\npedestrians defines the rhythm of life, integrating advanced technology for\nsafety and efficiency is increasingly crucial. This study delves into the\napplication of cutting-edge technological methods in smart cities, focusing on\nenhancing public safety through improved traffic accident detection. Action\nrecognition plays a pivotal role in interpreting visual data and tracking\nobject motion such as human pose estimation in video sequences. The challenges\nof action recognition include variability in rapid actions, limited dataset,\nand environmental factors such as (Weather, Illumination, and Occlusions). In\nthis paper, we present a novel comprehensive dataset for traffic accident\ndetection. This datasets is specifically designed to bolster computer vision\nand action recognition systems in predicting and detecting road traffic\naccidents. We integrated datasets from wide variety of data sources, road\nnetworks, weather conditions, and regions across the globe. This approach is\nunderpinned by empirical studies, aiming to contribute to the discourse on how\ntechnology can enhance the quality of life in densely populated areas. This\nresearch aims to bridge existing research gaps by introducing benchmark\ndatasets that leverage state-of-the-art algorithms tailored for traffic\naccident detection in smart cities. These dataset is expected to advance\nacademic research and also enhance real-time accident detection applications,\ncontributing significantly to the evolution of smart urban environments. Our\nstudy marks a pivotal step towards safer, more efficient smart cities,\nharnessing the power of AI and machine learning to transform urban living.\n","authors":["Victor Adewopo","Nelly Elsayed","Zag Elsayed","Murat Ozer","Constantinos Zekios","Ahmed Abdelgawad","Magdy Bayoumi"],"pdf_url":"https://arxiv.org/pdf/2401.03587v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03582v1","updated":"2024-01-07T21:22:42Z","published":"2024-01-07T21:22:42Z","title":"Invisible Reflections: Leveraging Infrared Laser Reflections to Target\n  Traffic Sign Perception","summary":"  All vehicles must follow the rules that govern traffic behavior, regardless\nof whether the vehicles are human-driven or Connected Autonomous Vehicles\n(CAVs). Road signs indicate locally active rules, such as speed limits and\nrequirements to yield or stop. Recent research has demonstrated attacks, such\nas adding stickers or projected colored patches to signs, that cause CAV\nmisinterpretation, resulting in potential safety issues. Humans can see and\npotentially defend against these attacks. But humans can not detect what they\ncan not observe. We have developed an effective physical-world attack that\nleverages the sensitivity of filterless image sensors and the properties of\nInfrared Laser Reflections (ILRs), which are invisible to humans. The attack is\ndesigned to affect CAV cameras and perception, undermining traffic sign\nrecognition by inducing misclassification. In this work, we formulate the\nthreat model and requirements for an ILR-based traffic sign perception attack\nto succeed. We evaluate the effectiveness of the ILR attack with real-world\nexperiments against two major traffic sign recognition architectures on four\nIR-sensitive cameras. Our black-box optimization methodology allows the attack\nto achieve up to a 100% attack success rate in indoor, static scenarios and a\n>80.5% attack success rate in our outdoor, moving vehicle scenarios. We find\nthe latest state-of-the-art certifiable defense is ineffective against ILR\nattacks as it mis-certifies >33.5% of cases. To address this, we propose a\ndetection strategy based on the physical properties of IR laser reflections\nwhich can detect 96% of ILR attacks.\n","authors":["Takami Sato","Sri Hrushikesh Varma Bhupathiraju","Michael Clifford","Takeshi Sugawara","Qi Alfred Chen","Sara Rampazzi"],"pdf_url":"https://arxiv.org/pdf/2401.03582v1.pdf","comment":"The first two authors are co-first. Accepted to NDSS '24"},{"id":"http://arxiv.org/abs/2401.03575v1","updated":"2024-01-07T20:08:17Z","published":"2024-01-07T20:08:17Z","title":"Involution Fused ConvNet for Classifying Eye-Tracking Patterns of\n  Children with Autism Spectrum Disorder","summary":"  Autism Spectrum Disorder (ASD) is a complicated neurological condition which\nis challenging to diagnose. Numerous studies demonstrate that children\ndiagnosed with autism struggle with maintaining attention spans and have less\nfocused vision. The eye-tracking technology has drawn special attention in the\ncontext of ASD since anomalies in gaze have long been acknowledged as a\ndefining feature of autism in general. Deep Learning (DL) approaches coupled\nwith eye-tracking sensors are exploiting additional capabilities to advance the\ndiagnostic and its applications. By learning intricate nonlinear input-output\nrelations, DL can accurately recognize the various gaze and eye-tracking\npatterns and adjust to the data. Convolutions alone are insufficient to capture\nthe important spatial information in gaze patterns or eye tracking. The dynamic\nkernel-based process known as involutions can improve the efficiency of\nclassifying gaze patterns or eye tracking data. In this paper, we utilise two\ndifferent image-processing operations to see how these processes learn\neye-tracking patterns. Since these patterns are primarily based on spatial\ninformation, we use involution with convolution making it a hybrid, which adds\nlocation-specific capability to a deep learning model. Our proposed model is\nimplemented in a simple yet effective approach, which makes it easier for\napplying in real life. We investigate the reasons why our approach works well\nfor classifying eye-tracking patterns. For comparative analysis, we experiment\nwith two separate datasets as well as a combined version of both. The results\nshow that IC with three involution layers outperforms the previous approaches.\n","authors":["Md. Farhadul Islam","Meem Arafat Manab","Joyanta Jyoti Mondal","Sarah Zabeen","Fardin Bin Rahman","Md. Zahidul Hasan","Farig Sadeque","Jannatun Noor"],"pdf_url":"https://arxiv.org/pdf/2401.03575v1.pdf","comment":"17 pages, 13 figures, Submitted to Engineering Applications of\n  Artificial Intelligence"},{"id":"http://arxiv.org/abs/2307.08919v2","updated":"2024-01-07T19:07:33Z","published":"2023-07-18T01:31:47Z","title":"Systematic comparison of semi-supervised and self-supervised learning\n  for medical image classification","summary":"  In many medical image classification problems, labeled data is scarce while\nunlabeled data is more available. Semi-supervised learning and self-supervised\nlearning are two different research directions that can improve accuracy by\nlearning from extra unlabeled data. Recent methods from both directions have\nreported significant gains on traditional benchmarks. Yet past benchmarks do\nnot focus on medical tasks and rarely compare self- and semi- methods together\non equal footing. Furthermore, past benchmarks often handle hyperparameter\ntuning suboptimally. First, they may not tune hyperparameters at all, leading\nto underfitting. Second, when tuning does occur, it often unrealistically uses\na labeled validation set much larger than the train set. Both cases make\npreviously published rankings of methods difficult to translate to practical\nsettings. This study contributes a systematic evaluation of self- and semi-\nmethods with a unified experimental protocol intended to guide a practitioner\nwith scarce overall labeled data and a limited compute budget. We answer two\nkey questions: Can hyperparameter tuning be effective with realistic-sized\nvalidation sets? If so, when all methods are tuned well, which self- or\nsemi-supervised methods reach the best accuracy? Our study compares 13\nrepresentative semi- and self-supervised methods to strong labeled-set-only\nbaselines on 4 medical datasets. From 20000+ total GPU hours of computation, we\nprovide valuable best practices to resource-constrained, results-focused\npractitioners.\n","authors":["Zhe Huang","Ruijie Jiang","Shuchin Aeron","Michael C. Hughes"],"pdf_url":"https://arxiv.org/pdf/2307.08919v2.pdf","comment":"Semi-supervised Learning; Self-supervised Learning; Medical Imaging"},{"id":"http://arxiv.org/abs/2401.03540v1","updated":"2024-01-07T16:52:49Z","published":"2024-01-07T16:52:49Z","title":"SeTformer is What You Need for Vision and Language","summary":"  The dot product self-attention (DPSA) is a fundamental component of\ntransformers. However, scaling them to long sequences, like documents or\nhigh-resolution images, becomes prohibitively expensive due to quadratic time\nand memory complexities arising from the softmax operation. Kernel methods are\nemployed to simplify computations by approximating softmax but often lead to\nperformance drops compared to softmax attention. We propose SeTformer, a novel\ntransformer, where DPSA is purely replaced by Self-optimal Transport (SeT) for\nachieving better performance and computational efficiency. SeT is based on two\nessential softmax properties: maintaining a non-negative attention matrix and\nusing a nonlinear reweighting mechanism to emphasize important tokens in input\nsequences. By introducing a kernel cost function for optimal transport,\nSeTformer effectively satisfies these properties. In particular, with small and\nbasesized models, SeTformer achieves impressive top-1 accuracies of 84.7% and\n86.2% on ImageNet-1K. In object detection, SeTformer-base outperforms the\nFocalNet counterpart by +2.2 mAP, using 38% fewer parameters and 29% fewer\nFLOPs. In semantic segmentation, our base-size model surpasses NAT by +3.5 mIoU\nwith 33% fewer parameters. SeTformer also achieves state-of-the-art results in\nlanguage modeling on the GLUE benchmark. These findings highlight SeTformer's\napplicability in vision and language tasks.\n","authors":["Pourya Shamsolmoali","Masoumeh Zareapoor","Eric Granger","Michael Felsberg"],"pdf_url":"https://arxiv.org/pdf/2401.03540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03522v1","updated":"2024-01-07T15:47:19Z","published":"2024-01-07T15:47:19Z","title":"Text-Driven Traffic Anomaly Detection with Temporal High-Frequency\n  Modeling in Driving Videos","summary":"  Traffic anomaly detection (TAD) in driving videos is critical for ensuring\nthe safety of autonomous driving and advanced driver assistance systems.\nPrevious single-stage TAD methods primarily rely on frame prediction, making\nthem vulnerable to interference from dynamic backgrounds induced by the rapid\nmovement of the dashboard camera. While two-stage TAD methods appear to be a\nnatural solution to mitigate such interference by pre-extracting\nbackground-independent features (such as bounding boxes and optical flow) using\nperceptual algorithms, they are susceptible to the performance of first-stage\nperceptual algorithms and may result in error propagation. In this paper, we\nintroduce TTHF, a novel single-stage method aligning video clips with text\nprompts, offering a new perspective on traffic anomaly detection. Unlike\nprevious approaches, the supervised signal of our method is derived from\nlanguages rather than orthogonal one-hot vectors, providing a more\ncomprehensive representation. Further, concerning visual representation, we\npropose to model the high frequency of driving videos in the temporal domain.\nThis modeling captures the dynamic changes of driving scenes, enhances the\nperception of driving behavior, and significantly improves the detection of\ntraffic anomalies. In addition, to better perceive various types of traffic\nanomalies, we carefully design an attentive anomaly focusing mechanism that\nvisually and linguistically guides the model to adaptively focus on the visual\ncontext of interest, thereby facilitating the detection of traffic anomalies.\nIt is shown that our proposed TTHF achieves promising performance,\noutperforming state-of-the-art competitors by +5.4% AUC on the DoTA dataset and\nachieving high generalization on the DADA dataset.\n","authors":["Rongqin Liang","Yuanman Li","Jiantao Zhou","Xia Li"],"pdf_url":"https://arxiv.org/pdf/2401.03522v1.pdf","comment":"12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2304.00501v6","updated":"2024-01-07T15:38:42Z","published":"2023-04-02T10:27:34Z","title":"A Comprehensive Review of YOLO Architectures in Computer Vision: From\n  YOLOv1 to YOLOv8 and YOLO-NAS","summary":"  YOLO has become a central real-time object detection system for robotics,\ndriverless cars, and video monitoring applications. We present a comprehensive\nanalysis of YOLO's evolution, examining the innovations and contributions in\neach iteration from the original YOLO up to YOLOv8, YOLO-NAS, and YOLO with\nTransformers. We start by describing the standard metrics and postprocessing;\nthen, we discuss the major changes in network architecture and training tricks\nfor each model. Finally, we summarize the essential lessons from YOLO's\ndevelopment and provide a perspective on its future, highlighting potential\nresearch directions to enhance real-time object detection systems.\n","authors":["Juan Terven","Diana Cordova-Esparza"],"pdf_url":"https://arxiv.org/pdf/2304.00501v6.pdf","comment":"36 pages, 21 figures, 4 tables, published in Machine Learning and\n  Knowledge Extraction. This version contains the last changes made to the\n  published version"},{"id":"http://arxiv.org/abs/2302.10630v2","updated":"2024-01-07T15:18:37Z","published":"2023-02-21T12:43:42Z","title":"LIT-Former: Linking In-plane and Through-plane Transformers for\n  Simultaneous CT Image Denoising and Deblurring","summary":"  This paper studies 3D low-dose computed tomography (CT) imaging. Although\nvarious deep learning methods were developed in this context, typically they\nfocus on 2D images and perform denoising due to low-dose and deblurring for\nsuper-resolution separately. Up to date, little work was done for simultaneous\nin-plane denoising and through-plane deblurring, which is important to obtain\nhigh-quality 3D CT images with lower radiation and faster imaging speed. For\nthis task, a straightforward method is to directly train an end-to-end 3D\nnetwork. However, it demands much more training data and expensive\ncomputational costs. Here, we propose to link in-plane and through-plane\ntransformers for simultaneous in-plane denoising and through-plane deblurring,\ntermed as LIT-Former, which can efficiently synergize in-plane and\nthrough-plane sub-tasks for 3D CT imaging and enjoy the advantages of both\nconvolution and transformer networks. LIT-Former has two novel designs:\nefficient multi-head self-attention modules (eMSM) and efficient convolutional\nfeedforward networks (eCFN). First, eMSM integrates in-plane 2D self-attention\nand through-plane 1D self-attention to efficiently capture global interactions\nof 3D self-attention, the core unit of transformer networks. Second, eCFN\nintegrates 2D convolution and 1D convolution to extract local information of 3D\nconvolution in the same fashion. As a result, the proposed LIT-Former synergize\nthese two subtasks, significantly reducing the computational complexity as\ncompared to 3D counterparts and enabling rapid convergence. Extensive\nexperimental results on simulated and clinical datasets demonstrate superior\nperformance over state-of-the-art models. The source code is made available at\nhttps://github.com/hao1635/LIT-Former.\n","authors":["Zhihao Chen","Chuang Niu","Qi Gao","Ge Wang","Hongming Shan"],"pdf_url":"https://arxiv.org/pdf/2302.10630v2.pdf","comment":"15 pages, 12 figures"},{"id":"http://arxiv.org/abs/2312.12478v2","updated":"2024-01-07T15:05:38Z","published":"2023-12-19T14:39:11Z","title":"ProS: Prompting-to-simulate Generalized knowledge for Universal\n  Cross-Domain Retrieval","summary":"  The goal of Universal Cross-Domain Retrieval (UCDR) is to achieve robust\nperformance in generalized test scenarios, wherein data may belong to strictly\nunknown domains and categories during training. Recently, pre-trained models\nwith prompt tuning have shown strong generalization capabilities and attained\nnoteworthy achievements in various downstream tasks, such as few-shot learning\nand video-text retrieval. However, applying them directly to UCDR may not\nsufficiently to handle both domain shift (i.e., adapting to unfamiliar domains)\nand semantic shift (i.e., transferring to unknown categories). To this end, we\npropose Prompting-to-Simulate (ProS), the first method to apply prompt tuning\nfor UCDR. ProS employs a two-step process to simulate Content-aware Dynamic\nPrompts (CaDP) which can impact models to produce generalized features for\nUCDR. Concretely, in Prompt Units Learning stage, we introduce two Prompt Units\nto individually capture domain and semantic knowledge in a mask-and-align way.\nThen, in Context-aware Simulator Learning stage, we train a Content-aware\nPrompt Simulator under a simulated test scenarios to produce the corresponding\nCaDP. Extensive experiments conducted on three benchmark datasets show that our\nmethod achieves new state-of-the-art performance without bringing excessive\nparameters. Our method is publicly available at\nhttps://anonymous.4open.science/r/ProS\n","authors":["Kaipeng Fang","Jingkuan Song","Lianli Gao","Pengpeng Zeng","Zhi-Qi Cheng","Xiyao Li","Heng Tao Shen"],"pdf_url":"https://arxiv.org/pdf/2312.12478v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12075v2","updated":"2024-01-07T15:04:31Z","published":"2023-11-20T02:21:49Z","title":"BadCLIP: Dual-Embedding Guided Backdoor Attack on Multimodal Contrastive\n  Learning","summary":"  Studying backdoor attacks is valuable for model copyright protection and\nenhancing defenses. While existing backdoor attacks have successfully infected\nmultimodal contrastive learning models such as CLIP, they can be easily\ncountered by specialized backdoor defenses for MCL models. This paper reveals\nthe threats in this practical scenario that backdoor attacks can remain\neffective even after defenses and introduces the \\emph{\\toolns} attack, which\nis resistant to backdoor detection and model fine-tuning defenses. To achieve\nthis, we draw motivations from the perspective of the Bayesian rule and propose\na dual-embedding guided framework for backdoor attacks. Specifically, we ensure\nthat visual trigger patterns approximate the textual target semantics in the\nembedding space, making it challenging to detect the subtle parameter\nvariations induced by backdoor learning on such natural trigger patterns.\nAdditionally, we optimize the visual trigger patterns to align the poisoned\nsamples with target vision features in order to hinder the backdoor unlearning\nthrough clean fine-tuning. Extensive experiments demonstrate that our attack\nsignificantly outperforms state-of-the-art baselines (+45.3% ASR) in the\npresence of SoTA backdoor defenses, rendering these mitigation and detection\nstrategies virtually ineffective. Furthermore, our approach effectively attacks\nsome more rigorous scenarios like downstream tasks. We believe that this paper\nraises awareness regarding the potential threats associated with the practical\napplication of multimodal contrastive learning and encourages the development\nof more robust defense mechanisms.\n","authors":["Siyuan Liang","Mingli Zhu","Aishan Liu","Baoyuan Wu","Xiaochun Cao","Ee-Chien Chang"],"pdf_url":"https://arxiv.org/pdf/2311.12075v2.pdf","comment":"The paper lacks some work that needs to be cited"},{"id":"http://arxiv.org/abs/2301.00970v3","updated":"2024-01-07T15:02:26Z","published":"2023-01-03T06:47:31Z","title":"Benchmarking the Robustness of LiDAR Semantic Segmentation Models","summary":"  When using LiDAR semantic segmentation models for safety-critical\napplications such as autonomous driving, it is essential to understand and\nimprove their robustness with respect to a large range of LiDAR corruptions. In\nthis paper, we aim to comprehensively analyze the robustness of LiDAR semantic\nsegmentation models under various corruptions. To rigorously evaluate the\nrobustness and generalizability of current approaches, we propose a new\nbenchmark called SemanticKITTI-C, which features 16 out-of-domain LiDAR\ncorruptions in three groups, namely adverse weather, measurement noise and\ncross-device discrepancy. Then, we systematically investigate 11 LiDAR semantic\nsegmentation models, especially spanning different input representations (e.g.,\npoint clouds, voxels, projected images, and etc.), network architectures and\ntraining schemes. Through this study, we obtain two insights: 1) We find out\nthat the input representation plays a crucial role in robustness. Specifically,\nunder specific corruptions, different representations perform variously. 2)\nAlthough state-of-the-art methods on LiDAR semantic segmentation achieve\npromising results on clean data, they are less robust when dealing with noisy\ndata. Finally, based on the above observations, we design a robust LiDAR\nsegmentation model (RLSeg) which greatly boosts the robustness with simple but\neffective modifications. It is promising that our benchmark, comprehensive\nanalysis, and observations can boost future research in robust LiDAR semantic\nsegmentation for safety-critical applications.\n","authors":["Xu Yan","Chaoda Zheng","Ying Xue","Zhen Li","Shuguang Cui","Dengxin Dai"],"pdf_url":"https://arxiv.org/pdf/2301.00970v3.pdf","comment":"IJCV-2024. The benchmark will be made available at\n  https://yanx27.github.io/RobustLidarSeg/"},{"id":"http://arxiv.org/abs/2401.03499v1","updated":"2024-01-07T14:34:34Z","published":"2024-01-07T14:34:34Z","title":"Re:Draw -- Context Aware Translation as a Controllable Method for\n  Artistic Production","summary":"  We introduce context-aware translation, a novel method that combines the\nbenefits of inpainting and image-to-image translation, respecting\nsimultaneously the original input and contextual relevance -- where existing\nmethods fall short. By doing so, our method opens new avenues for the\ncontrollable use of AI within artistic creation, from animation to digital art.\n  As an use case, we apply our method to redraw any hand-drawn animated\ncharacter eyes based on any design specifications - eyes serve as a focal point\nthat captures viewer attention and conveys a range of emotions, however, the\nlabor-intensive nature of traditional animation often leads to compromises in\nthe complexity and consistency of eye design. Furthermore, we remove the need\nfor production data for training and introduce a new character recognition\nmethod that surpasses existing work by not requiring fine-tuning to specific\nproductions. This proposed use case could help maintain consistency throughout\nproduction and unlock bolder and more detailed design choices without the\nproduction cost drawbacks. A user study shows context-aware translation is\npreferred over existing work 95.16% of the time.\n","authors":["Joao Liborio Cardoso","Francesco Banterle","Paolo Cignoni","Michael Wimmer"],"pdf_url":"https://arxiv.org/pdf/2401.03499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03495v1","updated":"2024-01-07T14:25:42Z","published":"2024-01-07T14:25:42Z","title":"Segment Anything Model for Medical Image Segmentation: Current\n  Applications and Future Directions","summary":"  Due to the inherent flexibility of prompting, foundation models have emerged\nas the predominant force in the fields of natural language processing and\ncomputer vision. The recent introduction of the Segment Anything Model (SAM)\nsignifies a noteworthy expansion of the prompt-driven paradigm into the domain\nof image segmentation, thereby introducing a plethora of previously unexplored\ncapabilities. However, the viability of its application to medical image\nsegmentation remains uncertain, given the substantial distinctions between\nnatural and medical images. In this work, we provide a comprehensive overview\nof recent endeavors aimed at extending the efficacy of SAM to medical image\nsegmentation tasks, encompassing both empirical benchmarking and methodological\nadaptations. Additionally, we explore potential avenues for future research\ndirections in SAM's role within medical image segmentation. While direct\napplication of SAM to medical image segmentation does not yield satisfactory\nperformance on multi-modal and multi-target medical datasets so far, numerous\ninsights gleaned from these efforts serve as valuable guidance for shaping the\ntrajectory of foundational models in the realm of medical image analysis. To\nsupport ongoing research endeavors, we maintain an active repository that\ncontains an up-to-date paper list and a succinct summary of open-source\nprojects at https://github.com/YichiZhang98/SAM4MIS.\n","authors":["Yichi Zhang","Zhenrong Shen","Rushi Jiao"],"pdf_url":"https://arxiv.org/pdf/2401.03495v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.03374v3","updated":"2024-01-07T14:05:41Z","published":"2023-06-06T03:25:09Z","title":"PGformer: Proxy-Bridged Game Transformer for Multi-Person Highly\n  Interactive Extreme Motion Prediction","summary":"  Multi-person motion prediction is a challenging task, especially for\nreal-world scenarios of highly interacted persons. Most previous works have\nbeen devoted to studying the case of weak interactions (e.g., walking\ntogether), in which typically forecasting each human pose in isolation can\nstill achieve good performances. This paper focuses on collaborative motion\nprediction for multiple persons with extreme motions and attempts to explore\nthe relationships between the highly interactive persons' pose trajectories.\nSpecifically, a novel cross-query attention (XQA) module is proposed to\nbilaterally learn the cross-dependencies between the two pose sequences\ntailored for this situation. A proxy unit is additionally introduced to bridge\nthe involved persons, which cooperates with our proposed XQA module and subtly\ncontrols the bidirectional spatial information flows. These designs are then\nintegrated into a Transformer-based architecture and the resulting model is\ncalled Proxy-bridged Game Transformer (PGformer) for multi-person interactive\nmotion prediction. Its effectiveness has been evaluated on the challenging ExPI\ndataset, which involves highly interactive actions. Our PGformer consistently\noutperforms the state-of-the-art methods in both short- and long-term\npredictions by a large margin. Besides, our approach can also be compatible\nwith the weakly interacted CMU-Mocap and MuPoTS-3D datasets and extended to the\ncase of more than 2 individuals with encouraging results.\n","authors":["Yanwen Fang","Jintai Chen","Peng-Tao Jiang","Chao Li","Yifeng Geng","Eddy K. F. Lam","Guodong Li"],"pdf_url":"https://arxiv.org/pdf/2306.03374v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.09272v2","updated":"2024-01-07T13:15:02Z","published":"2023-09-17T13:40:15Z","title":"Deep Neighbor Layer Aggregation for Lightweight Self-Supervised\n  Monocular Depth Estimation","summary":"  With the frequent use of self-supervised monocular depth estimation in\nrobotics and autonomous driving, the model's efficiency is becoming\nincreasingly important. Most current approaches apply much larger and more\ncomplex networks to improve the precision of depth estimation. Some researchers\nincorporated Transformer into self-supervised monocular depth estimation to\nachieve better performance. However, this method leads to high parameters and\nhigh computation. We present a fully convolutional depth estimation network\nusing contextual feature fusion. Compared to UNet++ and HRNet, we use\nhigh-resolution and low-resolution features to reserve information on small\ntargets and fast-moving objects instead of long-range fusion. We further\npromote depth estimation results employing lightweight channel attention based\non convolution in the decoder stage. Our method reduces the parameters without\nsacrificing accuracy. Experiments on the KITTI benchmark show that our method\ncan get better results than many large models, such as Monodepth2, with only 30\nparameters. The source code is available at\nhttps://github.com/boyagesmile/DNA-Depth.\n","authors":["Wang Boya","Wang Shuo","Ye Dong","Dou Ziwen"],"pdf_url":"https://arxiv.org/pdf/2309.09272v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03470v1","updated":"2024-01-07T12:34:45Z","published":"2024-01-07T12:34:45Z","title":"FurniScene: A Large-scale 3D Room Dataset with Intricate Furnishing\n  Scenes","summary":"  Indoor scene generation has attracted significant attention recently as it is\ncrucial for applications of gaming, virtual reality, and interior design.\nCurrent indoor scene generation methods can produce reasonable room layouts but\noften lack diversity and realism. This is primarily due to the limited coverage\nof existing datasets, including only large furniture without tiny furnishings\nin daily life. To address these challenges, we propose FurniScene, a\nlarge-scale 3D room dataset with intricate furnishing scenes from interior\ndesign professionals. Specifically, the FurniScene consists of 11,698 rooms and\n39,691 unique furniture CAD models with 89 different types, covering things\nfrom large beds to small teacups on the coffee table. To better suit\nfine-grained indoor scene layout generation, we introduce a novel Two-Stage\nDiffusion Scene Model (TSDSM) and conduct an evaluation benchmark for various\nindoor scene generation based on FurniScene. Quantitative and qualitative\nevaluations demonstrate the capability of our method to generate highly\nrealistic indoor scenes. Our dataset and code will be publicly available soon.\n","authors":["Genghao Zhang","Yuxi Wang","Chuanchen Luo","Shibiao Xu","Junran Peng","Zhaoxiang Zhang","Man Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.03470v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03459v1","updated":"2024-01-07T11:38:15Z","published":"2024-01-07T11:38:15Z","title":"BCLNet: Bilateral Consensus Learning for Two-View Correspondence Pruning","summary":"  Correspondence pruning aims to establish reliable correspondences between two\nrelated images and recover relative camera motion. Existing approaches often\nemploy a progressive strategy to handle the local and global contexts, with a\nprominent emphasis on transitioning from local to global, resulting in the\nneglect of interactions between different contexts. To tackle this issue, we\npropose a parallel context learning strategy that involves acquiring bilateral\nconsensus for the two-view correspondence pruning task. In our approach, we\ndesign a distinctive self-attention block to capture global context and\nparallel process it with the established local context learning module, which\nenables us to simultaneously capture both local and global consensuses. By\ncombining these local and global consensuses, we derive the required bilateral\nconsensus. We also design a recalibration block, reducing the influence of\nerroneous consensus information and enhancing the robustness of the model. The\nculmination of our efforts is the Bilateral Consensus Learning Network\n(BCLNet), which efficiently estimates camera pose and identifies inliers (true\ncorrespondences). Extensive experiments results demonstrate that our network\nnot only surpasses state-of-the-art methods on benchmark datasets but also\nshowcases robust generalization abilities across various feature extraction\ntechniques. Noteworthily, BCLNet obtains 3.98\\% mAP5$^{\\circ}$ gains over the\nsecond best method on unknown outdoor dataset, and obviously accelerates model\ntraining speed. The source code will be available at:\nhttps://github.com/guobaoxiao/BCLNet.\n","authors":["Xiangyang Miao","Guobao Xiao","Shiping Wang","Jun Yu"],"pdf_url":"https://arxiv.org/pdf/2401.03459v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03450v1","updated":"2024-01-07T11:11:03Z","published":"2024-01-07T11:11:03Z","title":"A Classification of Critical Configurations for any Number of Projective\n  Views","summary":"  Structure from motion is the process of recovering information about cameras\nand 3D scene from a set of images. Generally, in a noise-free setting, all\ninformation can be uniquely recovered if enough images and image points are\nprovided. There are, however, certain cases where unique recovery is\nimpossible, even in theory; these are called critical configurations. We use a\nrecently developed algebraic approach to classify all critical configurations\nfor any number of projective cameras. We show that they form well-known\nalgebraic varieties, such as quadric surfaces and curves of degree at most 4.\nThis paper also improves upon earlier results both by finding previously\nunknown critical configurations and by showing that some configurations\npreviously believed to be critical are in fact not.\n","authors":["Martin Bråtelund"],"pdf_url":"https://arxiv.org/pdf/2401.03450v1.pdf","comment":"44 pages, 10 figures, submitted to International Journal of Computer\n  Vision. arXiv admin note: text overlap with arXiv:2112.05478,\n  arXiv:2112.05074"},{"id":"http://arxiv.org/abs/2312.14924v3","updated":"2024-01-07T10:18:53Z","published":"2023-12-22T18:56:35Z","title":"Training Convolutional Neural Networks with the Forward-Forward\n  algorithm","summary":"  The recent successes in analyzing images with deep neural networks are almost\nexclusively achieved with Convolutional Neural Networks (CNNs). The training of\nthese CNNs, and in fact of all deep neural network architectures, uses the\nbackpropagation algorithm where the output of the network is compared with the\ndesired result and the difference is then used to tune the weights of the\nnetwork towards the desired outcome. In a 2022 preprint, Geoffrey Hinton\nsuggested an alternative way of training which passes the desired results\ntogether with the images at the input of the network. This so called Forward\nForward (FF) algorithm has up to now only been used in fully connected\nnetworks. In this paper, we show how the FF paradigm can be extended to CNNs.\nOur FF-trained CNN, featuring a novel spatially-extended labeling technique,\nachieves a classification accuracy of 99.16% on the MNIST hand-written digits\ndataset. We show how different hyperparameters affect the performance of the\nproposed algorithm and compare the results with CNN trained with the standard\nbackpropagation approach. Furthermore, we use Class Activation Maps to\ninvestigate which type of features are learnt by the FF algorithm.\n","authors":["Riccardo Scodellaro","Ajinkya Kulkarni","Frauke Alves","Matthias Schröter"],"pdf_url":"https://arxiv.org/pdf/2312.14924v3.pdf","comment":"19 pages, 9 figures"},{"id":"http://arxiv.org/abs/2304.09691v4","updated":"2024-01-07T09:42:39Z","published":"2023-04-19T14:32:56Z","title":"DarSwin: Distortion Aware Radial Swin Transformer","summary":"  Wide-angle lenses are commonly used in perception tasks requiring a large\nfield of view. Unfortunately, these lenses produce significant distortions,\nmaking conventional models that ignore the distortion effects unable to adapt\nto wide-angle images. In this paper, we present a novel transformer-based model\nthat automatically adapts to the distortion produced by wide-angle lenses. Our\nproposed image encoder architecture, dubbed DarSwin, leverages the physical\ncharacteristics of such lenses analytically defined by the radial distortion\nprofile. In contrast to conventional transformer-based architectures, DarSwin\ncomprises a radial patch partitioning, a distortion-based sampling technique\nfor creating token embeddings, and an angular position encoding for radial\npatch merging. Compared to other baselines, DarSwin achieves the best results\non different datasets with significant gains when trained on bounded levels of\ndistortions (very low, low, medium, and high) and tested on all, including\nout-of-distribution distortions. While the base DarSwin architecture requires\nknowledge of the radial distortion profile, we show it can be combined with a\nself-calibration network that estimates such a profile from the input image\nitself, resulting in a completely uncalibrated pipeline. Finally, we also\npresent DarSwin-Unet, which extends DarSwin, to an encoder-decoder architecture\nsuitable for pixel-level tasks. We demonstrate its performance on depth\nestimation and show through extensive experiments that DarSwin-Unet can perform\nzero-shot adaptation to unseen distortions of different wide-angle lenses. The\ncode and models are publicly available at https://lvsn.github.io/darswin/\n","authors":["Akshaya Athwale","Ichrak Shili","Émile Bergeron","Arman Afrasiyabi","Justin Lagüe","Ola Ahmad","Jean-François Lalonde"],"pdf_url":"https://arxiv.org/pdf/2304.09691v4.pdf","comment":"18 pages, 12 figures"},{"id":"http://arxiv.org/abs/2401.03433v1","updated":"2024-01-07T09:23:06Z","published":"2024-01-07T09:23:06Z","title":"SpecRef: A Fast Training-free Baseline of Specific Reference-Condition\n  Real Image Editing","summary":"  Text-conditional image editing based on large diffusion generative model has\nattracted the attention of both the industry and the research community. Most\nexisting methods are non-reference editing, with the user only able to provide\na source image and text prompt. However, it restricts user's control over the\ncharacteristics of editing outcome. To increase user freedom, we propose a new\ntask called Specific Reference Condition Real Image Editing, which allows user\nto provide a reference image to further control the outcome, such as replacing\nan object with a particular one. To accomplish this, we propose a fast baseline\nmethod named SpecRef. Specifically, we design a Specific Reference Attention\nController to incorporate features from the reference image, and adopt a mask\nmechanism to prevent interference between editing and non-editing regions. We\nevaluate SpecRef on typical editing tasks and show that it can achieve\nsatisfactory performance. The source code is available on\nhttps://github.com/jingjiqinggong/specp2p.\n","authors":["Songyan Chen","Jiancheng Huang"],"pdf_url":"https://arxiv.org/pdf/2401.03433v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03431v1","updated":"2024-01-07T09:17:32Z","published":"2024-01-07T09:17:32Z","title":"See360: Novel Panoramic View Interpolation","summary":"  We present See360, which is a versatile and efficient framework for 360\npanoramic view interpolation using latent space viewpoint estimation. Most of\nthe existing view rendering approaches only focus on indoor or synthetic 3D\nenvironments and render new views of small objects. In contrast, we suggest to\ntackle camera-centered view synthesis as a 2D affine transformation without\nusing point clouds or depth maps, which enables an effective 360? panoramic\nscene exploration. Given a pair of reference images, the See360 model learns to\nrender novel views by a proposed novel Multi-Scale Affine Transformer (MSAT),\nenabling the coarse-to-fine feature rendering. We also propose a Conditional\nLatent space AutoEncoder (C-LAE) to achieve view interpolation at any arbitrary\nangle. To show the versatility of our method, we introduce four training\ndatasets, namely UrbanCity360, Archinterior360, HungHom360 and Lab360, which\nare collected from indoor and outdoor environments for both real and synthetic\nrendering. Experimental results show that the proposed method is generic enough\nto achieve real-time rendering of arbitrary views for all four datasets. In\naddition, our See360 model can be applied to view synthesis in the wild: with\nonly a short extra training time (approximately 10 mins), and is able to render\nunknown real-world scenes. The superior performance of See360 opens up a\npromising direction for camera-centered view rendering and 360 panoramic view\ninterpolation.\n","authors":["Zhi-Song Liu","Marie-Paule Cani","Wan-Chi Siu"],"pdf_url":"https://arxiv.org/pdf/2401.03431v1.pdf","comment":"12 pages, 10 figures"},{"id":"http://arxiv.org/abs/2401.03411v1","updated":"2024-01-07T08:03:06Z","published":"2024-01-07T08:03:06Z","title":"GRAM: Global Reasoning for Multi-Page VQA","summary":"  The increasing use of transformer-based large language models brings forward\nthe challenge of processing long sequences. In document visual question\nanswering (DocVQA), leading methods focus on the single-page setting, while\ndocuments can span hundreds of pages. We present GRAM, a method that seamlessly\nextends pre-trained single-page models to the multi-page setting, without\nrequiring computationally-heavy pretraining. To do so, we leverage a\nsingle-page encoder for local page-level understanding, and enhance it with\ndocument-level designated layers and learnable tokens, facilitating the flow of\ninformation across pages for global reasoning. To enforce our model to utilize\nthe newly introduced document-level tokens, we propose a tailored bias\nadaptation method. For additional computational savings during decoding, we\nintroduce an optional compression stage using our C-Former model, which reduces\nthe encoded sequence length, thereby allowing a tradeoff between quality and\nlatency. Extensive experiments showcase GRAM's state-of-the-art performance on\nthe benchmarks for multi-page DocVQA, demonstrating the effectiveness of our\napproach.\n","authors":["Tsachi Blau","Sharon Fogel","Roi Ronen","Alona Golts","Roy Ganz","Elad Ben Avraham","Aviad Aberdam","Shahar Tsiper","Ron Litman"],"pdf_url":"https://arxiv.org/pdf/2401.03411v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03407v1","updated":"2024-01-07T07:56:47Z","published":"2024-01-07T07:56:47Z","title":"Bilateral Reference for High-Resolution Dichotomous Image Segmentation","summary":"  We introduce a novel bilateral reference framework (***BiRefNet***) for\nhigh-resolution dichotomous image segmentation (DIS). It comprises two\nessential components: the localization module (LM) and the reconstruction\nmodule (RM) with our proposed bilateral reference (BiRef). The LM aids in\nobject localization using global semantic information. Within the RM, we\nutilize BiRef for the reconstruction process, where hierarchical patches of\nimages provide the source reference and gradient maps serve as the target\nreference. These components collaborate to generate the final predicted maps.\nWe also introduce auxiliary gradient supervision to enhance focus on regions\nwith finer details. Furthermore, we outline practical training strategies\ntailored for DIS to improve map quality and training process. To validate the\ngeneral applicability of our approach, we conduct extensive experiments on four\ntasks to evince that *BiRefNet* exhibits remarkable performance, outperforming\ntask-specific cutting-edge methods across all benchmarks.\n","authors":["Peng Zheng","Dehong Gao","Deng-Ping Fan","Li Liu","Jorma Laaksonen","Wanli Ouyang","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2401.03407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.14624v3","updated":"2024-01-07T07:08:57Z","published":"2022-09-29T08:38:30Z","title":"Is Complexity Required for Neural Network Pruning? A Case Study on\n  Global Magnitude Pruning","summary":"  Pruning neural networks has become popular in the last decade when it was\nshown that a large number of weights can be safely removed from modern neural\nnetworks without compromising accuracy. Numerous pruning methods have been\nproposed since, each claiming to be better than prior art, however, at the cost\nof increasingly complex pruning methodologies. These methodologies include\nutilizing importance scores, getting feedback through back-propagation or\nhaving heuristics-based pruning rules amongst others. In this work, we question\nwhether this pattern of introducing complexity is really necessary to achieve\nbetter pruning results. We benchmark these SOTA techniques against a simple\npruning baseline, namely, Global Magnitude Pruning (Global MP), that ranks\nweights in order of their magnitudes and prunes the smallest ones.\nSurprisingly, we find that vanilla Global MP performs very well against the\nSOTA techniques. When considering sparsity-accuracy trade-off, Global MP\nperforms better than all SOTA techniques at all sparsity ratios. When\nconsidering FLOPs-accuracy trade-off, some SOTA techniques outperform Global MP\nat lower sparsity ratios, however, Global MP starts performing well at high\nsparsity ratios and performs very well at extremely high sparsity ratios.\nMoreover, we find that a common issue that many pruning algorithms run into at\nhigh sparsity rates, namely, layer-collapse, can be easily fixed in Global MP.\nWe explore why layer collapse occurs in networks and how it can be mitigated in\nGlobal MP by utilizing a technique called Minimum Threshold. We showcase the\nabove findings on various models (WRN-28-8, ResNet-32, ResNet-50, MobileNet-V1\nand FastGRNN) and multiple datasets (CIFAR-10, ImageNet and HAR-2). Code is\navailable at https://github.com/manasgupta-1/GlobalMP.\n","authors":["Manas Gupta","Efe Camci","Vishandi Rudy Keneta","Abhishek Vaidyanathan","Ritwik Kanodia","Chuan-Sheng Foo","Wu Min","Lin Jie"],"pdf_url":"https://arxiv.org/pdf/2209.14624v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03395v1","updated":"2024-01-07T05:50:12Z","published":"2024-01-07T05:50:12Z","title":"Deep Learning-based Image and Video Inpainting: A Survey","summary":"  Image and video inpainting is a classic problem in computer vision and\ncomputer graphics, aiming to fill in the plausible and realistic content in the\nmissing areas of images and videos. With the advance of deep learning, this\nproblem has achieved significant progress recently. The goal of this paper is\nto comprehensively review the deep learning-based methods for image and video\ninpainting. Specifically, we sort existing methods into different categories\nfrom the perspective of their high-level inpainting pipeline, present different\ndeep learning architectures, including CNN, VAE, GAN, diffusion models, etc.,\nand summarize techniques for module design. We review the training objectives\nand the common benchmark datasets. We present evaluation metrics for low-level\npixel and high-level perceptional similarity, conduct a performance evaluation,\nand discuss the strengths and weaknesses of representative inpainting methods.\nWe also discuss related real-world applications. Finally, we discuss open\nchallenges and suggest potential future research directions.\n","authors":["Weize Quan","Jiaxi Chen","Yanli Liu","Dong-Ming Yan","Peter Wonka"],"pdf_url":"https://arxiv.org/pdf/2401.03395v1.pdf","comment":"accepted to IJCV"},{"id":"http://arxiv.org/abs/2303.10428v3","updated":"2024-01-07T05:06:26Z","published":"2023-03-18T14:46:44Z","title":"A Region-Prompted Adapter Tuning for Visual Abductive Reasoning","summary":"  Visual Abductive Reasoning is an emerging vision-language (VL) topic where\nthe model needs to retrieve/generate a likely textual hypothesis from a visual\ninput (image or its part) using backward reasoning based on commonsense. Unlike\nin conventional VL retrieval or captioning tasks, where entities of texts\nappear in the image, in abductive inferences, the relevant facts about\ninferences are not readily apparent in the input images. Besides, these\ninferences are causally linked to specific regional visual cues and would\nchange as cues change. Existing works highlight cues utilizing a specific\nprompt (e.g., colorful prompt). Then, a full fine-tuning of a VL foundation\nmodel is launched to tweak its function from perception to deduction. However,\nthe colorful prompt uniformly patchify ``regional hints'' and ``global\ncontext'' at the same granularity level and may lose fine-grained visual\ndetails crucial for VAR. Meanwhile, full fine-tuning of VLF on limited data\nwould easily be overfitted.\n  To tackle this, we propose a simple yet effective Region-Prompted Adapter\n(RPA), a hybrid parameter-efficient fine-tuning method that leverages the\nstrengths of detailed cues and efficient training for the VAR task.\nRPA~consists of two novel modules: Regional Prompt Generator (RPG) and\nAdapter$^\\textbf{+}$. The prior encodes ``regional visual hints'' and ``global\ncontexts'' into visual prompts separately at fine and coarse-grained levels.\nThe latter extends the vanilla adapters with a new Map Adapter, which modifies\nthe attention map using a trainable low-dim query/key projection. Additionally,\nwe propose a new Dual-Contrastive Loss to regress the visual feature toward\nfeatures of factual description and plausible hypothesis. Experiments on the\nSherlock demonstrate that RPA outperforms previous SOTAs, achieving the 1st\nrank on leaderboards (Comparison to Human Accuracy: RPA~31.74 vs CPT-CLIP\n29.58).\n","authors":["Hao Zhang","Yeo Keat Ee","Basura Fernando"],"pdf_url":"https://arxiv.org/pdf/2303.10428v3.pdf","comment":"13 pages, 11 figures, Under Review of IEEE Transaction"},{"id":"http://arxiv.org/abs/2401.03384v1","updated":"2024-01-07T04:30:12Z","published":"2024-01-07T04:30:12Z","title":"conv_einsum: A Framework for Representation and Fast Evaluation of\n  Multilinear Operations in Convolutional Tensorial Neural Networks","summary":"  Modern ConvNets continue to achieve state-of-the-art results over a vast\narray of vision and image classification tasks, but at the cost of increasing\nparameters. One strategy for compactifying a network without sacrificing much\nexpressive power is to reshape it into a tensorial neural network (TNN), which\nis a higher-order tensorization of its layers, followed by a factorization,\nsuch as a CP-decomposition, which strips a weight down to its critical basis\ncomponents. Passes through TNNs can be represented as sequences of multilinear\noperations (MLOs), where the evaluation path can greatly affect the number of\nfloating point operations (FLOPs) incurred. While functions such as the popular\neinsum can evaluate simple MLOs such as contractions, existing implementations\ncannot process multi-way convolutions, resulting in scant assessments of how\noptimal evaluation paths through tensorized convolutional layers can improve\ntraining speed. In this paper, we develop a unifying framework for representing\ntensorial convolution layers as einsum-like strings and a meta-algorithm\nconv_einsum which is able to evaluate these strings in a FLOPs-minimizing\nmanner. Comprehensive experiments, using our open-source implementation, over a\nwide range of models, tensor decompositions, and diverse tasks, demonstrate\nthat conv_einsum significantly increases both computational and\nmemory-efficiency of convolutional TNNs.\n","authors":["Tahseen Rabbani","Jiahao Su","Xiaoyu Liu","David Chan","Geoffrey Sangston","Furong Huang"],"pdf_url":"https://arxiv.org/pdf/2401.03384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06583v2","updated":"2024-01-07T04:17:34Z","published":"2023-03-12T05:28:55Z","title":"Improving Masked Autoencoders by Learning Where to Mask","summary":"  Masked image modeling is a promising self-supervised learning method for\nvisual data. It is typically built upon image patches with random masks, which\nlargely ignores the variation of information density between them. The question\nis: Is there a better masking strategy than random sampling and how can we\nlearn it? We empirically study this problem and initially find that introducing\nobject-centric priors in mask sampling can significantly improve the learned\nrepresentations. Inspired by this observation, we present AutoMAE, a fully\ndifferentiable framework that uses Gumbel-Softmax to interlink an\nadversarially-trained mask generator and a mask-guided image modeling process.\nIn this way, our approach can adaptively find patches with higher information\ndensity for different images, and further strike a balance between the\ninformation gain obtained from image reconstruction and its practical training\ndifficulty. In our experiments, AutoMAE is shown to provide effective\npretraining models on standard self-supervised benchmarks and downstream tasks.\n","authors":["Haijian Chen","Wendong Zhang","Yunbo Wang","Xiaokang Yang"],"pdf_url":"https://arxiv.org/pdf/2303.06583v2.pdf","comment":"14 pages, 8 figures. This paper has been accepted by PRCV 2023"},{"id":"http://arxiv.org/abs/2401.00260v2","updated":"2024-01-07T04:17:20Z","published":"2023-12-30T15:24:50Z","title":"GazeCLIP: Towards Enhancing Gaze Estimation via Text Guidance","summary":"  Over the past decade, visual gaze estimation has garnered growing attention\nwithin the research community, thanks to its wide-ranging application\nscenarios. While existing estimation approaches have achieved remarkable\nsuccess in enhancing prediction accuracy, they primarily infer gaze directions\nfrom single-image signals and discard the huge potentials of the currently\ndominant text guidance. Notably, visual-language collaboration has been\nextensively explored across a range of visual tasks, such as image synthesis\nand manipulation, leveraging the remarkable transferability of large-scale\nContrastive Language-Image Pre-training (CLIP) model. Nevertheless, existing\ngaze estimation approaches ignore the rich semantic cues conveyed by linguistic\nsignals and priors in CLIP feature space, thereby yielding performance\nsetbacks. In pursuit of making up this gap, we delve deeply into the text-eye\ncollaboration protocol and introduce a novel gaze estimation framework in this\npaper, referred to as GazeCLIP. Specifically, we intricately design a\nlinguistic description generator to produce text signals with coarse\ndirectional cues. Additionally, a CLIP-based backbone that excels in\ncharacterizing text-eye pairs for gaze estimation is presented. This is\nfollowed by the implementation of a fine-grained multi-modal fusion module\naimed at modeling the interrelationships between heterogeneous inputs.\nExtensive experiments on three challenging datasets demonstrate the superiority\nof the proposed GazeCLIP which surpasses the previous approaches and achieves\nthe state-of-the-art estimation accuracy.\n","authors":["Jun Wang","Hao Ruan","Mingjie Wang","Chuanghui Zhang","Huachun Li","Jun Zhou"],"pdf_url":"https://arxiv.org/pdf/2401.00260v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01074v2","updated":"2024-01-07T04:14:16Z","published":"2024-01-02T07:28:21Z","title":"AliFuse: Aligning and Fusing Multi-modal Medical Data for Computer-Aided\n  Diagnosis","summary":"  Medical data collected for making a diagnostic decision are typically\nmulti-modal and provide complementary perspectives of a subject. A\ncomputer-aided diagnosis system welcomes multi-modal inputs; however, how to\neffectively fuse such multi-modal data is a challenging task and attracts a lot\nof attention in the medical research field. In this paper, we propose a\ntransformer-based framework, called Alifuse, for aligning and fusing\nmulti-modal medical data. Specifically, we convert images and unstructured and\nstructured texts into vision and language tokens, and use intramodal and\nintermodal attention mechanisms to learn holistic representations of all\nimaging and non-imaging data for classification. We apply Alifuse to classify\nAlzheimer's disease and obtain state-of-the-art performance on five public\ndatasets, by outperforming eight baselines. The source code will be available\nonline later.\n","authors":["Qiuhui Chen","Yi Hong"],"pdf_url":"https://arxiv.org/pdf/2401.01074v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.15421v5","updated":"2024-01-07T03:52:03Z","published":"2023-07-28T09:11:37Z","title":"MLIC++: Linear Complexity Attention-based Multi-Reference Entropy\n  Modeling for Learned Image Compression","summary":"  Recently, learned image compression has achieved impressive performance. The\nentropy model, which estimates the distribution of the latent representation,\nplays a crucial role in enhancing rate-distortion performance. However,\nexisting global context modules rely on computationally intensive quadratic\ncomplexity computations to capture global correlations. This quadratic\ncomplexity imposes limitations on the potential of high-resolution image\ncoding. Moreover, effectively capturing local, global, and channel-wise\ncontexts with acceptable even linear complexity within a single entropy model\nremains a challenge. To address these limitations, we propose the Linear\nComplexity Attention-based Multi-Reference Entropy Model (MEM++). MEM++\neffectively captures the diverse range of correlations inherent in the latent\nrepresentation. Specifically, the latent representation is first divided into\nmultiple slices. When compressing a particular slice, the previously compressed\nslices serve as its channel-wise contexts. To capture local contexts without\nsacrificing performance, we introduce a novel checkerboard attention module.\nAdditionally, to capture global contexts, we propose the linear complexity\nattention-based global correlations capturing by leveraging the decomposition\nof the softmax operation. The attention map of the previously decoded slice is\nimplicitly computed and employed to predict global correlations in the current\nslice. Based on MEM++, we propose image compression model MLIC++. Extensive\nexperimental evaluations demonstrate that our MLIC++ achieves state-of-the-art\nperformance, reducing BD-rate by 13.39% on the Kodak dataset compared to\nVTM-17.0 in PSNR. Furthermore, MLIC++ exhibits linear GPU memory consumption\nwith resolution, making it highly suitable for high-resolution image coding.\nCode and pre-trained models are available at\nhttps://github.com/JiangWeibeta/MLIC.\n","authors":["Wei Jiang","Jiayu Yang","Yongqi Zhai","Feng Gao","Ronggang Wang"],"pdf_url":"https://arxiv.org/pdf/2307.15421v5.pdf","comment":"v5 is the journel version. Short version (v1, v2, v3) is accepted at\n  ICML 2023 Neural Compression Workshop. MLIC++ is an extension of our ACMMM\n  2023 paper MLIC: Multi-Reference Entropy Model for Learned Image Compression"},{"id":"http://arxiv.org/abs/2401.03379v1","updated":"2024-01-07T03:35:04Z","published":"2024-01-07T03:35:04Z","title":"Towards Effective Multiple-in-One Image Restoration: A Sequential and\n  Prompt Learning Strategy","summary":"  While single task image restoration (IR) has achieved significant successes,\nit remains a challenging issue to train a single model which can tackle\nmultiple IR tasks. In this work, we investigate in-depth the multiple-in-one\n(MiO) IR problem, which comprises seven popular IR tasks. We point out that MiO\nIR faces two pivotal challenges: the optimization of diverse objectives and the\nadaptation to multiple tasks. To tackle these challenges, we present two simple\nyet effective strategies. The first strategy, referred to as sequential\nlearning, attempts to address how to optimize the diverse objectives, which\nguides the network to incrementally learn individual IR tasks in a sequential\nmanner rather than mixing them together. The second strategy, i.e., prompt\nlearning, attempts to address how to adapt to the different IR tasks, which\nassists the network to understand the specific task and improves the\ngeneralization ability. By evaluating on 19 test sets, we demonstrate that the\nsequential and prompt learning strategies can significantly enhance the MiO\nperformance of commonly used CNN and Transformer backbones. Our experiments\nalso reveal that the two strategies can supplement each other to learn better\ndegradation representations and enhance the model robustness. It is expected\nthat our proposed MiO IR formulation and strategies could facilitate the\nresearch on how to train IR models with higher generalization capabilities.\n","authors":["Xiangtao Kong","Chao Dong","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.03379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01505v2","updated":"2024-01-07T02:58:51Z","published":"2024-01-03T02:22:34Z","title":"Sports-QA: A Large-Scale Video Question Answering Benchmark for Complex\n  and Professional Sports","summary":"  Reasoning over sports videos for question answering is an important task with\nnumerous applications, such as player training and information retrieval.\nHowever, this task has not been explored due to the lack of relevant datasets\nand the challenging nature it presents. Most datasets for video question\nanswering (VideoQA) focus mainly on general and coarse-grained understanding of\ndaily-life videos, which is not applicable to sports scenarios requiring\nprofessional action understanding and fine-grained motion analysis. In this\npaper, we introduce the first dataset, named Sports-QA, specifically designed\nfor the sports VideoQA task. The Sports-QA dataset includes various types of\nquestions, such as descriptions, chronologies, causalities, and counterfactual\nconditions, covering multiple sports. Furthermore, to address the\ncharacteristics of the sports VideoQA task, we propose a new Auto-Focus\nTransformer (AFT) capable of automatically focusing on particular scales of\ntemporal information for question answering. We conduct extensive experiments\non Sports-QA, including baseline studies and the evaluation of different\nmethods. The results demonstrate that our AFT achieves state-of-the-art\nperformance.\n","authors":["Haopeng Li","Andong Deng","Qiuhong Ke","Jun Liu","Hossein Rahmani","Yulan Guo","Bernt Schiele","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2401.01505v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03020v2","updated":"2024-01-07T00:29:42Z","published":"2023-12-05T06:58:14Z","title":"Enhanced Breast Cancer Tumor Classification using MobileNetV2: A\n  Detailed Exploration on Image Intensity, Error Mitigation, and\n  Streamlit-driven Real-time Deployment","summary":"  This research introduces a sophisticated transfer learning model based on\nGoogle's MobileNetV2 for breast cancer tumor classification into normal,\nbenign, and malignant categories, utilizing a dataset of 1576 ultrasound images\n(265 normal, 891 benign, 420 malignant). The model achieves an accuracy of\n0.82, precision of 0.83, recall of 0.81, ROC-AUC of 0.94, PR-AUC of 0.88, and\nMCC of 0.74. It examines image intensity distributions and misclassification\nerrors, offering improvements for future applications. Addressing dataset\nimbalances, the study ensures a generalizable model. This work, using a dataset\nfrom Baheya Hospital, Cairo, Egypt, compiled by Walid Al-Dhabyani et al.,\nemphasizes MobileNetV2's potential in medical imaging, aiming to improve\ndiagnostic precision in oncology. Additionally, the paper explores\nStreamlit-based deployment for real-time tumor classification, demonstrating\nMobileNetV2's applicability in medical imaging and setting a benchmark for\nfuture research in oncology diagnostics.\n","authors":["Aaditya Surya","Aditya Shah","Jarnell Kabore","Subash Sasikumar"],"pdf_url":"https://arxiv.org/pdf/2312.03020v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.15462v4","updated":"2024-01-07T00:24:21Z","published":"2022-06-30T17:55:12Z","title":"Improving Visual Grounding by Encouraging Consistent Gradient-based\n  Explanations","summary":"  We propose a margin-based loss for tuning joint vision-language models so\nthat their gradient-based explanations are consistent with region-level\nannotations provided by humans for relatively smaller grounding datasets. We\nrefer to this objective as Attention Mask Consistency (AMC) and demonstrate\nthat it produces superior visual grounding results than previous methods that\nrely on using vision-language models to score the outputs of object detectors.\nParticularly, a model trained with AMC on top of standard vision-language\nmodeling objectives obtains a state-of-the-art accuracy of 86.49% in the\nFlickr30k visual grounding benchmark, an absolute improvement of 5.38% when\ncompared to the best previous model trained under the same level of\nsupervision. Our approach also performs exceedingly well on established\nbenchmarks for referring expression comprehension where it obtains 80.34%\naccuracy in the easy test of RefCOCO+, and 64.55% in the difficult split. AMC\nis effective, easy to implement, and is general as it can be adopted by any\nvision-language model, and can use any type of region annotations.\n","authors":["Ziyan Yang","Kushal Kafle","Franck Dernoncourt","Vicente Ordonez"],"pdf_url":"https://arxiv.org/pdf/2206.15462v4.pdf","comment":"CVPR 2023. Fix ReferIt results. Code:\n  https://github.com/uvavision/AMC-grounding Project Webpage:\n  https://vislang.ai/amc"}],"Robotics":[{"id":"http://arxiv.org/abs/2310.01775v3","updated":"2024-01-07T22:52:09Z","published":"2023-10-03T03:53:51Z","title":"STAMP: Differentiable Task and Motion Planning via Stein Variational\n  Gradient Descent","summary":"  Planning for many manipulation tasks, such as using tools or assembling\nparts, often requires both symbolic and geometric reasoning. Task and Motion\nPlanning (TAMP) algorithms typically solve these problems by conducting a tree\nsearch over high-level task sequences while checking for kinematic and dynamic\nfeasibility. This can be inefficient as the width of the tree can grow\nexponentially with the number of possible actions and objects. In this paper,\nwe propose a novel approach to TAMP that relaxes discrete-and-continuous TAMP\nproblems into inference problems on a continuous domain. Our method, Stein Task\nand Motion Planning (STAMP) subsequently solves this new problem using a\ngradient-based variational inference algorithm called Stein Variational\nGradient Descent, by obtaining gradients from a parallelized differentiable\nphysics simulator. By introducing relaxations to the discrete variables,\nleveraging parallelization, and approaching TAMP as an Bayesian inference\nproblem, our method is able to efficiently find multiple diverse plans in a\nsingle optimization run. We demonstrate our method on two TAMP problems and\nbenchmark them against existing TAMP baselines.\n","authors":["Yewon Lee","Philip Huang","Krishna Murthy Jatavallabhula","Andrew Z. Li","Fabian Damken","Eric Heiden","Kevin Smith","Derek Nowrouzezahrai","Fabio Ramos","Florian Shkurti"],"pdf_url":"https://arxiv.org/pdf/2310.01775v3.pdf","comment":"14 pages, 9 figures, Learning Effective Abstractions for Planning\n  (LEAP) Workshop at CoRL 2023"},{"id":"http://arxiv.org/abs/2401.03599v1","updated":"2024-01-07T22:49:24Z","published":"2024-01-07T22:49:24Z","title":"Disentangled Neural Relational Inference for Interpretable Motion\n  Prediction","summary":"  Effective interaction modeling and behavior prediction of dynamic agents play\na significant role in interactive motion planning for autonomous robots.\nAlthough existing methods have improved prediction accuracy, few research\nefforts have been devoted to enhancing prediction model interpretability and\nout-of-distribution (OOD) generalizability. This work addresses these two\nchallenging aspects by designing a variational auto-encoder framework that\nintegrates graph-based representations and time-sequence models to efficiently\ncapture spatio-temporal relations between interactive agents and predict their\ndynamics. Our model infers dynamic interaction graphs in a latent space\naugmented with interpretable edge features that characterize the interactions.\nMoreover, we aim to enhance model interpretability and performance in OOD\nscenarios by disentangling the latent space of edge features, thereby\nstrengthening model versatility and robustness. We validate our approach\nthrough extensive experiments on both simulated and real-world datasets. The\nresults show superior performance compared to existing methods in modeling\nspatio-temporal relations, motion prediction, and identifying time-invariant\nlatent features.\n","authors":["Victoria M. Dax","Jiachen Li","Enna Sachdeva","Nakul Agarwal","Mykel J. Kochenderfer"],"pdf_url":"https://arxiv.org/pdf/2401.03599v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03581v1","updated":"2024-01-07T21:14:32Z","published":"2024-01-07T21:14:32Z","title":"Evaluating and Personalizing User-Perceived Quality of Text-to-Speech\n  Voices for Delivering Mindfulness Meditation with Different Physical\n  Embodiments","summary":"  Mindfulness-based therapies have been shown to be effective in improving\nmental health, and technology-based methods have the potential to expand the\naccessibility of these therapies. To enable real-time personalized content\ngeneration for mindfulness practice in these methods, high-quality\ncomputer-synthesized text-to-speech (TTS) voices are needed to provide verbal\nguidance and respond to user performance and preferences. However, the\nuser-perceived quality of state-of-the-art TTS voices has not yet been\nevaluated for administering mindfulness meditation, which requires emotional\nexpressiveness. In addition, work has not yet been done to study the effect of\nphysical embodiment and personalization on the user-perceived quality of TTS\nvoices for mindfulness. To that end, we designed a two-phase human subject\nstudy. In Phase 1, an online Mechanical Turk between-subject study (N=471)\nevaluated 3 (feminine, masculine, child-like) state-of-the-art TTS voices with\n2 (feminine, masculine) human therapists' voices in 3 different physical\nembodiment settings (no agent, conversational agent, socially assistive robot)\nwith remote participants. Building on findings from Phase 1, in Phase 2, an\nin-person within-subject study (N=94), we used a novel framework we developed\nfor personalizing TTS voices based on user preferences, and evaluated\nuser-perceived quality compared to best-rated non-personalized voices from\nPhase 1. We found that the best-rated human voice was perceived better than all\nTTS voices; the emotional expressiveness and naturalness of TTS voices were\npoorly rated, while users were satisfied with the clarity of TTS voices.\nSurprisingly, by allowing users to fine-tune TTS voice features, the\nuser-personalized TTS voices could perform almost as well as human voices,\nsuggesting user personalization could be a simple and very effective tool to\nimprove user-perceived quality of TTS voice.\n","authors":["Zhonghao Shi","Han Chen","Anna-Maria Velentza","Siqi Liu","Nathaniel Dennler","Allison O'Connell","Maja Matarić"],"pdf_url":"https://arxiv.org/pdf/2401.03581v1.pdf","comment":"Published in Proceedings of the 2023 ACM/IEEE International\n  Conference on Human-Robot Interaction, pp. 516-524. 2023"},{"id":"http://arxiv.org/abs/2401.03547v1","updated":"2024-01-07T17:18:26Z","published":"2024-01-07T17:18:26Z","title":"Overview of Dialogue Robot Competition 2023","summary":"  We have held dialogue robot competitions in 2020 and 2022 to compare the\nperformances of interactive robots using an android that closely resembles a\nhuman. In 2023, the third competition DRC2023 was held. The task of DRC2023 was\ndesigned to be more challenging than the previous travel agent dialogue tasks.\nSince anyone can now develop a dialogue system using LLMs, the participating\nteams are required to develop a system that effectively uses information about\nthe situation on the spot (real-time information), which is not handled by\nChatGPT and other systems. DRC2023 has two rounds, a preliminary round and the\nfinal round as well as the previous competitions. The preliminary round has\nheld on Oct.27 -- Nov.20, 2023 at real travel agency stores. The final round\nwill be held on December 23, 2023. This paper provides an overview of the task\nsettings and evaluation method of DRC2023 and the preliminary round results.\n","authors":["Takashi Minato","Ryuichiro Higashinaka","Kurima Sakai","Tomo Funayama","Hiromitsu Nishizaki","Takayuki Naga"],"pdf_url":"https://arxiv.org/pdf/2401.03547v1.pdf","comment":"Proceedings of Dialogue Robot Competition 2023. arXiv admin note:\n  text overlap with arXiv:2210.12863"},{"id":"http://arxiv.org/abs/2401.03500v1","updated":"2024-01-07T14:39:41Z","published":"2024-01-07T14:39:41Z","title":"Quadrotor Stabilization with Safety Guarantees: A Universal Formula\n  Approach","summary":"  Safe stabilization is a significant challenge for quadrotors, which involves\nreaching a goal position while avoiding obstacles. Most of the existing\nsolutions for this problem rely on optimization-based methods, demanding\nsubstantial onboard computational resources. This paper introduces a novel\napproach to address this issue and provides a solution that offers fast\ncomputational capabilities tailored for onboard execution. Drawing inspiration\nfrom Sontag's universal formula, we propose an analytical control strategy that\nincorporates the conditions of control Lyapunov functions (CLFs) and control\nbarrier functions (CBFs), effectively avoiding the need for solving\noptimization problems onboard. Moreover, we extend our approach by\nincorporating the concepts of input-to-state stability (ISS) and input-to-state\nsafety (ISSf), enhancing the universal formula's capacity to effectively manage\ndisturbances. Furthermore, we present a projection-based approach to ensure\nthat the universal formula remains effective even when faced with control input\nconstraints. The basic idea of this approach is to project the control input\nderived from the universal formula onto the closest point within the control\ninput domain. Through comprehensive simulations and experimental results, we\nvalidate the efficacy and highlight the advantages of our methodology.\n","authors":["Ming Li","Zhiyong Sun","Siep Weiland"],"pdf_url":"https://arxiv.org/pdf/2401.03500v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03477v1","updated":"2024-01-07T13:06:13Z","published":"2024-01-07T13:06:13Z","title":"Robots and Social Sustainability","summary":"  Sustainability is no longer a matter of choice but is invariably linked to\nthe survival of the entire ecosystem of our planet Earth. As robotics\ntechnology is growing at an exponential rate, it is crucial to examine its\nimplications for sustainability. Our focus is on social sustainability,\nspecifically analyzing the role of robotics technology in this domain by\nidentifying six distinct ways robots influence social sustainability.\n","authors":["Bipin Indurkhya","Barbara Sienkiewicz"],"pdf_url":"https://arxiv.org/pdf/2401.03477v1.pdf","comment":"Full version of \"B.Indurkhya and B. Sienkiewicz, Robots and Social\n  Sustainability, European Robotics Forum, 13-15 March, 2024, Rimini, Italy\""},{"id":"http://arxiv.org/abs/2401.03412v1","updated":"2024-01-07T08:06:23Z","published":"2024-01-07T08:06:23Z","title":"N$^{3}$-Mapping: Normal Guided Neural Non-Projective Signed Distance\n  Fields for Large-scale 3D Mapping","summary":"  Accurate and dense mapping in large-scale environments is essential for\nvarious robot applications. Recently, implicit neural signed distance fields\n(SDFs) have shown promising advances in this task. However, most existing\napproaches employ projective distances from range data as SDF supervision,\nintroducing approximation errors and thus degrading the mapping quality. To\naddress this problem, we introduce N3-Mapping, an implicit neural mapping\nsystem featuring normal-guided neural non-projective signed distance fields.\nSpecifically, we directly sample points along the surface normal, instead of\nthe ray, to obtain more accurate non-projective distance values from range\ndata. Then these distance values are used as supervision to train the implicit\nmap. For large-scale mapping, we apply a voxel-oriented sliding window\nmechanism to alleviate the forgetting issue with a bounded memory footprint.\nBesides, considering the uneven distribution of measured point clouds, a\nhierarchical sampling strategy is designed to improve training efficiency.\nExperiments demonstrate that our method effectively mitigates SDF approximation\nerrors and achieves state-of-the-art mapping quality compared to existing\napproaches.\n","authors":["Shuangfu Song","Junqiao Zhao","Kai Huang","Jiaye Lin","Chen Ye","Tiantian Feng"],"pdf_url":"https://arxiv.org/pdf/2401.03412v1.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2401.03410v1","updated":"2024-01-07T08:01:25Z","published":"2024-01-07T08:01:25Z","title":"Engineering Features to Improve Pass Prediction in Soccer Simulation 2D\n  Games","summary":"  Soccer Simulation 2D (SS2D) is a simulation of a real soccer game in two\ndimensions. In soccer, passing behavior is an essential action for keeping the\nball in possession of our team and creating goal opportunities. Similarly, for\nSS2D, predicting the passing behaviors of both opponents and our teammates\nhelps manage resources and score more goals. Therefore, in this research, we\nhave tried to address the modeling of passing behavior of soccer 2D players\nusing Deep Neural Networks (DNN) and Random Forest (RF). We propose an embedded\ndata extraction module that can record the decision-making of agents in an\nonline format. Afterward, we apply four data sorting techniques for training\ndata preparation. After, we evaluate the trained models' performance playing\nagainst 6 top teams of RoboCup 2019 that have distinctive playing strategies.\nFinally, we examine the importance of different feature groups on the\nprediction of a passing strategy. All results in each step of this work prove\nour suggested methodology's effectiveness and improve the performance of the\npass prediction in Soccer Simulation 2D games ranging from 5\\% (e.g., playing\nagainst the same team) to 10\\% (e.g., playing against Robocup top teams).\n","authors":["Nader Zare","Mahtab Sarvmaili","Aref Sayareh","Omid Amini","Stan Matwin Amilcar Soares"],"pdf_url":"https://arxiv.org/pdf/2401.03410v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03406v1","updated":"2024-01-07T07:54:26Z","published":"2024-01-07T07:54:26Z","title":"Improving Dribbling, Passing, and Marking Actions in Soccer Simulation\n  2D Games Using Machine Learning","summary":"  The RoboCup competition was started in 1997, and is known as the oldest\nRoboCup league. The RoboCup 2D Soccer Simulation League is a stochastic,\npartially observable soccer environment in which 24 autonomous agents play on\ntwo opposing teams. In this paper, we detail the main strategies and\nfunctionalities of CYRUS, the RoboCup 2021 2D Soccer Simulation League\nchampions. The new functionalities presented and discussed in this work are (i)\nMulti Action Dribble, (ii) Pass Prediction and (iii) Marking Decision. The\nMulti Action Dribbling strategy enabled CYRUS to succeed more often and to be\nsafer when dribbling actions were performed during a game. The Pass Prediction\nenhanced our gameplay by predicting our teammate's passing behavior,\nanticipating and making our agents collaborate better towards scoring goals.\nFinally, the Marking Decision addressed the multi-agent matching problem to\nimprove CYRUS defensive strategy by finding an optimal solution to mark\nopponents' players.\n","authors":["Nader Zare","Omid Amini","Aref Sayareh","Mahtab Sarvmaili","Arad Firouzkouhi","Stan Matwin","Amilcar Soares"],"pdf_url":"https://arxiv.org/pdf/2401.03406v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03398v1","updated":"2024-01-07T06:55:41Z","published":"2024-01-07T06:55:41Z","title":"Amplifying robotics capacities with a human touch: An immersive\n  low-latency panoramic remote system","summary":"  AI and robotics technologies have witnessed remarkable advancements in the\npast decade, revolutionizing work patterns and opportunities in various\ndomains. The application of these technologies has propelled society towards an\nera of symbiosis between humans and machines. To facilitate efficient\ncommunication between humans and intelligent robots, we propose the \"Avatar\"\nsystem, an immersive low-latency panoramic human-robot interaction platform. We\nhave designed and tested a prototype of a rugged mobile platform integrated\nwith edge computing units, panoramic video capture devices, power batteries,\nrobot arms, and network communication equipment. Under favorable network\nconditions, we achieved a low-latency high-definition panoramic visual\nexperience with a delay of 357ms. Operators can utilize VR headsets and\ncontrollers for real-time immersive control of robots and devices. The system\nenables remote control over vast physical distances, spanning campuses,\nprovinces, countries, and even continents (New York to Shenzhen). Additionally,\nthe system incorporates visual SLAM technology for map and trajectory\nrecording, providing autonomous navigation capabilities. We believe that this\nintuitive system platform can enhance efficiency and situational experience in\nhuman-robot collaboration, and with further advancements in related\ntechnologies, it will become a versatile tool for efficient and symbiotic\ncooperation between AI and humans.\n","authors":["Junjie Li","Jian Xu","Dewei Han","Kang Li","Zhaoyuan Ma"],"pdf_url":"https://arxiv.org/pdf/2401.03398v1.pdf","comment":"9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2309.09011v2","updated":"2024-01-07T05:15:07Z","published":"2023-09-16T14:45:34Z","title":"Optimal Initialization Strategies for Range-Only Trajectory Estimation","summary":"  Range-only (RO) pose estimation involves determining a robot's pose over time\nby measuring the distance between multiple devices on the robot, known as tags,\nand devices installed in the environment, known as anchors. The nonconvex\nnature of the range measurement model results in a cost function with possible\nlocal minima. In the absence of a good initialization, commonly used iterative\nsolvers can get stuck in these local minima resulting in poor trajectory\nestimation accuracy. In this work, we propose convex relaxations to the\noriginal nonconvex problem based on semidefinite programs (SDPs). Specifically,\nwe formulate computationally tractable SDP relaxations to obtain accurate\ninitial pose and trajectory estimates for RO trajectory estimation under static\nand dynamic (i.e., constant-velocity motion) conditions. Through simulation and\nreal experiments, we demonstrate that our proposed initialization strategies\nestimate the initial state accurately compared to iterative local solvers.\nAdditionally, the proposed relaxations recover global minima under moderate\nrange measurement noise levels.\n","authors":["Abhishek Goudar","Frederike Dümbgen","Timothy D. Barfoot","Angela P. Schoellig"],"pdf_url":"https://arxiv.org/pdf/2309.09011v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03388v1","updated":"2024-01-07T04:46:23Z","published":"2024-01-07T04:46:23Z","title":"LLMs for Robotic Object Disambiguation","summary":"  The advantages of pre-trained large language models (LLMs) are apparent in a\nvariety of language processing tasks. But can a language model's knowledge be\nfurther harnessed to effectively disambiguate objects and navigate\ndecision-making challenges within the realm of robotics? Our study reveals the\nLLM's aptitude for solving complex decision making challenges that are often\npreviously modeled by Partially Observable Markov Decision Processes (POMDPs).\nA pivotal focus of our research is the object disambiguation capability of\nLLMs. We detail the integration of an LLM into a tabletop environment\ndisambiguation task, a decision making problem where the robot's task is to\ndiscern and retrieve a user's desired object from an arbitrarily large and\ncomplex cluster of objects. Despite multiple query attempts with zero-shot\nprompt engineering (details can be found in the Appendix), the LLM struggled to\ninquire about features not explicitly provided in the scene description. In\nresponse, we have developed a few-shot prompt engineering system to improve the\nLLM's ability to pose disambiguating queries. The result is a model capable of\nboth using given features when they are available and inferring new relevant\nfeatures when necessary, to successfully generate and navigate down a precise\ndecision tree to the correct object--even when faced with identical options.\n","authors":["Connie Jiang","Yiqing Xu","David Hsu"],"pdf_url":"https://arxiv.org/pdf/2401.03388v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00315v2","updated":"2024-01-07T01:23:49Z","published":"2023-12-30T20:23:27Z","title":"Bidirectional Temporal Plan Graph: Enabling Switchable Passing Orders\n  for More Efficient Multi-Agent Path Finding Plan Execution","summary":"  The Multi-Agent Path Finding (MAPF) problem involves planning collision-free\npaths for multiple agents in a shared environment. The majority of MAPF solvers\nrely on the assumption that an agent can arrive at a specific location at a\nspecific timestep. However, real-world execution uncertainties can cause agents\nto deviate from this assumption, leading to collisions and deadlocks. Prior\nresearch solves this problem by having agents follow a Temporal Plan Graph\n(TPG), enforcing a consistent passing order at every location as defined in the\nMAPF plan. However, we show that TPGs are overly strict because, in some\ncircumstances, satisfying the passing order requires agents to wait\nunnecessarily, leading to longer execution time. To overcome this issue, we\nintroduce a new graphical representation called a Bidirectional Temporal Plan\nGraph (BTPG), which allows switching passing orders during execution to avoid\nunnecessary waiting time. We design two anytime algorithms for constructing a\nBTPG: BTPG-na\\\"ive and BTPG-optimized. Experimental results show that following\nBTPGs consistently outperforms following TPGs, reducing unnecessary waits by\n8-20%.\n","authors":["Yifan Su","Rishi Veerapaneni","Jiaoyang Li"],"pdf_url":"https://arxiv.org/pdf/2401.00315v2.pdf","comment":"Accepted by AAAI-2024"}],"Multimedia":[{"id":"http://arxiv.org/abs/2401.03499v1","updated":"2024-01-07T14:34:34Z","published":"2024-01-07T14:34:34Z","title":"Re:Draw -- Context Aware Translation as a Controllable Method for\n  Artistic Production","summary":"  We introduce context-aware translation, a novel method that combines the\nbenefits of inpainting and image-to-image translation, respecting\nsimultaneously the original input and contextual relevance -- where existing\nmethods fall short. By doing so, our method opens new avenues for the\ncontrollable use of AI within artistic creation, from animation to digital art.\n  As an use case, we apply our method to redraw any hand-drawn animated\ncharacter eyes based on any design specifications - eyes serve as a focal point\nthat captures viewer attention and conveys a range of emotions, however, the\nlabor-intensive nature of traditional animation often leads to compromises in\nthe complexity and consistency of eye design. Furthermore, we remove the need\nfor production data for training and introduce a new character recognition\nmethod that surpasses existing work by not requiring fine-tuning to specific\nproductions. This proposed use case could help maintain consistency throughout\nproduction and unlock bolder and more detailed design choices without the\nproduction cost drawbacks. A user study shows context-aware translation is\npreferred over existing work 95.16% of the time.\n","authors":["Joao Liborio Cardoso","Francesco Banterle","Paolo Cignoni","Michael Wimmer"],"pdf_url":"https://arxiv.org/pdf/2401.03499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03476v1","updated":"2024-01-07T13:01:29Z","published":"2024-01-07T13:01:29Z","title":"Freetalker: Controllable Speech and Text-Driven Gesture Generation Based\n  on Diffusion Models for Enhanced Speaker Naturalness","summary":"  Current talking avatars mostly generate co-speech gestures based on audio and\ntext of the utterance, without considering the non-speaking motion of the\nspeaker. Furthermore, previous works on co-speech gesture generation have\ndesigned network structures based on individual gesture datasets, which results\nin limited data volume, compromised generalizability, and restricted speaker\nmovements. To tackle these issues, we introduce FreeTalker, which, to the best\nof our knowledge, is the first framework for the generation of both spontaneous\n(e.g., co-speech gesture) and non-spontaneous (e.g., moving around the podium)\nspeaker motions. Specifically, we train a diffusion-based model for speaker\nmotion generation that employs unified representations of both speech-driven\ngestures and text-driven motions, utilizing heterogeneous data sourced from\nvarious motion datasets. During inference, we utilize classifier-free guidance\nto highly control the style in the clips. Additionally, to create smooth\ntransitions between clips, we utilize DoubleTake, a method that leverages a\ngenerative prior and ensures seamless motion blending. Extensive experiments\nshow that our method generates natural and controllable speaker movements. Our\ncode, model, and demo are are available at\n\\url{https://youngseng.github.io/FreeTalker/}.\n","authors":["Sicheng Yang","Zunnan Xu","Haiwei Xue","Yongkang Cheng","Shaoli Huang","Mingming Gong","Zhiyong Wu"],"pdf_url":"https://arxiv.org/pdf/2401.03476v1.pdf","comment":"6 pages, 3 figures, ICASSP 2024"}]},"2024-01-06T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2310.02489v2","updated":"2024-01-06T23:41:15Z","published":"2023-10-03T23:31:48Z","title":"ResidualTransformer: Residual Low-Rank Learning with Weight-Sharing for\n  Transformer Layers","summary":"  Memory constraint of always-on devices is one of the major concerns when\ndeploying speech processing models on these devices. While larger models\ntrained with sufficiently large amount of data generally perform better, making\nthem fit in the device memory is a demanding challenge. In this paper, we aim\nto reduce model size by reparameterizing model weights across Transformer\nencoder layers and assuming a special weight composition and structure. More\nspecifically, inspired by ResNet and the more recent LoRA work, we propose an\napproach named ResidualTransformer, where each weight matrix in a Transformer\nlayer comprises 1) a shared full-rank component with its adjacent layers, and\n2) a unique low-rank component to itself. The low-rank matrices only account\nfor a small amount of model size increase. In addition, we add diagonal weight\nmatrices to improve modeling capacity of the low-rank matrices. Experiments of\nour 10k-hour speech recognition and speech translation tasks show that the\nTransformer encoder size can be reduced by ~3X with very slight performance\ndegradation.\n","authors":["Yiming Wang","Jinyu Li"],"pdf_url":"https://arxiv.org/pdf/2310.02489v2.pdf","comment":"Accepted at IEEE ICASSP 2024. 5 pages, 1 figure"},{"id":"http://arxiv.org/abs/2401.03321v1","updated":"2024-01-06T22:49:38Z","published":"2024-01-06T22:49:38Z","title":"PIXAR: Auto-Regressive Language Modeling in Pixel Space","summary":"  Recent works showed the possibility of building open-vocabulary large\nlanguage models (LLMs) that directly operate on pixel representations and are\nimplemented as encoder-decoder models that reconstruct masked image patches of\nrendered text. However, these pixel-based LLMs are limited to autoencoding\ntasks and cannot generate new text as images. As such, they cannot be used for\nopen-answer or generative language tasks. In this work, we overcome this\nlimitation and introduce PIXAR, the first pixel-based autoregressive LLM that\ndoes not rely on a pre-defined vocabulary for both input and output text.\nConsisting of only a decoder, PIXAR can answer free-form generative tasks while\nkeeping the text representation learning performance on par with previous\nencoder-decoder models. Furthermore, we highlight the challenges to\nautoregressively generate non-blurred text as images and link this to the usual\nmaximum likelihood objective. We propose a simple adversarial pretraining that\nsignificantly improves the readability and performance of PIXAR making it\ncomparable to GPT2 on short text generation tasks. This paves the way to\nbuilding open-vocabulary LLMs that are usable for free-form generative tasks\nand questions the necessity of the usual symbolic input representation -- text\nas tokens -- for these challenging tasks.\n","authors":["Yintao Tai","Xiyang Liao","Alessandro Suglia","Antonio Vergari"],"pdf_url":"https://arxiv.org/pdf/2401.03321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03314v1","updated":"2024-01-06T22:13:51Z","published":"2024-01-06T22:13:51Z","title":"Enhancing Context Through Contrast","summary":"  Neural machine translation benefits from semantically rich representations.\nConsiderable progress in learning such representations has been achieved by\nlanguage modelling and mutual information maximization objectives using\ncontrastive learning. The language-dependent nature of language modelling\nintroduces a trade-off between the universality of the learned representations\nand the model's performance on the language modelling tasks. Although\ncontrastive learning improves performance, its success cannot be attributed to\nmutual information alone. We propose a novel Context Enhancement step to\nimprove performance on neural machine translation by maximizing mutual\ninformation using the Barlow Twins loss. Unlike other approaches, we do not\nexplicitly augment the data but view languages as implicit augmentations,\neradicating the risk of disrupting semantic information. Further, our method\ndoes not learn embeddings from scratch and can be generalised to any set of\npre-trained embeddings. Finally, we evaluate the language-agnosticism of our\nembeddings through language classification and use them for neural machine\ntranslation to compare with state-of-the-art approaches.\n","authors":["Kshitij Ambilduke","Aneesh Shetye","Diksha Bagade","Rishika Bhagwatkar","Khurshed Fitter","Prasad Vagdargi","Shital Chiddarwar"],"pdf_url":"https://arxiv.org/pdf/2401.03314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.03750v2","updated":"2024-01-06T17:29:12Z","published":"2023-12-01T20:14:16Z","title":"Analyzing the Impact of Fake News on the Anticipated Outcome of the 2024\n  Election Ahead of Time","summary":"  Despite increasing awareness and research around fake news, there is still a\nsignificant need for datasets that specifically target racial slurs and biases\nwithin North American political speeches. This is particulary important in the\ncontext of upcoming North American elections. This study introduces a\ncomprehensive dataset that illuminates these critical aspects of\nmisinformation. To develop this fake news dataset, we scraped and built a\ncorpus of 40,000 news articles about political discourses in North America. A\nportion of this dataset (4000) was then carefully annotated, using a blend of\nadvanced language models and human verification methods. We have made both\nthese datasets openly available to the research community and have conducted\nbenchmarking on the annotated data to demonstrate its utility. We release the\nbest-performing language model along with data. We encourage researchers and\ndevelopers to make use of this dataset and contribute to this ongoing\ninitiative.\n","authors":["Shaina Raza","Mizanur Rahman","Shardul Ghuge"],"pdf_url":"https://arxiv.org/pdf/2312.03750v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03253v1","updated":"2024-01-06T16:33:39Z","published":"2024-01-06T16:33:39Z","title":"Large Language Models as Visual Cross-Domain Learners","summary":"  Recent advances achieved by deep learning models rely on the independent and\nidentically distributed assumption, hindering their applications in real-world\nscenarios with domain shifts. To address the above issues, cross-domain\nlearning aims at extracting domain-invariant knowledge to reduce the domain\nshift between training and testing data. However, in visual cross-domain\nlearning, traditional methods concentrate solely on the image modality,\nneglecting the use of the text modality to alleviate the domain shift. In this\nwork, we propose Large Language models as Visual cross-dOmain learners (LLaVO).\nLLaVO uses vision-language models to convert images into detailed textual\ndescriptions. A large language model is then finetuned on textual descriptions\nof the source/target domain generated by a designed instruction template.\nExtensive experimental results on various cross-domain tasks under the domain\ngeneralization and unsupervised domain adaptation settings have demonstrated\nthe effectiveness of the proposed method.\n","authors":["Shuhao Chen","Yulong Zhang","Weisen Jiang","Jiangang Lu","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.03253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03239v1","updated":"2024-01-06T15:34:38Z","published":"2024-01-06T15:34:38Z","title":"Reflections on Inductive Thematic Saturation as a potential metric for\n  measuring the validity of an inductive Thematic Analysis with LLMs","summary":"  This paper presents a set of reflections on saturation and the use of Large\nLanguage Models (LLMs) for performing Thematic Analysis (TA). The paper\nsuggests that initial thematic saturation (ITS) could be used as a metric to\nassess part of the transactional validity of TA with LLM, focusing on the\ninitial coding. The paper presents the initial coding of two datasets of\ndifferent sizes, and it reflects on how the LLM reaches some form of analytical\nsaturation during the coding. The procedure proposed in this work leads to the\ncreation of two codebooks, one comprising the total cumulative initial codes\nand the other the total unique codes. The paper proposes a metric to\nsynthetically measure ITS using a simple mathematical calculation employing the\nratio between slopes of cumulative codes and unique codes. The paper\ncontributes to the initial body of work exploring how to perform qualitative\nanalysis with LLMs.\n","authors":["Stefano De Paoli","Walter Stan Mathis"],"pdf_url":"https://arxiv.org/pdf/2401.03239v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.16582v2","updated":"2024-01-06T14:17:40Z","published":"2023-10-25T12:16:33Z","title":"Tailoring Personality Traits in Large Language Models via\n  Unsupervisedly-Built Personalized Lexicons","summary":"  Personality plays a pivotal role in shaping human expression patterns, thus\nregulating the personality of large language models (LLMs) holds significant\npotential in enhancing the user experience of LLMs. Previous methods either\nrelied on fine-tuning LLMs on specific corpora or necessitated manually crafted\nprompts to elicit specific personalities from LLMs. However, the former\napproach is inefficient and costly, while the latter cannot precisely\nmanipulate personality traits at a fine-grained level. To address the above\nchallenges, we have employed a novel Unsupervisedly-Built Personalized Lexicons\n(UBPL) in a pluggable manner during the decoding phase of LLMs to manipulate\ntheir personality traits. UBPL is a lexicon built through an unsupervised\napproach from a situational judgment test dataset (SJTs4LLM). Users can utilize\nUBPL to adjust the probability vectors of predicted words in the decoding phase\nof LLMs, thus influencing the personality expression of LLMs. Extensive\nexperimentation demonstrates the remarkable effectiveness and pluggability of\nour method for fine-grained manipulation of LLM's personality.\n","authors":["Tianlong Li","Shihan Dou","Changze Lv","Wenhao Liu","Jianhan Xu","Muling Wu","Zixuan Ling","Xiaoqing Zheng","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2310.16582v2.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2312.17581v2","updated":"2024-01-06T13:33:23Z","published":"2023-12-29T12:33:21Z","title":"Action-Item-Driven Summarization of Long Meeting Transcripts","summary":"  The increased prevalence of online meetings has significantly enhanced the\npracticality of a model that can automatically generate the summary of a given\nmeeting. This paper introduces a novel and effective approach to automate the\ngeneration of meeting summaries. Current approaches to this problem generate\ngeneral and basic summaries, considering the meeting simply as a long dialogue.\nHowever, our novel algorithms can generate abstractive meeting summaries that\nare driven by the action items contained in the meeting transcript. This is\ndone by recursively generating summaries and employing our action-item\nextraction algorithm for each section of the meeting in parallel. All of these\nsectional summaries are then combined and summarized together to create a\ncoherent and action-item-driven summary. In addition, this paper introduces\nthree novel methods for dividing up long transcripts into topic-based sections\nto improve the time efficiency of our algorithm, as well as to resolve the\nissue of large language models (LLMs) forgetting long-term dependencies. Our\npipeline achieved a BERTScore of 64.98 across the AMI corpus, which is an\napproximately 4.98% increase from the current state-of-the-art result produced\nby a fine-tuned BART (Bidirectional and Auto-Regressive Transformers) model.\n","authors":["Logan Golia","Jugal Kalita"],"pdf_url":"https://arxiv.org/pdf/2312.17581v2.pdf","comment":"Accepted into the 7th International Conference on Natural Language\n  Processing and Information Retrieval (NLPIR 2023)"},{"id":"http://arxiv.org/abs/2311.08648v2","updated":"2024-01-06T12:59:43Z","published":"2023-11-15T01:58:54Z","title":"Explore Spurious Correlations at the Concept Level in Language Models\n  for Text Classification","summary":"  Language models (LMs) have achieved notable success in numerous NLP tasks,\nemploying both fine-tuning and in-context learning (ICL) methods. While\nlanguage models demonstrate exceptional performance, they face robustness\nchallenges due to spurious correlations arising from imbalanced label\ndistributions in training data or ICL exemplars. Previous research has\nprimarily concentrated on word, phrase, and syntax features, neglecting the\nconcept level, often due to the absence of concept labels and difficulty in\nidentifying conceptual content in input texts. This paper introduces two main\ncontributions. First, we employ ChatGPT to assign concept labels to texts,\nassessing concept bias in models during fine-tuning or ICL on test data. We\nfind that LMs, when encountering spurious correlations between a concept and a\nlabel in training or prompts, resort to shortcuts for predictions. Second, we\nintroduce a data rebalancing technique that incorporates ChatGPT-generated\ncounterfactual data, thereby balancing label distribution and mitigating\nspurious correlations. Our method's efficacy, surpassing traditional token\nremoval approaches, is validated through extensive testing.\n","authors":["Yuhang Zhou","Paiheng Xu","Xiaoyu Liu","Bang An","Wei Ai","Furong Huang"],"pdf_url":"https://arxiv.org/pdf/2311.08648v2.pdf","comment":"14 pages, 4 page appendix"},{"id":"http://arxiv.org/abs/2401.03205v1","updated":"2024-01-06T12:40:45Z","published":"2024-01-06T12:40:45Z","title":"The Dawn After the Dark: An Empirical Study on Factuality Hallucination\n  in Large Language Models","summary":"  In the era of large language models (LLMs), hallucination (i.e., the tendency\nto generate factually incorrect content) poses great challenge to trustworthy\nand reliable deployment of LLMs in real-world applications. To tackle the LLM\nhallucination, three key questions should be well studied: how to detect\nhallucinations (detection), why do LLMs hallucinate (source), and what can be\ndone to mitigate them (mitigation). To address these challenges, this work\npresents a systematic empirical study on LLM hallucination, focused on the the\nthree aspects of hallucination detection, source and mitigation. Specially, we\nconstruct a new hallucination benchmark HaluEval 2.0, and designs a simple yet\neffective detection method for LLM hallucination. Furthermore, we zoom into the\ndifferent training or utilization stages of LLMs and extensively analyze the\npotential factors that lead to the LLM hallucination. Finally, we implement and\nexamine a series of widely used techniques to mitigate the hallucinations in\nLLMs. Our work has led to several important findings to understand the\nhallucination origin and mitigate the hallucinations in LLMs. Our code and data\ncan be accessed at https://github.com/RUCAIBox/HaluEval-2.0.\n","authors":["Junyi Li","Jie Chen","Ruiyang Ren","Xiaoxue Cheng","Wayne Xin Zhao","Jian-Yun Nie","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2401.03205v1.pdf","comment":"24 pages, 8 figures, 13 tables"},{"id":"http://arxiv.org/abs/2306.00789v3","updated":"2024-01-06T11:59:32Z","published":"2023-06-01T15:19:06Z","title":"Cross-Lingual Transfer Learning for Low-Resource Speech Translation","summary":"  The paper presents a novel three-step transfer learning framework for\nenhancing cross-lingual transfer from high- to low-resource languages in the\ndownstream application of Automatic Speech Translation. The approach integrates\na semantic knowledge-distillation step into the existing two-step cross-lingual\ntransfer learning framework XLS-R. This extra step aims to encode semantic\nknowledge in the multilingual speech encoder pre-trained via Self-Supervised\nLearning using unlabeled speech. Our proposed three-step cross-lingual transfer\nlearning framework addresses the large cross-lingual transfer gap (TRFGap)\nobserved in the XLS-R framework between high-resource and low-resource\nlanguages. We validate our proposal through extensive experiments and\ncomparisons on the CoVoST-2 benchmark, showing significant improvements in\ntranslation performance, especially for low-resource languages, and a notable\nreduction in the TRFGap.\n","authors":["Sameer Khurana","Nauman Dawalatabad","Antoine Laurent","Luis Vicente","Pablo Gimeno","Victoria Mingote","James Glass"],"pdf_url":"https://arxiv.org/pdf/2306.00789v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03190v1","updated":"2024-01-06T10:40:24Z","published":"2024-01-06T10:40:24Z","title":"MPN: Leveraging Multilingual Patch Neuron for Cross-lingual Model\n  Editing","summary":"  Large language models are known for encoding a vast amount of factual\nknowledge, but they often becomes outdated due to the ever-changing nature of\nexternal information. A promising solution to this challenge is the utilization\nof model editing methods to update the knowledge in an efficient manner.\nHowever, the majority of existing model editing techniques are limited to\nmonolingual frameworks, thus failing to address the crucial issue of\ncross-lingual knowledge synchronization for multilingual models. To tackle this\nproblem, we propose a simple yet effective method that trains multilingual\npatch neuron to store cross-lingual knowledge. It can be easily adapted to\nexisting approaches to enhance their cross-lingual editing capabilities. To\nevaluate our method, we conduct experiments using both the XNLI dataset and a\nself-constructed XFEVER dataset. Experimental results demonstrate that our\nproposed method achieves improved performance in cross-lingual editing tasks\nwithout requiring excessive modifications to the original methodology, thereby\nshowcasing its user-friendly characteristics. Codes will be released soon.\n","authors":["Nianwen Si","Hao Zhang","Weiqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.03190v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2401.03183v1","updated":"2024-01-06T10:08:33Z","published":"2024-01-06T10:08:33Z","title":"δ-CAUSAL: Exploring Defeasibility in Causal Reasoning","summary":"  Defeasibility in causal reasoning implies that the causal relationship\nbetween cause and effect can be strengthened or weakened. Namely, the causal\nstrength between cause and effect should increase or decrease with the\nincorporation of strengthening arguments (supporters) or weakening arguments\n(defeaters), respectively. However, existing works ignore defeasibility in\ncausal reasoning and fail to evaluate existing causal strength metrics in\ndefeasible settings. In this work, we present {\\delta}-CAUSAL, the first\nbenchmark dataset for studying defeasibility in causal reasoning.\n{\\delta}-CAUSAL includes around 11K events spanning ten domains, featuring\ndefeasible causality pairs, i.e., cause-effect pairs accompanied by supporters\nand defeaters. We further show current causal strength metrics fail to reflect\nthe change of causal strength with the incorporation of supporters or defeaters\nin {\\delta}-CAUSAL. To this end, we propose CESAR (Causal Embedding aSsociation\nwith Attention Rating), a metric that measures causal strength based on\ntoken-level causal relationships. CESAR achieves a significant 69.7% relative\nimprovement over existing metrics, increasing from 47.2% to 80.1% in capturing\nthe causal strength change brought by supporters and defeaters. We further\ndemonstrate even Large Language Models (LLMs) like GPT-3.5 still lag 4.5 and\n10.7 points behind humans in generating supporters and defeaters, emphasizing\nthe challenge posed by {\\delta}-CAUSAL.\n","authors":["Shaobo Cui","Lazar Milikic","Yiyang Feng","Mete Ismayilzada","Debjit Paul","Antoine Bosselut","Boi Faltings"],"pdf_url":"https://arxiv.org/pdf/2401.03183v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00793v2","updated":"2024-01-06T10:05:23Z","published":"2024-01-01T15:40:35Z","title":"SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for\n  Large Language Models","summary":"  With the growing use of large language models hosted on cloud platforms to\noffer inference services, privacy concerns are escalating, especially\nconcerning sensitive data like investment plans and bank account details.\nSecure Multi-Party Computing (SMPC) emerges as a promising solution to protect\nthe privacy of inference data and model parameters. However, the application of\nSMPC in Privacy-Preserving Inference (PPI) for large language models,\nparticularly those based on the Transformer architecture, often leads to\nconsiderable slowdowns or declines in performance. This is largely due to the\nmultitude of nonlinear operations in the Transformer architecture, which are\nnot well-suited to SMPC and difficult to circumvent or optimize effectively. To\naddress this concern, we introduce an advanced optimization framework called\nSecFormer, to achieve fast and accurate PPI for Transformer models. By\nimplementing model design optimization, we successfully eliminate the high-cost\nexponential and maximum operations in PPI without sacrificing model\nperformance. Additionally, we have developed a suite of efficient SMPC\nprotocols that utilize segmented polynomials, Fourier series and Goldschmidt's\nmethod to handle other complex nonlinear functions within PPI, such as GeLU,\nLayerNorm, and Softmax. Our extensive experiments reveal that SecFormer\noutperforms MPCFormer in performance, showing improvements of $5.6\\%$ and\n$24.2\\%$ for BERT$_{\\text{BASE}}$ and BERT$_{\\text{LARGE}}$, respectively. In\nterms of efficiency, SecFormer is 3.56 and 3.58 times faster than Puma for\nBERT$_{\\text{BASE}}$ and BERT$_{\\text{LARGE}}$, demonstrating its effectiveness\nand speed.\n","authors":["Jinglong Luo","Yehong Zhang","Jiaqi Zhang","Xin Mu","Hui Wang","Yue Yu","Zenglin Xu"],"pdf_url":"https://arxiv.org/pdf/2401.00793v2.pdf","comment":"12pages, 9figures"},{"id":"http://arxiv.org/abs/2401.03181v1","updated":"2024-01-06T09:55:22Z","published":"2024-01-06T09:55:22Z","title":"A Joint-Reasoning based Disease Q&A System","summary":"  Medical question answer (QA) assistants respond to lay users' health-related\nqueries by synthesizing information from multiple sources using natural\nlanguage processing and related techniques. They can serve as vital tools to\nalleviate issues of misinformation, information overload, and complexity of\nmedical language, thus addressing lay users' information needs while reducing\nthe burden on healthcare professionals. QA systems, the engines of such\nassistants, have typically used either language models (LMs) or knowledge\ngraphs (KG), though the approaches could be complementary. LM-based QA systems\nexcel at understanding complex questions and providing well-formed answers, but\nare prone to factual mistakes. KG-based QA systems, which represent facts well,\nare mostly limited to answering short-answer questions with pre-created\ntemplates. While a few studies have jointly used LM and KG approaches for\ntext-based QA, this was done to answer multiple-choice questions. Extant QA\nsystems also have limitations in terms of automation and performance. We\naddress these challenges by designing a novel, automated disease QA system\nwhich effectively utilizes both LM and KG techniques through a joint-reasoning\napproach to answer disease-related questions appropriate for lay users. Our\nevaluation of the system using a range of quality metrics demonstrates its\nefficacy over benchmark systems, including the popular ChatGPT.\n","authors":["Prakash Chandra Sukhwal","Vaibhav Rajan","Atreyi Kankanhalli"],"pdf_url":"https://arxiv.org/pdf/2401.03181v1.pdf","comment":"36 pages, 6 figures, submitted to TMIS on 14 July 2023 (status: under\n  review)"},{"id":"http://arxiv.org/abs/2401.01943v2","updated":"2024-01-06T09:51:09Z","published":"2024-01-03T19:03:32Z","title":"Generalist embedding models are better at short-context clinical\n  semantic search than specialized embedding models","summary":"  The increasing use of tools and solutions based on Large Language Models\n(LLMs) for various tasks in the medical domain has become a prominent trend.\nTheir use in this highly critical and sensitive domain has thus raised\nimportant questions about their robustness, especially in response to\nvariations in input, and the reliability of the generated outputs. This study\naddresses these questions by constructing a textual dataset based on the\nICD-10-CM code descriptions, widely used in US hospitals and containing many\nclinical terms, and their easily reproducible rephrasing. We then benchmarked\nexisting embedding models, either generalist or specialized in the clinical\ndomain, in a semantic search task where the goal was to correctly match the\nrephrased text to the original description. Our results showed that generalist\nmodels performed better than clinical models, suggesting that existing clinical\nspecialized models are more sensitive to small changes in input that confuse\nthem. The highlighted problem of specialized models may be due to the fact that\nthey have not been trained on sufficient data, and in particular on datasets\nthat are not diverse enough to have a reliable global language understanding,\nwhich is still necessary for accurate handling of medical documents.\n","authors":["Jean-Baptiste Excoffier","Tom Roehr","Alexei Figueroa","Jens-Michalis Papaioannou","Keno Bressem","Matthieu Ortala"],"pdf_url":"https://arxiv.org/pdf/2401.01943v2.pdf","comment":"11 pages, 1 figure, 5 tables"},{"id":"http://arxiv.org/abs/2401.03177v1","updated":"2024-01-06T09:38:55Z","published":"2024-01-06T09:38:55Z","title":"Text-Video Retrieval via Variational Multi-Modal Hypergraph Networks","summary":"  Text-video retrieval is a challenging task that aims to identify relevant\nvideos given textual queries. Compared to conventional textual retrieval, the\nmain obstacle for text-video retrieval is the semantic gap between the textual\nnature of queries and the visual richness of video content. Previous works\nprimarily focus on aligning the query and the video by finely aggregating\nword-frame matching signals. Inspired by the human cognitive process of\nmodularly judging the relevance between text and video, the judgment needs\nhigh-order matching signal due to the consecutive and complex nature of video\ncontents. In this paper, we propose chunk-level text-video matching, where the\nquery chunks are extracted to describe a specific retrieval unit, and the video\nchunks are segmented into distinct clips from videos. We formulate the\nchunk-level matching as n-ary correlations modeling between words of the query\nand frames of the video and introduce a multi-modal hypergraph for n-ary\ncorrelation modeling. By representing textual units and video frames as nodes\nand using hyperedges to depict their relationships, a multi-modal hypergraph is\nconstructed. In this way, the query and the video can be aligned in a\nhigh-order semantic space. In addition, to enhance the model's generalization\nability, the extracted features are fed into a variational inference component\nfor computation, obtaining the variational representation under the Gaussian\ndistribution. The incorporation of hypergraphs and variational inference allows\nour model to capture complex, n-ary interactions among textual and visual\ncontents. Experimental results demonstrate that our proposed method achieves\nstate-of-the-art performance on the text-video retrieval task.\n","authors":["Qian Li","Lixin Su","Jiashu Zhao","Long Xia","Hengyi Cai","Suqi Cheng","Hengzhu Tang","Junfeng Wang","Dawei Yin"],"pdf_url":"https://arxiv.org/pdf/2401.03177v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03175v1","updated":"2024-01-06T09:37:56Z","published":"2024-01-06T09:37:56Z","title":"Part-of-Speech Tagger for Bodo Language using Deep Learning approach","summary":"  Language Processing systems such as Part-of-speech tagging, Named entity\nrecognition, Machine translation, Speech recognition, and Language modeling\n(LM) are well-studied in high-resource languages. Nevertheless, research on\nthese systems for several low-resource languages, including Bodo, Mizo,\nNagamese, and others, is either yet to commence or is in its nascent stages.\nLanguage model plays a vital role in the downstream tasks of modern NLP.\nExtensive studies are carried out on LMs for high-resource languages.\nNevertheless, languages such as Bodo, Rabha, and Mising continue to lack\ncoverage. In this study, we first present BodoBERT, a language model for the\nBodo language. To the best of our knowledge, this work is the first such effort\nto develop a language model for Bodo. Secondly, we present an ensemble DL-based\nPOS tagging model for Bodo. The POS tagging model is based on combinations of\nBiLSTM with CRF and stacked embedding of BodoBERT with BytePairEmbeddings. We\ncover several language models in the experiment to see how well they work in\nPOS tagging tasks. The best-performing model achieves an F1 score of 0.8041. A\ncomparative experiment was also conducted on Assamese POS taggers, considering\nthat the language is spoken in the same region as Bodo.\n","authors":["Dhrubajyoti Pathak","Sanjib Narzary","Sukumar Nandi","Bidisha Som"],"pdf_url":"https://arxiv.org/pdf/2401.03175v1.pdf","comment":"Accepted to Natural Language Engineering"},{"id":"http://arxiv.org/abs/2401.03158v1","updated":"2024-01-06T08:28:20Z","published":"2024-01-06T08:28:20Z","title":"Quartet Logic: A Four-Step Reasoning (QLFR) framework for advancing\n  Short Text Classification","summary":"  Short Text Classification (STC) is crucial for processing and comprehending\nthe brief but substantial content prevalent on contemporary digital platforms.\nThe STC encounters difficulties in grasping semantic and syntactic intricacies,\nan issue that is apparent in traditional pre-trained language models. Although\nGraph Convolutional Networks enhance performance by integrating external\nknowledge bases, these methods are limited by the quality and extent of the\nknowledge applied. Recently, the emergence of Large Language Models (LLMs) and\nChain-of-Thought (CoT) has significantly improved the performance of complex\nreasoning tasks. However, some studies have highlighted the limitations of\ntheir application in fundamental NLP tasks. Consequently, this study sought to\nemploy CoT to investigate the capabilities of LLMs in STC tasks. This study\nintroduces Quartet Logic: A Four-Step Reasoning (QLFR) framework. This\nframework primarily incorporates Syntactic and Semantic Enrichment CoT,\neffectively decomposing the STC task into four distinct steps: (i) essential\nconcept identification, (ii) common-sense knowledge retrieval, (iii) text\nrewriting, and (iv) classification. This elicits the inherent knowledge and\nabilities of LLMs to address the challenges in STC. Surprisingly, we found that\nQLFR can also improve the performance of smaller models. Therefore, we\ndeveloped a CoT-Driven Multi-task learning (QLFR-CML) method to facilitate the\nknowledge transfer from LLMs to smaller models. Extensive experimentation\nacross six short-text benchmarks validated the efficacy of the proposed\nmethods. Notably, QLFR achieved state-of-the-art performance on all datasets,\nwith significant improvements, particularly on the Ohsumed and TagMyNews\ndatasets.\n","authors":["Hui Wu","Yuanben Zhang","Zhonghe Han","Yingyan Hou","Lei Wang","Siye Liu","Qihang Gong","Yunping Ge"],"pdf_url":"https://arxiv.org/pdf/2401.03158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.00378v2","updated":"2024-01-06T06:47:33Z","published":"2023-09-01T10:27:04Z","title":"Long-Term Ad Memorability: Understanding and Generating Memorable Ads","summary":"  Marketers spend billions of dollars on advertisements but to what end? At the\ntime of purchase, if customers cannot recognize the brand for which they saw an\nad, the money spent on the ad is essentially wasted. Despite its importance in\nmarketing, until now, there has been no study on the memorability of ads in the\nML literature. Most studies have been conducted on short-term recall (<5 mins)\non specific content types like object and action videos. On the other hand, the\nadvertising industry only cares about long-term memorability, and ads are\nalmost always highly multimodal, depicting a story through its different\nmodalities. With this motivation, we release the first large-scale memorability\ndataset, LAMDBA, consisting of 1749 participants and 2205 ads covering 276\nbrands. Running statistical tests over different participant subpopulations and\nad types, we find many interesting insights into what makes an ad memorable.\nFor e.g., we find that brands that use commercials with fast-moving scenes are\nmore memorable than those with slower scenes (p=8e-10) and that people who use\nad-blockers remember fewer ads than those who don't (p=5e-3). Next, to simulate\nthe memorability of marketing materials for a particular audience, we present a\nnovel model, Henry, trained to leverage real-world knowledge of LLMs and visual\nknowledge to predict the memorability. We test Henry on all the prominent\nmemorability datasets in literature (both images and videos) and achieve\nstate-of-the-art performance across all of them. Henry shows strong\ngeneralization showing better results in 0-shot on unseen datasets. Next, we\npropose the task of memorable ad generation and release a large-scale ad\ndataset, UltraLAMBDA, consisting of 4 million ads with their Henry-assigned\nmemorability scores. We show that aligning Henry to generate memorable content\nimproves memorability scores by more than 25%.\n","authors":["Harini S I","Somesh Singh","Yaman K Singla","Aanisha Bhattacharyya","Veeky Baths","Changyou Chen","Rajiv Ratn Shah","Balaji Krishnamurthy"],"pdf_url":"https://arxiv.org/pdf/2309.00378v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03129v1","updated":"2024-01-06T05:34:09Z","published":"2024-01-06T05:34:09Z","title":"Examining Forgetting in Continual Pre-training of Aligned Large Language\n  Models","summary":"  Recent advances in Large Language Models (LLMs) have exhibited remarkable\nproficiency across various tasks. Given the potent applications of LLMs in\nnumerous fields, there has been a surge in LLM development. In developing LLMs,\na common practice involves continual pre-training on previously fine-tuned\nmodels. However, this can lead to catastrophic forgetting. In our work, we\ninvestigate the phenomenon of forgetting that occurs during continual\npre-training on an existing fine-tuned LLM. We evaluate the impact of\ncontinuous pre-training on the fine-tuned LLM across various dimensions,\nincluding output format, knowledge, and reliability. Experiment results\nhighlight the non-trivial challenge of addressing catastrophic forgetting\nduring continual pre-training, especially the repetition issue.\n","authors":["Chen-An Li","Hung-Yi Lee"],"pdf_url":"https://arxiv.org/pdf/2401.03129v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2401.01053v2","updated":"2024-01-06T04:48:10Z","published":"2024-01-02T06:24:13Z","title":"Cheetah: Natural Language Generation for 517 African Languages","summary":"  Low-resource African languages pose unique challenges for natural language\nprocessing (NLP) tasks, including natural language generation (NLG). In this\npaper, we develop Cheetah, a massively multilingual NLG language model for\nAfrican languages. Cheetah supports 517 African languages and language\nvarieties, allowing us to address the scarcity of NLG resources and provide a\nsolution to foster linguistic diversity. We demonstrate the effectiveness of\nCheetah through comprehensive evaluations across seven generation downstream\ntasks. In five of the seven tasks, Cheetah significantly outperforms other\nmodels, showcasing its remarkable performance for generating coherent and\ncontextually appropriate text in a wide range of African languages. We\nadditionally conduct a detailed human evaluation to delve deeper into the\nlinguistic capabilities of Cheetah. The introduction of Cheetah has\nfar-reaching benefits for linguistic diversity. By leveraging pretrained models\nand adapting them to specific languages, our approach facilitates the\ndevelopment of practical NLG applications for African communities. The findings\nof this study contribute to advancing NLP research in low-resource settings,\nenabling greater accessibility and inclusion for African languages in a rapidly\nexpanding digital landscape. We publicly release our models for research.\n","authors":["Ife Adebara","AbdelRahim Elmadany","Muhammad Abdul-Mageed"],"pdf_url":"https://arxiv.org/pdf/2401.01053v2.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2204.13492v4","updated":"2024-01-06T23:30:03Z","published":"2022-04-28T13:35:14Z","title":"Representation Recycling for Streaming Video Analysis","summary":"  We present StreamDEQ, a method that aims to infer frame-wise representations\non videos with minimal per-frame computation. Conventional deep networks do\nfeature extraction from scratch at each frame in the absence of ad-hoc\nsolutions. We instead aim to build streaming recognition models that can\nnatively exploit temporal smoothness between consecutive video frames. We\nobserve that the recently emerging implicit layer models provide a convenient\nfoundation to construct such models, as they define representations as the\nfixed-points of shallow networks, which need to be estimated using iterative\nmethods. Our main insight is to distribute the inference iterations over the\ntemporal axis by using the most recent representation as a starting point at\neach frame. This scheme effectively recycles the recent inference computations\nand greatly reduces the needed processing time. Through extensive experimental\nanalysis, we show that StreamDEQ is able to recover near-optimal\nrepresentations in a few frames' time and maintain an up-to-date representation\nthroughout the video duration. Our experiments on video semantic segmentation,\nvideo object detection, and human pose estimation in videos show that StreamDEQ\nachieves on-par accuracy with the baseline while being more than 2-4x faster.\n","authors":["Can Ufuk Ertenli","Ramazan Gokberk Cinbis","Emre Akbas"],"pdf_url":"https://arxiv.org/pdf/2204.13492v4.pdf","comment":"v3: ECCV2022 paper. This version: extended version under review at\n  TPAMI"},{"id":"http://arxiv.org/abs/2401.03317v1","updated":"2024-01-06T22:32:34Z","published":"2024-01-06T22:32:34Z","title":"Spatiotemporally adaptive compression for scientific dataset with\n  feature preservation -- a case study on simulation data with extreme climate\n  events analysis","summary":"  Scientific discoveries are increasingly constrained by limited storage space\nand I/O capacities. For time-series simulations and experiments, their data\noften need to be decimated over timesteps to accommodate storage and I/O\nlimitations. In this paper, we propose a technique that addresses storage costs\nwhile improving post-analysis accuracy through spatiotemporal adaptive,\nerror-controlled lossy compression. We investigate the trade-off between data\nprecision and temporal output rates, revealing that reducing data precision and\nincreasing timestep frequency lead to more accurate analysis outcomes.\nAdditionally, we integrate spatiotemporal feature detection with data\ncompression and demonstrate that performing adaptive error-bounded compression\nin higher dimensional space enables greater compression ratios, leveraging the\nerror propagation theory of a transformation-based compressor.\n  To evaluate our approach, we conduct experiments using the well-known E3SM\nclimate simulation code and apply our method to compress variables used for\ncyclone tracking. Our results show a significant reduction in storage size\nwhile enhancing the quality of cyclone tracking analysis, both quantitatively\nand qualitatively, in comparison to the prevalent timestep decimation approach.\nCompared to three state-of-the-art lossy compressors lacking feature\npreservation capabilities, our adaptive compression framework improves\nperfectly matched cases in TC tracking by 26.4-51.3% at medium compression\nratios and by 77.3-571.1% at large compression ratios, with a merely 5-11%\ncomputational overhead.\n","authors":["Qian Gong","Chengzhu Zhang","Xin Liang","Viktor Reshniak","Jieyang Chen","Anand Rangarajan","Sanjay Ranka","Nicolas Vidal","Lipeng Wan","Paul Ullrich","Norbert Podhorszki","Robert Jacob","Scott Klasky"],"pdf_url":"https://arxiv.org/pdf/2401.03317v1.pdf","comment":"10 pages, 13 figures, 2023 IEEE International Conference on e-Science\n  and Grid Computing"},{"id":"http://arxiv.org/abs/2401.03312v1","updated":"2024-01-06T21:47:49Z","published":"2024-01-06T21:47:49Z","title":"Exploiting Data Hierarchy as a New Modality for Contrastive Learning","summary":"  This work investigates how hierarchically structured data can help neural\nnetworks learn conceptual representations of cathedrals. The underlying\nWikiScenes dataset provides a spatially organized hierarchical structure of\ncathedral components. We propose a novel hierarchical contrastive training\napproach that leverages a triplet margin loss to represent the data's spatial\nhierarchy in the encoder's latent space. As such, the proposed approach\ninvestigates if the dataset structure provides valuable information for\nself-supervised learning. We apply t-SNE to visualize the resultant latent\nspace and evaluate the proposed approach by comparing it with other\ndataset-specific contrastive learning methods using a common downstream\nclassification task. The proposed method outperforms the comparable\nweakly-supervised and baseline methods. Our findings suggest that dataset\nstructure is a valuable modality for weakly-supervised learning.\n","authors":["Arjun Bhalla","Daniel Levenson","Jan Bernhard","Anton Abilov"],"pdf_url":"https://arxiv.org/pdf/2401.03312v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03302v1","updated":"2024-01-06T20:53:02Z","published":"2024-01-06T20:53:02Z","title":"Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical\n  Images Using YOLOv8 and DeiT","summary":"  In the field of medical sciences, reliable detection and classification of\nbrain tumors from images remains a formidable challenge due to the rarity of\ntumors within the population of patients. Therefore, the ability to detect\ntumors in anomaly scenarios is paramount for ensuring timely interventions and\nimproved patient outcomes. This study addresses the issue by leveraging deep\nlearning (DL) techniques to detect and classify brain tumors in challenging\nsituations. The curated data set from the National Brain Mapping Lab (NBML)\ncomprises 81 patients, including 30 Tumor cases and 51 Normal cases. The\ndetection and classification pipelines are separated into two consecutive\ntasks. The detection phase involved comprehensive data analysis and\npre-processing to modify the number of image samples and the number of patients\nof each class to anomaly distribution (9 Normal per 1 Tumor) to comply with\nreal world scenarios. Next, in addition to common evaluation metrics for the\ntesting, we employed a novel performance evaluation method called Patient to\nPatient (PTP), focusing on the realistic evaluation of the model. In the\ndetection phase, we fine-tuned a YOLOv8n detection model to detect the tumor\nregion. Subsequent testing and evaluation yielded competitive performance both\nin Common Evaluation Metrics and PTP metrics. Furthermore, using the Data\nEfficient Image Transformer (DeiT) module, we distilled a Vision Transformer\n(ViT) model from a fine-tuned ResNet152 as a teacher in the classification\nphase. This approach demonstrates promising strides in reliable tumor detection\nand classification, offering potential advancements in tumor diagnosis for\nreal-world medical imaging scenarios.\n","authors":["Seyed Mohammad Hossein Hashemi","Leila Safari","Amirhossein Dadashzade Taromi"],"pdf_url":"https://arxiv.org/pdf/2401.03302v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03298v1","updated":"2024-01-06T20:39:20Z","published":"2024-01-06T20:39:20Z","title":"Multi-View 3D Instance Segmentation of Structural Anomalies for Enhanced\n  Structural Inspection of Concrete Bridges","summary":"  For effective structural damage assessment, the instances of damages need to\nbe localized in the world of a 3D model. Due to a lack of data, the detection\nof structural anomalies can currently not be directly learned and performed in\n3D space. In this work, a three-stage approach is presented, which uses the\ngood performance of detection models on image level to segment instances of\nanomalies in the 3D space. In the detection stage, semantic segmentation\npredictions are produced on image level. The mapping stage transfers the\nimage-level prediction onto the respective point cloud. In the extraction\nstage, 3D anomaly instances are extracted from the segmented point cloud. Cloud\ncontraction is used to transform cracks into their medial axis representation.\nFor areal anomalies the bounding polygon is extracted by means of alpha shapes.\nThe approach covers the classes crack, spalling, and corrosion and the three\nimage-level segmentation models TopoCrack, nnU-Net, and DetectionHMA are\ncompared. Granted a localization tolerance of 4cm, IoUs of over 90% can be\nachieved for crack and corrosion and 41% for spalling, which appears to be a\nspecifically challenging class. Detection on instance-level measured in AP is\nabout 45% for crack and spalling and 73% for corrosion.\n","authors":["Christian Benz","Volker Rodehorst"],"pdf_url":"https://arxiv.org/pdf/2401.03298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03275v1","updated":"2024-01-06T18:28:01Z","published":"2024-01-06T18:28:01Z","title":"Real Time Human Detection by Unmanned Aerial Vehicles","summary":"  One of the most important problems in computer vision and remote sensing is\nobject detection, which identifies particular categories of diverse things in\npictures. Two crucial data sources for public security are the thermal infrared\n(TIR) remote sensing multi-scenario photos and videos produced by unmanned\naerial vehicles (UAVs). Due to the small scale of the target, complex scene\ninformation, low resolution relative to the viewable videos, and dearth of\npublicly available labeled datasets and training models, their object detection\nprocedure is still difficult. A UAV TIR object detection framework for pictures\nand videos is suggested in this study. The Forward-looking Infrared (FLIR)\ncameras used to gather ground-based TIR photos and videos are used to create\nthe ``You Only Look Once'' (YOLO) model, which is based on CNN architecture.\nResults indicated that in the validating task, detecting human object had an\naverage precision at IOU (Intersection over Union) = 0.5, which was 72.5\\%,\nusing YOLOv7 (YOLO version 7) state of the art model \\cite{1}, while the\ndetection speed around 161 frames per second (FPS/second). The usefulness of\nthe YOLO architecture is demonstrated in the application, which evaluates the\ncross-detection performance of people in UAV TIR videos under a YOLOv7 model in\nterms of the various UAVs' observation angles. The qualitative and quantitative\nevaluation of object detection from TIR pictures and videos using deep-learning\nmodels is supported favorably by this work.\n","authors":["Walid Guettala","Ali Sayah","Laid Kahloul","Ahmed Tibermacine"],"pdf_url":"https://arxiv.org/pdf/2401.03275v1.pdf","comment":"6 pages, 5 figures, 2 tables"},{"id":"http://arxiv.org/abs/2401.03271v1","updated":"2024-01-06T18:17:55Z","published":"2024-01-06T18:17:55Z","title":"Analysis and Validation of Image Search Engines in Histopathology","summary":"  Searching for similar images in archives of histology and histopathology\nimages is a crucial task that may aid in patient matching for various purposes,\nranging from triaging and diagnosis to prognosis and prediction. Whole slide\nimages (WSIs) are highly detailed digital representations of tissue specimens\nmounted on glass slides. Matching WSI to WSI can serve as the critical method\nfor patient matching. In this paper, we report extensive analysis and\nvalidation of four search methods bag of visual words (BoVW), Yottixel, SISH,\nRetCCL, and some of their potential variants. We analyze their algorithms and\nstructures and assess their performance. For this evaluation, we utilized four\ninternal datasets ($1269$ patients) and three public datasets ($1207$\npatients), totaling more than $200,000$ patches from $38$ different\nclasses/subtypes across five primary sites. Certain search engines, for\nexample, BoVW, exhibit notable efficiency and speed but suffer from low\naccuracy. Conversely, search engines like Yottixel demonstrate efficiency and\nspeed, providing moderately accurate results. Recent proposals, including SISH,\ndisplay inefficiency and yield inconsistent outcomes, while alternatives like\nRetCCL prove inadequate in both accuracy and efficiency. Further research is\nimperative to address the dual aspects of accuracy and minimal storage\nrequirements in histopathological image search.\n","authors":["Isaiah Lahr","Saghir Alfasly","Peyman Nejat","Jibran Khan","Luke Kottom","Vaishnavi Kumbhar","Areej Alsaafin","Abubakr Shafique","Sobhan Hemati","Ghazal Alabtah","Nneka Comfere","Dennis Murphee","Aaron Mangold","Saba Yasir","Chady Meroueh","Lisa Boardman","Vijay H. Shah","Joaquin J. Garcia","H. R. Tizhoosh"],"pdf_url":"https://arxiv.org/pdf/2401.03271v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03267v1","updated":"2024-01-06T18:05:06Z","published":"2024-01-06T18:05:06Z","title":"Autonomous Navigation in Complex Environments","summary":"  This paper explores the application of CNN-DNN network fusion to construct a\nrobot navigation controller within a simulated environment. The simulated\nenvironment is constructed to model a subterranean rescue situation, such that\nan autonomous agent is tasked with finding a goal within an unknown cavernous\nsystem. Imitation learning is used to train the control algorithm to use LiDAR\nand camera data to navigate the space and find the goal. The trained model is\nthen tested for robustness using Monte-Carlo.\n","authors":["Andrew Gerstenslager","Jomol Lewis","Liam McKenna","Poorva Patel"],"pdf_url":"https://arxiv.org/pdf/2401.03267v1.pdf","comment":"7 pages, 3 figures, independent paper"},{"id":"http://arxiv.org/abs/2401.03262v1","updated":"2024-01-06T17:36:13Z","published":"2024-01-06T17:36:13Z","title":"Group Activity Recognition using Unreliable Tracked Pose","summary":"  Group activity recognition in video is a complex task due to the need for a\nmodel to recognise the actions of all individuals in the video and their\ncomplex interactions. Recent studies propose that optimal performance is\nachieved by individually tracking each person and subsequently inputting the\nsequence of poses or cropped images/optical flow into a model. This helps the\nmodel to recognise what actions each person is performing before they are\nmerged to arrive at the group action class. However, all previous models are\nhighly reliant on high quality tracking and have only been evaluated using\nground truth tracking information. In practice it is almost impossible to\nachieve highly reliable tracking information for all individuals in a group\nactivity video. We introduce an innovative deep learning-based group activity\nrecognition approach called Rendered Pose based Group Activity Recognition\nSystem (RePGARS) which is designed to be tolerant of unreliable tracking and\npose information. Experimental results confirm that RePGARS outperforms all\nexisting group activity recognition algorithms tested which do not use ground\ntruth detection and tracking information.\n","authors":["Haritha Thilakarathne","Aiden Nibali","Zhen He","Stuart Morgan"],"pdf_url":"https://arxiv.org/pdf/2401.03262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03257v1","updated":"2024-01-06T16:54:02Z","published":"2024-01-06T16:54:02Z","title":"RustNeRF: Robust Neural Radiance Field with Low-Quality Images","summary":"  Recent work on Neural Radiance Fields (NeRF) exploits multi-view 3D\nconsistency, achieving impressive results in 3D scene modeling and\nhigh-fidelity novel-view synthesis. However, there are limitations. First,\nexisting methods assume enough high-quality images are available for training\nthe NeRF model, ignoring real-world image degradation. Second, previous methods\nstruggle with ambiguity in the training set due to unmodeled inconsistencies\namong different views. In this work, we present RustNeRF for real-world\nhigh-quality NeRF. To improve NeRF's robustness under real-world inputs, we\ntrain a 3D-aware preprocessing network that incorporates real-world degradation\nmodeling. We propose a novel implicit multi-view guidance to address\ninformation loss during image degradation and restoration. Extensive\nexperiments demonstrate RustNeRF's advantages over existing approaches under\nreal-world degradation. The code will be released.\n","authors":["Mengfei Li","Ming Lu","Xiaofang Li","Shanghang Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.03257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03253v1","updated":"2024-01-06T16:33:39Z","published":"2024-01-06T16:33:39Z","title":"Large Language Models as Visual Cross-Domain Learners","summary":"  Recent advances achieved by deep learning models rely on the independent and\nidentically distributed assumption, hindering their applications in real-world\nscenarios with domain shifts. To address the above issues, cross-domain\nlearning aims at extracting domain-invariant knowledge to reduce the domain\nshift between training and testing data. However, in visual cross-domain\nlearning, traditional methods concentrate solely on the image modality,\nneglecting the use of the text modality to alleviate the domain shift. In this\nwork, we propose Large Language models as Visual cross-dOmain learners (LLaVO).\nLLaVO uses vision-language models to convert images into detailed textual\ndescriptions. A large language model is then finetuned on textual descriptions\nof the source/target domain generated by a designed instruction template.\nExtensive experimental results on various cross-domain tasks under the domain\ngeneralization and unsupervised domain adaptation settings have demonstrated\nthe effectiveness of the proposed method.\n","authors":["Shuhao Chen","Yulong Zhang","Weisen Jiang","Jiangang Lu","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.03253v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03250v1","updated":"2024-01-06T16:17:58Z","published":"2024-01-06T16:17:58Z","title":"Interpersonal Relationship Analysis with Dyadic EEG Signals via Learning\n  Spatial-Temporal Patterns","summary":"  Interpersonal relationship quality is pivotal in social and occupational\ncontexts. Existing analysis of interpersonal relationships mostly rely on\nsubjective self-reports, whereas objective quantification remains challenging.\nIn this paper, we propose a novel social relationship analysis framework using\nspatio-temporal patterns derived from dyadic EEG signals, which can be applied\nto quantitatively measure team cooperation in corporate team building, and\nevaluate interpersonal dynamics between therapists and patients in psychiatric\ntherapy. First, we constructed a dyadic-EEG dataset from 72 pairs of\nparticipants with two relationships (stranger or friend) when watching\nemotional videos simultaneously. Then we proposed a deep neural network on\ndyadic-subject EEG signals, in which we combine the dynamic graph convolutional\nneural network for characterizing the interpersonal relationships among the EEG\nchannels and 1-dimension convolution for extracting the information from the\ntime sequence. To obtain the feature vectors from two EEG recordings that well\nrepresent the relationship of two subjects, we integrate deep canonical\ncorrelation analysis and triplet loss for training the network. Experimental\nresults show that the social relationship type (stranger or friend) between two\nindividuals can be effectively identified through their EEG data.\n","authors":["Wenqi Ji","Fang liu","Xinxin Du","Niqi Liu","Chao Zhou","Mingjin Yu","Guozhen Zhao","Yong-Jin Liu"],"pdf_url":"https://arxiv.org/pdf/2401.03250v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03221v1","updated":"2024-01-06T14:12:16Z","published":"2024-01-06T14:12:16Z","title":"MirrorDiffusion: Stabilizing Diffusion Process in Zero-shot Image\n  Translation by Prompts Redescription and Beyond","summary":"  Recently, text-to-image diffusion models become a new paradigm in image\nprocessing fields, including content generation, image restoration and\nimage-to-image translation. Given a target prompt, Denoising Diffusion\nProbabilistic Models (DDPM) are able to generate realistic yet eligible images.\nWith this appealing property, the image translation task has the potential to\nbe free from target image samples for supervision. By using a target text\nprompt for domain adaption, the diffusion model is able to implement zero-shot\nimage-to-image translation advantageously. However, the sampling and inversion\nprocesses of DDPM are stochastic, and thus the inversion process often fail to\nreconstruct the input content. Specifically, the displacement effect will\ngradually accumulated during the diffusion and inversion processes, which led\nto the reconstructed results deviating from the source domain. To make\nreconstruction explicit, we propose a prompt redescription strategy to realize\na mirror effect between the source and reconstructed image in the diffusion\nmodel (MirrorDiffusion). More specifically, a prompt redescription mechanism is\ninvestigated to align the text prompts with latent code at each time step of\nthe Denoising Diffusion Implicit Models (DDIM) inversion to pursue a\nstructure-preserving reconstruction. With the revised DDIM inversion,\nMirrorDiffusion is able to realize accurate zero-shot image translation by\nediting optimized text prompts and latent code. Extensive experiments\ndemonstrate that MirrorDiffusion achieves superior performance over the\nstate-of-the-art methods on zero-shot image translation benchmarks by clear\nmargins and practical model stability.\n","authors":["Yupei Lin","Xiaoyu Xian","Yukai Shi","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2401.03221v1.pdf","comment":"A prompt re-description strategy is proposed for stabilizing the\n  diffusion model in image-to-image translation. Code and dataset page:\n  https://mirrordiffusion.github.io/"},{"id":"http://arxiv.org/abs/2401.03220v1","updated":"2024-01-06T14:06:29Z","published":"2024-01-06T14:06:29Z","title":"MetaISP -- Exploiting Global Scene Structure for Accurate Multi-Device\n  Color Rendition","summary":"  Image signal processors (ISPs) are historically grown legacy software systems\nfor reconstructing color images from noisy raw sensor measurements. Each\nsmartphone manufacturer has developed its ISPs with its own characteristic\nheuristics for improving the color rendition, for example, skin tones and other\nvisually essential colors. The recent interest in replacing the historically\ngrown ISP systems with deep-learned pipelines to match DSLR's image quality\nimproves structural features in the image. However, these works ignore the\nsuperior color processing based on semantic scene analysis that distinguishes\nmobile phone ISPs from DSLRs. Here, we present MetaISP, a single model designed\nto learn how to translate between the color and local contrast characteristics\nof different devices. MetaISP takes the RAW image from device A as input and\ntranslates it to RGB images that inherit the appearance characteristics of\ndevices A, B, and C. We achieve this result by employing a lightweight deep\nlearning technique that conditions its output appearance based on the device of\ninterest. In this approach, we leverage novel attention mechanisms inspired by\ncross-covariance to learn global scene semantics. Additionally, we use the\nmetadata that typically accompanies RAW images and estimate scene illuminants\nwhen they are unavailable.\n","authors":["Matheus Souza","Wolfgang Heidrich"],"pdf_url":"https://arxiv.org/pdf/2401.03220v1.pdf","comment":"VMV 2023, Project page: https://www.github.com/vccimaging/MetaISP"},{"id":"http://arxiv.org/abs/2312.06164v2","updated":"2024-01-06T13:35:07Z","published":"2023-12-11T07:09:32Z","title":"Implicit Shape Modeling for Anatomical Structure Refinement of\n  Volumetric Medical Images","summary":"  Shape modeling of volumetric data is essential for medical image analysis and\ncomputer-aided intervention. In practice, automated shape reconstruction cannot\nalways achieve satisfactory results due to limited image resolution and a lack\nof sufficiently detailed shape priors used as constraints. In this paper, a\nunified framework is proposed for 3D shape modelling and segmentation\nrefinement based on implicit neural networks. To learn a sharable shape prior\nfrom different instances within the same category during training, physical\ndetails of volumetric data are firstly used to construct Physical-Informed\nContinuous Coordinate Transform (PICCT) for implicit shape modeling. For\nimproved shape representation, implicit shape constraints based on Signed\nDistance Function (SDF) are used for both instances and latent templates. For\ninference, a Template Interaction Module (TIM) is proposed to refine 3D shapes\nproduced by Convolutional Neural Networks (CNNs) via deforming deep implicit\ntemplates with latent codes. Experimental results on validation datasets\ninvolving liver, pancreas and lung segmentation demonstrate the superiority of\nour approach in shape refinement and reconstruction. The Chamfer Distance/Earth\nMover's Distance achieved by the proposed method are 0.232/0.087 for the Liver\ndataset, 0.128/0.069 for the Pancreas dataset, and 0.417/0.100 for the Lung\nLobe dataset, respectively.\n","authors":["Minghui Zhang","Hanxiao Zhang","Xin You","Guang-Zhong Yang","Yun Gu"],"pdf_url":"https://arxiv.org/pdf/2312.06164v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.09782v2","updated":"2024-01-06T12:51:00Z","published":"2022-11-17T18:54:35Z","title":"Assessing Neural Network Robustness via Adversarial Pivotal Tuning","summary":"  The robustness of image classifiers is essential to their deployment in the\nreal world. The ability to assess this resilience to manipulations or\ndeviations from the training data is thus crucial. These modifications have\ntraditionally consisted of minimal changes that still manage to fool\nclassifiers, and modern approaches are increasingly robust to them. Semantic\nmanipulations that modify elements of an image in meaningful ways have thus\ngained traction for this purpose. However, they have primarily been limited to\nstyle, color, or attribute changes. While expressive, these manipulations do\nnot make use of the full capabilities of a pretrained generative model. In this\nwork, we aim to bridge this gap. We show how a pretrained image generator can\nbe used to semantically manipulate images in a detailed, diverse, and\nphotorealistic way while still preserving the class of the original image.\nInspired by recent GAN-based image inversion methods, we propose a method\ncalled Adversarial Pivotal Tuning (APT). Given an image, APT first finds a\npivot latent space input that reconstructs the image using a pretrained\ngenerator. It then adjusts the generator's weights to create small yet semantic\nmanipulations in order to fool a pretrained classifier. APT preserves the full\nexpressive editing capabilities of the generative model. We demonstrate that\nAPT is capable of a wide range of class-preserving semantic image manipulations\nthat fool a variety of pretrained classifiers. Finally, we show that\nclassifiers that are robust to other benchmarks are not robust to APT\nmanipulations and suggest a method to improve them. Code available at:\nhttps://captaine.github.io/apt/\n","authors":["Peter Ebert Christensen","Vésteinn Snæbjarnarson","Andrea Dittadi","Serge Belongie","Sagie Benaim"],"pdf_url":"https://arxiv.org/pdf/2211.09782v2.pdf","comment":"Major changes include new experiments in Table 1 on page 5 and Table\n  2-4 on page 6, new figure 5 on page 8. Paper accepted at WACV (oral)"},{"id":"http://arxiv.org/abs/2304.11906v3","updated":"2024-01-06T12:36:04Z","published":"2023-04-24T08:29:45Z","title":"Transformer-based stereo-aware 3D object detection from binocular images","summary":"  Transformers have shown promising progress in various visual object detection\ntasks, including monocular 2D/3D detection and surround-view 3D detection. More\nimportantly, the attention mechanism in the Transformer model and the image\ncorrespondence in binocular stereo are both similarity-based. However, directly\napplying existing Transformer-based detectors to binocular stereo 3D object\ndetection leads to slow convergence and significant precision drops. We argue\nthat a key cause of this defect is that existing Transformers ignore the\nstereo-specific image correspondence information. In this paper, we explore the\nmodel design of Transformers in binocular 3D object detection, focusing\nparticularly on extracting and encoding the task-specific image correspondence\ninformation. To achieve this goal, we present TS3D, a Transformer-based\nStereo-aware 3D object detector. In the TS3D, a Disparity-Aware Positional\nEncoding (DAPE) module is proposed to embed the image correspondence\ninformation into stereo features. The correspondence is encoded as normalized\nsub-pixel-level disparity and is used in conjunction with sinusoidal 2D\npositional encoding to provide the 3D location information of the scene. To\nextract enriched multi-scale stereo features, we propose a Stereo Preserving\nFeature Pyramid Network (SPFPN). The SPFPN is designed to preserve the\ncorrespondence information while fusing intra-scale and aggregating cross-scale\nstereo features. Our proposed TS3D achieves a 41.29% Moderate Car detection\naverage precision on the KITTI test set and takes 88 ms to detect objects from\neach binocular image pair. It is competitive with advanced counterparts in\nterms of both precision and inference speed.\n","authors":["Hanqing Sun","Yanwei Pang","Jiale Cao","Jin Xie","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2304.11906v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03203v1","updated":"2024-01-06T12:32:25Z","published":"2024-01-06T12:32:25Z","title":"Hi-Map: Hierarchical Factorized Radiance Field for High-Fidelity\n  Monocular Dense Mapping","summary":"  In this paper, we introduce Hi-Map, a novel monocular dense mapping approach\nbased on Neural Radiance Field (NeRF). Hi-Map is exceptional in its capacity to\nachieve efficient and high-fidelity mapping using only posed RGB inputs. Our\nmethod eliminates the need for external depth priors derived from e.g., a depth\nestimation model. Our key idea is to represent the scene as a hierarchical\nfeature grid that encodes the radiance and then factorizes it into feature\nplanes and vectors. As such, the scene representation becomes simpler and more\ngeneralizable for fast and smooth convergence on new observations. This allows\nfor efficient computation while alleviating noise patterns by reducing the\ncomplexity of the scene representation. Buttressed by the hierarchical\nfactorized representation, we leverage the Sign Distance Field (SDF) as a proxy\nof rendering for inferring the volume density, demonstrating high mapping\nfidelity. Moreover, we introduce a dual-path encoding strategy to strengthen\nthe photometric cues and further boost the mapping quality, especially for the\ndistant and textureless regions. Extensive experiments demonstrate our method's\nsuperiority in geometric and textural accuracy over the state-of-the-art\nNeRF-based monocular mapping methods.\n","authors":["Tongyan Hua","Haotian Bai","Zidong Cao","Ming Liu","Dacheng Tao","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2401.03203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03201v1","updated":"2024-01-06T12:20:18Z","published":"2024-01-06T12:20:18Z","title":"3DMIT: 3D Multi-modal Instruction Tuning for Scene Understanding","summary":"  The remarkable potential of multi-modal large language models (MLLMs) in\ncomprehending both vision and language information has been widely\nacknowledged. However, the scarcity of 3D scenes-language pairs in comparison\nto their 2D counterparts, coupled with the inadequacy of existing approaches in\nunderstanding of 3D scenes by LLMs, poses a significant challenge. In response,\nwe collect and construct an extensive dataset comprising 75K\ninstruction-response pairs tailored for 3D scenes. This dataset addresses tasks\nrelated to 3D VQA, 3D grounding, and 3D conversation. To further enhance the\nintegration of 3D spatial information into LLMs, we introduce a novel and\nefficient prompt tuning paradigm, 3DMIT. This paradigm eliminates the alignment\nstage between 3D scenes and language and extends the instruction prompt with\nthe 3D modality information including the entire scene and segmented objects.\nWe evaluate the effectiveness of our method across diverse tasks in the 3D\nscene domain and find that our approach serves as a strategic means to enrich\nLLMs' comprehension of the 3D world. Our code is available at\nhttps://github.com/staymylove/3DMIT.\n","authors":["Zeju Li","Chao Zhang","Xiaoyan Wang","Ruilong Ren","Yifan Xu","Ruifei Ma","Xiangde Liu"],"pdf_url":"https://arxiv.org/pdf/2401.03201v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2401.03195v1","updated":"2024-01-06T11:37:20Z","published":"2024-01-06T11:37:20Z","title":"Efficient Bitrate Ladder Construction using Transfer Learning and\n  Spatio-Temporal Features","summary":"  Providing high-quality video with efficient bitrate is a main challenge in\nvideo industry. The traditional one-size-fits-all scheme for bitrate ladders is\ninefficient and reaching the best content-aware decision computationally\nimpractical due to extensive encodings required. To mitigate this, we propose a\nbitrate and complexity efficient bitrate ladder prediction method using\ntransfer learning and spatio-temporal features. We propose: (1) using feature\nmaps from well-known pre-trained DNNs to predict rate-quality behavior with\nlimited training data; and (2) improving highest quality rung efficiency by\npredicting minimum bitrate for top quality and using it for the top rung. The\nmethod tested on 102 video scenes demonstrates 94.1% reduction in complexity\nversus brute-force at 1.71% BD-Rate expense. Additionally, transfer learning\nwas thoroughly studied through four networks and ablation studies.\n","authors":["Ali Falahati","Mohammad Karim Safavi","Ardavan Elahi","Farhad Pakdaman","Moncef Gabbouj"],"pdf_url":"https://arxiv.org/pdf/2401.03195v1.pdf","comment":"7 pages, 9 figures, 7 tables"},{"id":"http://arxiv.org/abs/2401.03191v1","updated":"2024-01-06T10:56:36Z","published":"2024-01-06T10:56:36Z","title":"DistFormer: Enhancing Local and Global Features for Monocular Per-Object\n  Distance Estimation","summary":"  Accurate per-object distance estimation is crucial in safety-critical\napplications such as autonomous driving, surveillance, and robotics. Existing\napproaches rely on two scales: local information (i.e., the bounding box\nproportions) or global information, which encodes the semantics of the scene as\nwell as the spatial relations with neighboring objects. However, these\napproaches may struggle with long-range objects and in the presence of strong\nocclusions or unusual visual patterns. In this respect, our work aims to\nstrengthen both local and global cues. Our architecture -- named DistFormer --\nbuilds upon three major components acting jointly: i) a robust context encoder\nextracting fine-grained per-object representations; ii) a masked\nencoder-decoder module exploiting self-supervision to promote the learning of\nuseful per-object features; iii) a global refinement module that aggregates\nobject representations and computes a joint, spatially-consistent estimation.\nTo evaluate the effectiveness of DistFormer, we conduct experiments on the\nstandard KITTI dataset and the large-scale NuScenes and MOTSynth datasets. Such\ndatasets cover various indoor/outdoor environments, changing weather\nconditions, appearances, and camera viewpoints. Our comprehensive analysis\nshows that DistFormer outperforms existing methods. Moreover, we further delve\ninto its generalization capabilities, showing its regularization benefits in\nzero-shot synth-to-real transfer.\n","authors":["Aniello Panariello","Gianluca Mancusi","Fedy Haj Ali","Angelo Porrello","Simone Calderara","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2401.03191v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03190v1","updated":"2024-01-06T10:40:24Z","published":"2024-01-06T10:40:24Z","title":"MPN: Leveraging Multilingual Patch Neuron for Cross-lingual Model\n  Editing","summary":"  Large language models are known for encoding a vast amount of factual\nknowledge, but they often becomes outdated due to the ever-changing nature of\nexternal information. A promising solution to this challenge is the utilization\nof model editing methods to update the knowledge in an efficient manner.\nHowever, the majority of existing model editing techniques are limited to\nmonolingual frameworks, thus failing to address the crucial issue of\ncross-lingual knowledge synchronization for multilingual models. To tackle this\nproblem, we propose a simple yet effective method that trains multilingual\npatch neuron to store cross-lingual knowledge. It can be easily adapted to\nexisting approaches to enhance their cross-lingual editing capabilities. To\nevaluate our method, we conduct experiments using both the XNLI dataset and a\nself-constructed XFEVER dataset. Experimental results demonstrate that our\nproposed method achieves improved performance in cross-lingual editing tasks\nwithout requiring excessive modifications to the original methodology, thereby\nshowcasing its user-friendly characteristics. Codes will be released soon.\n","authors":["Nianwen Si","Hao Zhang","Weiqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.03190v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2401.03182v1","updated":"2024-01-06T09:58:09Z","published":"2024-01-06T09:58:09Z","title":"Distribution-aware Interactive Attention Network and Large-scale Cloud\n  Recognition Benchmark on FY-4A Satellite Image","summary":"  Accurate cloud recognition and warning are crucial for various applications,\nincluding in-flight support, weather forecasting, and climate research.\nHowever, recent deep learning algorithms have predominantly focused on\ndetecting cloud regions in satellite imagery, with insufficient attention to\nthe specificity required for accurate cloud recognition. This limitation\ninspired us to develop the novel FY-4A-Himawari-8 (FYH) dataset, which includes\nnine distinct cloud categories and uses precise domain adaptation methods to\nalign 70,419 image-label pairs in terms of projection, temporal resolution, and\nspatial resolution, thereby facilitating the training of supervised deep\nlearning networks. Given the complexity and diversity of cloud formations, we\nhave thoroughly analyzed the challenges inherent to cloud recognition tasks,\nexamining the intricate characteristics and distribution of the data. To\neffectively address these challenges, we designed a Distribution-aware\nInteractive-Attention Network (DIAnet), which preserves pixel-level details\nthrough a high-resolution branch and a parallel multi-resolution cross-branch.\nWe also integrated a distribution-aware loss (DAL) to mitigate the imbalance\nacross cloud categories. An Interactive Attention Module (IAM) further enhances\nthe robustness of feature extraction combined with spatial and channel\ninformation. Empirical evaluations on the FYH dataset demonstrate that our\nmethod outperforms other cloud recognition networks, achieving superior\nperformance in terms of mean Intersection over Union (mIoU). The code for\nimplementing DIAnet is available at https://github.com/icey-zhang/DIAnet.\n","authors":["Jiaqing Zhang","Jie Lei","Weiying Xie","Kai Jiang","Mingxiang Cao","Yunsong Li"],"pdf_url":"https://arxiv.org/pdf/2401.03182v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03179v1","updated":"2024-01-06T09:53:33Z","published":"2024-01-06T09:53:33Z","title":"Multimodal Informative ViT: Information Aggregation and Distribution for\n  Hyperspectral and LiDAR Classification","summary":"  In multimodal land cover classification (MLCC), a common challenge is the\nredundancy in data distribution, where irrelevant information from multiple\nmodalities can hinder the effective integration of their unique features. To\ntackle this, we introduce the Multimodal Informative Vit (MIVit), a system with\nan innovative information aggregate-distributing mechanism. This approach\nredefines redundancy levels and integrates performance-aware elements into the\nfused representation, facilitating the learning of semantics in both forward\nand backward directions. MIVit stands out by significantly reducing redundancy\nin the empirical distribution of each modality's separate and fused features.\nIt employs oriented attention fusion (OAF) for extracting shallow local\nfeatures across modalities in horizontal and vertical dimensions, and a\nTransformer feature extractor for extracting deep global features through\nlong-range attention. We also propose an information aggregation constraint\n(IAC) based on mutual information, designed to remove redundant information and\npreserve complementary information within embedded features. Additionally, the\ninformation distribution flow (IDF) in MIVit enhances performance-awareness by\ndistributing global classification information across different modalities'\nfeature maps. This architecture also addresses missing modality challenges with\nlightweight independent modality classifiers, reducing the computational load\ntypically associated with Transformers. Our results show that MIVit's\nbidirectional aggregate-distributing mechanism between modalities is highly\neffective, achieving an average overall accuracy of 95.56% across three\nmultimodal datasets. This performance surpasses current state-of-the-art\nmethods in MLCC. The code for MIVit is accessible at\nhttps://github.com/icey-zhang/MIViT.\n","authors":["Jiaqing Zhang","Jie Lei","Weiying Xie","Geng Yang","Daixun Li","Yunsong Li","Karim Seghouane"],"pdf_url":"https://arxiv.org/pdf/2401.03179v1.pdf","comment":null}],"Robotics":[{"id":"http://arxiv.org/abs/2401.03329v1","updated":"2024-01-06T23:23:02Z","published":"2024-01-06T23:23:02Z","title":"Designing a Socially Assistive Robot to Support Older Adults with Low\n  Vision","summary":"  Socially assistive robots (SARs) have shown great promise in supplementing\nand augmenting interventions to support the physical and mental well-being of\nolder adults. However, past work has not yet explored the potential of applying\nSAR to lower the barriers of long-term low vision rehabilitation (LVR)\ninterventions for older adults. In this work, we present a user-informed design\nprocess to validate the motivation and identify major design principles for\ndeveloping SAR for long-term LVR. To evaluate user-perceived usefulness and\nacceptance of SAR in this novel domain, we performed a two-phase study through\nuser surveys. First, a group (n=38) of older adults with LV completed a\nmailed-in survey. Next, a new group (n=13) of older adults with LV saw an\nin-clinic SAR demo and then completed the survey. The study participants\nreported that SARs would be useful, trustworthy, easy to use, and enjoyable\nwhile providing socio-emotional support to augment LVR interventions. The\nin-clinic demo group reported significantly more positive opinions of the SAR's\ncapabilities than did the baseline survey group that used mailed-in forms\nwithout the SAR demo.\n","authors":["Emily Zhou","Zhonghao Shi","Xiaoyang Qiao","Maja J Matarić","Ava K Bittner"],"pdf_url":"https://arxiv.org/pdf/2401.03329v1.pdf","comment":"Published in Social Robotics: 13th International Conference, ICSR\n  2021. Springer International Publishing"},{"id":"http://arxiv.org/abs/2401.03306v1","updated":"2024-01-06T21:04:31Z","published":"2024-01-06T21:04:31Z","title":"MOTO: Offline Pre-training to Online Fine-tuning for Model-based Robot\n  Learning","summary":"  We study the problem of offline pre-training and online fine-tuning for\nreinforcement learning from high-dimensional observations in the context of\nrealistic robot tasks. Recent offline model-free approaches successfully use\nonline fine-tuning to either improve the performance of the agent over the data\ncollection policy or adapt to novel tasks. At the same time, model-based RL\nalgorithms have achieved significant progress in sample efficiency and the\ncomplexity of the tasks they can solve, yet remain under-utilized in the\nfine-tuning setting. In this work, we argue that existing model-based offline\nRL methods are not suitable for offline-to-online fine-tuning in\nhigh-dimensional domains due to issues with distribution shifts, off-dynamics\ndata, and non-stationary rewards. We propose an on-policy model-based method\nthat can efficiently reuse prior data through model-based value expansion and\npolicy regularization, while preventing model exploitation by controlling\nepistemic uncertainty. We find that our approach successfully solves tasks from\nthe MetaWorld benchmark, as well as the Franka Kitchen robot manipulation\nenvironment completely from images. To the best of our knowledge, MOTO is the\nfirst method to solve this environment from pixels.\n","authors":["Rafael Rafailov","Kyle Hatch","Victor Kolev","John D. Martin","Mariano Phielipp","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2401.03306v1.pdf","comment":"This is an updated version of a manuscript that originally appeared\n  at CoRL 2023. The project website is here https://sites.google.com/view/mo2o"},{"id":"http://arxiv.org/abs/2401.03286v1","updated":"2024-01-06T19:21:44Z","published":"2024-01-06T19:21:44Z","title":"Theoretical Framework for the Optimization of Microphone Array\n  Configuration for Humanoid Robot Audition","summary":"  An important aspect of a humanoid robot is audition. Previous work has\npresented robot systems capable of sound localization and source segregation\nbased on microphone arrays with various configurations. However, no theoretical\nframework for the design of these arrays has been presented. In the current\npaper, a design framework is proposed based on a novel array quality measure.\nThe measure is based on the effective rank of a matrix composed of the\ngeneralized head related transfer functions (GHRTFs) that account for\nmicrophone positions other than the ears. The measure is shown to be\ntheoretically related to standard array performance measures such as\nbeamforming robustness and DOA estimation accuracy. Then, the measure is\napplied to produce sample designs of microphone arrays. Their performance is\ninvestigated numerically, verifying the advantages of array design based on the\nproposed theoretical framework.\n","authors":["Vladimir Tourbabin","Boaz Rafaely"],"pdf_url":"https://arxiv.org/pdf/2401.03286v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03267v1","updated":"2024-01-06T18:05:06Z","published":"2024-01-06T18:05:06Z","title":"Autonomous Navigation in Complex Environments","summary":"  This paper explores the application of CNN-DNN network fusion to construct a\nrobot navigation controller within a simulated environment. The simulated\nenvironment is constructed to model a subterranean rescue situation, such that\nan autonomous agent is tasked with finding a goal within an unknown cavernous\nsystem. Imitation learning is used to train the control algorithm to use LiDAR\nand camera data to navigate the space and find the goal. The trained model is\nthen tested for robustness using Monte-Carlo.\n","authors":["Andrew Gerstenslager","Jomol Lewis","Liam McKenna","Poorva Patel"],"pdf_url":"https://arxiv.org/pdf/2401.03267v1.pdf","comment":"7 pages, 3 figures, independent paper"},{"id":"http://arxiv.org/abs/2401.03236v1","updated":"2024-01-06T15:11:14Z","published":"2024-01-06T15:11:14Z","title":"Challenges of Data-Driven Simulation of Diverse and Consistent Human\n  Driving Behaviors","summary":"  Building simulation environments for developing and testing autonomous\nvehicles necessitates that the simulators accurately model the statistical\nrealism of the real-world environment, including the interaction with other\nvehicles driven by human drivers. To address this requirement, an accurate\nhuman behavior model is essential to incorporate the diversity and consistency\nof human driving behavior. We propose a mathematical framework for designing a\ndata-driven simulation model that simulates human driving behavior more\nrealistically than the currently used physics-based simulation models.\nExperiments conducted using the NGSIM dataset validate our hypothesis regarding\nthe necessity of considering the complexity, diversity, and consistency of\nhuman driving behavior when aiming to develop realistic simulators.\n","authors":["Kalle Kujanpää","Daulet Baimukashev","Shibei Zhu","Shoaib Azam","Farzeen Munir","Gokhan Alcan","Ville Kyrki"],"pdf_url":"https://arxiv.org/pdf/2401.03236v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03217v1","updated":"2024-01-06T13:40:43Z","published":"2024-01-06T13:40:43Z","title":"Understanding Large-Language Model (LLM)-powered Human-Robot Interaction","summary":"  Large-language models (LLMs) hold significant promise in improving\nhuman-robot interaction, offering advanced conversational skills and\nversatility in managing diverse, open-ended user requests in various tasks and\ndomains. Despite the potential to transform human-robot interaction, very\nlittle is known about the distinctive design requirements for utilizing LLMs in\nrobots, which may differ from text and voice interaction and vary by task and\ncontext. To better understand these requirements, we conducted a user study (n\n= 32) comparing an LLM-powered social robot against text- and voice-based\nagents, analyzing task-based requirements in conversational tasks, including\nchoose, generate, execute, and negotiate. Our findings show that LLM-powered\nrobots elevate expectations for sophisticated non-verbal cues and excel in\nconnection-building and deliberation, but fall short in logical communication\nand may induce anxiety. We provide design implications both for robots\nintegrating LLMs and for fine-tuning LLMs for use with robots.\n","authors":["Callie Y. Kim","Christine P. Lee","Bilge Mutlu"],"pdf_url":"https://arxiv.org/pdf/2401.03217v1.pdf","comment":"10 pages, 4 figures. Callie Y. Kim and Christine P. Lee contributed\n  equally to the work. To be published in Proceedings of the 2024 ACM/IEEE\n  International Conference on Human-Robot Interaction (HRI '24), March 11--14,\n  2024, Boulder, CO, USA"},{"id":"http://arxiv.org/abs/2401.03160v1","updated":"2024-01-06T08:30:14Z","published":"2024-01-06T08:30:14Z","title":"Human as AI Mentor: Enhanced Human-in-the-loop Reinforcement Learning\n  for Safe and Efficient Autonomous Driving","summary":"  Despite significant progress in autonomous vehicles (AVs), the development of\ndriving policies that ensure both the safety of AVs and traffic flow efficiency\nhas not yet been fully explored. In this paper, we propose an enhanced\nhuman-in-the-loop reinforcement learning method, termed the Human as AI\nmentor-based deep reinforcement learning (HAIM-DRL) framework, which\nfacilitates safe and efficient autonomous driving in mixed traffic platoon.\nDrawing inspiration from the human learning process, we first introduce an\ninnovative learning paradigm that effectively injects human intelligence into\nAI, termed Human as AI mentor (HAIM). In this paradigm, the human expert serves\nas a mentor to the AI agent. While allowing the agent to sufficiently explore\nuncertain environments, the human expert can take control in dangerous\nsituations and demonstrate correct actions to avoid potential accidents. On the\nother hand, the agent could be guided to minimize traffic flow disturbance,\nthereby optimizing traffic flow efficiency. In detail, HAIM-DRL leverages data\ncollected from free exploration and partial human demonstrations as its two\ntraining sources. Remarkably, we circumvent the intricate process of manually\ndesigning reward functions; instead, we directly derive proxy state-action\nvalues from partial human demonstrations to guide the agents' policy learning.\nAdditionally, we employ a minimal intervention technique to reduce the human\nmentor's cognitive load. Comparative results show that HAIM-DRL outperforms\ntraditional methods in driving safety, sampling efficiency, mitigation of\ntraffic flow disturbance, and generalizability to unseen traffic scenarios. The\ncode and demo videos for this paper can be accessed at:\nhttps://zilin-huang.github.io/HAIM-DRL-website/}{https://zilin-huang.github.io/HAIM-DRL-website/.\n","authors":["Zilin Huang","Zihao Sheng","Chengyuan Ma","Sikai Chen"],"pdf_url":"https://arxiv.org/pdf/2401.03160v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.03154v1","updated":"2024-01-06T08:10:58Z","published":"2024-01-06T08:10:58Z","title":"Decentralized Multi-Agent Active Search and Tracking when Targets\n  Outnumber Agents","summary":"  Multi-agent multi-target tracking has a wide range of applications, including\nwildlife patrolling, security surveillance or environment monitoring. Such\nalgorithms often make restrictive assumptions: the number of targets and/or\ntheir initial locations may be assumed known, or agents may be pre-assigned to\nmonitor disjoint partitions of the environment, reducing the burden of\nexploration. This also limits applicability when there are fewer agents than\ntargets, since agents are unable to continuously follow the targets in their\nfields of view. Multi-agent tracking algorithms additionally assume inter-agent\nsynchronization of observations, or the presence of a central controller to\ncoordinate joint actions. Instead, we focus on the setting of decentralized\nmulti-agent, multi-target, simultaneous active search-and-tracking with\nasynchronous inter-agent communication. Our proposed algorithm DecSTER uses a\nsequential monte carlo implementation of the probability hypothesis density\nfilter for posterior inference combined with Thompson sampling for\ndecentralized multi-agent decision making. We compare different action\nselection policies, focusing on scenarios where targets outnumber agents. In\nsimulation, we demonstrate that DecSTER is robust to unreliable inter-agent\ncommunication and outperforms information-greedy baselines in terms of the\nOptimal Sub-Pattern Assignment (OSPA) metric for different numbers of targets\nand varying teamsizes.\n","authors":["Arundhati Banerjee","Jeff Schneider"],"pdf_url":"https://arxiv.org/pdf/2401.03154v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2401.03141v1","updated":"2024-01-06T07:05:48Z","published":"2024-01-06T07:05:48Z","title":"Estimating the Lateral Motion States of an Underwater Robot by Propeller\n  Wake Sensing Using an Artificial Lateral Line","summary":"  An artificial lateral line (ALL) is a bioinspired flow sensing system of an\nunderwater robot that consists of distributed flow sensors. The ALL has\nachieved great success in sensing the motion states of bioinspired underwater\nrobots, e.g., robotic fish, that are driven by body undulation and/or tail\nflapping. However, the ALL has not been systematically tested and studied in\nthe sensing of underwater robots driven by rotating propellers due to the\nhighly dynamic and complex flow field therein. This paper makes a bold\nhypothesis that the distributed flow measurements sampled from the propeller\nwake flow, although infeasible to represent the entire flow dynamics, provides\nsufficient information for estimating the lateral motion states of the leader\nunderwater robot. An experimental testbed is constructed to investigate the\nfeasibility of such a state estimator which comprises a cylindrical ALL sensory\nsystem, a rotating leader propeller, and a water tank with a planar sliding\nguide. Specifically, a hybrid network that consists of a one-dimensional\nconvolution network (1DCNN) and a bidirectional long short-term memory network\n(BiLSTM) is designed to extract the spatiotemporal features of the time series\nof distributed pressure measurements. A multi-output deep learning network is\nadopted to estimate the lateral motion states of the leader propeller. In\naddition, the state estimator is optimized using the whale optimization\nalgorithm (WOA) considering the comprehensive estimation performance. Extensive\nexperiments are conducted the results of which validate the proposed\ndata-driven algorithm in estimating the motion states of the leader underwater\nrobot by propeller wake sensing.\n","authors":["Jun Wang","Dexin Zhao","Youxi Zhao","Feitian Zhang","Tongsheng Shen"],"pdf_url":"https://arxiv.org/pdf/2401.03141v1.pdf","comment":"10 pages, 8 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2401.03201v1","updated":"2024-01-06T12:20:18Z","published":"2024-01-06T12:20:18Z","title":"3DMIT: 3D Multi-modal Instruction Tuning for Scene Understanding","summary":"  The remarkable potential of multi-modal large language models (MLLMs) in\ncomprehending both vision and language information has been widely\nacknowledged. However, the scarcity of 3D scenes-language pairs in comparison\nto their 2D counterparts, coupled with the inadequacy of existing approaches in\nunderstanding of 3D scenes by LLMs, poses a significant challenge. In response,\nwe collect and construct an extensive dataset comprising 75K\ninstruction-response pairs tailored for 3D scenes. This dataset addresses tasks\nrelated to 3D VQA, 3D grounding, and 3D conversation. To further enhance the\nintegration of 3D spatial information into LLMs, we introduce a novel and\nefficient prompt tuning paradigm, 3DMIT. This paradigm eliminates the alignment\nstage between 3D scenes and language and extends the instruction prompt with\nthe 3D modality information including the entire scene and segmented objects.\nWe evaluate the effectiveness of our method across diverse tasks in the 3D\nscene domain and find that our approach serves as a strategic means to enrich\nLLMs' comprehension of the 3D world. Our code is available at\nhttps://github.com/staymylove/3DMIT.\n","authors":["Zeju Li","Chao Zhang","Xiaoyan Wang","Ruilong Ren","Yifan Xu","Ruifei Ma","Xiangde Liu"],"pdf_url":"https://arxiv.org/pdf/2401.03201v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2401.03195v1","updated":"2024-01-06T11:37:20Z","published":"2024-01-06T11:37:20Z","title":"Efficient Bitrate Ladder Construction using Transfer Learning and\n  Spatio-Temporal Features","summary":"  Providing high-quality video with efficient bitrate is a main challenge in\nvideo industry. The traditional one-size-fits-all scheme for bitrate ladders is\ninefficient and reaching the best content-aware decision computationally\nimpractical due to extensive encodings required. To mitigate this, we propose a\nbitrate and complexity efficient bitrate ladder prediction method using\ntransfer learning and spatio-temporal features. We propose: (1) using feature\nmaps from well-known pre-trained DNNs to predict rate-quality behavior with\nlimited training data; and (2) improving highest quality rung efficiency by\npredicting minimum bitrate for top quality and using it for the top rung. The\nmethod tested on 102 video scenes demonstrates 94.1% reduction in complexity\nversus brute-force at 1.71% BD-Rate expense. Additionally, transfer learning\nwas thoroughly studied through four networks and ablation studies.\n","authors":["Ali Falahati","Mohammad Karim Safavi","Ardavan Elahi","Farhad Pakdaman","Moncef Gabbouj"],"pdf_url":"https://arxiv.org/pdf/2401.03195v1.pdf","comment":"7 pages, 9 figures, 7 tables"},{"id":"http://arxiv.org/abs/2401.03115v1","updated":"2024-01-06T03:03:28Z","published":"2024-01-06T03:03:28Z","title":"Transferable Learned Image Compression-Resistant Adversarial\n  Perturbations","summary":"  Adversarial attacks can readily disrupt the image classification system,\nrevealing the vulnerability of DNN-based recognition tasks. While existing\nadversarial perturbations are primarily applied to uncompressed images or\ncompressed images by the traditional image compression method, i.e., JPEG,\nlimited studies have investigated the robustness of models for image\nclassification in the context of DNN-based image compression. With the rapid\nevolution of advanced image compression, DNN-based learned image compression\nhas emerged as the promising approach for transmitting images in many\nsecurity-critical applications, such as cloud-based face recognition and\nautonomous driving, due to its superior performance over traditional\ncompression. Therefore, there is a pressing need to fully investigate the\nrobustness of a classification system post-processed by learned image\ncompression. To bridge this research gap, we explore the adversarial attack on\na new pipeline that targets image classification models that utilize learned\nimage compressors as pre-processing modules. Furthermore, to enhance the\ntransferability of perturbations across various quality levels and\narchitectures of learned image compression models, we introduce a saliency\nscore-based sampling method to enable the fast generation of transferable\nperturbation. Extensive experiments with popular attack methods demonstrate the\nenhanced transferability of our proposed method when attacking images that have\nbeen post-processed with different learned image compression models.\n","authors":["Yang Sui","Zhuohang Li","Ding Ding","Xiang Pan","Xiaozhong Xu","Shan Liu","Zhenzhong Chen"],"pdf_url":"https://arxiv.org/pdf/2401.03115v1.pdf","comment":"Accepted as poster at Data Compression Conference 2024 (DCC 2024)"},{"id":"http://arxiv.org/abs/2401.03105v1","updated":"2024-01-06T02:02:34Z","published":"2024-01-06T02:02:34Z","title":"Incorporating Visual Experts to Resolve the Information Loss in\n  Multimodal Large Language Models","summary":"  Multimodal Large Language Models (MLLMs) are experiencing rapid growth,\nyielding a plethora of noteworthy contributions in recent months. The\nprevailing trend involves adopting data-driven methodologies, wherein diverse\ninstruction-following datasets are collected. However, a prevailing challenge\npersists in these approaches, specifically in relation to the limited visual\nperception ability, as CLIP-like encoders employed for extracting visual\ninformation from inputs. Though these encoders are pre-trained on billions of\nimage-text pairs, they still grapple with the information loss dilemma, given\nthat textual captions only partially capture the contents depicted in images.\nTo address this limitation, this paper proposes to improve the visual\nperception ability of MLLMs through a mixture-of-experts knowledge enhancement\nmechanism. Specifically, we introduce a novel method that incorporates\nmulti-task encoders and visual tools into the existing MLLMs training and\ninference pipeline, aiming to provide a more comprehensive and accurate\nsummarization of visual inputs. Extensive experiments have evaluated its\neffectiveness of advancing MLLMs, showcasing improved visual perception\nachieved through the integration of visual experts.\n","authors":["Xin He","Longhui Wei","Lingxi Xie","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2401.03105v1.pdf","comment":null}]}}