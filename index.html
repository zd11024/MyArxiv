<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-01-08T00:00:00Z">2024-01-08</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">43</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mixtral of Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.04088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.04088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Sandeep Subramanian, Sophia Yang, Szymon Antoniak, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model.
Mixtral has the same architecture as Mistral 7B, with the difference that each
layer is composed of 8 feedforward blocks (i.e. experts). For every token, at
each layer, a router network selects two experts to process the current state
and combine their outputs. Even though each token only sees two experts, the
selected experts can be different at each timestep. As a result, each token has
access to 47B parameters, but only uses 13B active parameters during inference.
Mixtral was trained with a context size of 32k tokens and it outperforms or
matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular,
Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and
multilingual benchmarks. We also provide a model fine-tuned to follow
instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo,
Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both
the base and instruct models are released under the Apache 2.0 license.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>See more details at https://mistral.ai/news/mixtral-of-experts/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MoE-Mamba: Efficient Selective State Space Models with Mixture of
  Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.04081v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.04081v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maciej Pióro, Kamil Ciebiera, Krystian Król, Jan Ludziejewski, Sebastian Jaszczur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State Space Models (SSMs) have become serious contenders in the field of
sequential modeling, challenging the dominance of Transformers. At the same
time, Mixture of Experts (MoE) has significantly improved Transformer-based
LLMs, including recent state-of-the-art open-source models. We propose that to
unlock the potential of SSMs for scaling, they should be combined with MoE. We
showcase this on Mamba, a recent SSM-based model that achieves remarkable,
Transformer-like performance. Our model, MoE-Mamba, outperforms both Mamba and
Transformer-MoE. In particular, MoE-Mamba reaches the same performance as Mamba
in 2.2x less training steps while preserving the inference performance gains of
Mamba against the Transformer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Empirical Analysis of Efficient Fine-Tuning Methods for Large
  Pre-Trained Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.04051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.04051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nigel Doering, Cyril Gorlla, Trevor Tuttle, Adhvaith Vijay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning large pre-trained language models for downstream tasks remains a
critical challenge in natural language processing. This paper presents an
empirical analysis comparing two efficient fine-tuning methods - BitFit and
adapter modules - to standard full model fine-tuning. Experiments conducted on
GLUE benchmark datasets (MRPC, COLA, STS-B) reveal several key insights. The
BitFit approach, which trains only bias terms and task heads, matches full
fine-tuning performance across varying amounts of training data and time
constraints. It demonstrates remarkable stability even with only 30\% of data,
outperforming full fine-tuning at intermediate data levels. Adapter modules
exhibit high variability, with inconsistent gains over default models. The
findings indicate BitFit offers an attractive balance between performance and
parameter efficiency. Our work provides valuable perspectives on model tuning,
emphasizing robustness and highlighting BitFit as a promising alternative for
resource-constrained or streaming task settings. The analysis offers actionable
guidelines for efficient adaptation of large pre-trained models, while
illustrating open challenges in stabilizing techniques like adapter modules.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FFSplit: Split Feed-Forward Network For Optimizing Accuracy-Efficiency
  Trade-off in Language Model Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.04044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.04044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zirui Liu, Qingquan Song, Qiang Charles Xiao, Sathiya Keerthi Selvaraj, Rahul Mazumder, Aman Gupta, Xia Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The large number of parameters in Pretrained Language Models enhance their
performance, but also make them resource-intensive, making it challenging to
deploy them on commodity hardware like a single GPU. Due to the memory and
power limitations of these devices, model compression techniques are often used
to decrease both the model's size and its inference latency. This usually
results in a trade-off between model accuracy and efficiency. Therefore,
optimizing this balance is essential for effectively deploying LLMs on
commodity hardware. A significant portion of the efficiency challenge is the
Feed-forward network (FFN) component, which accounts for roughly $\frac{2}{3}$
total parameters and inference latency. In this paper, we first observe that
only a few neurons of FFN module have large output norm for any input tokens,
a.k.a. heavy hitters, while the others are sparsely triggered by different
tokens. Based on this observation, we explicitly split the FFN into two parts
according to the heavy hitters. We improve the efficiency-accuracy trade-off of
existing compression methods by allocating more resource to FFN parts with
heavy hitters. In practice, our method can reduce model size by 43.1\% and
bring $1.25\sim1.56\times$ wall clock time speedup on different hardware with
negligible accuracy drop.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IDoFew: Intermediate Training Using Dual-Clustering in Language Models
  for Few Labels Text Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.04025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.04025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdullah Alsuhaibani, Hamad Zogan, Imran Razzak, Shoaib Jameel, Guandong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models such as Bidirectional Encoder Representations from
Transformers (BERT) have been very effective in various Natural Language
Processing (NLP) and text mining tasks including text classification. However,
some tasks still pose challenges for these models, including text
classification with limited labels. This can result in a cold-start problem.
Although some approaches have attempted to address this problem through
single-stage clustering as an intermediate training step coupled with a
pre-trained language model, which generates pseudo-labels to improve
classification, these methods are often error-prone due to the limitations of
the clustering algorithms. To overcome this, we have developed a novel
two-stage intermediate clustering with subsequent fine-tuning that models the
pseudo-labels reliably, resulting in reduced prediction errors. The key novelty
in our model, IDoFew, is that the two-stage clustering coupled with two
different clustering algorithms helps exploit the advantages of the
complementary algorithms that reduce the errors in generating reliable
pseudo-labels for fine-tuning. Our approach has shown significant improvements
compared to strong comparative models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in The 17th ACM International Conference on Web Search and
  Data Mining</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Spatial Reasoning in Large Language Models: An In-Depth
  Evaluation and Enhancement Using the StepGame <span class="highlight-title">Benchmark</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03991v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03991v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangjun Li, David C. Hogg, Anthony G. Cohn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI) has made remarkable progress across various
domains, with large language models like ChatGPT gaining substantial attention
for their human-like text-generation capabilities. Despite these achievements,
spatial reasoning remains a significant challenge for these models. Benchmarks
like StepGame evaluate AI spatial reasoning, where ChatGPT has shown
unsatisfactory performance. However, the presence of template errors in the
benchmark has an impact on the evaluation results. Thus there is potential for
ChatGPT to perform better if these template errors are addressed, leading to
more accurate assessments of its spatial reasoning capabilities. In this study,
we refine the StepGame benchmark, providing a more accurate dataset for model
evaluation. We analyze GPT's spatial reasoning performance on the rectified
benchmark, identifying proficiency in mapping natural language text to spatial
relations but limitations in multi-hop reasoning. We provide a flawless
solution to the benchmark by combining template-to-relation mapping with
logic-based reasoning. This combination demonstrates proficiency in performing
qualitative reasoning on StepGame without encountering any errors. We then
address the limitations of GPT models in spatial reasoning. We deploy
Chain-of-thought and Tree-of-thoughts prompting strategies, offering insights
into GPT's ``cognitive process", and achieving remarkable improvements in
accuracy. Our investigation not only sheds light on model deficiencies but also
proposes enhancements, contributing to the advancement of AI with more robust
spatial reasoning capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-Ready version for AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TextMachina: Seamless Generation of Machine-Generated Text <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03946v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03946v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Areg Mikael Sarvazyan, José Ángel González, Marc Franco-Salvador
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have led to high-quality
Machine-Generated Text (MGT), giving rise to countless new use cases and
applications. However, easy access to LLMs is posing new challenges due to
misuse. To address malicious usage, researchers have released datasets to
effectively train models on MGT-related tasks. Similar strategies are used to
compile these datasets, but no tool currently unifies them. In this scenario,
we introduce TextMachina, a modular and extensible Python framework, designed
to aid in the creation of high-quality, unbiased datasets to build robust
models for MGT-related tasks such as detection, attribution, or boundary
detection. It provides a user-friendly pipeline that abstracts away the
inherent intricacies of building MGT datasets, such as LLM integrations, prompt
templating, and bias mitigation. The quality of the datasets generated by
TextMachina has been assessed in previous works, including shared tasks where
more than one hundred teams trained robust MGT detectors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpeechAgents: Human-Communication Simulation with Multi-Modal
  Multi-Agent Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03945v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03945v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Zhang, Zhaowei Li, Pengyu Wang, Xin Zhang, Yaqian Zhou, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human communication is a complex and diverse process that not only involves
multiple factors such as language, commonsense, and cultural backgrounds but
also requires the participation of multimodal information, such as speech.
Large Language Model (LLM)-based multi-agent systems have demonstrated
promising performance in simulating human society. Can we leverage LLM-based
multi-agent systems to simulate human communication? However, current LLM-based
multi-agent systems mainly rely on text as the primary medium. In this paper,
we propose SpeechAgents, a multi-modal LLM based multi-agent system designed
for simulating human communication. SpeechAgents utilizes multi-modal LLM as
the control center for individual agent and employes multi-modal signals as the
medium for exchanged messages among agents. Additionally, we propose
Multi-Agent Tuning to enhance the multi-agent capabilities of LLM without
compromising general abilities. To strengthen and evaluate the effectiveness of
human communication simulation, we build the Human-Communication Simulation
Benchmark. Experimental results demonstrate that SpeechAgents can simulate
human communication dialogues with consistent content, authentic rhythm, and
rich emotions and demonstrate excellent scalability even with up to 25 agents,
which can apply to tasks such as drama creation and audio novels generation.
Code and models will be open-sourced at https://github.
com/0nutation/SpeechAgents
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Philosophical Introduction to Language Models -- Part I: Continuity
  With Classic Debates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03910v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03910v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raphaël Millière, Cameron Buckner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models like GPT-4 have achieved remarkable proficiency in a
broad spectrum of language-based tasks, some of which are traditionally
associated with hallmarks of human intelligence. This has prompted ongoing
disagreements about the extent to which we can meaningfully ascribe any kind of
linguistic or cognitive competence to language models. Such questions have deep
philosophical roots, echoing longstanding debates about the status of
artificial neural networks as cognitive models. This article -- the first part
of two companion papers -- serves both as a primer on language models for
philosophers, and as an opinionated survey of their significance in relation to
classic debates in the philosophy cognitive science, artificial intelligence,
and linguistics. We cover topics such as compositionality, language
acquisition, semantic competence, grounding, world models, and the transmission
of cultural knowledge. We argue that the success of language models challenges
several long-held assumptions about artificial neural networks. However, we
also highlight the need for further empirical investigation to better
understand their internal mechanisms. This sets the stage for the companion
paper (Part II), which turns to novel empirical methods for probing the inner
workings of language models, and new philosophical questions prompted by their
latest developments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WEBDial, a Multi-domain, Multitask Statistical Dialogue Framework with
  RDF 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Morgan Veyret, Jean-Baptiste Duchene, Kekeli Afonouvi, Quentin Brabant, Gwenole Lecorve, Lina M. Rojas-Barahona
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Typically available dialogue frameworks have adopted a semantic
representation based on dialogue-acts and slot-value pairs. Despite its
simplicity, this representation has disadvantages such as the lack of
expressivity, scalability and explainability. We present WEBDial: a dialogue
framework that relies on a graph formalism by using RDF triples instead of
slot-value pairs. We describe its overall architecture and the graph-based
semantic representation. We show its applicability from simple to complex
applications, by varying the complexity of domains and tasks: from single
domain and tasks to multiple domains and complex tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STAIR: Spatial-Temporal Reasoning with Auditable Intermediate Results
  for Video Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03901v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03901v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueqian Wang, Yuxuan Wang, Kai Chen, Dongyan Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently we have witnessed the rapid development of video question answering
models. However, most models can only handle simple videos in terms of temporal
reasoning, and their performance tends to drop when answering
temporal-reasoning questions on long and informative videos. To tackle this
problem we propose STAIR, a Spatial-Temporal Reasoning model with Auditable
Intermediate Results for video question answering. STAIR is a neural module
network, which contains a program generator to decompose a given question into
a hierarchical combination of several sub-tasks, and a set of lightweight
neural modules to complete each of these sub-tasks. Though neural module
networks are already widely studied on image-text tasks, applying them to
videos is a non-trivial task, as reasoning on videos requires different
abilities. In this paper, we define a set of basic video-text sub-tasks for
video question answering and design a set of lightweight modules to complete
them. Different from most prior works, modules of STAIR return intermediate
outputs specific to their intentions instead of always returning attention
maps, which makes it easier to interpret and collaborate with pre-trained
models. We also introduce intermediate supervision to make these intermediate
outputs more accurate. We conduct extensive experiments on several video
question answering datasets under various settings to show STAIR's performance,
explainability, compatibility with pre-trained models, and applicability when
program annotations are not available. Code:
https://github.com/yellow-binary-tree/STAIR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boldly Going Where No <span class="highlight-title">Benchmark</span> Has Gone Before: Exposing Bias and
  Shortcomings in Code Generation Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03855v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03855v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankit Yadav, Mayank Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by the increasing popularity of code generation from human
descriptions using large language models (LLMs), several benchmarks have been
proposed to assess the capabilities of existing and emerging models. This study
presents a large-scale human evaluation of HumanEval and MBPP, two widely used
benchmarks for Python code generation, focusing on their diversity and
difficulty. Our findings reveal a significant bias towards a limited number of
programming concepts, with negligible or no representation of most concepts.
Additionally, we identify a concerningly high proportion of easy programming
questions, potentially leading to an overestimation of model performance on
code generation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ We Need to Talk About Classification Evaluation Metrics in NLP <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Vickers, Loïc Barrault, Emilio Monti, Nikolaos Aletras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Natural Language Processing (NLP) classification tasks such as topic
categorisation and sentiment analysis, model generalizability is generally
measured with standard metrics such as Accuracy, F-Measure, or AUC-ROC. The
diversity of metrics, and the arbitrariness of their application suggest that
there is no agreement within NLP on a single best metric to use. This lack
suggests there has not been sufficient examination of the underlying heuristics
which each metric encodes. To address this we compare several standard
classification metrics with more 'exotic' metrics and demonstrate that a
random-guess normalised Informedness metric is a parsimonious baseline for task
performance. To show how important the choice of metric is, we perform
extensive experiments on a wide range of NLP tasks including a synthetic
scenario, natural language understanding, question answering and machine
translation. Across these tasks we use a superset of metrics to rank models and
find that Informedness best captures the ideal model characteristics. Finally,
we release a Python implementation of Informedness following the SciKitLearn
classifier format.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Appeared in AACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TeleChat Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Wang, Xinzhang Liu, Shixuan Liu, Yitong Yao, Yuyao Huang, Zhongjiang He, Xuelong Li, Yongxiang Li, Zhonghao Che, Zhaoxi Zhang, Yan Wang, Xin Wang, Luwen Pu, Huihan Xu, Ruiyu Fang, Yu Zhao, Jie Zhang, Xiaomeng Huang, Zhilong Lu, Jiaxin Peng, Wenjun Zheng, Shiquan Wang, Bingkai Yang, Xuewei he, Zhuoru Jiang, Qiyi Xie, Yanhan Zhang, Zhongqiu Li, Lingling Shi, Weiwei Fu, Yin Zhang, Zilu Huang, Sishi Xiong, Yuxiang Zhang, Chao Wang, Shuangyong Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this technical report, we present TeleChat, a collection of large language
models (LLMs) with parameters of 3 billion, 7 billion and 12 billion. It
includes pretrained language models as well as fine-tuned chat models that is
aligned with human preferences. TeleChat is initially pretrained on an
extensive corpus containing a diverse collection of texts from both English and
Chinese languages, including trillions of tokens. Subsequently, the model
undergoes fine-tuning to align with human preferences, following a detailed
methodology that we describe. We evaluate the performance of TeleChat on
various tasks, including language understanding, mathematics, reasoning, code
generation, and knowledge-based question answering. Our findings indicate that
TeleChat achieves comparable performance to other open-source models of similar
size across a wide range of public benchmarks. To support future research and
applications utilizing LLMs, we release the fine-tuned model checkpoints of
TeleChat's 7B and 12B variant, along with code and a portion of our pretraining
data, to the public community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Anatomy of Neural Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Majd Saleh, Stéphane Paquelet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI and transfer learning fields have experienced remarkable
advancements in recent years especially in the domain of Natural Language
Processing (NLP). Transformers were at the heart of these advancements where
the cutting-edge transformer-based Language Models (LMs) enabled new
state-of-the-art results in a wide spectrum of applications. While the number
of research works involving neural LMs is exponentially increasing, their vast
majority are high-level and far from self-contained. Consequently, a deep
understanding of the literature in this area is a tough task especially at the
absence of a unified mathematical framework explaining the main types of neural
LMs. We address the aforementioned problem in this tutorial where the objective
is to explain neural LMs in a detailed, simplified and unambiguous mathematical
framework accompanied with clear graphical illustrations. Concrete examples on
widely used models like BERT and GPT2 are explored. Finally, since transformers
pretrained on language-modeling-like tasks have been widely adopted in computer
vision and time series applications, we briefly explore some examples of such
solutions in order to enable readers understand how transformers work in the
aforementioned domains and compare this use with the original one in NLP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 Pages; 25 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced Automated Code Vulnerability Repair using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03741v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03741v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David de-Fitero-Dominguez, Eva Garcia-Lopez, Antonio Garcia-Cabot, Jose-Javier Martinez-Herraiz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research addresses the complex challenge of automated repair of code
vulnerabilities, vital for enhancing digital security in an increasingly
technology-driven world. The study introduces a novel and efficient format for
the representation of code modification, using advanced Large Language Models
(LLMs) such as Code Llama and Mistral. These models, fine-tuned on datasets
featuring C code vulnerabilities, significantly improve the accuracy and
adaptability of automated code repair techniques. A key finding is the enhanced
repair accuracy of these models when compared to previous methods such as
VulRepair, which underscores their practical utility and efficiency. The
research also offers a critical assessment of current evaluation metrics, such
as perfect predictions, and their limitations in reflecting the true
capabilities of automated repair models in real-world scenarios. Following
this, it underscores the importance of using test datasets devoid of train
samples, emphasizing the need for dataset integrity to enhance the
effectiveness of LLMs in code repair tasks. The significance of this work is
its contribution to digital security, setting new standards for automated code
vulnerability repair and paving the way for future advancements in the fields
of cybersecurity and artificial intelligence. The study does not only highlight
the potential of LLMs in enhancing code security but also fosters further
exploration and research in these crucial areas.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Large Language Models Beat Wall Street? Unveiling the Potential of
  AI in Stock Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Fatouros, Konstantinos Metaxas, John Soldatos, Dimosthenis Kyriazis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the dynamic and data-driven landscape of financial markets, this paper
introduces MarketSenseAI, a novel AI-driven framework leveraging the advanced
reasoning capabilities of GPT-4 for scalable stock selection. MarketSenseAI
incorporates Chain of Thought and In-Context Learning methodologies to analyze
a wide array of data sources, including market price dynamics, financial news,
company fundamentals, and macroeconomic reports emulating the decision making
process of prominent financial investment teams. The development,
implementation, and empirical validation of MarketSenseAI are detailed, with a
focus on its ability to provide actionable investment signals (buy, hold, sell)
backed by cogent explanations. A notable aspect of this study is the use of
GPT-4 not only as a predictive tool but also as an evaluator, revealing the
significant impact of the AI-generated explanations on the reliability and
acceptance of the suggested investment signals. In an extensive empirical
evaluation with S&P 100 stocks, MarketSenseAI outperformed the benchmark index
by 13%, achieving returns up to 40%, while maintaining a risk profile
comparable to the market. These results demonstrate the efficacy of Large
Language Models in complex financial decision-making and mark a significant
advancement in the integration of AI into financial analysis and investment
strategies. This research contributes to the financial AI field, presenting an
innovative approach and underscoring the transformative potential of AI in
revolutionizing traditional financial analysis investment methodologies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 12 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Models Understand Numbers, at Least Partially 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangwei Zhu, Damai Dai, Zhifang Sui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have exhibited impressive competency in various
text-related tasks. However, their opaque internal mechanisms become a
hindrance to leveraging them in mathematical problems. In this paper, we study
a fundamental question: whether language models understand numbers, which play
a basic element in mathematical problems. We assume that to solve mathematical
problems, language models should be capable of understanding numbers and
compressing these numbers in their hidden states. We construct a synthetic
dataset comprising addition problems and utilize linear probes to read out
input numbers from the hidden states of models. Experimental results
demonstrate evidence supporting the existence of compressed numbers in the
LLaMA-2 model family from early layers. However, the compression process seems
to be not lossless, presenting difficulty in precisely reconstructing the
original numbers. Further experiments show that language models can utilize the
encoded numbers to perform arithmetic computations, and the computational
ability scales up with the model size. Our preliminary research suggests that
language models exhibit a partial understanding of numbers, offering insights
into future investigations about the models' capability of solving mathematical
problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Butterfly Effect of Altering Prompts: How Small Changes and
  Jailbreaks Affect Large Language Model Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abel Salinas, Fred Morstatter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are regularly being used to label data across
many domains and for myriad tasks. By simply asking the LLM for an answer, or
``prompting,'' practitioners are able to use LLMs to quickly get a response for
an arbitrary task. This prompting is done through a series of decisions by the
practitioner, from simple wording of the prompt, to requesting the output in a
certain data format, to jailbreaking in the case of prompts that address more
sensitive topics. In this work, we ask: do variations in the way a prompt is
constructed change the ultimate decision of the LLM? We answer this using a
series of prompt variations across a variety of text classification tasks. We
find that even the smallest of perturbations, such as adding a space at the end
of a prompt, can cause the LLM to change its answer. Further, we find that
requesting responses in XML and commonly used jailbreaks can have cataclysmic
effects on the data labeled by LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Overview of the 2023 ICON Shared Task on Gendered Abuse Detection in
  Indic Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aatman Vaidya, Arnav Arora, Aditya Joshi, Tarunima Prabhakar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper reports the findings of the ICON 2023 on Gendered Abuse Detection
in Indic Languages. The shared task deals with the detection of gendered abuse
in online text. The shared task was conducted as a part of ICON 2023, based on
a novel dataset in Hindi, Tamil and the Indian dialect of English. The
participants were given three subtasks with the train dataset consisting of
approximately 6500 posts sourced from Twitter. For the test set, approximately
1200 posts were provided. The shared task received a total of 9 registrations.
The best F-1 scores are 0.616 for subtask 1, 0.572 for subtask 2 and, 0.616 and
0.582 for subtask 3. The paper contains examples of hateful content owing to
its topic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted at 20th International Conference on
  Natural Language Processing (ICON), it is of 5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Content-Based Novelty Measure for Scholarly Publications: A Proof of
  Concept 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haining Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Novelty, akin to gene mutation in evolution, opens possibilities for
scientific advancement. Despite peer review being the gold standard for
evaluating novelty in scholarly communication and resource allocation, the vast
volume of submissions necessitates an automated measure of scientific novelty.
Adopting a perspective that views novelty as the atypical combination of
existing knowledge, we introduce an information-theoretic measure of novelty in
scholarly publications. This measure is quantified by the degree of `surprise'
perceived by a language model that represents the distribution of scientific
discourse. The proposed measure is accompanied by face and construct validity
evidence; the former demonstrates correspondence to scientific common sense,
and the latter is endorsed through alignments with novelty evaluations from a
select panel of domain experts. Additionally, characterized by its
interpretability, fine granularity, and accessibility, this measure addresses
gaps prevalent in existing methods. We believe this measure holds great
potential to benefit editors, stakeholders, and policymakers, and it provides a
confident lens for examining the relationship between novelty and scientific
dynamics such as creativity, interdisciplinarity, scientific advances, and
more.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in the proceedings of iConference2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging the Skills Gap: Evaluating an AI-Assisted Provider Platform to
  Support Care Providers with Empathetic Delivery of Protocolized Therapy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William R. Kearns, Jessica Bertram, Myra Divina, Lauren Kemp, Yinzhou Wang, Alex Marin, Trevor Cohen, Weichao Yuwen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the high prevalence and burden of mental health conditions, there is
a global shortage of mental health providers. Artificial Intelligence (AI)
methods have been proposed as a way to address this shortage, by supporting
providers with less extensive training as they deliver care. To this end, we
developed the AI-Assisted Provider Platform (A2P2), a text-based virtual
therapy interface that includes a response suggestion feature, which supports
providers in delivering protocolized therapies empathetically. We studied
providers with and without expertise in mental health treatment delivering a
therapy session using the platform with (intervention) and without (control)
AI-assistance features. Upon evaluation, the AI-assisted system significantly
decreased response times by 29.34% (p=0.002), tripled empathic response
accuracy (p=0.0001), and increased goal recommendation accuracy by 66.67%
(p=0.001) across both user groups compared to the control. Both groups rated
the system as having excellent usability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted: AMIA Annual Symposium 2023. To appear as: Kearns W, Bertram
  J, Divina M, Kemp L, Wang Y, Marin A, Cohen T, Yuwen W. Bridging the Skills
  Gap: Evaluating an AI-Assisted Provider Platform to Support Care Providers
  with Empathetic Delivery of Protocolized Therapy. AMIA Annual Symposium
  Proceedings 2023. American Medical Informatics Association</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Why Solving Multi-agent Path Finding with Large Language Model has not
  Succeeded Yet 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03630v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03630v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weizhe Chen, Sven Koenig, Bistra Dilkina
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the explosive influence caused by the success of large language models
(LLM) like ChatGPT and GPT-4, there has been an extensive amount of recent work
showing that foundation models can be used to solve a large variety of tasks.
However, there is very limited work that shares insights on multi-agent
planning. Multi-agent planning is different from other domains by combining the
difficulty of multi-agent coordination and planning, and making it hard to
leverage external tools to facilitate the reasoning needed. In this paper, we
focus on the problem of multi-agent path finding (MAPF), which is also known as
multi-robot route planning, and study how to solve MAPF with LLMs. We first
show the motivating success on an empty room map without obstacles, then the
failure to plan on a slightly harder room map. We present our hypothesis of why
directly solving MAPF with LLMs has not been successful yet, and we use various
experiments to support our hypothesis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toward A Reinforcement-Learning-Based System for Adjusting Medication to
  Minimize Speech Disfluency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11509v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11509v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pavlos Constas, Vikram Rawal, Matthew Honorio Oliveira, Andreas Constas, Aditya Khan, Kaison Cheung, Najma Sultani, Carrie Chen, Micol Altomare, Michael Akzam, Jiacheng Chen, Vhea He, Lauren Altomare, Heraa Murqi, Asad Khan, Nimit Amikumar Bhanshali, Youssef Rachad, Michael Guerzhoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a Reinforcement-Learning-based system that would automatically
prescribe a hypothetical patient medication that may help the patient with
their mental-health-related speech disfluency, and adjust the medication and
the dosages in response to zero-cost frequent measurement of the fluency of the
patient. We demonstrate the components of the system: a module that detects and
evaluates speech disfluency on a large dataset we built, and a Reinforcement
Learning algorithm that automatically finds good combinations of medications.
To support the two modules, we collect data on the effect of psychiatric
medications for speech disfluency from the literature, and build a plausible
patient simulation system. We demonstrate that the Reinforcement Learning
system is, under some circumstances, able to converge to a good medication
regime. We collect and label a dataset of people with possible speech
disfluency and demonstrate our methods using that dataset. Our work is a proof
of concept: we show that there is promise in the idea of using automatic data
collection to address disfluency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Proc. Machine Learning for Cognitive and Mental Health Workshop
  (ML4CMH) at AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive Study of Knowledge Editing for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01286v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01286v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, Siyuan Cheng, Ziwen Xu, Xin Xu, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown extraordinary capabilities in
understanding and generating text that closely mirrors human communication.
However, a primary limitation lies in the significant computational demands
during training, arising from their extensive parameterization. This challenge
is further intensified by the dynamic nature of the world, necessitating
frequent updates to LLMs to correct outdated information or integrate new
knowledge, thereby ensuring their continued relevance. Note that many
applications demand continual model adjustments post-training to address
deficiencies or undesirable behaviors. There is an increasing interest in
efficient, lightweight methods for on-the-fly model modifications. To this end,
recent years have seen a burgeoning in the techniques of knowledge editing for
LLMs, which aim to efficiently modify LLMs' behaviors within specific domains
while preserving overall performance across various inputs. In this paper, we
first define the knowledge editing problem and then provide a comprehensive
review of cutting-edge approaches. Drawing inspiration from educational and
cognitive research theories, we propose a unified categorization criterion that
classifies knowledge editing methods into three groups: resorting to external
knowledge, merging knowledge into the model, and editing intrinsic knowledge.
Furthermore, we introduce a new benchmark, KnowEdit, for a comprehensive
empirical evaluation of representative knowledge editing approaches.
Additionally, we provide an in-depth analysis of knowledge location, which can
provide a deeper understanding of the knowledge structures inherent within
LLMs. Finally, we discuss several potential applications of knowledge editing,
outlining its broad and impactful implications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing work; 52 pages, 280 citations; benchmark is available at
  https://huggingface.co/datasets/zjunlp/KnowEdit code is available at
  https://github.com/zjunlp/EasyEdit paper list is available at
  https://github.com/zjunlp/KnowledgeEditingPapers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code
  Empowers Large Language Models to Serve as Intelligent Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00812v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00812v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Yang, Jiateng Liu, John Wu, Chaoqi Yang, Yi R. Fung, Sha Li, Zixuan Huang, Xu Cao, Xingyao Wang, Yiquan Wang, Heng Ji, Chengxiang Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prominent large language models (LLMs) of today differ from past language
models not only in size, but also in the fact that they are trained on a
combination of natural language and formal language (code). As a medium between
humans and computers, code translates high-level goals into executable steps,
featuring standard syntax, logical consistency, abstraction, and modularity. In
this survey, we present an overview of the various benefits of integrating code
into LLMs' training data. Specifically, beyond enhancing LLMs in code
generation, we observe that these unique properties of code help (i) unlock the
reasoning ability of LLMs, enabling their applications to a range of more
complex natural language tasks; (ii) steer LLMs to produce structured and
precise intermediate steps, which can then be connected to external execution
ends through function calls; and (iii) take advantage of code compilation and
execution environment, which also provides diverse feedback for model
improvement. In addition, we trace how these profound capabilities of LLMs,
brought by code, have led to their emergence as intelligent agents (IAs) in
situations where the ability to understand instructions, decompose goals, plan
and execute actions, and refine from feedback are crucial to their success on
downstream tasks. Finally, we present several key challenges and future
directions of empowering LLMs with code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive Survey of Hallucination Mitigation Techniques in Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01313v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01313v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. M Towhidul Islam Tonmoy, S M Mehedi Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, Amitava Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) continue to advance in their ability to write
human-like text, a key challenge remains around their tendency to hallucinate
generating content that appears factual but is ungrounded. This issue of
hallucination is arguably the biggest hindrance to safely deploying these
powerful LLMs into real-world production systems that impact people's lives.
The journey toward widespread adoption of LLMs in practical settings heavily
relies on addressing and mitigating hallucinations. Unlike traditional AI
systems focused on limited tasks, LLMs have been exposed to vast amounts of
online text data during training. While this allows them to display impressive
language fluency, it also means they are capable of extrapolating information
from the biases in training data, misinterpreting ambiguous prompts, or
modifying the information to align superficially with the input. This becomes
hugely alarming when we rely on language generation capabilities for sensitive
applications, such as summarizing medical records, financial analysis reports,
etc. This paper presents a comprehensive survey of over 32 techniques developed
to mitigate hallucination in LLMs. Notable among these are Retrieval Augmented
Generation (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023),
CoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we
introduce a detailed taxonomy categorizing these methods based on various
parameters, such as dataset utilization, common tasks, feedback mechanisms, and
retriever types. This classification helps distinguish the diverse approaches
specifically designed to tackle hallucination issues in LLMs. Additionally, we
analyze the challenges and limitations inherent in these techniques, providing
a solid foundation for future research in addressing hallucinations and related
phenomena within the realm of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Deep Latent Position Topic Model for Clustering and Representation
  of Networks with Textual Edges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.08242v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.08242v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rémi Boutin, Pierre Latouche, Charles Bouveyron
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerical interactions leading to users sharing textual content published by
others are naturally represented by a network where the individuals are
associated with the nodes and the exchanged texts with the edges. To understand
those heterogeneous and complex data structures, clustering nodes into
homogeneous groups as well as rendering a comprehensible visualisation of the
data is mandatory. To address both issues, we introduce Deep-LPTM, a
model-based clustering strategy relying on a variational graph auto-encoder
approach as well as a probabilistic model to characterise the topics of
discussion. Deep-LPTM allows to build a joint representation of the nodes and
of the edges in two embeddings spaces. The parameters are inferred using a
variational inference algorithm. We also introduce IC2L, a model selection
criterion specifically designed to choose models with relevant clustering and
visualisation properties. An extensive benchmark study on synthetic data is
provided. In particular, we find that Deep-LPTM better recovers the partitions
of the nodes than the state-of-the art ETSBM and STBM. Eventually, the emails
of the Enron company are analysed and visualisations of the results are
presented, with meaningful highlights of the graph structure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages including the appendix, 13 figures, 6 tables, journal paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning-Based Knowledge Injection for Metaphor Detection: A
  Comprehensive Review 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.04306v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.04306v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Yang, Zheng Li, Zhiyue Liu, Qingbao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Metaphor as an advanced cognitive modality works by extracting familiar
concepts in the target domain in order to understand vague and abstract
concepts in the source domain. This helps humans to quickly understand and
master new domains and thus adapt to changing environments. With the continuous
development of metaphor research in the natural language community, many
studies using knowledge-assisted models to detect textual metaphors have
emerged in recent years. Compared to not using knowledge, systems that
introduce various kinds of knowledge achieve greater performance gains and
reach SOTA in a recent study. Based on this, the goal of this paper is to
provide a comprehensive review of research advances in the application of deep
learning for knowledge injection in metaphor detection tasks. We will first
systematically summarize and generalize the mainstream knowledge and knowledge
injection principles. Then, the datasets, evaluation metrics, and benchmark
models used in metaphor detection tasks are examined. Finally, we explore the
current issues facing knowledge injection methods and provide an outlook on
future research directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comparative Experimentation of Accuracy Metrics in Automated Medical
  Reporting: The Case of Otitis Consultations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13273v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13273v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wouter Faber, Renske Eline Bootsma, Tom Huibers, Sandra van Dulmen, Sjaak Brinkkemper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Artificial Intelligence (AI) can be used to automatically generate
medical reports based on transcripts of medical consultations. The aim is to
reduce the administrative burden that healthcare professionals face. The
accuracy of the generated reports needs to be established to ensure their
correctness and usefulness. There are several metrics for measuring the
accuracy of AI generated reports, but little work has been done towards the
application of these metrics in medical reporting. A comparative
experimentation of 10 accuracy metrics has been performed on AI generated
medical reports against their corresponding General Practitioner's (GP) medical
reports concerning Otitis consultations. The number of missing, incorrect, and
additional statements of the generated reports have been correlated with the
metric scores. In addition, we introduce and define a Composite Accuracy Score
which produces a single score for comparing the metrics within the field of
automated medical reporting. Findings show that based on the correlation study
and the Composite Accuracy Score, the ROUGE-L and Word Mover's Distance metrics
are the preferred metrics, which is not in line with previous work. These
findings help determine the accuracy of an AI generated medical report, which
aids the development of systems that generate medical reports for GPs to reduce
the administrative burden.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 1 figure, to be presented at HEALTHINF 2024, Author
  contributions: Wouter Faber and Renske Eline Bootsma performed research and
  wrote paper, Tom Huibers provided needed software and research inspiration,
  Sandra van Dulmen provided the data and feedback on paper, Sjaak Brinkkemper
  supervised the project and provided continuous feedback</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Format Consistency for Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.15504v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.15504v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shihao Liang, Runchu Tian, Kunlun Zhu, Yujia Qin, Huadong Wang, Xin Cong, Zhiyuan Liu, Xiaojiang Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning has emerged as a promising approach to enhancing large
language models in following human instructions. It is shown that increasing
the diversity and number of instructions in the training data can consistently
enhance generalization performance, which facilitates a recent endeavor to
collect various instructions and integrate existing instruction tuning datasets
into larger collections. However, different users have their unique ways of
expressing instructions, and there often exist variations across different
datasets in the instruction styles and formats, i.e., format inconsistency. In
this work, we propose a framework named Unified Instruction Tuning (UIT), which
calls OpenAI APIs for automatic format transfer among different instruction
tuning datasets such as PromptSource, FLAN and CrossFit. With the framework, we
(1) demonstrate the necessity of maintaining format consistency in instruction
tuning; (2) improve the generalization performance on unseen instructions on
T5-LM-xl; (3) provide a novel perplexity-based denoising method to reduce the
noise of automatic format transfer to make the UIT framework more practical and
a smaller offline model based on GPT-J that achieves comparable format transfer
capability to OpenAI APIs to reduce costs in practice. Further analysis
regarding variations of targeted formats and other effects is intended.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Synthetic Data Generation in Low-Resource Settings via Fine-Tuning of
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01119v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01119v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean Kaddour, Qi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The in-context learning ability of large language models (LLMs) enables them
to generalize to novel downstream tasks with relatively few labeled examples.
However, they require enormous computational resources to be deployed.
Alternatively, smaller models can solve specific tasks if fine-tuned with
enough labeled examples. These examples, however, are expensive to obtain. In
pursuit of the best of both worlds, we study synthetic data generation of
fine-tuning training data via fine-tuned teacher LLMs to improve the downstream
performance of much smaller models. In four text classification and two text
generation tasks, we find that both data generation and annotation dramatically
improve the respective downstream model's performance, occasionally
necessitating only a minor fraction of the original training dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TextBind: Multi-turn Interleaved Multimodal Instruction-following in the
  Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08637v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08637v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huayang Li, Siheng Li, Deng Cai, Longyue Wang, Lemao Liu, Taro Watanabe, Yujiu Yang, Shuming Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models with instruction-following abilities have
revolutionized the field of artificial intelligence. These models show
exceptional generalizability to tackle various real-world tasks through their
natural language interfaces. However, their performance heavily relies on
high-quality exemplar data, which is often difficult to obtain. This challenge
is further exacerbated when it comes to multimodal instruction following. We
introduce TextBind, an almost annotation-free framework for empowering larger
language models with the multi-turn interleaved multimodal
instruction-following capabilities. Our approach requires only image-caption
pairs and generates multi-turn multimodal instruction-response conversations
from a language model. To accommodate interleaved image-text inputs and
outputs, we devise MIM, a language model-centric architecture that seamlessly
integrates image encoder and decoder models. We release our dataset, model, and
demo to foster future research in the area of multimodal instruction following.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://textbind.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convoifilter: A case study of doing cocktail party speech recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.11380v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.11380v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thai-Binh Nguyen, Alexander Waibel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an end-to-end model designed to improve automatic speech
recognition (ASR) for a particular speaker in a crowded, noisy environment. The
model utilizes a single-channel speech enhancement module that isolates the
speaker's voice from background noise (ConVoiFilter) and an ASR module. The
model can decrease ASR's word error rate (WER) from 80% to 26.4% through this
approach. Typically, these two components are adjusted independently due to
variations in data requirements. However, speech enhancement can create
anomalies that decrease ASR efficiency. By implementing a joint fine-tuning
strategy, the model can reduce the WER from 26.4% in separate tuning to 14.5%
in joint tuning. We openly share our pre-trained model to foster further
research hf.co/nguyenvulebinh/voice-filter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Granularity Information Interaction Framework for Incomplete
  Utterance Rewriting <span class="chip">EMNLP2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11945v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11945v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowei Du, Dinghao Zhang, Chen Li, Yang Li, Dongyan Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent approaches in Incomplete Utterance Rewriting (IUR) fail to capture the
source of important words, which is crucial to edit the incomplete utterance,
and introduce words from irrelevant utterances. We propose a novel and
effective multi-task information interaction framework including context
selection, edit matrix construction, and relevance merging to capture the
multi-granularity of semantic information. Benefiting from fetching the
relevant utterance and figuring out the important words, our approach
outperforms existing state-of-the-art models on two benchmark datasets
Restoration-200K and CANAND in this field. Code will be provided on
\url{https://github.com/yanmenxue/QR}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of EMNLP2023 (short)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LEXTREME: A Multi-Lingual and Multi-Task <span class="highlight-title">Benchmark</span> for the Legal Domain <span class="chip">EMNLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.13126v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.13126v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joel Niklaus, Veton Matoshi, Pooja Rani, Andrea Galassi, Matthias Stürmer, Ilias Chalkidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lately, propelled by the phenomenal advances around the transformer
architecture, the legal NLP field has enjoyed spectacular growth. To measure
progress, well curated and challenging benchmarks are crucial. However, most
benchmarks are English only and in legal NLP specifically there is no
multilingual benchmark available yet. Additionally, many benchmarks are
saturated, with the best models clearly outperforming the best humans and
achieving near perfect scores. We survey the legal NLP literature and select 11
datasets covering 24 languages, creating LEXTREME. To provide a fair
comparison, we propose two aggregate scores, one based on the datasets and one
on the languages. The best baseline (XLM-R large) achieves both a dataset
aggregate score a language aggregate score of 61.3. This indicates that
LEXTREME is still very challenging and leaves ample room for improvement. To
make it easy for researchers and practitioners to use, we release LEXTREME on
huggingface together with all the code required to evaluate models and a public
Weights and Biases project with all the runs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at EMNLP Findings 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quality and Quantity of Machine Translation References for Automated
  Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01283v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01283v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vilém Zouhar, Ondřej Bojar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic machine translation metrics often use human translations to
determine the quality of system translations. Common wisdom in the field
dictates that the human references should be of very high quality. However,
there are no cost-benefit analyses that could be used to guide practitioners
who plan to collect references for machine translation evaluation. We find that
higher-quality references lead to better metric correlations with humans at the
segment-level. Having up to 7 references per segment and taking their average
helps all metrics. Interestingly, the references from vendors of different
qualities can be mixed together and improve metric success. Higher quality
references, however, cost more to create and we frame this as an optimization
problem: given a specific budget, what references should be collected to
maximize metric success. These findings can be used by evaluators of shared
tasks when references need to be created under a certain budget.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Large Language Models Understand Real-World Complex Instructions? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09150v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09150v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianyu He, Jie Zeng, Wenhao Huang, Lina Chen, Jin Xiao, Qianxi He, Xunzhe Zhou, Lida Chen, Xintao Wang, Yuncheng Huang, Haoning Ye, Zihan Li, Shisong Chen, Yikai Zhang, Zhouhong Gu, Jiaqing Liang, Yanghua Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can understand human instructions, showing their
potential for pragmatic applications beyond traditional NLP tasks. However,
they still struggle with complex instructions, which can be either complex task
descriptions that require multiple tasks and constraints, or complex input that
contains long context, noise, heterogeneous information and multi-turn format.
Due to these features, LLMs often ignore semantic constraints from task
descriptions, generate incorrect formats, violate length or sample count
constraints, and be unfaithful to the input text. Existing benchmarks are
insufficient to assess LLMs' ability to understand complex instructions, as
they are close-ended and simple. To bridge this gap, we propose CELLO, a
benchmark for evaluating LLMs' ability to follow complex instructions
systematically. We design eight features for complex instructions and construct
a comprehensive evaluation dataset from real-world scenarios. We also establish
four criteria and develop corresponding metrics, as current ones are
inadequate, biased or too strict and coarse-grained. We compare the performance
of representative Chinese-oriented and English-oriented models in following
complex instructions through extensive experiments. Resources of CELLO are
publicly available at https://github.com/Abbey4799/CELLO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kiki or Bouba? Sound Symbolism in Vision-and-Language Models <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16781v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16781v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Morris Alper, Hadar Averbuch-Elor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although the mapping between sound and meaning in human language is assumed
to be largely arbitrary, research in cognitive science has shown that there are
non-trivial correlations between particular sounds and meanings across
languages and demographic groups, a phenomenon known as sound symbolism. Among
the many dimensions of meaning, sound symbolism is particularly salient and
well-demonstrated with regards to cross-modal associations between language and
the visual domain. In this work, we address the question of whether sound
symbolism is reflected in vision-and-language models such as CLIP and Stable
Diffusion. Using zero-shot knowledge probing to investigate the inherent
knowledge of these models, we find strong evidence that they do show this
pattern, paralleling the well-known kiki-bouba effect in psycholinguistics. Our
work provides a novel method for demonstrating sound symbolism and
understanding its nature using computational tools. Our code will be made
publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2023 (spotlight). Project webpage:
  https://kiki-bouba.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Natural Language Decomposition and Interpretation of Complex Utterances 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.08677v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.08677v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harsh Jhamtani, Hao Fang, Patrick Xia, Eran Levy, Jacob Andreas, Ben Van Durme
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing natural language interfaces has historically required collecting
supervised data to translate user requests into carefully designed intent
representations. This requires enumerating and labeling a long tail of user
requests, which is challenging. At the same time, large language models (LLMs)
encode knowledge about goals and plans that can help conversational assistants
interpret user requests requiring numerous steps to complete. We introduce an
approach to handle complex-intent-bearing utterances from a user via a process
of hierarchical natural language decomposition and interpretation. Our approach
uses a pre-trained language model to decompose a complex utterance into a
sequence of simpler natural language steps and interprets each step using the
language-to-program model designed for the interface. To test our approach, we
collect and release DeCU -- a new NL-to-program benchmark to evaluate
Decomposition of Complex Utterances. Experiments show that the proposed
approach enables the interpretation of complex utterances with almost no
complex training data, while outperforming standard few-shot prompting
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AlpacaFarm: A Simulation Framework for Methods that Learn from Human
  Feedback <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14387v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14387v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, Tatsunori B. Hashimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) such as ChatGPT have seen widespread adoption
due to their strong instruction-following abilities. Developing these LLMs
involves a complex yet poorly understood workflow requiring training with human
feedback. Replicating and understanding this instruction-following requires
tackling three major challenges: the high cost of data collection, the lack of
trustworthy evaluation, and the absence of reference method implementations. We
address these challenges with AlpacaFarm, a simulator that enables research and
development for learning from feedback at a low cost. First, we design LLM
prompts to simulate human feedback that are 50x cheaper than crowdworkers and
display high agreement with humans. Second, we propose an automatic evaluation
and validate it against human instructions obtained on real-world interactions.
Third, we contribute reference implementations for several methods (PPO, DPO,
best-of-n, expert iteration, and more) that learn from pairwise feedback.
Finally, as an end-to-end validation of AlpacaFarm, we train and evaluate
eleven models on 10k pairs of real human feedback and show that rankings of
models trained in AlpacaFarm match rankings of models trained on human data. As
a demonstration of the research possible in AlpacaFarm, we find that methods
that use a reward model can substantially improve over supervised fine-tuning
and that our reference PPO implementation leads to a +10% improvement in
win-rate against Davinci003. We release all components of AlpacaFarm at
https://github.com/tatsu-lab/alpaca_farm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Spotlight at NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ YAYI-UIE: A Chat-Enhanced Instruction Tuning Framework for Universal
  Information Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15548v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15548v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinglin Xiao, Yijie Wang, Nan Xu, Yuqi Wang, Hanxuan Yang, Minzheng Wang, Yin Luo, Lei Wang, Wenji Mao, Daniel Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The difficulty of the information extraction task lies in dealing with the
task-specific label schemas and heterogeneous data structures. Recent work has
proposed methods based on large language models to uniformly model different
information extraction tasks. However, these existing methods are deficient in
their information extraction capabilities for Chinese languages other than
English. In this paper, we propose an end-to-end chat-enhanced instruction
tuning framework for universal information extraction (YAYI-UIE), which
supports both Chinese and English. Specifically, we utilize dialogue data and
information extraction data to enhance the information extraction performance
jointly. Experimental results show that our proposed framework achieves
state-of-the-art performance on Chinese datasets while also achieving
comparable performance on English datasets under both supervised settings and
zero-shot settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Modal Discussion Transformer: Integrating Text, Images and Graph
  Transformers to Detect Hate Speech on Social Media 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09312v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09312v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liam Hebert, Gaurav Sahu, Yuxuan Guo, Nanda Kishore Sreenivas, Lukasz Golab, Robin Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the Multi-Modal Discussion Transformer (mDT), a novel methodfor
detecting hate speech in online social networks such as Reddit discussions. In
contrast to traditional comment-only methods, our approach to labelling a
comment as hate speech involves a holistic analysis of text and images grounded
in the discussion context. This is done by leveraging graph transformers to
capture the contextual relationships in the discussion surrounding a comment
and grounding the interwoven fusion layers that combine text and image
embeddings instead of processing modalities separately. To evaluate our work,
we present a new dataset, HatefulDiscussions, comprising complete multi-modal
discussions from multiple online communities on Reddit. We compare the
performance of our model to baselines that only process individual comments and
conduct extensive ablation studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2024 (AI for Social Impact Track)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">89</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dr$^2$Net: Dynamic Reversible Dual-Residual Networks for
  Memory-Efficient Finetuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.04105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.04105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Zhao, Shuming Liu, Karttikeya Mangalam, Guocheng Qian, Fatimah Zohra, Abdulmohsen Alghannam, Jitendra Malik, Bernard Ghanem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large pretrained models are increasingly crucial in modern computer vision
tasks. These models are typically used in downstream tasks by end-to-end
finetuning, which is highly memory-intensive for tasks with high-resolution
data, e.g., video understanding, small object detection, and point cloud
analysis. In this paper, we propose Dynamic Reversible Dual-Residual Networks,
or Dr$^2$Net, a novel family of network architectures that acts as a surrogate
network to finetune a pretrained model with substantially reduced memory
consumption. Dr$^2$Net contains two types of residual connections, one
maintaining the residual structure in the pretrained models, and the other
making the network reversible. Due to its reversibility, intermediate
activations, which can be reconstructed from output, are cleared from memory
during training. We use two coefficients on either type of residual connections
respectively, and introduce a dynamic training strategy that seamlessly
transitions the pretrained model to a reversible network with much higher
numerical precision. We evaluate Dr$^2$Net on various pretrained models and
various tasks, and show that it can reach comparable performance to
conventional finetuning but with significantly less memory usage.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AGG: Amortized Generative 3D Gaussians for Single Image to 3D 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.04099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.04099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dejia Xu, Ye Yuan, Morteza Mardani, Sifei Liu, Jiaming Song, Zhangyang Wang, Arash Vahdat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the growing need for automatic 3D content creation pipelines, various
3D representations have been studied to generate 3D objects from a single
image. Due to its superior rendering efficiency, 3D Gaussian splatting-based
models have recently excelled in both 3D reconstruction and generation. 3D
Gaussian splatting approaches for image to 3D generation are often
optimization-based, requiring many computationally expensive score-distillation
steps. To overcome these challenges, we introduce an Amortized Generative 3D
Gaussian framework (AGG) that instantly produces 3D Gaussians from a single
image, eliminating the need for per-instance optimization. Utilizing an
intermediate hybrid representation, AGG decomposes the generation of 3D
Gaussian locations and other appearance attributes for joint optimization.
Moreover, we propose a cascaded pipeline that first generates a coarse
representation of the 3D data and later upsamples it with a 3D Gaussian
super-resolution module. Our method is evaluated against existing
optimization-based 3D Gaussian frameworks and sampling-based pipelines
utilizing other 3D representations, where AGG showcases competitive generation
abilities both qualitatively and quantitatively while being several orders of
magnitude faster. Project page: https://ir1d.github.io/AGG/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://ir1d.github.io/AGG/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GPT-4V(ision) is a Human-Aligned Evaluator for Text-to-3D Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.04092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.04092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Wu, Guandao Yang, Zhibing Li, Kai Zhang, Ziwei Liu, Leonidas Guibas, Dahua Lin, Gordon Wetzstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advances in text-to-3D generative methods, there is a notable
absence of reliable evaluation metrics. Existing metrics usually focus on a
single criterion each, such as how well the asset aligned with the input text.
These metrics lack the flexibility to generalize to different evaluation
criteria and might not align well with human preferences. Conducting user
preference studies is an alternative that offers both adaptability and
human-aligned results. User studies, however, can be very expensive to scale.
This paper presents an automatic, versatile, and human-aligned evaluation
metric for text-to-3D generative models. To this end, we first develop a prompt
generator using GPT-4V to generate evaluating prompts, which serve as input to
compare text-to-3D models. We further design a method instructing GPT-4V to
compare two 3D assets according to user-defined criteria. Finally, we use these
pairwise comparison results to assign these models Elo ratings. Experimental
results suggest our metric strongly align with human preference across
different evaluation criteria.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://gpteval3d.github.io/; Code:
  https://github.com/3DTopia/GPTEval3D</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RudolfV: A Foundation Model by Pathologists for Pathologists 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.04079v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.04079v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Dippel, Barbara Feulner, Tobias Winterhoff, Simon Schallenberg, Gabriel Dernbach, Andreas Kunft, Stephan Tietz, Philipp Jurmeister, David Horst, Lukas Ruff, Klaus-Robert Müller, Frederick Klauschen, Maximilian Alber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Histopathology plays a central role in clinical medicine and biomedical
research. While artificial intelligence shows promising results on many
pathological tasks, generalization and dealing with rare diseases, where
training data is scarce, remains a challenge. Distilling knowledge from
unlabeled data into a foundation model before learning from, potentially
limited, labeled data provides a viable path to address these challenges. In
this work, we extend the state of the art of foundation models for digital
pathology whole slide images by semi-automated data curation and incorporating
pathologist domain knowledge. Specifically, we combine computational and
pathologist domain knowledge (1) to curate a diverse dataset of 103k slides
corresponding to 750 million image patches covering data from different
fixation, staining, and scanning protocols as well as data from different
indications and labs across the EU and US, (2) for grouping semantically
similar slides and tissue patches, and (3) to augment the input images during
training. We evaluate the resulting model on a set of public and internal
benchmarks and show that although our foundation model is trained with an order
of magnitude less slides, it performs on par or better than competing models.
We expect that scaling our approach to more data and larger models will further
increase its performance and capacity to deal with increasingly complex real
world tasks in diagnostics and biomedical research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fun with Flags: Robust Principal Directions via Flag Manifolds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.04071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.04071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Mankovich, Gustau Camps-Valls, Tolga Birdal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Principal component analysis (PCA), along with its extensions to manifolds
and outlier contaminated data, have been indispensable in computer vision and
machine learning. In this work, we present a unifying formalism for PCA and its
variants, and introduce a framework based on the flags of linear subspaces, \ie
a hierarchy of nested linear subspaces of increasing dimension, which not only
allows for a common implementation but also yields novel variants, not explored
previously. We begin by generalizing traditional PCA methods that either
maximize variance or minimize reconstruction error. We expand these
interpretations to develop a wide array of new dimensionality reduction
algorithms by accounting for outliers and the data manifold. To devise a common
computational approach, we recast robust and dual forms of PCA as optimization
problems on flag manifolds. We then integrate tangent space approximations of
principal geodesic analysis (tangent-PCA) into this flag-based framework,
creating novel robust and dual geodesic PCA variations. The remarkable
flexibility offered by the 'flagification' introduced here enables even more
algorithmic variants identified by specific flag types. Last but not least, we
propose an effective convergent solver for these flag-formulations employing
the Stiefel manifold. Our empirical results on both real-world and synthetic
scenarios, demonstrate the superiority of our novel algorithms, especially in
terms of robustness to outliers on manifolds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Multiscale Multimodal Bottleneck Transformer for Audio-Video
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.04023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.04023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, researchers combine both audio and video signals to deal
with challenges where actions are not well represented or captured by visual
cues. However, how to effectively leverage the two modalities is still under
development. In this work, we develop a multiscale multimodal Transformer (MMT)
that leverages hierarchical representation learning. Particularly, MMT is
composed of a novel multiscale audio Transformer (MAT) and a multiscale video
Transformer [43]. To learn a discriminative cross-modality fusion, we further
design multimodal supervised contrastive objectives called audio-video
contrastive loss (AVC) and intra-modal contrastive loss (IMC) that robustly
align the two modalities. MMT surpasses previous state-of-the-art approaches by
7.3% and 2.1% on Kinetics-Sounds and VGGSound in terms of the top-1 accuracy
without external training data. Moreover, the proposed MAT significantly
outperforms AST [28] by 22.2%, 4.4% and 4.7% on three public benchmark
datasets, and is about 3% more efficient based on the number of FLOPs and 9.8%
more efficient based on GPU memory usage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WACV 2024; well-formatted PDF is in
  https://drive.google.com/file/d/10Zo_ydJZFAm7YsxHDgTjhyc4dEJbW_dk/view?usp=sharing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Behavioural Cloning in VizDoom 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03993v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03993v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Spick, Timothy Bradley, Ayush Raina, Pierluigi Vito Amadori, Guy Moss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes methods for training autonomous agents to play the game
"Doom 2" through Imitation Learning (IL) using only pixel data as input. We
also explore how Reinforcement Learning (RL) compares to IL for humanness by
comparing camera movement and trajectory data. Through behavioural cloning, we
examine the ability of individual models to learn varying behavioural traits.
We attempt to mimic the behaviour of real players with different play styles,
and find we can train agents that behave aggressively, passively, or simply
more human-like than traditional AIs. We propose these methods of introducing
more depth and human-like behaviour to agents in video games. The trained IL
agents perform on par with the average players in our dataset, whilst
outperforming the worst players. While performance was not as strong as common
RL approaches, it provides much stronger human-like behavioural traits to the
agent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MS-DETR: Efficient DETR Training with Mixed Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03989v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03989v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuyang Zhao, Yifan Sun, Wenhao Wang, Qiang Chen, Errui Ding, Yi Yang, Jingdong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DETR accomplishes end-to-end object detection through iteratively generating
multiple object candidates based on image features and promoting one candidate
for each ground-truth object. The traditional training procedure using
one-to-one supervision in the original DETR lacks direct supervision for the
object detection candidates.
  We aim at improving the DETR training efficiency by explicitly supervising
the candidate generation procedure through mixing one-to-one supervision and
one-to-many supervision. Our approach, namely MS-DETR, is simple, and places
one-to-many supervision to the object queries of the primary decoder that is
used for inference. In comparison to existing DETR variants with one-to-many
supervision, such as Group DETR and Hybrid DETR, our approach does not need
additional decoder branches or object queries. The object queries of the
primary decoder in our approach directly benefit from one-to-many supervision
and thus are superior in object candidate prediction. Experimental results show
that our approach outperforms related DETR variants, such as DN-DETR, Hybrid
DETR, and Group DETR, and the combination with related DETR variants further
improves the performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-scale attention-based instance segmentation for measuring crystals
  with large size variation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03939v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03939v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theresa Neubauer, Astrid Berg, Maria Wimmer, Dimitrios Lenis, David Major, Philip Matthias Winter, Gaia Romana De Paolis, Johannes Novotny, Daniel Lüftner, Katja Reinharter, Katja Bühler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantitative measurement of crystals in high-resolution images allows for
important insights into underlying material characteristics. Deep learning has
shown great progress in vision-based automatic crystal size measurement, but
current instance segmentation methods reach their limits with images that have
large variation in crystal size or hard to detect crystal boundaries. Even
small image segmentation errors, such as incorrectly fused or separated
segments, can significantly lower the accuracy of the measured results. Instead
of improving the existing pixel-wise boundary segmentation methods, we propose
to use an instance-based segmentation method, which gives more robust
segmentation results to improve measurement accuracy. Our novel method enhances
flow maps with a size-aware multi-scale attention module. The attention module
adaptively fuses information from multiple scales and focuses on the most
relevant scale for each segmented image area. We demonstrate that our proposed
attention fusion strategy outperforms state-of-the-art instance and boundary
segmentation methods, as well as simple average fusion of multi-scale
predictions. We evaluate our method on a refractory raw material dataset of
high-resolution images with large variation in crystal size and show that our
model can be used to calculate the crystal size more accurately than existing
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>has been accepted for publication in IEEE Transactions on
  Instrumentation and Measurement</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structure-focused Neurodegeneration Convolutional Neural Network for
  Modeling and Classification of Alzheimer's Disease 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03922v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03922v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simisola Odimayo, Chollette C. Olisah, Khadija Mohammed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alzheimer's disease (AD), the predominant form of dementia, poses a growing
global challenge and underscores the urgency of accurate and early diagnosis.
The clinical technique radiologists adopt for distinguishing between mild
cognitive impairment (MCI) and AD using Machine Resonance Imaging (MRI)
encounter hurdles because they are not consistent and reliable. Machine
learning has been shown to offer promise for early AD diagnosis. However,
existing models focused on focal fine-grain features without considerations to
focal structural features that give off information on neurodegeneration of the
brain cerebral cortex. Therefore, this paper proposes a machine learning (ML)
framework that integrates Gamma correction, an image enhancement technique, and
includes a structure-focused neurodegeneration convolutional neural network
(CNN) architecture called SNeurodCNN for discriminating between AD and MCI. The
ML framework leverages the mid-sagittal and para-sagittal brain image
viewpoints of the structure-focused Alzheimer's Disease Neuroimaging Initiative
(ADNI) dataset. Through experiments, our proposed machine learning framework
shows exceptional performance. The parasagittal viewpoint set achieves 97.8%
accuracy, with 97.0% specificity and 98.5% sensitivity. The midsagittal
viewpoint is shown to present deeper insights into the structural brain changes
given the increase in accuracy, specificity, and sensitivity, which are 98.1%
97.2%, and 99.0%, respectively. Using GradCAM technique, we show that our
proposed model is capable of capturing the structural dynamics of MCI and AD
which exist about the frontal lobe, occipital lobe, cerebellum, and parietal
lobe. Therefore, our model itself as a potential brain structural change
Digi-Biomarker for early diagnosis of AD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 Pages, 2 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ D3PRefiner: A Diffusion-based Denoise Method for 3D Human Pose
  Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03914v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03914v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danqi Yan, Qing Gao, Yuepeng Qian, Xinxing Chen, Chenglong Fu, Yuquan Leng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Three-dimensional (3D) human pose estimation using a monocular camera has
gained increasing attention due to its ease of implementation and the abundance
of data available from daily life. However, owing to the inherent depth
ambiguity in images, the accuracy of existing monocular camera-based 3D pose
estimation methods remains unsatisfactory, and the estimated 3D poses usually
include much noise. By observing the histogram of this noise, we find each
dimension of the noise follows a certain distribution, which indicates the
possibility for a neural network to learn the mapping between noisy poses and
ground truth poses. In this work, in order to obtain more accurate 3D poses, a
Diffusion-based 3D Pose Refiner (D3PRefiner) is proposed to refine the output
of any existing 3D pose estimator. We first introduce a conditional
multivariate Gaussian distribution to model the distribution of noisy 3D poses,
using paired 2D poses and noisy 3D poses as conditions to achieve greater
accuracy. Additionally, we leverage the architecture of current diffusion
models to convert the distribution of noisy 3D poses into ground truth 3D
poses. To evaluate the effectiveness of the proposed method, two
state-of-the-art sequence-to-sequence 3D pose estimators are used as basic 3D
pose estimation models, and the proposed method is evaluated on different types
of 2D poses and different lengths of the input sequence. Experimental results
demonstrate the proposed architecture can significantly improve the performance
of current sequence-to-sequence 3D pose estimators, with a reduction of at
least 10.3% in the mean per joint position error (MPJPE) and at least 11.0% in
the Procrustes MPJPE (P-MPJPE).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention-Guided Erasing: A Novel Augmentation Method for Enhancing
  Downstream Breast Density Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03912v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03912v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adarsh Bhandary Panambur, Hui Yu, Sheethal Bhat, Prathmesh Madhu, Siming Bayer, Andreas Maier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The assessment of breast density is crucial in the context of breast cancer
screening, especially in populations with a higher percentage of dense breast
tissues. This study introduces a novel data augmentation technique termed
Attention-Guided Erasing (AGE), devised to enhance the downstream
classification of four distinct breast density categories in mammography
following the BI-RADS recommendation in the Vietnamese cohort. The proposed
method integrates supplementary information during transfer learning, utilizing
visual attention maps derived from a vision transformer backbone trained using
the self-supervised DINO method. These maps are utilized to erase background
regions in the mammogram images, unveiling only the potential areas of dense
breast tissues to the network. Through the incorporation of AGE during transfer
learning with varying random probabilities, we consistently surpass
classification performance compared to scenarios without AGE and the
traditional random erasing transformation. We validate our methodology using
the publicly available VinDr-Mammo dataset. Specifically, we attain a mean
F1-score of 0.5910, outperforming values of 0.5594 and 0.5691 corresponding to
scenarios without AGE and with random erasing (RE), respectively. This
superiority is further substantiated by t-tests, revealing a p-value of
p<0.0001, underscoring the statistical significance of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RoboFusion: Towards Robust Multi-Modal 3D obiect Detection via SAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03907v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03907v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziying Song, Guoxing Zhang, Lin Liu, Lei Yang, Shaoqing Xu, Caiyan Jia, Feiyang Jia, Li Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal 3D object detectors are dedicated to exploring secure and
reliable perception systems for autonomous driving (AD). However, while
achieving state-of-the-art (SOTA) performance on clean benchmark datasets, they
tend to overlook the complexity and harsh conditions of real-world
environments. Meanwhile, with the emergence of visual foundation models (VFMs),
opportunities and challenges are presented for improving the robustness and
generalization of multi-modal 3D object detection in autonomous driving.
Therefore, we propose RoboFusion, a robust framework that leverages VFMs like
SAM to tackle out-of-distribution (OOD) noise scenarios. We first adapt the
original SAM for autonomous driving scenarios named SAM-AD. To align SAM or
SAM-AD with multi-modal methods, we then introduce AD-FPN for upsampling the
image features extracted by SAM. We employ wavelet decomposition to denoise the
depth-guided images for further noise reduction and weather interference.
Lastly, we employ self-attention mechanisms to adaptively reweight the fused
features, enhancing informative features while suppressing excess noise. In
summary, our RoboFusion gradually reduces noise by leveraging the
generalization and robustness of VFMs, thereby enhancing the resilience of
multi-modal 3D object detection. Consequently, our RoboFusion achieves
state-of-the-art performance in noisy scenarios, as demonstrated by the KITTI-C
and nuScenes-C benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STAIR: Spatial-Temporal Reasoning with Auditable Intermediate Results
  for Video Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03901v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03901v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueqian Wang, Yuxuan Wang, Kai Chen, Dongyan Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently we have witnessed the rapid development of video question answering
models. However, most models can only handle simple videos in terms of temporal
reasoning, and their performance tends to drop when answering
temporal-reasoning questions on long and informative videos. To tackle this
problem we propose STAIR, a Spatial-Temporal Reasoning model with Auditable
Intermediate Results for video question answering. STAIR is a neural module
network, which contains a program generator to decompose a given question into
a hierarchical combination of several sub-tasks, and a set of lightweight
neural modules to complete each of these sub-tasks. Though neural module
networks are already widely studied on image-text tasks, applying them to
videos is a non-trivial task, as reasoning on videos requires different
abilities. In this paper, we define a set of basic video-text sub-tasks for
video question answering and design a set of lightweight modules to complete
them. Different from most prior works, modules of STAIR return intermediate
outputs specific to their intentions instead of always returning attention
maps, which makes it easier to interpret and collaborate with pre-trained
models. We also introduce intermediate supervision to make these intermediate
outputs more accurate. We conduct extensive experiments on several video
question answering datasets under various settings to show STAIR's performance,
explainability, compatibility with pre-trained models, and applicability when
program annotations are not available. Code:
https://github.com/yellow-binary-tree/STAIR
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Survey on 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03890v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03890v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guikun Chen, Wenguan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian splatting (3D GS) has recently emerged as a transformative
technique in the explicit radiance field and computer graphics landscape. This
innovative approach, characterized by the utilization of millions of 3D
Gaussians, represents a significant departure from the neural radiance field
(NeRF) methodologies, which predominantly use implicit, coordinate-based models
to map spatial coordinates to pixel values. 3D GS, with its explicit scene
representations and differentiable rendering algorithms, not only promises
real-time rendering capabilities but also introduces unprecedented levels of
control and editability. This positions 3D GS as a potential game-changer for
the next generation of 3D reconstruction and representation. In the present
paper, we provide the first systematic overview of the recent developments and
critical contributions in the domain of 3D GS. We begin with a detailed
exploration of the underlying principles and the driving forces behind the
advent of 3D GS, setting the stage for understanding its significance. A focal
point of our discussion is the practical applicability of 3D GS. By
facilitating real-time performance, 3D GS opens up a plethora of applications,
ranging from virtual reality to interactive media and beyond. This is
complemented by a comparative analysis of leading 3D GS models, evaluated
across various benchmark tasks to highlight their performance and practical
utility. The survey concludes by identifying current challenges and suggesting
potential avenues for future research in this domain. Through this survey, we
aim to provide a valuable resource for both newcomers and seasoned researchers,
fostering further exploration and advancement in applicable and explicit
radiance field representation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing project</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A New <span class="highlight-title">Dataset</span> and a Distractor-Aware Architecture for Transparent Object
  Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alan Lukezic, Ziga Trojer, Jiri Matas, Matej Kristan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Performance of modern trackers degrades substantially on transparent objects
compared to opaque objects. This is largely due to two distinct reasons.
Transparent objects are unique in that their appearance is directly affected by
the background. Furthermore, transparent object scenes often contain many
visually similar objects (distractors), which often lead to tracking failure.
However, development of modern tracking architectures requires large training
sets, which do not exist in transparent object tracking. We present two
contributions addressing the aforementioned issues. We propose the first
transparent object tracking training dataset Trans2k that consists of over 2k
sequences with 104,343 images overall, annotated by bounding boxes and
segmentation masks. Standard trackers trained on this dataset consistently
improve by up to 16%. Our second contribution is a new distractor-aware
transparent object tracker (DiTra) that treats localization accuracy and target
identification as separate tasks and implements them by a novel architecture.
DiTra sets a new state-of-the-art in transparent object tracking and
generalizes well to opaque objects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under the review. arXiv admin note: substantial text overlap with
  arXiv:2210.03436</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gramformer: Learning Crowd Counting via Graph-Modulated Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Lin, Zhiheng Ma, Xiaopeng Hong, Qinnan Shangguan, Deyu Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer has been popular in recent crowd counting work since it breaks
the limited receptive field of traditional CNNs. However, since crowd images
always contain a large number of similar patches, the self-attention mechanism
in Transformer tends to find a homogenized solution where the attention maps of
almost all patches are identical. In this paper, we address this problem by
proposing Gramformer: a graph-modulated transformer to enhance the network by
adjusting the attention and input node features respectively on the basis of
two different types of graphs. Firstly, an attention graph is proposed to
diverse attention maps to attend to complementary information. The graph is
building upon the dissimilarities between patches, modulating the attention in
an anti-similarity fashion. Secondly, a feature-based centrality encoding is
proposed to discover the centrality positions or importance of nodes. We encode
them with a proposed centrality indices scheme to modulate the node features
and similarity relationships. Extensive experiments on four challenging crowd
counting datasets have validated the competitiveness of the proposed method.
Code is available at {https://github.com/LoraLinH/Gramformer}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is the accepted version of the paper and supplemental material
  to appear in AAAI 2024. Please cite the final published version. Code is
  available at {https://github.com/LoraLinH/Gramformer}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TIER: Text and Image Encoder-based Regression for AIGC Image Quality
  Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiquan Yuan, Xinyan Cao, Jinming Che, Qinyuan Wang, Sen Liang, Wei Ren, Jinlong Lin, Xixin Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, AIGC image quality assessment (AIGCIQA), which aims to assess the
quality of AI-generated images from a human perception perspective, has emerged
as a new topic in computer vision. Unlike common image quality assessment tasks
where images are derived from original ones distorted by noise, blur, and
compression, in AIGCIQA tasks, images are typically generated by generative
models using text prompts. Considerable efforts have been made in the past
years to advance AIGCIQA. However, most existing AIGCIQA methods regress
predicted scores directly from individual generated images, overlooking the
information contained in the text prompts of these images. This oversight
partially limits the performance of these AIGCIQA methods. To address this
issue, we propose a text and image encoder-based regression (TIER) framework.
Specifically, we process the generated images and their corresponding text
prompts as inputs, utilizing a text encoder and an image encoder to extract
features from these text prompts and generated images, respectively. To
demonstrate the effectiveness of our proposed TIER method, we conduct extensive
experiments on several mainstream AIGCIQA databases, including AGIQA-1K,
AGIQA-3K, and AIGCIQA2023. The experimental results indicate that our proposed
TIER method generally demonstrates superior performance compared to baseline in
most cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures. arXiv admin note: text overlap with
  arXiv:2312.05897</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aligned with LLM: a new multi-modal training paradigm for encoding fMRI
  activity in visual cortex 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03851v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03851v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuxiao Ma, Linyuan Wang, Senbao Hou, Bin Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been a surge in the popularity of pre trained large
language models (LLMs) (such as GPT-4), sweeping across the entire Natural
Language Processing (NLP) and Computer Vision (CV) communities. These LLMs have
demonstrated advanced multi-modal understanding capabilities and showcased
strong performance across various benchmarks. The LLM has started to embody
traits of artificial general intelligence, which holds vital guidance for
enhancing brain-like characteristics within visual encoding models. Hence, This
paper proposes a new multi-modal training paradigm, aligning with LLM, for
encoding fMRI activity in visual cortex. Based on this paradigm, we trained an
encoding model in fMRI data named the LLM-Visual Encoding Model (LLM-VEM).
Specifically, we utilize LLM (miniGPT4) to generate descriptive text for all
stimulus images, forming a high-quality textual description set. Moreover, we
use the pre-trained text encoder (CLIP) to process these detailed descriptions,
obtaining the text embedding features. Next, we use the contrast loss function
to minimize the distance between the image embedding features and the text
embedding features to complete the alignment operation of the stimulus image
and text information. With the assistance of the pre-trained LLM, this
alignment process facilitates better learning of the visual encoding model,
resulting in higher precision. The final experimental results indicate that our
training paradigm has significantly aided in enhancing the performance of the
visual encoding model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UFO: Unidentified Foreground Object Detection in 3D Point Cloud 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunjun Choi, Hawook Jeong, Jin Young Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we raise a new issue on Unidentified Foreground Object (UFO)
detection in 3D point clouds, which is a crucial technology in autonomous
driving in the wild. UFO detection is challenging in that existing 3D object
detectors encounter extremely hard challenges in both 3D localization and
Out-of-Distribution (OOD) detection. To tackle these challenges, we suggest a
new UFO detection framework including three tasks: evaluation protocol,
methodology, and benchmark. The evaluation includes a new approach to measure
the performance on our goal, i.e. both localization and OOD detection of UFOs.
The methodology includes practical techniques to enhance the performance of our
goal. The benchmark is composed of the KITTI Misc benchmark and our additional
synthetic benchmark for modeling a more diverse range of UFOs. The proposed
framework consistently enhances performance by a large margin across all four
baseline detectors: SECOND, PointPillars, PV-RCNN, and PartA2, giving insight
for future work on UFO detection in the wild.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fully Attentional Networks with Self-emerging Token Labeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingyin Zhao, Zhiding Yu, Shiyi Lan, Yutao Cheng, Anima Anandkumar, Yingjie Lao, Jose M. Alvarez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies indicate that Vision Transformers (ViTs) are robust against
out-of-distribution scenarios. In particular, the Fully Attentional Network
(FAN) - a family of ViT backbones, has achieved state-of-the-art robustness. In
this paper, we revisit the FAN models and improve their pre-training with a
self-emerging token labeling (STL) framework. Our method contains a two-stage
training framework. Specifically, we first train a FAN token labeler (FAN-TL)
to generate semantically meaningful patch token labels, followed by a FAN
student model training stage that uses both the token labels and the original
class label. With the proposed STL framework, our best model based on
FAN-L-Hybrid (77.3M parameters) achieves 84.8% Top-1 accuracy and 42.1% mCE on
ImageNet-1K and ImageNet-C, and sets a new state-of-the-art for ImageNet-A
(46.1%) and ImageNet-R (56.6%) without using extra data, outperforming the
original FAN counterpart by significant margins. The proposed framework also
demonstrates significantly enhanced performance on downstream tasks such as
semantic segmentation, with up to 1.7% improvement in robustness over the
counterpart model. Code is available at https://github.com/NVlabs/STL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WidthFormer: Toward Efficient Transformer-based BEV View Transformation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03836v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03836v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhongyi Yang, Tianwei Lin, Lichao Huang, Elliot J. Crowley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present WidthFormer, a novel transformer-based
Bird's-Eye-View (BEV) 3D detection method tailored for real-time
autonomous-driving applications. WidthFormer is computationally efficient,
robust and does not require any special engineering effort to deploy. In this
work, we propose a novel 3D positional encoding mechanism capable of accurately
encapsulating 3D geometric information, which enables our model to generate
high-quality BEV representations with only a single transformer decoder layer.
This mechanism is also beneficial for existing sparse 3D object detectors.
Inspired by the recently-proposed works, we further improve our model's
efficiency by vertically compressing the image features when serving as
attention keys and values. We also introduce two modules to compensate for
potential information loss due to feature compression. Experimental evaluation
on the widely-used nuScenes 3D object detection benchmark demonstrates that our
method outperforms previous approaches across different 3D detection
architectures. More importantly, our model is highly efficient. For example,
when using $256\times 704$ input images, it achieves 1.5 ms latency on NVIDIA
3090 GPU. Furthermore, WidthFormer also exhibits strong robustness to different
degrees of camera perturbations. Our study offers valuable insights into the
deployment of BEV transformation methods in real-world, complex road
environments. Code is available at
https://github.com/ChenhongyiYang/WidthFormer .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Limitations of Data-Driven Spectral Reconstruction -- An Optics-Aware
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03835v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03835v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Fu, Matheus Souza, Eunsue Choi, Suhyun Shin, Seung-Hwan Baek, Wolfgang Heidrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral imaging empowers computer vision systems with the distinct
capability of identifying materials through recording their spectral
signatures. Recent efforts in data-driven spectral reconstruction aim at
extracting spectral information from RGB images captured by cost-effective RGB
cameras, instead of dedicated hardware.
  In this paper we systematically analyze the performance of such methods,
evaluating both the practical limitations with respect to current datasets and
overfitting, as well as fundamental limits with respect to the nature of the
information encoded in the RGB images, and the dependency of this information
on the optical system of the camera.
  We find that the current models are not robust under slight variations, e.g.,
in noise level or compression of the RGB file. Both the methods and the
datasets are also limited in their ability to cope with metameric colors. This
issue can in part be overcome with metameric data augmentation. Moreover,
optical lens aberrations can help to improve the encoding of the metameric
information into the RGB image, which paves the road towards higher performing
spectral imaging and reconstruction approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A foundation for exact binarized morphological neural networks <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Theodore Aouad, Hugues Talbot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training and running deep neural networks (NNs) often demands a lot of
computation and energy-intensive specialized hardware (e.g. GPU, TPU...). One
way to reduce the computation and power cost is to use binary weight NNs, but
these are hard to train because the sign function has a non-smooth gradient. We
present a model based on Mathematical Morphology (MM), which can binarize
ConvNets without losing performance under certain conditions, but these
conditions may not be easy to satisfy in real-world scenarios. To solve this,
we propose two new approximation methods and develop a robust theoretical
framework for ConvNets binarization using MM. We propose as well regularization
losses to improve the optimization. We empirically show that our model can
learn a complex morphological network, and explore its performance on a
classification task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at conference ICCV 2023 Workshop LBQNN. Same work, different
  format, accepted at conference NeurIPS 2023 Workshop WANT. 8 pages, 17 pages
  appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A multimodal gesture recognition <span class="highlight-title">dataset</span> for desktop human-computer
  interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Wang, Fengchao Zhu, Guangming Zhu, Liang Zhang, Ning Li, Eryang Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gesture recognition is an indispensable component of natural and efficient
human-computer interaction technology, particularly in desktop-level
applications, where it can significantly enhance people's productivity.
However, the current gesture recognition community lacks a suitable
desktop-level (top-view perspective) dataset for lightweight gesture capture
devices. In this study, we have established a dataset named GR4DHCI. What
distinguishes this dataset is its inherent naturalness, intuitive
characteristics, and diversity. Its primary purpose is to serve as a valuable
resource for the development of desktop-level portable applications. GR4DHCI
comprises over 7,000 gesture samples and a total of 382,447 frames for both
Stereo IR and skeletal modalities. We also address the variances in hand
positioning during desktop interactions by incorporating 27 different hand
positions into the dataset. Building upon the GR4DHCI dataset, we conducted a
series of experimental studies, the results of which demonstrate that the
fine-grained classification blocks proposed in this paper can enhance the
model's recognition accuracy. Our dataset and experimental findings presented
in this paper are anticipated to propel advancements in desktop-level gesture
recognition research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gnuastro: visualizing the full dynamic range in color images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raúl Infante-Sainz, Mohammad Akhlaghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Color plays a crucial role in the visualization, interpretation, and analysis
of multi-wavelength astronomical images. However, generating color images that
accurately represent the full dynamic range of astronomical sources is
challenging. In response, Gnuastro v0.22 introduces the program
'astscript-color-faint-gray', which is extensively documented in the Gnuastro
manual. It employs a non-linear transformation to assign an 8-bit RGB
(Red-Green-Blue) value to brighter pixels, while the fainter ones are shown in
an inverse grayscale. This approach enables the simultaneous visualization of
low surface brightness features within the same image. This research note is
reproducible with Maneage, on the Git commit 48f5408.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted RNAAS. Supplementary data on Zenodo
  (https://doi.org/10.5281/zenodo.10058165), project source on Codeberg
  (https://codeberg.org/gnuastro/papers/src/branch/color-faint-gray) and
  archived on Software Heritage
  (swh:1:dir:1064a48d4bb58d6684c3df33c6633a04d4141d2d)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FM-AE: Frequency-masked Multimodal Autoencoder for Zinc Electrolysis
  Plate Contact Abnormality Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Canzong Zhou, Can Zhou, Hongqiu Zhu, Tianhao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zinc electrolysis is one of the key processes in zinc smelting, and
maintaining stable operation of zinc electrolysis is an important factor in
ensuring production efficiency and product quality. However, poor contact
between the zinc electrolysis cathode and the anode is a common problem that
leads to reduced production efficiency and damage to the electrolysis cell.
Therefore, online monitoring of the contact status of the plates is crucial for
ensuring production quality and efficiency. To address this issue, we propose
an end-to-end network, the Frequency-masked Multimodal Autoencoder (FM-AE).
This method takes the cell voltage signal and infrared image information as
input, and through automatic encoding, fuses the two features together and
predicts the poor contact status of the plates through a cascaded detector.
Experimental results show that the proposed method maintains high accuracy
(86.2%) while having good robustness and generalization ability, effectively
detecting poor contact status of the zinc electrolysis cell, providing strong
support for production practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2023 The 34th Chinese Process Control Conference (CPCC 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MvKSR: Multi-view Knowledge-guided Scene Recovery for Hazy and Rainy
  Degradation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong Yang, Wenyu Xu, Yuxu Lu, Yuan Gao, Jingming Zhang, Yu Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality imaging is crucial for ensuring safety supervision and
intelligent deployment in fields like transportation and industry. It enables
precise and detailed monitoring of operations, facilitating timely detection of
potential hazards and efficient management. However, adverse weather
conditions, such as atmospheric haziness and precipitation, can have a
significant impact on image quality. When the atmosphere contains dense haze or
water droplets, the incident light scatters, leading to degraded captured
images. This degradation is evident in the form of image blur and reduced
contrast, increasing the likelihood of incorrect assessments and
interpretations by intelligent imaging systems (IIS). To address the challenge
of restoring degraded images in hazy and rainy conditions, this paper proposes
a novel multi-view knowledge-guided scene recovery network (termed MvKSR).
Specifically, guided filtering is performed on the degraded image to separate
high/low-frequency components. Subsequently, an en-decoder-based multi-view
feature coarse extraction module (MCE) is used to coarsely extract features
from different views of the degraded image. The multi-view feature fine fusion
module (MFF) will learn and infer the restoration of degraded images through
mixed supervision under different views. Additionally, we suggest an atrous
residual block to handle global restoration and local repair in
hazy/rainy/mixed scenes. Extensive experimental results demonstrate that MvKSR
outperforms other state-of-the-art methods in terms of efficiency and stability
for restoring degraded scenarios in IIS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Monitoring water contaminants in coastal areas through ML algorithms
  leveraging atmospherically corrected Sentinel-2 data <span class="chip">RSS2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03792v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03792v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesca Razzano, Francesco Mauro, Pietro Di Stasio, Gabriele Meoni, Marco Esposito, Gilda Schirinzi, Silvia Liberata Ullo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monitoring water contaminants is of paramount importance, ensuring public
health and environmental well-being. Turbidity, a key parameter, poses a
significant problem, affecting water quality. Its accurate assessment is
crucial for safeguarding ecosystems and human consumption, demanding meticulous
attention and action. For this, our study pioneers a novel approach to monitor
the Turbidity contaminant, integrating CatBoost Machine Learning (ML) with
high-resolution data from Sentinel-2 Level-2A. Traditional methods are
labor-intensive while CatBoost offers an efficient solution, excelling in
predictive accuracy. Leveraging atmospherically corrected Sentinel-2 data
through the Google Earth Engine (GEE), our study contributes to scalable and
precise Turbidity monitoring. A specific tabular dataset derived from Hong Kong
contaminants monitoring stations enriches our study, providing region-specific
insights. Results showcase the viability of this integrated approach, laying
the foundation for adopting advanced techniques in global water quality
management.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 3 figures, IGARSS2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-light Image Enhancement via CLIP-Fourier Guided Wavelet Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minglong Xue, Jinhong He, Yanyi He, Zhipu Liu, Wenhai Wang, Mingliang Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-light image enhancement techniques have significantly progressed, but
unstable image quality recovery and unsatisfactory visual perception are still
significant challenges. To solve these problems, we propose a novel and robust
low-light image enhancement method via CLIP-Fourier Guided Wavelet Diffusion,
abbreviated as CFWD. Specifically, we design a guided network with a multiscale
visual language in the frequency domain based on the wavelet transform to
achieve effective image enhancement iteratively. In addition, we combine the
advantages of Fourier transform in detail perception to construct a hybrid
frequency domain space with significant perceptual capabilities(HFDPM). This
operation guides wavelet diffusion to recover the fine-grained structure of the
image and avoid diversity confusion. Extensive quantitative and qualitative
experiments on publicly available real-world benchmarks show that our method
outperforms existing state-of-the-art methods and better reproduces images
similar to normal images. Code is available at
https://github.com/He-Jinhong/CFWD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identifying Important Group of Pixels using Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03785v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03785v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kosuke Sumiyasu, Kazuhiko Kawamoto, Hiroshi Kera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To better understand the behavior of image classifiers, it is useful to
visualize the contribution of individual pixels to the model prediction. In
this study, we propose a method, MoXI~($\textbf{Mo}$del e$\textbf{X}$planation
by $\textbf{I}$nteractions), that efficiently and accurately identifies a group
of pixels with high prediction confidence. The proposed method employs
game-theoretic concepts, Shapley values and interactions, taking into account
the effects of individual pixels and the cooperative influence of pixels on
model confidence. Theoretical analysis and experiments demonstrate that our
method better identifies the pixels that are highly contributing to the model
outputs than widely-used visualization methods using Grad-CAM, Attention
rollout, and Shapley value. While prior studies have suffered from the
exponential computational cost in the computation of Shapley value and
interactions, we show that this can be reduced to linear cost for our task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeRFmentation: NeRF-based Augmentation for Monocular Depth Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Casimir Feldmann, Niall Siegenheim, Nikolas Hars, Lovro Rabuzin, Mert Ertugrul, Luca Wolfart, Marc Pollefeys, Zuria Bauer, Martin R. Oswald
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The capabilities of monocular depth estimation (MDE) models are limited by
the availability of sufficient and diverse datasets. In the case of MDE models
for autonomous driving, this issue is exacerbated by the linearity of the
captured data trajectories. We propose a NeRF-based data augmentation pipeline
to introduce synthetic data with more diverse viewing directions into training
datasets and demonstrate the benefits of our approach to model performance and
robustness. Our data augmentation pipeline, which we call "NeRFmentation",
trains NeRFs on each scene in the dataset, filters out subpar NeRFs based on
relevant metrics, and uses them to generate synthetic RGB-D images captured
from new viewing directions. In this work, we apply our technique in
conjunction with three state-of-the-art MDE architectures on the popular
autonomous driving dataset KITTI, augmenting its training set of the Eigen
split. We evaluate the resulting performance gain on the original test set, a
separate popular driving set, and our own synthetic test set.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InvariantOODG: Learning Invariant Features of Point Clouds for
  Out-of-Distribution Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhimin Zhang, Xiang Gao, Wei Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The convenience of 3D sensors has led to an increase in the use of 3D point
clouds in various applications. However, the differences in acquisition devices
or scenarios lead to divergence in the data distribution of point clouds, which
requires good generalization of point cloud representation learning methods.
While most previous methods rely on domain adaptation, which involves
fine-tuning pre-trained models on target domain data, this may not always be
feasible in real-world scenarios where target domain data may be unavailable.
To address this issue, we propose InvariantOODG, which learns invariability
between point clouds with different distributions using a two-branch network to
extract local-to-global features from original and augmented point clouds.
Specifically, to enhance local feature learning of point clouds, we define a
set of learnable anchor points that locate the most useful local regions and
two types of transformations to augment the input point clouds. The
experimental results demonstrate the effectiveness of the proposed model on 3D
domain generalization benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D-SSGAN: Lifting 2D Semantics for 3D-Aware Compositional Portrait
  Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03764v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03764v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiqi Liu, Peng Zheng, Ye Wang, Rui Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing 3D-aware portrait synthesis methods can generate impressive
high-quality images while preserving strong 3D consistency. However, most of
them cannot support the fine-grained part-level control over synthesized
images. Conversely, some GAN-based 2D portrait synthesis methods can achieve
clear disentanglement of facial regions, but they cannot preserve view
consistency due to a lack of 3D modeling abilities. To address these issues, we
propose 3D-SSGAN, a novel framework for 3D-aware compositional portrait image
synthesis. First, a simple yet effective depth-guided 2D-to-3D lifting module
maps the generated 2D part features and semantics to 3D. Then, a volume
renderer with a novel 3D-aware semantic mask renderer is utilized to produce
the composed face features and corresponding masks. The whole framework is
trained end-to-end by discriminating between real and synthesized 2D images and
their semantic masks. Quantitative and qualitative evaluations demonstrate the
superiority of 3D-SSGAN in controllable part-level synthesis while preserving
3D view consistency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Color-$S^{4}L$: Self-supervised Semi-supervised Learning with Image
  Colorization <span class="chip">ECCV 2020</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanxiao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work addresses the problem of semi-supervised image classification tasks
with the integration of several effective self-supervised pretext tasks.
Different from widely-used consistency regularization within semi-supervised
learning, we explored a novel self-supervised semi-supervised learning
framework (Color-$S^{4}L$) especially with image colorization proxy task and
deeply evaluate performances of various network architectures in such special
pipeline. Also, we demonstrated its effectiveness and optimal performance on
CIFAR-10, SVHN and CIFAR-100 datasets in comparison to previous supervised and
semi-supervised optimal methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This original work has been accepted and presented in the Poster
  Session at ECCV 2020 WiCV Workshop.
  (https://sites.google.com/view/wicvworkshop-eccv2020/program/presentations)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flying Bird Object Detection Algorithm in Surveillance Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziwei Sun, Zexi Hua, Hengchao Li, Yan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aiming at the characteristics of the flying bird object in surveillance
video, such as the single frame image feature is not obvious, the size is small
in most cases, and asymmetric, this paper proposes a Flying Bird Object
Detection method for Surveillance Video (FBOD-SV). Firstly, a new feature
aggregation module, the Correlation Attention Feature Aggregation
(Co-Attention-FA) module, is designed to aggregate the features of the flying
bird object according to the bird object's correlation on multiple consecutive
frames of images. Secondly, a Flying Bird Object Detection Network (FBOD-Net)
with down-sampling and then up-sampling is designed, which uses a large feature
layer that fuses fine spatial information and large receptive field information
to detect special multi-scale (mostly small-scale) bird objects. Finally, the
SimOTA dynamic label allocation method is applied to One-Category object
detection, and the SimOTA-OC dynamic label strategy is proposed to solve the
difficult problem of label allocation caused by irregular flying bird objects.
In this paper, the algorithm's performance is verified by the experimental data
set of the surveillance video of the flying bird object of the traction
substation. The experimental results show that the surveillance video flying
bird object detection method proposed in this paper effectively improves the
detection performance of flying bird objects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flowmind2Digital: The First Comprehensive Flowmind Recognition and
  Conversion Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03742v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03742v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanyu Liu, Jianfeng Cai, Tingjia Zhang, Hongsheng Li, Siyuan Wang, Guangming Zhu, Syed Afaq Ali Shah, Mohammed Bennamoun, Liang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Flowcharts and mind maps, collectively known as flowmind, are vital in daily
activities, with hand-drawn versions facilitating real-time collaboration.
However, there's a growing need to digitize them for efficient processing.
Automated conversion methods are essential to overcome manual conversion
challenges. Existing sketch recognition methods face limitations in practical
situations, being field-specific and lacking digital conversion steps. Our
paper introduces the Flowmind2digital method and hdFlowmind dataset to address
these challenges. Flowmind2digital, utilizing neural networks and keypoint
detection, achieves a record 87.3% accuracy on our dataset, surpassing previous
methods by 11.9%. The hdFlowmind dataset, comprising 1,776 annotated flowminds
across 22 scenarios, outperforms existing datasets. Additionally, our
experiments emphasize the importance of simple graphics, enhancing accuracy by
9.3%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FMA-Net: Flow-Guided Dynamic Filtering and Iterative Feature Refinement
  with Multi-Attention for Joint Video Super-Resolution and Deblurring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geunhyuk Youk, Jihyong Oh, Munchurl Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a joint learning scheme of video super-resolution and deblurring,
called VSRDB, to restore clean high-resolution (HR) videos from blurry
low-resolution (LR) ones. This joint restoration problem has drawn much less
attention compared to single restoration problems. In this paper, we propose a
novel flow-guided dynamic filtering (FGDF) and iterative feature refinement
with multi-attention (FRMA), which constitutes our VSRDB framework, denoted as
FMA-Net. Specifically, our proposed FGDF enables precise estimation of both
spatio-temporally-variant degradation and restoration kernels that are aware of
motion trajectories through sophisticated motion representation learning.
Compared to conventional dynamic filtering, the FGDF enables the FMA-Net to
effectively handle large motions into the VSRDB. Additionally, the stacked FRMA
blocks trained with our novel temporal anchor (TA) loss, which temporally
anchors and sharpens features, refine features in a course-to-fine manner
through iterative updates. Extensive experiments demonstrate the superiority of
the proposed FMA-Net over state-of-the-art methods in terms of both
quantitative and qualitative quality. Codes and pre-trained models are
available at: https://kaist-viclab.github.io/fmanet-site
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The last two authors advised equally to this work (equal advising).
  Please visit our project page at https://kaist-viclab.github.io/fmanet-site</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sur2f: A Hybrid Representation for High-Quality and Efficient Surface
  Reconstruction from Multi-view Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangjin Huang, Zhihao Liang, Haojie Zhang, Yangkai Lin, Kui Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-view surface reconstruction is an ill-posed, inverse problem in 3D
vision research. It involves modeling the geometry and appearance with
appropriate surface representations. Most of the existing methods rely either
on explicit meshes, using surface rendering of meshes for reconstruction, or on
implicit field functions, using volume rendering of the fields for
reconstruction. The two types of representations in fact have their respective
merits. In this work, we propose a new hybrid representation, termed Sur2f,
aiming to better benefit from both representations in a complementary manner.
Technically, we learn two parallel streams of an implicit signed distance field
and an explicit surrogate surface Sur2f mesh, and unify volume rendering of the
implicit signed distance function (SDF) and surface rendering of the surrogate
mesh with a shared, neural shader; the unified shading promotes their
convergence to the same, underlying surface. We synchronize learning of the
surrogate mesh by driving its deformation with functions induced from the
implicit SDF. In addition, the synchronized surrogate mesh enables
surface-guided volume sampling, which greatly improves the sampling efficiency
per ray in volume rendering. We conduct thorough experiments showing that
Sur$^2$f outperforms existing reconstruction methods and surface
representations, including hybrid ones, in terms of both recovery quality and
recovery efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Large-scale Empirical Study on Improving the Fairness of Deep Learning
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Yang, Jiajun Jiang, Zeyu Sun, Junjie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fairness has been a critical issue that affects the adoption of deep learning
models in real practice. To improve model fairness, many existing methods have
been proposed and evaluated to be effective in their own contexts. However,
there is still no systematic evaluation among them for a comprehensive
comparison under the same context, which makes it hard to understand the
performance distinction among them, hindering the research progress and
practical adoption of them. To fill this gap, this paper endeavours to conduct
the first large-scale empirical study to comprehensively compare the
performance of existing state-of-the-art fairness improving techniques.
Specifically, we target the widely-used application scenario of image
classification, and utilized three different datasets and five commonly-used
performance metrics to assess in total 13 methods from diverse categories. Our
findings reveal substantial variations in the performance of each method across
different datasets and sensitive attributes, indicating over-fitting on
specific datasets by many existing methods. Furthermore, different fairness
evaluation metrics, due to their distinct focuses, yield significantly
different assessment results. Overall, we observe that pre-processing methods
and in-processing methods outperform post-processing methods, with
pre-processing methods exhibiting the best performance. Our empirical study
offers comprehensive recommendations for enhancing fairness in deep learning
models. We approach the problem from multiple dimensions, aiming to provide a
uniform evaluation platform and inspire researchers to explore more effective
fairness solutions via a set of implications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GloTSFormer: Global Video Text Spotting Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Wang, Yanjie Wang, Yang Li, Can Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Text Spotting (VTS) is a fundamental visual task that aims to predict
the trajectories and content of texts in a video. Previous works usually
conduct local associations and apply IoU-based distance and complex
post-processing procedures to boost performance, ignoring the abundant temporal
information and the morphological characteristics in VTS. In this paper, we
propose a novel Global Video Text Spotting Transformer GloTSFormer to model the
tracking problem as global associations and utilize the Gaussian Wasserstein
distance to guide the morphological correlation between frames. Our main
contributions can be summarized as three folds. 1). We propose a
Transformer-based global tracking method GloTSFormer for VTS and associate
multiple frames simultaneously. 2). We introduce a Wasserstein distance-based
method to conduct positional associations between frames. 3). We conduct
extensive experiments on public datasets. On the ICDAR2015 video dataset,
GloTSFormer achieves 56.0 MOTA with 4.6 absolute improvement compared with the
previous SOTA method and outperforms the previous Transformer-based method by a
significant 8.3 MOTA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Primitive Geometry Segment Pre-training for 3D Medical Image
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03665v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03665v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryu Tadokoro, Ryosuke Yamada, Kodai Nakashima, Ryo Nakamura, Hirokatsu Kataoka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The construction of 3D medical image datasets presents several issues,
including requiring significant financial costs in data collection and
specialized expertise for annotation, as well as strict privacy concerns for
patient confidentiality compared to natural image datasets. Therefore, it has
become a pressing issue in 3D medical image segmentation to enable
data-efficient learning with limited 3D medical data and supervision. A
promising approach is pre-training, but improving its performance in 3D medical
image segmentation is difficult due to the small size of existing 3D medical
image datasets. We thus present the Primitive Geometry Segment Pre-training
(PrimGeoSeg) method to enable the learning of 3D semantic features by
pre-training segmentation tasks using only primitive geometric objects for 3D
medical image segmentation. PrimGeoSeg performs more accurate and efficient 3D
medical image segmentation without manual data collection and annotation.
Further, experimental results show that PrimGeoSeg on SwinUNETR improves
performance over learning from scratch on BTCV, MSD (Task06), and BraTS
datasets by 3.7%, 4.4%, and 0.3%, respectively. Remarkably, the performance was
equal to or better than state-of-the-art self-supervised learning despite the
equal number of pre-training data. From experimental results, we conclude that
effective pre-training can be achieved by looking at primitive geometric
objects only. Code and dataset are available at
https://github.com/SUPER-TADORY/PrimGeoSeg.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to BMVC2023 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual-Channel Reliable Breast Ultrasound Image Classification Based on
  Explainable Attribution and Uncertainty Quantification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuge Lei, Haonan Hu, Dasheng Sun, Huabin Zhang, Kehong Yuan, Jian Dai, Jijun Tang, Yan Tong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on the classification task of breast ultrasound images and
researches on the reliability measurement of classification results. We
proposed a dual-channel evaluation framework based on the proposed inference
reliability and predictive reliability scores. For the inference reliability
evaluation, human-aligned and doctor-agreed inference rationales based on the
improved feature attribution algorithm SP-RISA are gracefully applied.
Uncertainty quantification is used to evaluate the predictive reliability via
the Test Time Enhancement. The effectiveness of this reliability evaluation
framework has been verified on our breast ultrasound clinical dataset YBUS, and
its robustness is verified on the public dataset BUSI. The expected calibration
errors on both datasets are significantly lower than traditional evaluation
methods, which proves the effectiveness of our proposed reliability
measurement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DME-Driver: Integrating Human Decision Logic and 3D Scene Perception in
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wencheng Han, Dongqian Guo, Cheng-Zhong Xu, Jianbing Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of autonomous driving, two important features of autonomous
driving car systems are the explainability of decision logic and the accuracy
of environmental perception. This paper introduces DME-Driver, a new autonomous
driving system that enhances the performance and reliability of autonomous
driving system. DME-Driver utilizes a powerful vision language model as the
decision-maker and a planning-oriented perception model as the control signal
generator. To ensure explainable and reliable driving decisions, the logical
decision-maker is constructed based on a large vision language model. This
model follows the logic employed by experienced human drivers and makes
decisions in a similar manner. On the other hand, the generation of accurate
control signals relies on precise and detailed environmental perception, which
is where 3D scene perception models excel. Therefore, a planning oriented
perception model is employed as the signal generator. It translates the logical
decisions made by the decision-maker into accurate control signals for the
self-driving cars. To effectively train the proposed model, a new dataset for
autonomous driving was created. This dataset encompasses a diverse range of
human driver behaviors and their underlying motivations. By leveraging this
dataset, our model achieves high-precision planning accuracy through a logical
thinking process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning for Visual Neuroprosthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03639v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03639v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Beech, Shanshan Jia, Zhaofei Yu, Jian K. Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The visual pathway involves complex networks of cells and regions which
contribute to the encoding and processing of visual information. While some
aspects of visual perception are understood, there are still many unanswered
questions regarding the exact mechanisms of visual encoding and the
organization of visual information along the pathway. This chapter discusses
the importance of visual perception and the challenges associated with
understanding how visual information is encoded and represented in the brain.
Furthermore, this chapter introduces the concept of neuroprostheses: devices
designed to enhance or replace bodily functions, and highlights the importance
of constructing computational models of the visual pathway in the
implementation of such devices. A number of such models, employing the use of
deep learning models, are outlined, and their value to understanding visual
coding and natural vision is discussed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unifying Graph Contrastive Learning via Graph Message Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyan Zhang, Bo Jiang, Jin Tang, Bin Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph contrastive learning is usually performed by first conducting Graph
Data Augmentation (GDA) and then employing a contrastive learning pipeline to
train GNNs. As we know that GDA is an important issue for graph contrastive
learning. Various GDAs have been developed recently which mainly involve
dropping or perturbing edges, nodes, node attributes and edge attributes.
However, to our knowledge, it still lacks a universal and effective augmentor
that is suitable for different types of graph data. To address this issue, in
this paper, we first introduce the graph message representation of graph data.
Based on it, we then propose a novel Graph Message Augmentation (GMA), a
universal scheme for reformulating many existing GDAs. The proposed unified GMA
not only gives a new perspective to understand many existing GDAs but also
provides a universal and more effective graph data augmentation for graph
self-supervised learning tasks. Moreover, GMA introduces an easy way to
implement the mixup augmentor which is natural for images but usually
challengeable for graphs. Based on the proposed GMA, we then propose a unified
graph contrastive learning, termed Graph Message Contrastive Learning (GMCL),
that employs attribution-guided universal GMA for graph contrastive learning.
Experiments on many graph learning tasks demonstrate the effectiveness and
benefits of the proposed GMA and GMCL approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inverse-like Antagonistic Scene Text Spotting via Reading-Order
  Estimation and Dynamic Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi-Xue Zhang, Chun Yang, Xiaobin Zhu, Hongyang Zhou, Hongfa Wang, Xu-Cheng Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene text spotting is a challenging task, especially for inverse-like scene
text, which has complex layouts, e.g., mirrored, symmetrical, or retro-flexed.
In this paper, we propose a unified end-to-end trainable inverse-like
antagonistic text spotting framework dubbed IATS, which can effectively spot
inverse-like scene texts without sacrificing general ones. Specifically, we
propose an innovative reading-order estimation module (REM) that extracts
reading-order information from the initial text boundary generated by an
initial boundary module (IBM). To optimize and train REM, we propose a joint
reading-order estimation loss consisting of a classification loss, an
orthogonality loss, and a distribution loss. With the help of IBM, we can
divide the initial text boundary into two symmetric control points and
iteratively refine the new text boundary using a lightweight boundary
refinement module (BRM) for adapting to various shapes and scales. To alleviate
the incompatibility between text detection and recognition, we propose a
dynamic sampling module (DSM) with a thin-plate spline that can dynamically
sample appropriate features for recognition in the detected text region.
Without extra supervision, the DSM can proactively learn to sample appropriate
features for text recognition through the gradient returned by the recognition
module. Extensive experiments on both challenging scene text and inverse-like
scene text datasets demonstrate that our method achieves superior performance
both on irregular and inverse-like text spotting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 16 figures, Accepted by TIP-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Learning Applications in Traumatic Brain Injury Diagnosis and
  Prognosis: A Spotlight on Mild TBI and CT Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanem Ellethy, Shekhar S. Chandra, Viktor Vegh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traumatic Brain Injury (TBI) poses a significant global public health
challenge, contributing to high morbidity and mortality rates and placing a
substantial economic burden on healthcare systems worldwide. The diagnosis and
prognosis of TBI relies on a combination of clinical and imaging data often
acquired using a Computed Tomography (CT) scanner. Addressing the multifaceted
challenges posed by TBI requires innovative, data-driven approaches, for this
complex condition. As such, we provide a summary of the state-of-the-art
Machine Learning (ML) and Deep Learning (DL) techniques applied to clinical and
images in TBI, with a particular focus on mild TBI (mTBI). We explore the rich
spectrum of ML and DL techniques used and highlight their impact in TBI . We
categorize ML and DL methods by TBI severity and showcase their application in
mTBI and moderate-to-severe TBI scenarios. Finally, we emphasize the role of ML
and DL in mTBI diagnosis, where conventional methods often fall short, and
comment on the potential of CT-based ML applications in TBI. This review may
serve as a source of inspiration for future research endeavours aimed at
improving the diagnosis and prognosis of TBI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The manuscript has 36 pages, 2 figures, and 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Detection of Myopic Maculopathy in MMAC 2023: Achievements in
  Classification, Segmentation, and Spherical Equivalent Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihao Li, Philippe Zhang, Yubo Tan, Jing Zhang, Zhihan Wang, Weili Jiang, Pierre-Henri Conze, Mathieu Lamard, Gwenolé Quellec, Mostafa El Habib Daho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Myopic macular degeneration is the most common complication of myopia and the
primary cause of vision loss in individuals with pathological myopia. Early
detection and prompt treatment are crucial in preventing vision impairment due
to myopic maculopathy. This was the focus of the Myopic Maculopathy Analysis
Challenge (MMAC), in which we participated. In task 1, classification of myopic
maculopathy, we employed the contrastive learning framework, specifically
SimCLR, to enhance classification accuracy by effectively capturing enriched
features from unlabeled data. This approach not only improved the intrinsic
understanding of the data but also elevated the performance of our
classification model. For Task 2 (segmentation of myopic maculopathy plus
lesions), we have developed independent segmentation models tailored for
different lesion segmentation tasks and implemented a test-time augmentation
strategy to further enhance the model's performance. As for Task 3 (prediction
of spherical equivalent), we have designed a deep regression model based on the
data distribution of the dataset and employed an integration strategy to
enhance the model's prediction accuracy. The results we obtained are promising
and have allowed us to position ourselves in the Top 6 of the classification
task, the Top 2 of the segmentation task, and the Top 1 of the prediction task.
The code is available at
\url{https://github.com/liyihao76/MMAC_LaTIM_Solution}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vulnerabilities Unveiled: Adversarially Attacking a Multimodal Vision
  Language Model for Pathology Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02565v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02565v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jai Prakash Veerla, Poojitha Thota, Partha Sai Guttikonda, Shirin Nilizadeh, Jacob M. Luber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the dynamic landscape of medical artificial intelligence, this study
explores the vulnerabilities of the Pathology Language-Image Pretraining (PLIP)
model, a Vision Language Foundation model, under targeted adversarial
conditions. Leveraging the Kather Colon dataset with 7,180 H&E images across
nine tissue types, our investigation employs Projected Gradient Descent (PGD)
adversarial attacks to intentionally induce misclassifications. The outcomes
reveal a 100% success rate in manipulating PLIP's predictions, underscoring its
susceptibility to adversarial perturbations. The qualitative analysis of
adversarial examples delves into the interpretability challenges, shedding
light on nuanced changes in predictions induced by adversarial manipulations.
These findings contribute crucial insights into the interpretability, domain
adaptation, and trustworthiness of Vision Language Models in medical imaging.
The study emphasizes the pressing need for robust defenses to ensure the
reliability of AI models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TPC-ViT: Token Propagation Controller for Efficient Vision Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01470v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01470v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision transformers (ViTs) have achieved promising results on a variety of
Computer Vision tasks, however their quadratic complexity in the number of
input tokens has limited their application specially in resource-constrained
settings. Previous approaches that employ gradual token reduction to address
this challenge assume that token redundancy in one layer implies redundancy in
all the following layers. We empirically demonstrate that this assumption is
often not correct, i.e., tokens that are redundant in one layer can be useful
in later layers. We employ this key insight to propose a novel token
propagation controller (TPC) that incorporates two different
token-distributions, i.e., pause probability and restart probability to control
the reduction and reuse of tokens respectively, which results in more efficient
token utilization. To improve the estimates of token distributions, we propose
a smoothing mechanism that acts as a regularizer and helps remove noisy
outliers. Furthermore, to improve the training-stability of our proposed TPC,
we introduce a model stabilizer that is able to implicitly encode local image
structures and minimize accuracy fluctuations during model training. We present
extensive experimental results on the ImageNet-1K dataset using DeiT, LV-ViT
and Swin models to demonstrate the effectiveness of our proposed method. For
example, compared to baseline models, our proposed method improves the
inference speed of the DeiT-S by 250% while increasing the classification
accuracy by 1.0%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the main conference of WACV 2024; well-formatted PDF is
  in
  https://drive.google.com/file/d/1Id3oEdYv3OWing1qojQMyjvhZO-gG-Dm/view?usp=sharing
  ; supplementary is in
  https://drive.google.com/file/d/15LhYlBdCXtompA0_TLAp_ZJb4_sq2N5V/view?usp=sharing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive Study of Knowledge Editing for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01286v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01286v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, Siyuan Cheng, Ziwen Xu, Xin Xu, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown extraordinary capabilities in
understanding and generating text that closely mirrors human communication.
However, a primary limitation lies in the significant computational demands
during training, arising from their extensive parameterization. This challenge
is further intensified by the dynamic nature of the world, necessitating
frequent updates to LLMs to correct outdated information or integrate new
knowledge, thereby ensuring their continued relevance. Note that many
applications demand continual model adjustments post-training to address
deficiencies or undesirable behaviors. There is an increasing interest in
efficient, lightweight methods for on-the-fly model modifications. To this end,
recent years have seen a burgeoning in the techniques of knowledge editing for
LLMs, which aim to efficiently modify LLMs' behaviors within specific domains
while preserving overall performance across various inputs. In this paper, we
first define the knowledge editing problem and then provide a comprehensive
review of cutting-edge approaches. Drawing inspiration from educational and
cognitive research theories, we propose a unified categorization criterion that
classifies knowledge editing methods into three groups: resorting to external
knowledge, merging knowledge into the model, and editing intrinsic knowledge.
Furthermore, we introduce a new benchmark, KnowEdit, for a comprehensive
empirical evaluation of representative knowledge editing approaches.
Additionally, we provide an in-depth analysis of knowledge location, which can
provide a deeper understanding of the knowledge structures inherent within
LLMs. Finally, we discuss several potential applications of knowledge editing,
outlining its broad and impactful implications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing work; 52 pages, 280 citations; benchmark is available at
  https://huggingface.co/datasets/zjunlp/KnowEdit code is available at
  https://github.com/zjunlp/EasyEdit paper list is available at
  https://github.com/zjunlp/KnowledgeEditingPapers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Standardized CycleGAN training for unsupervised stain adaptation in
  invasive carcinoma classification for breast histopathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.13128v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.13128v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Nerrienet, Rémy Peyret, Marie Sockeel, Stéphane Sockeel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalization is one of the main challenges of computational pathology.
Slide preparation heterogeneity and the diversity of scanners lead to poor
model performance when used on data from medical centers not seen during
training. In order to achieve stain invariance in breast invasive carcinoma
patch classification, we implement a stain translation strategy using cycleGANs
for unsupervised image-to-image translation. We compare three cycleGAN-based
approaches to a baseline classification model obtained without any stain
invariance strategy. Two of the proposed approaches use cycleGAN's translations
at inference or training in order to build stain-specific classification
models. The last method uses them for stain data augmentation during training.
This constrains the classification model to learn stain-invariant features.
Baseline metrics are set by training and testing the baseline classification
model on a reference stain. We assessed performances using three medical
centers with H&E and H&E&S staining. Every approach tested in this study
improves baseline metrics without needing labels on target stains. The stain
augmentation-based approach produced the best results on every stain. Each
method's pros and cons are studied and discussed in this paper. However,
training highly performing cycleGANs models in itself represents a challenge.
In this work, we introduce a systematical method for optimizing cycleGAN
training by setting a novel stopping criterion. This method has the benefit of
not requiring any visual inspection of cycleGAN results and proves superiority
to methods using a predefined number of training epochs. In addition, we also
study the minimal amount of data required for cycleGAN training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ARFA: An Asymmetric Receptive Field Autoencoder Model for Spatiotemporal
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.00314v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.00314v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Zhang, Xuechao Zou, Li Wu, Xiaoying Wang, Jianqiang Huang, Junliang Xing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatiotemporal prediction aims to generate future sequences by paradigms
learned from historical contexts. It is essential in numerous domains, such as
traffic flow prediction and weather forecasting. Recently, research in this
field has been predominantly driven by deep neural networks based on
autoencoder architectures. However, existing methods commonly adopt autoencoder
architectures with identical receptive field sizes. To address this issue, we
propose an Asymmetric Receptive Field Autoencoder (ARFA) model, which
introduces corresponding sizes of receptive field modules tailored to the
distinct functionalities of the encoder and decoder. In the encoder, we present
a large kernel module for global spatiotemporal feature extraction. In the
decoder, we develop a small kernel module for local spatiotemporal information
reconstruction. Experimental results demonstrate that ARFA consistently
achieves state-of-the-art performance on popular datasets. Additionally, we
construct the RainBench, a large-scale radar echo dataset for precipitation
prediction, to address the scarcity of meteorological data in the domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language-free Compositional Action Generation via Decoupling Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03538v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03538v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Liu, Guangyi Chen, Yansong Tang, Guangrun Wang, Xiao-Ping Zhang, Ser-Nam Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Composing simple elements into complex concepts is crucial yet challenging,
especially for 3D action generation. Existing methods largely rely on extensive
neural language annotations to discern composable latent semantics, a process
that is often costly and labor-intensive. In this study, we introduce a novel
framework to generate compositional actions without reliance on language
auxiliaries. Our approach consists of three main components: Action Coupling,
Conditional Action Generation, and Decoupling Refinement. Action Coupling
utilizes an energy model to extract the attention masks of each sub-action,
subsequently integrating two actions using these attentions to generate
pseudo-training examples. Then, we employ a conditional generative model, CVAE,
to learn a latent space, facilitating the diverse generation. Finally, we
propose Decoupling Refinement, which leverages a self-supervised pre-trained
model MAE to ensure semantic consistency between the sub-actions and
compositional actions. This refinement process involves rendering generated 3D
actions into 2D space, decoupling these images into two sub-segments, using the
MAE model to restore the complete image from sub-segments, and constraining the
recovered images to match images rendered from raw sub-actions. Due to the lack
of existing datasets containing both sub-actions and compositional actions, we
created two new datasets, named HumanAct-C and UESTC-C, and present a
corresponding evaluation metric. Both qualitative and quantitative assessments
are conducted to show our efficacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LEFormer: A Hybrid CNN-Transformer Architecture for Accurate Lake
  Extraction from Remote Sensing Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.04397v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.04397v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ben Chen, Xuechao Zou, Yu Zhang, Jiayu Li, Kai Li, Junliang Xing, Pin Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lake extraction from remote sensing images is challenging due to the complex
lake shapes and inherent data noises. Existing methods suffer from blurred
segmentation boundaries and poor foreground modeling. This paper proposes a
hybrid CNN-Transformer architecture, called LEFormer, for accurate lake
extraction. LEFormer contains three main modules: CNN encoder, Transformer
encoder, and cross-encoder fusion. The CNN encoder effectively recovers local
spatial information and improves fine-scale details. Simultaneously, the
Transformer encoder captures long-range dependencies between sequences of any
length, allowing them to obtain global features and context information. The
cross-encoder fusion module integrates the local and global features to improve
mask prediction. Experimental results show that LEFormer consistently achieves
state-of-the-art performance and efficiency on the Surface Water and the
Qinghai-Tibet Plateau Lake datasets. Specifically, LEFormer achieves 90.86% and
97.42% mIoU on two datasets with a parameter count of 3.61M, respectively,
while being 20 minor than the previous best lake extraction method. The source
code is available at https://github.com/BastianChen/LEFormer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ranking-based Adaptive Query Generation for DETRs in Crowded Pedestrian
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.15725v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.15725v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Gao, Jiaxu Leng, Ji Gan, Xinbo Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DEtection TRansformer (DETR) and its variants (DETRs) have been successfully
applied to crowded pedestrian detection, which achieved promising performance.
However, we find that, in different degrees of crowded scenes, the number of
DETRs' queries must be adjusted manually, otherwise, the performance would
degrade to varying degrees. In this paper, we first analyze the two current
query generation methods and summarize four guidelines for designing the
adaptive query generation method. Then, we propose Rank-based Adaptive Query
Generation (RAQG) to alleviate the problem. Specifically, we design a rank
prediction head that can predict the rank of the lowest confidence positive
training sample produced by the encoder. Based on the predicted rank, we design
an adaptive selection method that can adaptively select coarse detection
results produced by the encoder to generate queries. Moreover, to train the
rank prediction head better, we propose Soft Gradient L1 Loss. The gradient of
Soft Gradient L1 Loss is continuous, which can describe the relationship
between the loss value and the updated value of model parameters granularly.
Our method is simple and effective, which can be plugged into any DETRs to make
it query-adaptive in theory. The experimental results on Crowdhuman dataset and
Citypersons dataset show that our method can adaptively generate queries for
DETRs and achieve competitive results. Especially, our method achieves
state-of-the-art 39.4% MR on Crowdhuman dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unlocking Pre-trained Image Backbones for Semantic Image Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.13314v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.13314v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tariq Berrada, Jakob Verbeek, Camille Couprie, Karteek Alahari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic image synthesis, i.e., generating images from user-provided semantic
label maps, is an important conditional image generation task as it allows to
control both the content as well as the spatial layout of generated images.
Although diffusion models have pushed the state of the art in generative image
modeling, the iterative nature of their inference process makes them
computationally demanding. Other approaches such as GANs are more efficient as
they only need a single feed-forward pass for generation, but the image quality
tends to suffer on large and diverse datasets. In this work, we propose a new
class of GAN discriminators for semantic image synthesis that generates highly
realistic images by exploiting feature backbone networks pre-trained for tasks
such as image classification. We also introduce a new generator architecture
with better context modeling and using cross-attention to inject noise into
latent variables, leading to more diverse generated images. Our model, which we
dub DP-SIMS, achieves state-of-the-art results in terms of image quality and
consistency with the input label maps on ADE-20K, COCO-Stuff, and Cityscapes,
surpassing recent diffusion models while requiring two orders of magnitude less
compute for inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DamWorld: Progressive Reasoning with World Models for Robotic
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11335v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11335v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengzhen Ren, Kaidong Zhang, Hetao Zheng, Zixuan Li, Yuhang Wen, Fengda Zhu, Mas Ma, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The research on embodied AI has greatly promoted the development of robot
manipulation. However, it still faces significant challenges in various aspects
such as benchmark construction, multi-modal perception and decision-making, and
physical execution. Previous robot manipulation simulators were primarily
designed to enrich manipulation types and types of objects while neglecting the
balance between physical manipulation and language instruction complexity in
multi-modal environments. This paper proposes a new robot manipulation
simulator and builds a comprehensive and systematic robot manipulation
benchmark with progressive reasoning tasks called SeaWave (i.e., a progressive
reasoning benchmark). It provides a standard test platform for embedded AI
agents in a multi-modal environment, which can evaluate and execute four levels
of human natural language instructions at the same time.
  Previous world model-based robot manipulation work lacked research on the
perception and decision-making of complex instructions in multi-modal
environments. To this end, we propose a new world model tailored for
cross-modal robot manipulation called DamWorld. Specifically, DamWorld takes
the current visual scene and predicted execution actions based on natural
language instructions as input, and uses the next action frame to supervise the
output of the world model to force the model to learn robot manipulation
consistent with world knowledge. Compared with the renowned baselines (e.g.,
RT-1), our DamWorld improves the manipulation success rate by 5.6% on average
on four levels of progressive reasoning tasks. It is worth noting that on the
most challenging level 4 manipulation task, DamWorld still improved by 9.0%
compared to prior works.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Color-Event based Tracking: A Unified Network, <span class="highlight-title">Dataset</span>, and
  Metric 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.11010v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.11010v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanming Tang, Xiao Wang, Ju Huang, Bo Jiang, Lin Zhu, Jianlin Zhang, Yaowei Wang, Yonghong Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combining the Color and Event cameras (also called Dynamic Vision Sensors,
DVS) for robust object tracking is a newly emerging research topic in recent
years. Existing color-event tracking framework usually contains multiple
scattered modules which may lead to low efficiency and high computational
complexity, including feature extraction, fusion, matching, interactive
learning, etc. In this paper, we propose a single-stage backbone network for
Color-Event Unified Tracking (CEUTrack), which achieves the above functions
simultaneously. Given the event points and RGB frames, we first transform the
points into voxels and crop the template and search regions for both
modalities, respectively. Then, these regions are projected into tokens and
parallelly fed into the unified Transformer backbone network. The output
features will be fed into a tracking head for target object localization. Our
proposed CEUTrack is simple, effective, and efficient, which achieves over 75
FPS and new SOTA performance. To better validate the effectiveness of our model
and address the data deficiency of this task, we also propose a generic and
large-scale benchmark dataset for color-event tracking, termed COESOT, which
contains 90 categories and 1354 video sequences. Additionally, a new evaluation
metric named BOC is proposed in our evaluation toolkit to evaluate the
prominence with respect to the baseline methods. We hope the newly proposed
method, dataset, and evaluation metric provide a better platform for
color-event-based tracking. The dataset, toolkit, and source code will be
released on: \url{https://github.com/Event-AHU/COESOT}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Peer Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Contrastive Learning Scheme with Transformer Innate Patches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.14806v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.14806v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sander Riisøen Jyhne, Per-Arne Andersen, Morten Goodwin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents Contrastive Transformer, a contrastive learning scheme
using the Transformer innate patches. Contrastive Transformer enables existing
contrastive learning techniques, often used for image classification, to
benefit dense downstream prediction tasks such as semantic segmentation. The
scheme performs supervised patch-level contrastive learning, selecting the
patches based on the ground truth mask, subsequently used for hard-negative and
hard-positive sampling. The scheme applies to all vision-transformer
architectures, is easy to implement, and introduces minimal additional memory
footprint. Additionally, the scheme removes the need for huge batch sizes, as
each patch is treated as an image.
  We apply and test Contrastive Transformer for the case of aerial image
segmentation, known for low-resolution data, large class imbalance, and similar
semantic classes. We perform extensive experiments to show the efficacy of the
Contrastive Transformer scheme on the ISPRS Potsdam aerial image segmentation
dataset. Additionally, we show the generalizability of our scheme by applying
it to multiple inherently different Transformer architectures. Ultimately, the
results show a consistent increase in mean IoU across all classes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodal Parameter-Efficient Few-Shot Class Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04751v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04751v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco D'Alessandro, Alberto Alonso, Enrique Calabrés, Mikel Galar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-Shot Class Incremental Learning (FSCIL) is a challenging continual
learning task, where limited training examples are available during several
learning sessions. To succeed in this task, it is necessary to avoid
over-fitting new classes caused by biased distributions in the few-shot
training sets. The general approach to address this issue involves enhancing
the representational capability of a pre-defined backbone architecture by
adding special modules for backward compatibility with older classes. However,
this approach has not yet solved the dilemma of ensuring high classification
accuracy over time while reducing the gap between the performance obtained on
larger training sets and the smaller ones. In this work, we propose an
alternative approach called Continual Parameter-Efficient CLIP (CPE-CLIP) to
reduce the loss of information between different learning sessions. Instead of
adapting additional modules to address information loss, we leverage the vast
knowledge acquired by CLIP in large-scale pre-training and its effectiveness in
generalizing to new concepts. Our approach is multimodal and
parameter-efficient, relying on learnable prompts for both the language and
vision encoders to enable transfer learning across sessions. We also introduce
prompt regularization to improve performance and prevent forgetting. Our
experimental results demonstrate that CPE-CLIP significantly improves FSCIL
performance compared to state-of-the-art proposals while also drastically
reducing the number of learnable parameters and training costs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recovering Sign Bits of DCT Coefficients in Digital Images as an
  Optimization Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.01096v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.01096v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiyuan Lin, Sheng Liu, Jun Jiang, Shujun Li, Chengqing Li, C. -C. Jay Kuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recovering unknown, missing, damaged, distorted, or lost information in DCT
coefficients is a common task in multiple applications of digital image
processing, including image compression, selective image encryption, and image
communication. This paper investigates the recovery of sign bits in DCT
coefficients of digital images, by proposing two different approximation
methods to solve a mixed integer linear programming (MILP) problem, which is
NP-hard in general. One method is a relaxation of the MILP problem to a linear
programming (LP) problem, and the other splits the original MILP problem into
some smaller MILP problems and an LP problem. We considered how the proposed
methods can be applied to JPEG-encoded images and conducted extensive
experiments to validate their performances. The experimental results showed
that the proposed methods outperformed other existing methods by a substantial
margin, both according to objective quality metrics and our subjective
evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predicting Infant Brain Connectivity with Federated Multi-Trajectory
  GNNs using Scarce Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01383v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01383v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michalis Pistos, Gang Li, Weili Lin, Dinggang Shen, Islem Rekik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The understanding of the convoluted evolution of infant brain networks during
the first postnatal year is pivotal for identifying the dynamics of early brain
connectivity development. Existing deep learning solutions suffer from three
major limitations. First, they cannot generalize to multi-trajectory prediction
tasks, where each graph trajectory corresponds to a particular imaging modality
or connectivity type (e.g., T1-w MRI). Second, existing models require
extensive training datasets to achieve satisfactory performance which are often
challenging to obtain. Third, they do not efficiently utilize incomplete time
series data. To address these limitations, we introduce FedGmTE-Net++, a
federated graph-based multi-trajectory evolution network. Using the power of
federation, we aggregate local learnings among diverse hospitals with limited
datasets. As a result, we enhance the performance of each hospital's local
generative model, while preserving data privacy. The three key innovations of
FedGmTE-Net++ are: (i) presenting the first federated learning framework
specifically designed for brain multi-trajectory evolution prediction in a
data-scarce environment, (ii) incorporating an auxiliary regularizer in the
local objective function to exploit all the longitudinal brain connectivity
within the evolution trajectory and maximize data utilization, (iii)
introducing a two-step imputation process, comprising a preliminary KNN-based
precompletion followed by an imputation refinement step that employs regressors
to improve similarity scores and refine imputations. Our comprehensive
experimental results showed the outperformance of FedGmTE-Net++ in brain
multi-trajectory prediction from a single baseline graph in comparison with
benchmark methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive FSS: A Novel Few-Shot Segmentation Framework via Prototype
  Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15731v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15731v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Wang, Jinagyun Li, Chen Chen, Yisi Zhang, Haoran Shen, Tianxiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Few-Shot Segmentation (FSS) aims to accomplish the novel class
segmentation task with a few annotated images. Current FSS research based on
meta-learning focus on designing a complex interaction mechanism between the
query and support feature. However, unlike humans who can rapidly learn new
things from limited samples, the existing approach relies solely on fixed
feature matching to tackle new tasks, lacking adaptability. In this paper, we
propose a novel framework based on the adapter mechanism, namely Adaptive FSS,
which can efficiently adapt the existing FSS model to the novel classes. In
detail, we design the Prototype Adaptive Module (PAM), which utilizes accurate
category information provided by the support set to derive class prototypes,
enhancing class-specific information in the multi-stage representation. In
addition, our approach is compatible with in diverse FSS methods with different
backbones by simply inserting PAM between the layers of the encoder.
Experiments demonstrate that our method effectively improves the performance of
the FSS models (e.g., MSANet, HDMNet, FPTrans, and DCAMA) and achieve new
state-of-the-art (SOTA) results (i.e., 72.4\% and 79.1\% mIoU on PASCAL-5$^i$
1-shot and 5-shot settings, 52.7\% and 60.0\% mIoU on COCO-20$^i$ 1-shot and
5-shot settings). Our code can be available at
https://github.com/jingw193/AdaptiveFSS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distilling Temporal Knowledge with Masked Feature Reconstruction for 3D
  Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01918v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01918v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowen Zheng, Dong Cao, Jintao Xu, Rui Ai, Weihao Gu, Yang Yang, Yanyan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Striking a balance between precision and efficiency presents a prominent
challenge in the bird's-eye-view (BEV) 3D object detection. Although previous
camera-based BEV methods achieved remarkable performance by incorporating
long-term temporal information, most of them still face the problem of low
efficiency. One potential solution is knowledge distillation. Existing
distillation methods only focus on reconstructing spatial features, while
overlooking temporal knowledge. To this end, we propose TempDistiller, a
Temporal knowledge Distiller, to acquire long-term memory from a teacher
detector when provided with a limited number of frames. Specifically, a
reconstruction target is formulated by integrating long-term temporal knowledge
through self-attention operation applied to feature teachers. Subsequently,
novel features are generated for masked student features via a generator.
Ultimately, we utilize this reconstruction target to reconstruct the student
features. In addition, we also explore temporal relational knowledge when
inputting full frames for the student model. We verify the effectiveness of the
proposed method on the nuScenes benchmark. The experimental results show our
method obtain an enhancement of +1.6 mAP and +1.1 NDS compared to the baseline,
a speed improvement of approximately 6 FPS after compressing temporal
knowledge, and the most accurate velocity estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Human Action Recognition in Still Images Using ConViT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.08994v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.08994v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyed Rohollah Hosseyni, Sanaz Seyedin, Hasan Taheri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the relationship between different parts of an image is crucial
in a variety of applications, including object recognition, scene
understanding, and image classification. Despite the fact that Convolutional
Neural Networks (CNNs) have demonstrated impressive results in classifying and
detecting objects, they lack the capability to extract the relationship between
different parts of an image, which is a crucial factor in Human Action
Recognition (HAR). To address this problem, this paper proposes a new module
that functions like a convolutional layer that uses Vision Transformer (ViT).
In the proposed model, the Vision Transformer can complement a convolutional
neural network in a variety of tasks by helping it to effectively extract the
relationship among various parts of an image. It is shown that the proposed
model, compared to a simple CNN, can extract meaningful parts of an image and
suppress the misleading parts. The proposed model has been evaluated on the
Stanford40 and PASCAL VOC 2012 action datasets and has achieved 95.5% mean
Average Precision (mAP) and 91.5% mAP results, respectively, which are
promising compared to other state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BA-SAM: Scalable Bias-Mode Attention Mask for Segment Anything Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02317v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02317v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiran Song, Qianyu Zhou, Xiangtai Li, Deng-Ping Fan, Xuequan Lu, Lizhuang Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the challenge of image resolution variation for the
Segment Anything Model (SAM). SAM, known for its zero-shot generalizability,
exhibits a performance degradation when faced with datasets with varying image
sizes. Previous approaches tend to resize the image to a fixed size or adopt
structure modifications, hindering the preservation of SAM's rich prior
knowledge. Besides, such task-specific tuning necessitates a complete
retraining of the model, which is cost-expensive and unacceptable for
deployment in the downstream tasks. In this paper, we reformulate this issue as
a length extrapolation problem, where token sequence length varies while
maintaining a consistent patch size for images of different sizes. To this end,
we propose Scalable Bias-Mode Attention Mask (BA-SAM) to enhance SAM's
adaptability to varying image resolutions while eliminating the need for
structure modifications. Firstly, we introduce a new scaling factor to ensure
consistent magnitude in the attention layer's dot product values when the token
sequence length changes. Secondly, we present a bias-mode attention mask that
allows each token to prioritize neighboring information, mitigating the impact
of untrained distant information. Our BA-SAM demonstrates efficacy in two
scenarios: zero-shot and fine-tuning. Extensive evaluation on diverse datasets,
including DIS5K, DUTS, ISIC, COD10K, and COCO, reveals its ability to
significantly mitigate performance degradation in the zero-shot setting and
achieve state-of-the-art performance with minimal fine-tuning. Furthermore, we
propose a generalized model and benchmark, showcasing BA-SAM's generalizability
across all four datasets simultaneously.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code:https://github.com/zongzi13545329/BA-SAM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Revisit of the Normalized Eight-Point Algorithm and A Self-Supervised
  Deep Solution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.10771v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.10771v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Fan, Yuchao Dai, Yongduek Seo, Mingyi He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Normalized Eight-Point algorithm has been widely viewed as the
cornerstone in two-view geometry computation, where the seminal Hartley's
normalization greatly improves the performance of the direct linear
transformation (DLT) algorithm. A natural question is, whether there exists and
how to find other normalization methods that may further improve the
performance as per each input sample. In this paper, we provide a novel
perspective and make two contributions towards this fundamental problem: 1) We
revisit the normalized eight-point algorithm and make a theoretical
contribution by showing the existence of different and better normalization
algorithms; 2) We present a deep convolutional neural network with a
self-supervised learning strategy to the normalization. Given eight pairs of
correspondences, our network directly predicts the normalization matrices, thus
learning to normalize each input sample. Our learning-based normalization
module could be integrated with both traditional (e.g., RANSAC) and deep
learning framework (affording good interpretability) with minimal efforts.
Extensive experiments on both synthetic and real images show the effectiveness
of our proposed approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Visual Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AdaMesh: Personalized Facial Expressions and Head Poses for Adaptive
  Speech-Driven 3D Facial Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07236v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07236v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liyang Chen, Weihong Bao, Shun Lei, Boshi Tang, Zhiyong Wu, Shiyin Kang, Haozhi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech-driven 3D facial animation aims at generating facial movements that
are synchronized with the driving speech, which has been widely explored
recently. Existing works mostly neglect the person-specific talking style in
generation, including facial expression and head pose styles. Several works
intend to capture the personalities by fine-tuning modules. However, limited
training data leads to the lack of vividness. In this work, we propose AdaMesh,
a novel adaptive speech-driven facial animation approach, which learns the
personalized talking style from a reference video of about 10 seconds and
generates vivid facial expressions and head poses. Specifically, we propose
mixture-of-low-rank adaptation (MoLoRA) to fine-tune the expression adapter,
which efficiently captures the facial expression style. For the personalized
pose style, we propose a pose adapter by building a discrete pose prior and
retrieving the appropriate style embedding with a semantic-aware pose style
matrix without fine-tuning. Extensive experimental results show that our
approach outperforms state-of-the-art methods, preserves the talking style in
the reference video, and generates vivid facial animation. The supplementary
video and code will be available at https://adamesh.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://adamesh.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SAME: Sample Reconstruction against Model Extraction Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10578v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10578v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Xie, Jie Zhang, Shiqian Zhao, Tianwei Zhang, Xiaofeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While deep learning models have shown significant performance across various
domains, their deployment needs extensive resources and advanced computing
infrastructure. As a solution, Machine Learning as a Service (MLaaS) has
emerged, lowering the barriers for users to release or productize their deep
learning models. However, previous studies have highlighted potential privacy
and security concerns associated with MLaaS, and one primary threat is model
extraction attacks. To address this, there are many defense solutions but they
suffer from unrealistic assumptions and generalization issues, making them less
practical for reliable protection. Driven by these limitations, we introduce a
novel defense mechanism, SAME, based on the concept of sample reconstruction.
This strategy imposes minimal prerequisites on the defender's capabilities,
eliminating the need for auxiliary Out-of-Distribution (OOD) datasets, user
query history, white-box model access, and additional intervention during model
training. It is compatible with existing active defense methods. Our extensive
experiments corroborate the superior efficacy of SAME over state-of-the-art
solutions. Our code is available at https://github.com/xythink/SAME.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kiki or Bouba? Sound Symbolism in Vision-and-Language Models <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16781v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16781v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Morris Alper, Hadar Averbuch-Elor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although the mapping between sound and meaning in human language is assumed
to be largely arbitrary, research in cognitive science has shown that there are
non-trivial correlations between particular sounds and meanings across
languages and demographic groups, a phenomenon known as sound symbolism. Among
the many dimensions of meaning, sound symbolism is particularly salient and
well-demonstrated with regards to cross-modal associations between language and
the visual domain. In this work, we address the question of whether sound
symbolism is reflected in vision-and-language models such as CLIP and Stable
Diffusion. Using zero-shot knowledge probing to investigate the inherent
knowledge of these models, we find strong evidence that they do show this
pattern, paralleling the well-known kiki-bouba effect in psycholinguistics. Our
work provides a novel method for demonstrating sound symbolism and
understanding its nature using computational tools. Our code will be made
publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to NeurIPS 2023 (spotlight). Project webpage:
  https://kiki-bouba.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncovering the human motion pattern: Pattern Memory-based Diffusion
  Model for Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02916v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02916v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Yang, Pengfei Zhu, Mengshi Qi, Huadong Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human trajectory forecasting is a critical challenge in fields such as
robotics and autonomous driving. Due to the inherent uncertainty of human
actions and intentions in real-world scenarios, various unexpected occurrences
may arise. To uncover latent motion patterns in human behavior, we introduce a
novel memory-based method, named Motion Pattern Priors Memory Network. Our
method involves constructing a memory bank derived from clustered prior
knowledge of motion patterns observed in the training set trajectories. We
introduce an addressing mechanism to retrieve the matched pattern and the
potential target distributions for each prediction from the memory bank, which
enables the identification and retrieval of natural motion patterns exhibited
by agents, subsequently using the target priors memory token to guide the
diffusion model to generate predictions. Extensive experiments validate the
effectiveness of our approach, achieving state-of-the-art trajectory prediction
accuracy. The code will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coordinate-based Neural Network for Fourier Phase Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.14925v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.14925v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingyou Li, Zixin Xu, Yong S. Chu, Xiaojing Huang, Jizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fourier phase retrieval is essential for high-definition imaging of nanoscale
structures across diverse fields, notably coherent diffraction imaging. This
study presents the Single impliCit neurAl Network (SCAN), a tool built upon
coordinate neural networks meticulously designed for enhanced phase retrieval
performance. Remedying the drawbacks of conventional iterative methods which
are easiliy trapped into local minimum solutions and sensitive to noise, SCAN
adeptly connects object coordinates to their amplitude and phase within a
unified network in an unsupervised manner. While many existing methods
primarily use Fourier magnitude in their loss function, our approach
incorporates both the predicted magnitude and phase, enhancing retrieval
accuracy. Comprehensive tests validate SCAN's superiority over traditional and
other deep learning models regarding accuracy and noise robustness. We also
demonstrate that SCAN excels in the ptychography setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bringing Back the Context: Camera Trap Species Identification as Link
  Prediction on Multimodal Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00608v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00608v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vardaan Pahuja, Weidi Luo, Yu Gu, Cheng-Hao Tu, Hong-You Chen, Tanya Berger-Wolf, Charles Stewart, Song Gao, Wei-Lun Chao, Yu Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Camera traps are valuable tools in animal ecology for biodiversity monitoring
and conservation. However, challenges like poor generalization to deployment at
new unseen locations limit their practical application. Images are naturally
associated with heterogeneous forms of context possibly in different
modalities. In this work, we leverage the structured context associated with
the camera trap images to improve out-of-distribution generalization for the
task of species identification in camera traps. For example, a photo of a wild
animal may be associated with information about where and when it was taken, as
well as structured biology knowledge about the animal species. While typically
overlooked by existing work, bringing back such context offers several
potential benefits for better image understanding, such as addressing data
scarcity and enhancing generalization. However, effectively integrating such
heterogeneous context into the visual domain is a challenging problem. To
address this, we propose a novel framework that reformulates species
classification as link prediction in a multimodal knowledge graph (KG). This
framework seamlessly integrates various forms of multimodal context for visual
recognition. We apply this framework for out-of-distribution species
classification on the iWildCam2020-WILDS and Snapshot Mountain Zebra datasets
and achieve competitive performance with state-of-the-art approaches.
Furthermore, our framework successfully incorporates biological taxonomy for
improved generalization and enhances sample efficiency for recognizing
under-represented species.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Benchmark</span>ing Joint Face Spoofing and Forgery Detection with Visual and
  Physiological Cues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.05401v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.05401v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zitong Yu, Rizhao Cai, Zhi Li, Wenhan Yang, Jingang Shi, Alex C. Kot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face anti-spoofing (FAS) and face forgery detection play vital roles in
securing face biometric systems from presentation attacks (PAs) and vicious
digital manipulation (e.g., deepfakes). Despite promising performance upon
large-scale data and powerful deep models, the generalization problem of
existing approaches is still an open issue. Most of recent approaches focus on
1) unimodal visual appearance or physiological (i.e., remote
photoplethysmography (rPPG)) cues; and 2) separated feature representation for
FAS or face forgery detection. On one side, unimodal appearance and rPPG
features are respectively vulnerable to high-fidelity face 3D mask and video
replay attacks, inspiring us to design reliable multi-modal fusion mechanisms
for generalized face attack detection. On the other side, there are rich common
features across FAS and face forgery detection tasks (e.g., periodic rPPG
rhythms and vanilla appearance for bonafides), providing solid evidence to
design a joint FAS and face forgery detection system in a multi-task learning
fashion. In this paper, we establish the first joint face spoofing and forgery
detection benchmark using both visual appearance and physiological rPPG cues.
To enhance the rPPG periodicity discrimination, we design a two-branch
physiological network using both facial spatio-temporal rPPG signal map and its
continuous wavelet transformed counterpart as inputs. To mitigate the modality
bias and improve the fusion efficacy, we conduct a weighted batch and layer
normalization for both appearance and rPPG features before multi-modal fusion.
We find that the generalization capacities of both unimodal (appearance or
rPPG) and multi-modal (appearance+rPPG) models can be obviously improved via
joint training on these two tasks. We hope this new benchmark will facilitate
the future research of both FAS and deepfake detection communities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Dependable and Secure Computing
  (TDSC). Corresponding authors: Zitong Yu and Wenhan Yang</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Pyramid Channel Attention Network for Pathological Myopia
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09196v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09196v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoqing Zhang, Jilu Zhao, Yan Li, Hao Wu, Xiangtian Zhou, Jiang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pathological myopia (PM) is the leading ocular disease for impaired vision
worldwide. Clinically, the characteristic of pathology distribution in PM is
global-local on the fundus image, which plays a significant role in assisting
clinicians in diagnosing PM. However, most existing deep neural networks
focused on designing complex architectures but rarely explored the pathology
distribution prior of PM. To tackle this issue, we propose an efficient pyramid
channel attention (EPCA) module, which fully leverages the potential of the
clinical pathology prior of PM with pyramid pooling and multi-scale context
fusion. Then, we construct EPCA-Net for automatic PM recognition based on
fundus images by stacking a sequence of EPCA modules. Moreover, motivated by
the recent pretraining-and-finetuning paradigm, we attempt to adapt pre-trained
natural image models for PM recognition by freezing them and treating the EPCA
and other attention modules as adapters. In addition, we construct a PM
recognition benchmark termed PM-fundus by collecting fundus images of PM from
publicly available datasets. The comprehensive experiments demonstrate the
superiority of our EPCA-Net over state-of-the-art methods in the PM recognition
task. The results also show that our method based on the
pretraining-and-finetuning paradigm achieves competitive performance through
comparisons to part of previous methods based on traditional fine-tuning
paradigm with fewer tunable parameters, which has the potential to leverage
more natural image foundation models to address the PM recognition task in
limited medical data regime.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Point Transformer with Federated Learning for Predicting Breast Cancer
  HER2 Status from Hematoxylin and Eosin-Stained Whole Slide Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06454v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06454v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bao Li, Zhenyu Liu, Lizhi Shao, Bensheng Qiu, Hong Bu, Jie Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Directly predicting human epidermal growth factor receptor 2 (HER2) status
from widely available hematoxylin and eosin (HE)-stained whole slide images
(WSIs) can reduce technical costs and expedite treatment selection. Accurately
predicting HER2 requires large collections of multi-site WSIs. Federated
learning enables collaborative training of these WSIs without gigabyte-size
WSIs transportation and data privacy concerns. However, federated learning
encounters challenges in addressing label imbalance in multi-site WSIs from the
real world. Moreover, existing WSI classification methods cannot simultaneously
exploit local context information and long-range dependencies in the site-end
feature representation of federated learning. To address these issues, we
present a point transformer with federated learning for multi-site HER2 status
prediction from HE-stained WSIs. Our approach incorporates two novel designs.
We propose a dynamic label distribution strategy and an auxiliary classifier,
which helps to establish a well-initialized model and mitigate label
distribution variations across sites. Additionally, we propose a farthest
cosine sampling based on cosine distance. It can sample the most distinctive
features and capture the long-range dependencies. Extensive experiments and
analysis show that our method achieves state-of-the-art performance at four
sites with a total of 2687 WSIs. Furthermore, we demonstrate that our model can
generalize to two unseen sites with 229 WSIs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SimDistill: Simulated Multi-modal Distillation for BEV 3D Object
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.16818v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.16818v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haimei Zhao, Qiming Zhang, Shanshan Zhao, Zhe Chen, Jing Zhang, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-view camera-based 3D object detection has become popular due to its low
cost, but accurately inferring 3D geometry solely from camera data remains
challenging and may lead to inferior performance. Although distilling precise
3D geometry knowledge from LiDAR data could help tackle this challenge, the
benefits of LiDAR information could be greatly hindered by the significant
modality gap between different sensory modalities. To address this issue, we
propose a Simulated multi-modal Distillation (SimDistill) method by carefully
crafting the model architecture and distillation strategy. Specifically, we
devise multi-modal architectures for both teacher and student models, including
a LiDAR-camera fusion-based teacher and a simulated fusion-based student. Owing
to the ``identical'' architecture design, the student can mimic the teacher to
generate multi-modal features with merely multi-view images as input, where a
geometry compensation module is introduced to bridge the modality gap.
Furthermore, we propose a comprehensive multi-modal distillation scheme that
supports intra-modal, cross-modal, and multi-modal fusion distillation
simultaneously in the Bird's-eye-view space. Incorporating them together, our
SimDistill can learn better feature representations for 3D object detection
while maintaining a cost-effective camera-only deployment. Extensive
experiments validate the effectiveness and superiority of SimDistill over
state-of-the-art methods, achieving an improvement of 4.8\% mAP and 4.1\% NDS
over the baseline detector. The source code will be released at
https://github.com/ViTAE-Transformer/SimDistill.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffBody: Diffusion-based Pose and Shape Editing of Human Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02804v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02804v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuta Okuyama, Yuki Endo, Yoshihiro Kanamori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pose and body shape editing in a human image has received increasing
attention. However, current methods often struggle with dataset biases and
deteriorate realism and the person's identity when users make large edits. We
propose a one-shot approach that enables large edits with identity
preservation. To enable large edits, we fit a 3D body model, project the input
image onto the 3D model, and change the body's pose and shape. Because this
initial textured body model has artifacts due to occlusion and the inaccurate
body shape, the rendered image undergoes a diffusion-based refinement, in which
strong noise destroys body structure and identity whereas insufficient noise
does not help. We thus propose an iterative refinement with weak noise, applied
first for the whole body and then for the face. We further enhance the realism
by fine-tuning text embeddings via self-supervised learning. Our quantitative
and qualitative evaluations demonstrate that our method outperforms other
existing methods across various datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WACV 2024, project page:
  https://www.cgg.cs.tsukuba.ac.jp/~okuyama/pub/diffbody/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain Generalization with Vital Phase Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16451v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16451v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ingyun Lee, Wooju Lee, Hyun Myung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have shown remarkable performance in image
classification. However, their performance significantly deteriorates with
corrupted input data. Domain generalization methods have been proposed to train
robust models against out-of-distribution data. Data augmentation in the
frequency domain is one of such approaches that enable a model to learn phase
features to establish domain-invariant representations. This approach changes
the amplitudes of the input data while preserving the phases. However, using
fixed phases leads to susceptibility to phase fluctuations because amplitudes
and phase fluctuations commonly occur in out-of-distribution. In this study, to
address this problem, we introduce an approach using finite variation of the
phases of input data rather than maintaining fixed phases. Based on the
assumption that the degree of domain-invariant features varies for each phase,
we propose a method to distinguish phases based on this degree. In addition, we
propose a method called vital phase augmentation (VIPAug) that applies the
variation to the phases differently according to the degree of domain-invariant
features of given phases. The model depends more on the vital phases that
contain more domain-invariant features for attaining robustness to amplitude
and phase fluctuations. We present experimental evaluations of our proposed
approach, which exhibited improved performance for both clean and corrupted
data. VIPAug achieved SOTA performance on the benchmark CIFAR-10 and CIFAR-100
datasets, as well as near-SOTA performance on the ImageNet-100 and ImageNet
datasets. Our code is available at https://github.com/excitedkid/vipaug.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI-24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EFHQ: Multi-purpose ExtremePose-Face-HQ <span class="highlight-title">dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17205v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17205v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trung Tuan Dao, Duc Hong Vu, Cuong Pham, Anh Tran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The existing facial datasets, while having plentiful images at near frontal
views, lack images with extreme head poses, leading to the downgraded
performance of deep learning models when dealing with profile or pitched faces.
This work aims to address this gap by introducing a novel dataset named Extreme
Pose Face High-Quality Dataset (EFHQ), which includes a maximum of 450k
high-quality images of faces at extreme poses. To produce such a massive
dataset, we utilize a novel and meticulous dataset processing pipeline to
curate two publicly available datasets, VFHQ and CelebV-HQ, which contain many
high-resolution face videos captured in various settings. Our dataset can
complement existing datasets on various facial-related tasks, such as facial
synthesis with 2D/3D-aware GAN, diffusion-based text-to-image face generation,
and face reenactment. Specifically, training with EFHQ helps models generalize
well across diverse poses, significantly improving performance in scenarios
involving extreme views, confirmed by extensive experiments. Additionally, we
utilize EFHQ to define a challenging cross-view face verification benchmark, in
which the performance of SOTA face recognition models drops 5-37% compared to
frontal-to-frontal scenarios, aiming to stimulate studies on face recognition
under severe pose conditions in the wild.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://bomcon123456.github.io/efhq/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AIGCBench: Comprehensive Evaluation of Image-to-Video Content Generated
  by AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01651v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01651v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanda Fan, Chunjie Luo, Wanling Gao, Jianfeng Zhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The burgeoning field of Artificial Intelligence Generated Content (AIGC) is
witnessing rapid advancements, particularly in video generation. This paper
introduces AIGCBench, a pioneering comprehensive and scalable benchmark
designed to evaluate a variety of video generation tasks, with a primary focus
on Image-to-Video (I2V) generation. AIGCBench tackles the limitations of
existing benchmarks, which suffer from a lack of diverse datasets, by including
a varied and open-domain image-text dataset that evaluates different
state-of-the-art algorithms under equivalent conditions. We employ a novel text
combiner and GPT-4 to create rich text prompts, which are then used to generate
images via advanced Text-to-Image models. To establish a unified evaluation
framework for video generation tasks, our benchmark includes 11 metrics
spanning four dimensions to assess algorithm performance. These dimensions are
control-video alignment, motion effects, temporal consistency, and video
quality. These metrics are both reference video-dependent and video-free,
ensuring a comprehensive evaluation strategy. The evaluation standard proposed
correlates well with human judgment, providing insights into the strengths and
weaknesses of current I2V algorithms. The findings from our extensive
experiments aim to stimulate further research and development in the I2V field.
AIGCBench represents a significant step toward creating standardized benchmarks
for the broader AIGC landscape, proposing an adaptable and equitable framework
for future assessments of video generation tasks. We have open-sourced the
dataset and evaluation code on the project website:
https://www.benchcouncil.org/AIGCBench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GTP-ViT: Efficient Vision Transformers via Graph-based Token Propagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.03035v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.03035v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuwei Xu, Sen Wang, Yudong Chen, Yanping Zheng, Zhewei Wei, Jiajun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformers (ViTs) have revolutionized the field of computer vision,
yet their deployments on resource-constrained devices remain challenging due to
high computational demands. To expedite pre-trained ViTs, token pruning and
token merging approaches have been developed, which aim at reducing the number
of tokens involved in the computation. However, these methods still have some
limitations, such as image information loss from pruned tokens and inefficiency
in the token-matching process. In this paper, we introduce a novel Graph-based
Token Propagation (GTP) method to resolve the challenge of balancing model
efficiency and information preservation for efficient ViTs. Inspired by graph
summarization algorithms, GTP meticulously propagates less significant tokens'
information to spatially and semantically connected tokens that are of greater
importance. Consequently, the remaining few tokens serve as a summarization of
the entire token graph, allowing the method to reduce computational complexity
while preserving essential information of eliminated tokens. Combined with an
innovative token selection strategy, GTP can efficiently identify image tokens
to be propagated. Extensive experiments have validated GTP's effectiveness,
demonstrating both efficiency and performance improvements. Specifically, GTP
decreases the computational complexity of both DeiT-S and DeiT-B by up to 26%
with only a minimal 0.3% accuracy drop on ImageNet-1K without finetuning, and
remarkably surpasses the state-of-the-art token merging method on various
backbones at an even faster inference speed. The source code is available at
https://github.com/Ackesnal/GTP-ViT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WACV2024 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ScalableMap: Scalable Map Learning for Online Long-Range Vectorized HD
  Map Construction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13378v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13378v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyi Yu, Zizhao Zhang, Shengfu Xia, Jizhang Sang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel end-to-end pipeline for online long-range vectorized
high-definition (HD) map construction using on-board camera sensors. The
vectorized representation of HD maps, employing polylines and polygons to
represent map elements, is widely used by downstream tasks. However, previous
schemes designed with reference to dynamic object detection overlook the
structural constraints within linear map elements, resulting in performance
degradation in long-range scenarios. In this paper, we exploit the properties
of map elements to improve the performance of map construction. We extract more
accurate bird's eye view (BEV) features guided by their linear structure, and
then propose a hierarchical sparse map representation to further leverage the
scalability of vectorized map elements and design a progressive decoding
mechanism and a supervision strategy based on this representation. Our
approach, ScalableMap, demonstrates superior performance on the nuScenes
dataset, especially in long-range scenarios, surpassing previous
state-of-the-art model by 6.5 mAP while achieving 18.3 FPS. Code is available
at https://github.com/jingy1yu/ScalableMap.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Neural Implicit through Volume Rendering with Attentive Depth
  Fusion Priors <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11598v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11598v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengchong Hu, Zhizhong Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning neural implicit representations has achieved remarkable performance
in 3D reconstruction from multi-view images. Current methods use volume
rendering to render implicit representations into either RGB or depth images
that are supervised by multi-view ground truth. However, rendering a view each
time suffers from incomplete depth at holes and unawareness of occluded
structures from the depth supervision, which severely affects the accuracy of
geometry inference via volume rendering. To resolve this issue, we propose to
learn neural implicit representations from multi-view RGBD images through
volume rendering with an attentive depth fusion prior. Our prior allows neural
networks to perceive coarse 3D structures from the Truncated Signed Distance
Function (TSDF) fused from all depth images available for rendering. The TSDF
enables accessing the missing depth at holes on one depth image and the
occluded parts that are invisible from the current view. By introducing a novel
attention mechanism, we allow neural networks to directly use the depth fusion
prior with the inferred occupancy as the learned implicit function. Our
attention mechanism works with either a one-time fused TSDF that represents a
whole scene or an incrementally fused TSDF that represents a partial scene in
the context of Simultaneous Localization and Mapping (SLAM). Our evaluations on
widely used benchmarks including synthetic and real-world scans show our
superiority over the latest neural implicit methods. Project page:
https://machineperceptionlab.github.io/Attentive_DF_Prior/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accurate Leukocyte Detection Based on Deformable-DETR and Multi-Level
  Feature Fusion for Aiding Diagnosis of Blood Diseases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00926v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00926v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Chen, Chenyan Zhang, Ben Chen, Yiyu Huang, Yifei Sun, Changmiao Wang, Xianjun Fu, Yuxing Dai, Feiwei Qin, Yong Peng, Yu Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In standard hospital blood tests, the traditional process requires doctors to
manually isolate leukocytes from microscopic images of patients' blood using
microscopes. These isolated leukocytes are then categorized via automatic
leukocyte classifiers to determine the proportion and volume of different types
of leukocytes present in the blood samples, aiding disease diagnosis. This
methodology is not only time-consuming and labor-intensive, but it also has a
high propensity for errors due to factors such as image quality and
environmental conditions, which could potentially lead to incorrect subsequent
classifications and misdiagnosis. To address these issues, this paper proposes
an innovative method of leukocyte detection: the Multi-level Feature Fusion and
Deformable Self-attention DETR (MFDS-DETR). To tackle the issue of leukocyte
scale disparity, we designed the High-level Screening-feature Fusion Pyramid
(HS-FPN), enabling multi-level fusion. This model uses high-level features as
weights to filter low-level feature information via a channel attention module
and then merges the screened information with the high-level features, thus
enhancing the model's feature expression capability. Further, we address the
issue of leukocyte feature scarcity by incorporating a multi-scale deformable
self-attention module in the encoder and using the self-attention and
cross-deformable attention mechanisms in the decoder, which aids in the
extraction of the global features of the leukocyte feature maps. The
effectiveness, superiority, and generalizability of the proposed MFDS-DETR
method are confirmed through comparisons with other cutting-edge leukocyte
detection models using the private WBCDD, public LISC and BCCD datasets. Our
source code and private WBCCD dataset are available at
https://github.com/JustlfC03/MFDS-DETR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures, accept Computers in Biology and Medicine 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Novel real-time arrhythmia detection model using YOLOv8 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16727v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16727v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guang Jun Nicholas Ang, Aritejh Kr Goil, Henryk Chan, Jieyi Jeric Lew, Xin Chun Lee, Raihan Bin Ahmad Mustaffa, Timotius Jason, Ze Ting Woon, Bingquan Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a landscape characterized by heightened connectivity and mobility, coupled
with a surge in cardiovascular ailments, the imperative to curtail healthcare
expenses through remote monitoring of cardiovascular health has become more
pronounced. The accurate detection and classification of cardiac arrhythmias
are pivotal for diagnosing individuals with heart irregularities. This study
underscores the feasibility of employing electrocardiograms (ECG) measurements
in the home environment for real-time arrhythmia detection. Presenting a fresh
application for arrhythmia detection, this paper leverages the cutting-edge
You-Only-Look-Once (YOLO)v8 algorithm to categorize single-lead ECG signals. We
introduce a novel loss-modified YOLOv8 model, fine-tuned on the MIT-BIH
arrhythmia dataset, enabling real-time continuous monitoring. The obtained
results substantiate the efficacy of our approach, with the model attaining an
average accuracy of 99.5% and 0.992 mAP@50, and a rapid detection time of 0.002
seconds on an NVIDIA Tesla V100. Our investigation exemplifies the potential of
real-time arrhythmia detection, enabling users to visually interpret the model
output within the comfort of their homes. Furthermore, this study lays the
groundwork for an extension into a real-time explainable AI (XAI) model capable
of deployment in the healthcare sector, thereby significantly advancing the
realm of healthcare solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-modal Active Complementary Learning with Self-refining
  Correspondence <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17468v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17468v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Qin, Yuan Sun, Dezhong Peng, Joey Tianyi Zhou, Xi Peng, Peng Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, image-text matching has attracted more and more attention from
academia and industry, which is fundamental to understanding the latent
correspondence across visual and textual modalities. However, most existing
methods implicitly assume the training pairs are well-aligned while ignoring
the ubiquitous annotation noise, a.k.a noisy correspondence (NC), thereby
inevitably leading to a performance drop. Although some methods attempt to
address such noise, they still face two challenging problems: excessive
memorizing/overfitting and unreliable correction for NC, especially under high
noise. To address the two problems, we propose a generalized Cross-modal Robust
Complementary Learning framework (CRCL), which benefits from a novel Active
Complementary Loss (ACL) and an efficient Self-refining Correspondence
Correction (SCC) to improve the robustness of existing methods. Specifically,
ACL exploits active and complementary learning losses to reduce the risk of
providing erroneous supervision, leading to theoretically and experimentally
demonstrated robustness against NC. SCC utilizes multiple self-refining
processes with momentum correction to enlarge the receptive field for
correcting correspondences, thereby alleviating error accumulation and
achieving accurate and stable corrections. We carry out extensive experiments
on three image-text benchmarks, i.e., Flickr30K, MS-COCO, and CC152K, to verify
the superior robustness of our CRCL against synthetic and real-world noisy
correspondences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">18</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Digital Twin for Autonomous Surface Vessels for Safe Maritime <span class="highlight-title">Navigation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.04032v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.04032v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Menges, Andreas Von Brandis, Adil Rasheed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous surface vessels (ASVs) play an increasingly important role in the
safety and sustainability of open sea operations. Since most maritime accidents
are related to human failure, intelligent algorithms for autonomous collision
avoidance and path following can drastically reduce the risk in the maritime
sector. A DT is a virtual representative of a real physical system and can
enhance the situational awareness (SITAW) of such an ASV to generate optimal
decisions. This work builds on an existing DT framework for ASVs and
demonstrates foundations for enabling predictive, prescriptive, and autonomous
capabilities. In this context, sophisticated target tracking approaches are
crucial for estimating and predicting the position and motion of other dynamic
objects. The applied tracking method is enabled by real-time automatic
identification system (AIS) data and synthetic light detection and ranging
(Lidar) measurements. To guarantee safety during autonomous operations, we
applied a predictive safety filter, based on the concept of nonlinear model
predictive control (NMPC). The approaches are implemented into a DT built with
the Unity game engine. As a result, this work demonstrates the potential of a
DT capable of making predictions, playing through various what-if scenarios,
and providing optimal control decisions according to its enhanced SITAW.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Task-Oriented Active Learning of Model Preconditions for Inaccurate
  Dynamics Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.04007v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.04007v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex LaGrassa, Moonyoung Lee, Oliver Kroemer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When planning with an inaccurate dynamics model, a practical strategy is to
restrict planning to regions of state-action space where the model is accurate:
also known as a model precondition. Empirical real-world trajectory data is
valuable for defining data-driven model preconditions regardless of the model
form (analytical, simulator, learned, etc...). However, real-world data is
often expensive and dangerous to collect. In order to achieve data efficiency,
this paper presents an algorithm for actively selecting trajectories to learn a
model precondition for an inaccurate pre-specified dynamics model. Our proposed
techniques address challenges arising from the sequential nature of
trajectories, and potential benefit of prioritizing task-relevant data. The
experimental analysis shows how algorithmic properties affect performance in
three planning scenarios: icy gridworld, simulated plant watering, and
real-world plant watering. Results demonstrate an improvement of approximately
80% after only four real-world trajectories when using our proposed techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to International Conference on Robotics and Automation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simultaneous Task Allocation and Planning for Multi-Robots under
  Hierarchical Temporal Logic Specifications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.04003v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.04003v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xusheng Luo, Changliu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Past research into robotic planning with temporal logic specifications,
notably Linear Temporal Logic (LTL), was largely based on singular formulas for
individual or groups of robots. But with increasing task complexity, LTL
formulas unavoidably grow lengthy, complicating interpretation and
specification generation, and straining the computational capacities of the
planners. In order to maximize the potential of LTL specifications, we
capitalized on the intrinsic structure of tasks and introduced a hierarchical
structure to LTL specifications, and designed an algorithm to ascertain whether
they are satisfied given an input sequence. Second, we employ a search-based
approach to synthesize plans for a multi-robot system, accomplishing
simultaneous task allocation and planning. The search space is approximated by
loosely interconnected sub-spaces, with each sub-space corresponding to one LTL
specification. The search is predominantly confined to a single sub-space,
transitioning to another sub-space under certain conditions, determined by the
decomposition of automatons. Moreover, multiple heuristics are formulated to
expedite the search significantly. A theoretical analysis concerning
completeness and optimality is conducted under mild assumptions. When compared
with existing methods on service tasks, our method outperforms in terms of
execution times with comparable solution quality. Finally, scalability is
evaluated by testing a group of 30 robots and achieving reasonable runtimes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diegetic Graphical User Interfaces and Intuitive Control of Assistive
  Robots via Eye-gaze 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03944v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03944v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emanuel Nunez Sardinha, Marcela Munera, Nancy Zook, David Western, Virginia Ruiz Garate
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Individuals with tetraplegia and similar forms of paralysis suffer physically
and emotionally due to a lack of autonomy. To help regain part of this
autonomy, assistive robotic arms have been shown to increase living
independence. However, users with paralysis pose unique challenging conditions
for the control of these devices. In this article, we present the use of
Diegetic Graphical User Interfaces, a novel, intuitive, and computationally
inexpensive approach for gaze-controlled interfaces applied to robots. By using
symbols paired with fiducial markers, interactive buttons can be defined in the
real world which the user can trigger via gaze, and which can be embedded
easily into the environment. We apply this system to pilot a
3-degree-of-freedom robotic arm for precision pick-and-place tasks. The
interface is placed directly on the robot to allow intuitive and direct
interaction, eliminating the need for context-switching between external
screens, menus, and the robot. After calibration and a brief habituation
period, twenty-one participants from multiple backgrounds, ages and eye-sight
conditions completed the Yale-CMU-Berkeley (YCB) Block Pick and Place Protocol
to benchmark the system, achieving a mean score of 13.71 out of the maximum
16.00 points. Good usability and user experience were reported (System
Usability Score of 75.36) while achieving a low task workload measure (NASA-TLX
of 44.76). Results show that users can employ multiple interface elements to
perform actions with minimal practice and with a small cognitive load. To our
knowledge, this is the first easily reconfigurable screenless system that
enables robot control entirely via gaze for Cartesian robot control without the
need for eye or face gestures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 11 figures, 5 tables Submitted to Transactions in Robotics.
  Github repo at https://github.com/enunezs/D-GUI For the associated video, see
  https://youtu.be/hrXuNYLDFds</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recovering the 3D UUV Position using UAV Imagery in Shallow-Water
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03938v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03938v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antun Đuraš, Matija Sukno, Ivana Palunko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we propose a novel approach aimed at recovering the 3D position
of an UUV from UAV imagery in shallow-water environments. Through combination
of UAV and UUV measurements, we show that our method can be utilized as an
accurate and cost-effective alternative when compared to acoustic sensing
methods, typically required to obtain ground truth information in underwater
localization problems. Furthermore, our approach allows for a seamless
conversion to geo-referenced coordinates which can be utilized for navigation
purposes. To validate our method, we present the results with data collected
through a simulation environment and field experiments, demonstrating the
ability to successfully recover the UUV position with sub-meter accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using reinforcement learning to improve drone-based inference of
  greenhouse gas fluxes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03932v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03932v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alouette van Hove, Kristoffer Aalstad, Norbert Pirk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate mapping of greenhouse gas fluxes at the Earth's surface is essential
for the validation and calibration of climate models. In this study, we present
a framework for surface flux estimation with drones. Our approach uses data
assimilation (DA) to infer fluxes from drone-based observations, and
reinforcement learning (RL) to optimize the drone's sampling strategy. Herein,
we demonstrate that a RL-trained drone can quantify a CO2 hotspot more
accurately than a drone sampling along a predefined flight path that traverses
the emission plume. We find that information-based reward functions can match
the performance of an error-based reward function that quantifies the
difference between the estimated surface flux and the true value. Reward
functions based on information gain and information entropy can motivate
actions that increase the drone's confidence in its updated belief, without
requiring knowledge of the true surface flux. These findings provide valuable
insights for further development of the framework for the mapping of more
complex surface flux fields.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guided Time-optimal Model Predictive Control of a Multi-rotor 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03904v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03904v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyu Zhang, Yongjie Zheng, Yuqing He, Liying Yang, Hongyu Nie, Chaoxiong Huang, Yiwen Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time-optimal control of a multi-rotor remains an open problem due to the
under-actuation and nonlinearity of its dynamics, which make it difficult to
solve this problem directly. In this paper, the time-optimal control problem of
the multi-rotor is studied. Firstly, a thrust limit optimal decomposition
method is proposed, which can reasonably decompose the limited thrust into
three directions according to the current state and the target state. As a
result, the thrust limit constraint is decomposed as a linear constraint. With
the linear constraint and decoupled dynamics, a time-optimal guidance
trajectory can be obtained. Then, a cost function is defined based on the
time-optimal guidance trajectory, which has a quadratic form and can be used to
evaluate the time-optimal performance of the system outputs. Finally, based on
the cost function, the time-optimal control problem is reformulated as an MPC
(Model Predictive Control) problem. The experimental results demonstrate the
feasibility and validity of the proposed methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Aerial Manipulator for Robot-to-robot Torch Relay Task: System Design
  and Control Scheme 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03903v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03903v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyu Zhang, Yuqing He, Liying Yang, Chaoxiong Huang, Yanchun Chang, Siliang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Torch relay is an important tradition of the Olympics and heralds the start
of the Games. Robots applied in the torch relay activity can not only
demonstrate the technological capability of humans to the world but also
provide a sight of human lives with robots in the future. This article presents
an aerial manipulator designed for the robot-to-robot torch relay task of the
Beijing 2022 Winter Olympics. This aerial manipulator system is composed of a
quadrotor, a 3 DoF (Degree of Freedom) manipulator, and a monocular camera.
This article primarily describes the system design and system control scheme of
the aerial manipulator. The experimental results demonstrate that it can
complete robot-to-robot torch relay task under the guidance of vision in the
ice and snow field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "Oh, Sorry, I Think I Interrupted You'': Designing Repair Strategies for
  Robotic Longitudinal Well-being Coaching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minja Axelsson, Micol Spitale, Hatice Gunes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic well-being coaches have been shown to successfully promote people's
mental well-being. To provide successful coaching, a robotic coach should have
the capability to repair the mistakes it makes. Past investigations of robot
mistakes are limited to game or task-based, one-off and in-lab studies. This
paper presents a 4-phase design process to design repair strategies for robotic
longitudinal well-being coaching with the involvement of real-world
stakeholders: 1) designing repair strategies with a professional well-being
coach; 2) a longitudinal study with the involvement of experienced users (i.e.,
who had already interacted with a robotic coach) to investigate the repair
strategies defined in (1); 3) a design workshop with users from the study in
(2) to gather their perspectives on the robotic coach's repair strategies; 4)
discussing the results obtained in (2) and (3) with the mental well-being
professional to reflect on how to design repair strategies for robotic
coaching. Our results show that users have different expectations for a robotic
coach than a human coach, which influences how repair strategies should be
designed. We show that different repair strategies (e.g., apologizing,
explaining, or repairing empathically) are appropriate in different scenarios,
and that preferences for repair strategies change during longitudinal
interactions with the robotic coach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-term Safe Reinforcement Learning with Binary Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akifumi Wachi, Wataru Hashimoto, Kazumune Hashimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safety is an indispensable requirement for applying reinforcement learning
(RL) to real problems. Although there has been a surge of safe RL algorithms
proposed in recent years, most existing work typically 1) relies on receiving
numeric safety feedback; 2) does not guarantee safety during the learning
process; 3) limits the problem to a priori known, deterministic transition
dynamics; and/or 4) assume the existence of a known safe policy for any states.
Addressing the issues mentioned above, we thus propose Long-term Binaryfeedback
Safe RL (LoBiSaRL), a safe RL algorithm for constrained Markov decision
processes (CMDPs) with binary safety feedback and an unknown, stochastic state
transition function. LoBiSaRL optimizes a policy to maximize rewards while
guaranteeing a long-term safety that an agent executes only safe state-action
pairs throughout each episode with high probability. Specifically, LoBiSaRL
models the binary safety function via a generalized linear model (GLM) and
conservatively takes only a safe action at every time step while inferring its
effect on future safety under proper assumptions. Our theoretical results show
that LoBiSaRL guarantees the long-term safety constraint, with high
probability. Finally, our empirical results demonstrate that our algorithm is
safer than existing methods without significantly compromising performance in
terms of reward.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI-24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ExTraCT -- Explainable Trajectory Corrections from language inputs using
  Textual description of features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        J-Anne Yow, Neha Priyadarshini Garg, Manoj Ramanathan, Wei Tech Ang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language provides an intuitive and expressive way of conveying human
intent to robots. Prior works employed end-to-end methods for learning
trajectory deformations from language corrections. However, such methods do not
generalize to new initial trajectories or object configurations. This work
presents ExTraCT, a modular framework for trajectory corrections using natural
language that combines Large Language Models (LLMs) for natural language
understanding and trajectory deformation functions. Given a scene, ExTraCT
generates the trajectory modification features (scene-specific and
scene-independent) and their corresponding natural language textual
descriptions for the objects in the scene online based on a template. We use
LLMs for semantic matching of user utterances to the textual descriptions of
features. Based on the feature matched, a trajectory modification function is
applied to the initial trajectory, allowing generalization to unseen
trajectories and object configurations. Through user studies conducted both in
simulation and with a physical robot arm, we demonstrate that trajectories
deformed using our method were more accurate and were preferred in about 80\%
of cases, outperforming the baseline. We also showcase the versatility of our
system in a manipulation task and an assistive feeding task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DME-Driver: Integrating Human Decision Logic and 3D Scene Perception in
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wencheng Han, Dongqian Guo, Cheng-Zhong Xu, Jianbing Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of autonomous driving, two important features of autonomous
driving car systems are the explainability of decision logic and the accuracy
of environmental perception. This paper introduces DME-Driver, a new autonomous
driving system that enhances the performance and reliability of autonomous
driving system. DME-Driver utilizes a powerful vision language model as the
decision-maker and a planning-oriented perception model as the control signal
generator. To ensure explainable and reliable driving decisions, the logical
decision-maker is constructed based on a large vision language model. This
model follows the logic employed by experienced human drivers and makes
decisions in a similar manner. On the other hand, the generation of accurate
control signals relies on precise and detailed environmental perception, which
is where 3D scene perception models excel. Therefore, a planning oriented
perception model is employed as the signal generator. It translates the logical
decisions made by the decision-maker into accurate control signals for the
self-driving cars. To effectively train the proposed model, a new dataset for
autonomous driving was created. This dataset encompasses a diverse range of
human driver behaviors and their underlying motivations. By leveraging this
dataset, our model achieves high-precision planning accuracy through a logical
thinking process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DDM-Lag : A Diffusion-based Decision-making Model for Autonomous
  Vehicles with Lagrangian Safety Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqi Liu, Peng Hang, Xiaocong Zhao, Jianqiang Wang, Jian Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decision-making stands as a pivotal component in the realm of autonomous
vehicles (AVs), playing a crucial role in navigating the intricacies of
autonomous driving. Amidst the evolving landscape of data-driven methodologies,
enhancing decision-making performance in complex scenarios has emerged as a
prominent research focus. Despite considerable advancements, current
learning-based decision-making approaches exhibit potential for refinement,
particularly in aspects of policy articulation and safety assurance. To address
these challenges, we introduce DDM-Lag, a Diffusion Decision Model,augmented
with Lagrangian-based safety enhancements.In our approach, the autonomous
driving decision-making conundrum is conceptualized as a Constrained Markov
Decision Process (CMDP). We have crafted an Actor-Critic framework, wherein the
diffusion model is employed as the actor,facilitating policy exploration and
learning. The integration of safety constraints in the CMDP and the adoption of
a Lagrangian relaxation-based policy optimization technique ensure enhanced
decision safety. A PID controller is employed for the stable updating of model
parameters. The effectiveness of DDM-Lag is evaluated through different driving
tasks, showcasing improvements in decision-making safety and overall
performance compared to baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributed formation-enforcing control for UAVs robust to observation
  noise in relative pose measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.03057v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.03057v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viktor Walter, Matouš Vrba, Daniel Bonilla Licea, Matej Hilmer, Martin Saska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A technique that allows a formation-enforcing control (FEC) derived from
graph rigidity theory to interface with a realistic relative localization
system onboard lightweight Unmanned Aerial Vehicles (UAVs) is proposed in this
paper. The proposed methodology enables reliable real-world deployment of UAVs
in tight formation using real relative localization systems burdened by
non-negligible sensory noise, which is typically not fully taken into account
in FEC algorithms. The proposed solution is based on decomposition of the
gradient descent-based FEC command into interpretable elements, and then
modifying these individually based on the estimated distribution of sensory
noise, such that the resulting action limits the probability of overshooting
the desired formation. The behavior of the system has been analyzed and the
practicality of the proposed solution has been compared to pure
gradient-descent in real-world experiments where it presented significantly
better performance in terms of oscillations, deviation from the desired state
and convergence time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Transactions on Robotics journal on December 19.
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attentiveness Map Estimation for Haptic Teleoperation of Mobile Robot
  Obstacle Avoidance and Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08742v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08742v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ninghan Zhong, Kris Hauser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Haptic feedback can improve safety of teleoperated robots when situational
awareness is limited or operators are inattentive. Standard potential field
approaches increase haptic resistance as an obstacle is approached, which is
desirable when the operator is unaware of the obstacle but undesirable when the
movement is intentional, such as when the operator wishes to inspect or
manipulate an object. This paper presents a novel haptic teleoperation
framework that estimates the operator's attentiveness to obstacles and dampens
haptic feedback for intentional movement. A biologically-inspired attention
model is developed based on computational working memory theories to integrate
visual saliency estimation with spatial mapping. The attentiveness map is
generated in real-time, and our system renders lower haptic forces for
obstacles that the operator is estimated to be aware of. Experimental results
in simulation show that the proposed framework outperforms haptic teleoperation
without attentiveness estimation in terms of task performance, robot safety,
and user experience.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE RA-L</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DamWorld: Progressive Reasoning with World Models for Robotic
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11335v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11335v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengzhen Ren, Kaidong Zhang, Hetao Zheng, Zixuan Li, Yuhang Wen, Fengda Zhu, Mas Ma, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The research on embodied AI has greatly promoted the development of robot
manipulation. However, it still faces significant challenges in various aspects
such as benchmark construction, multi-modal perception and decision-making, and
physical execution. Previous robot manipulation simulators were primarily
designed to enrich manipulation types and types of objects while neglecting the
balance between physical manipulation and language instruction complexity in
multi-modal environments. This paper proposes a new robot manipulation
simulator and builds a comprehensive and systematic robot manipulation
benchmark with progressive reasoning tasks called SeaWave (i.e., a progressive
reasoning benchmark). It provides a standard test platform for embedded AI
agents in a multi-modal environment, which can evaluate and execute four levels
of human natural language instructions at the same time.
  Previous world model-based robot manipulation work lacked research on the
perception and decision-making of complex instructions in multi-modal
environments. To this end, we propose a new world model tailored for
cross-modal robot manipulation called DamWorld. Specifically, DamWorld takes
the current visual scene and predicted execution actions based on natural
language instructions as input, and uses the next action frame to supervise the
output of the world model to force the model to learn robot manipulation
consistent with world knowledge. Compared with the renowned baselines (e.g.,
RT-1), our DamWorld improves the manipulation success rate by 5.6% on average
on four levels of progressive reasoning tasks. It is worth noting that on the
most challenging level 4 manipulation task, DamWorld still improved by 9.0%
compared to prior works.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Sampling-based Particle Filter for Visual-inertial Gimbal in
  the Wild <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.10981v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.10981v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueyang Kang, Ariel Herrera, Henry Lema, Esteban Valencia, Patrick Vandewalle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a Computer Vision (CV) based tracking and fusion
algorithm, dedicated to a 3D printed gimbal system on drones operating in
nature. The whole gimbal system can stabilize the camera orientation robustly
in a challenging nature scenario by using skyline and ground plane as
references. Our main contributions are the following: a) a light-weight
Resnet-18 backbone network model was trained from scratch, and deployed onto
the Jetson Nano platform to segment the image into binary parts (ground and
sky); b) our geometry assumption from nature cues delivers the potential for
robust visual tracking by using the skyline and ground plane as a reference; c)
a spherical surface-based adaptive particle sampling, can fuse orientation from
multiple sensor sources flexibly. The whole algorithm pipeline is tested on our
customized gimbal module including Jetson and other hardware components. The
experiments were performed on top of a building in the real landscape.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>content in 6 pages, 9 figures, 2 pseudo codes, one table, accepted by
  ICRA 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ INVIGORATE: Interactive Visual Grounding and Grasping in Clutter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.11092v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.11092v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanbo Zhang, Yunfan Lu, Cunjun Yu, David Hsu, Xuguang Lan, Nanning Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents INVIGORATE, a robot system that interacts with human
through natural language and grasps a specified object in clutter. The objects
may occlude, obstruct, or even stack on top of one another. INVIGORATE embodies
several challenges: (i) infer the target object among other occluding objects,
from input language expressions and RGB images, (ii) infer object blocking
relationships (OBRs) from the images, and (iii) synthesize a multi-step plan to
ask questions that disambiguate the target object and to grasp it successfully.
We train separate neural networks for object detection, for visual grounding,
for question generation, and for OBR detection and grasping. They allow for
unrestricted object categories and language expressions, subject to the
training datasets. However, errors in visual perception and ambiguity in human
languages are inevitable and negatively impact the robot's performance. To
overcome these uncertainties, we build a partially observable Markov decision
process (POMDP) that integrates the learned neural network modules. Through
approximate POMDP planning, the robot tracks the history of observations and
asks disambiguation questions in order to achieve a near-optimal sequence of
actions that identify and grasp the target object. INVIGORATE combines the
benefits of model-based POMDP planning and data-driven deep learning.
Preliminary experiments with INVIGORATE on a Fetch robot show significant
benefits of this integrated approach to object grasping in clutter with natural
language interactions. A demonstration video is available at
https://youtu.be/zYakh80SGcU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, full version</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">9</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bjøntegaard Delta (BD): A Tutorial Overview of the Metric, Evolution,
  Challenges, and Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.04039v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.04039v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nabajeet Barman, Maria G. Martini, Yuriy Reznik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Bj{\o}ntegaard Delta (BD) method proposed in 2001 has become a popular
tool for comparing video codec compression efficiency. It was initially
proposed to compute bitrate and quality differences between two Rate-Distortion
curves using PSNR as a distortion metric. Over the years, many works have
calculated and reported BD results using other objective quality metrics such
as SSIM, VMAF and, in some cases, even subjective ratings (mean opinion
scores). However, the lack of consolidated literature explaining the metric,
its evolution over the years, and a systematic evaluation of the same under
different test conditions can result in a wrong interpretation of the BD
results thus obtained.
  Towards this end, this paper presents a detailed tutorial describing the BD
method and example cases where the metric might fail. We also provide a
detailed history of its evolution, including a discussion of various proposed
improvements and variations over the last 20 years. In addition, we evaluate
the various BD methods and their open-source implementations, considering
different objective quality metrics and subjective ratings taking into account
different RD characteristics. Based on our results, we present a set of
recommendations on using existing BD metrics and various insights for possible
exploration towards developing more effective tools for codec compression
efficiency evaluation and comparison.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Multiscale Multimodal Bottleneck Transformer for Audio-Video
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.04023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.04023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, researchers combine both audio and video signals to deal
with challenges where actions are not well represented or captured by visual
cues. However, how to effectively leverage the two modalities is still under
development. In this work, we develop a multiscale multimodal Transformer (MMT)
that leverages hierarchical representation learning. Particularly, MMT is
composed of a novel multiscale audio Transformer (MAT) and a multiscale video
Transformer [43]. To learn a discriminative cross-modality fusion, we further
design multimodal supervised contrastive objectives called audio-video
contrastive loss (AVC) and intra-modal contrastive loss (IMC) that robustly
align the two modalities. MMT surpasses previous state-of-the-art approaches by
7.3% and 2.1% on Kinetics-Sounds and VGGSound in terms of the top-1 accuracy
without external training data. Moreover, the proposed MAT significantly
outperforms AST [28] by 22.2%, 4.4% and 4.7% on three public benchmark
datasets, and is about 3% more efficient based on the number of FLOPs and 9.8%
more efficient based on GPU memory usage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WACV 2024; well-formatted PDF is in
  https://drive.google.com/file/d/10Zo_ydJZFAm7YsxHDgTjhyc4dEJbW_dk/view?usp=sharing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Survey on 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03890v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03890v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guikun Chen, Wenguan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian splatting (3D GS) has recently emerged as a transformative
technique in the explicit radiance field and computer graphics landscape. This
innovative approach, characterized by the utilization of millions of 3D
Gaussians, represents a significant departure from the neural radiance field
(NeRF) methodologies, which predominantly use implicit, coordinate-based models
to map spatial coordinates to pixel values. 3D GS, with its explicit scene
representations and differentiable rendering algorithms, not only promises
real-time rendering capabilities but also introduces unprecedented levels of
control and editability. This positions 3D GS as a potential game-changer for
the next generation of 3D reconstruction and representation. In the present
paper, we provide the first systematic overview of the recent developments and
critical contributions in the domain of 3D GS. We begin with a detailed
exploration of the underlying principles and the driving forces behind the
advent of 3D GS, setting the stage for understanding its significance. A focal
point of our discussion is the practical applicability of 3D GS. By
facilitating real-time performance, 3D GS opens up a plethora of applications,
ranging from virtual reality to interactive media and beyond. This is
complemented by a comparative analysis of leading 3D GS models, evaluated
across various benchmark tasks to highlight their performance and practical
utility. The survey concludes by identifying current challenges and suggesting
potential avenues for future research in this domain. Through this survey, we
aim to provide a valuable resource for both newcomers and seasoned researchers,
fostering further exploration and advancement in applicable and explicit
radiance field representation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing project</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TPC-ViT: Token Propagation Controller for Efficient Vision Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01470v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01470v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision transformers (ViTs) have achieved promising results on a variety of
Computer Vision tasks, however their quadratic complexity in the number of
input tokens has limited their application specially in resource-constrained
settings. Previous approaches that employ gradual token reduction to address
this challenge assume that token redundancy in one layer implies redundancy in
all the following layers. We empirically demonstrate that this assumption is
often not correct, i.e., tokens that are redundant in one layer can be useful
in later layers. We employ this key insight to propose a novel token
propagation controller (TPC) that incorporates two different
token-distributions, i.e., pause probability and restart probability to control
the reduction and reuse of tokens respectively, which results in more efficient
token utilization. To improve the estimates of token distributions, we propose
a smoothing mechanism that acts as a regularizer and helps remove noisy
outliers. Furthermore, to improve the training-stability of our proposed TPC,
we introduce a model stabilizer that is able to implicitly encode local image
structures and minimize accuracy fluctuations during model training. We present
extensive experimental results on the ImageNet-1K dataset using DeiT, LV-ViT
and Swin models to demonstrate the effectiveness of our proposed method. For
example, compared to baseline models, our proposed method improves the
inference speed of the DeiT-S by 250% while increasing the classification
accuracy by 1.0%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the main conference of WACV 2024; well-formatted PDF is
  in
  https://drive.google.com/file/d/1Id3oEdYv3OWing1qojQMyjvhZO-gG-Dm/view?usp=sharing
  ; supplementary is in
  https://drive.google.com/file/d/15LhYlBdCXtompA0_TLAp_ZJb4_sq2N5V/view?usp=sharing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recovering Sign Bits of DCT Coefficients in Digital Images as an
  Optimization Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.01096v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.01096v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiyuan Lin, Sheng Liu, Jun Jiang, Shujun Li, Chengqing Li, C. -C. Jay Kuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recovering unknown, missing, damaged, distorted, or lost information in DCT
coefficients is a common task in multiple applications of digital image
processing, including image compression, selective image encryption, and image
communication. This paper investigates the recovery of sign bits in DCT
coefficients of digital images, by proposing two different approximation
methods to solve a mixed integer linear programming (MILP) problem, which is
NP-hard in general. One method is a relaxation of the MILP problem to a linear
programming (LP) problem, and the other splits the original MILP problem into
some smaller MILP problems and an LP problem. We considered how the proposed
methods can be applied to JPEG-encoded images and conducted extensive
experiments to validate their performances. The experimental results showed
that the proposed methods outperformed other existing methods by a substantial
margin, both according to objective quality metrics and our subjective
evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MaskSearch: Querying Image Masks at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.02375v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.02375v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong He, Jieyu Zhang, Maureen Daum, Alexander Ratner, Magdalena Balazinska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning tasks over image databases often generate masks that
annotate image content (e.g., saliency maps, segmentation maps, depth maps) and
enable a variety of applications (e.g., determine if a model is learning
spurious correlations or if an image was maliciously modified to mislead a
model). While queries that retrieve examples based on mask properties are
valuable to practitioners, existing systems do not support them efficiently. In
this paper, we formalize the problem and propose MaskSearch, a system that
focuses on accelerating queries over databases of image masks while
guaranteeing the correctness of query results. MaskSearch leverages a novel
indexing technique and an efficient filter-verification query execution
framework. Experiments with our prototype show that MaskSearch, using indexes
approximately 5% of the compressed data size, accelerates individual queries by
up to two orders of magnitude and consistently outperforms existing methods on
various multi-query workloads that simulate dataset exploration and analysis
processes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AdaMesh: Personalized Facial Expressions and Head Poses for Adaptive
  Speech-Driven 3D Facial Animation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07236v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07236v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liyang Chen, Weihong Bao, Shun Lei, Boshi Tang, Zhiyong Wu, Shiyin Kang, Haozhi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech-driven 3D facial animation aims at generating facial movements that
are synchronized with the driving speech, which has been widely explored
recently. Existing works mostly neglect the person-specific talking style in
generation, including facial expression and head pose styles. Several works
intend to capture the personalities by fine-tuning modules. However, limited
training data leads to the lack of vividness. In this work, we propose AdaMesh,
a novel adaptive speech-driven facial animation approach, which learns the
personalized talking style from a reference video of about 10 seconds and
generates vivid facial expressions and head poses. Specifically, we propose
mixture-of-low-rank adaptation (MoLoRA) to fine-tune the expression adapter,
which efficiently captures the facial expression style. For the personalized
pose style, we propose a pose adapter by building a discrete pose prior and
retrieving the appropriate style embedding with a semantic-aware pose style
matrix without fine-tuning. Extensive experimental results show that our
approach outperforms state-of-the-art methods, preserves the talking style in
the reference video, and generates vivid facial animation. The supplementary
video and code will be available at https://adamesh.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://adamesh.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deformable Audio Transformer for Audio Event Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16228v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16228v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have achieved promising results on a variety of tasks. However,
the quadratic complexity in self-attention computation has limited the
applications, especially in low-resource settings and mobile or edge devices.
Existing works have proposed to exploit hand-crafted attention patterns to
reduce computation complexity. However, such hand-crafted patterns are
data-agnostic and may not be optimal. Hence, it is likely that relevant keys or
values are being reduced, while less important ones are still preserved. Based
on this key insight, we propose a novel deformable audio Transformer for audio
recognition, named DATAR, where a deformable attention equipping with a pyramid
transformer backbone is constructed and learnable. Such an architecture has
been proven effective in prediction tasks,~\textit{e.g.}, event classification.
Moreover, we identify that the deformable attention map computation may
over-simplify the input feature, which can be further enhanced. Hence, we
introduce a learnable input adaptor to alleviate this issue, and DATAR achieves
state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2024. arXiv admin note: substantial text overlap with
  arXiv:2201.00520 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Modal Discussion Transformer: Integrating Text, Images and Graph
  Transformers to Detect Hate Speech on Social Media 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09312v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09312v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liam Hebert, Gaurav Sahu, Yuxuan Guo, Nanda Kishore Sreenivas, Lukasz Golab, Robin Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the Multi-Modal Discussion Transformer (mDT), a novel methodfor
detecting hate speech in online social networks such as Reddit discussions. In
contrast to traditional comment-only methods, our approach to labelling a
comment as hate speech involves a holistic analysis of text and images grounded
in the discussion context. This is done by leveraging graph transformers to
capture the contextual relationships in the discussion surrounding a comment
and grounding the interwoven fusion layers that combine text and image
embeddings instead of processing modalities separately. To evaluate our work,
we present a new dataset, HatefulDiscussions, comprising complete multi-modal
discussions from multiple online communities on Reddit. We compare the
performance of our model to baselines that only process individual comments and
conduct extensive ablation studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2024 (AI for Social Impact Track)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-01-07T00:00:00Z">2024-01-07</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">35</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChatGPT for Conversational Recommendation: Refining Recommendations by
  Reprompting with Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyle Dylan Spurlock, Cagla Acun, Esin Saka, Olfa Nasraoui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommendation algorithms have been pivotal in handling the overwhelming
volume of online content. However, these algorithms seldom consider direct user
input, resulting in superficial interaction between them. Efforts have been
made to include the user directly in the recommendation process through
conversation, but these systems too have had limited interactivity. Recently,
Large Language Models (LLMs) like ChatGPT have gained popularity due to their
ease of use and their ability to adapt dynamically to various tasks while
responding to feedback. In this paper, we investigate the effectiveness of
ChatGPT as a top-n conversational recommendation system. We build a rigorous
pipeline around ChatGPT to simulate how a user might realistically probe the
model for recommendations: by first instructing and then reprompting with
feedback to refine a set of recommendations. We further explore the effect of
popularity bias in ChatGPT's recommendations, and compare its performance to
baseline models. We find that reprompting ChatGPT with feedback is an effective
strategy to improve recommendation relevancy, and that popularity bias can be
mitigated through prompt engineering.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ InFoBench: Evaluating Instruction Following Ability in Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Xuansheng Wu, Fei Liu, Pengfei Liu, Dong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the Decomposed Requirements Following Ratio (DRFR), a
new metric for evaluating Large Language Models' (LLMs) ability to follow
instructions. Addressing a gap in current methodologies, DRFR breaks down
complex instructions into simpler criteria, facilitating a detailed analysis of
LLMs' compliance with various aspects of tasks. Alongside this metric, we
present InFoBench, a benchmark comprising 500 diverse instructions and 2,250
decomposed questions across multiple constraint categories. Our experiments
compare DRFR with traditional scoring methods and explore annotation sources,
including human experts, crowd-sourced workers, and GPT-4. The findings
demonstrate DRFR's higher reliability and the effectiveness of using GPT-4 as a
cost-efficient annotator. The evaluation of several advanced LLMs using this
framework reveals their strengths and areas needing improvement, particularly
in complex instruction-following. This study contributes a novel metric and
benchmark, offering insights for future LLM development and evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text Classification Based on Knowledge Graphs and Improved Attention
  Mechanism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyu Li, Lu Chen, Chenwei Song, Xinyi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To resolve the semantic ambiguity in texts, we propose a model, which
innovatively combines a knowledge graph with an improved attention mechanism.
An existing knowledge base is utilized to enrich the text with relevant
contextual concepts. The model operates at both character and word levels to
deepen its understanding by integrating the concepts. We first adopt
information gain to select import words. Then an encoder-decoder framework is
used to encode the text along with the related concepts. The local attention
mechanism adjusts the weight of each concept, reducing the influence of
irrelevant or noisy concepts during classification. We improve the calculation
formula for attention scores in the local self-attention mechanism, ensuring
that words with different frequencies of occurrence in the text receive higher
attention scores. Finally, the model employs a Bi-directional Gated Recurrent
Unit (Bi-GRU), which is effective in feature extraction from texts for improved
classification accuracy. Its performance is demonstrated on datasets such as
AGNews, Ohsumed, and TagMyNews, achieving accuracy of 75.1%, 58.7%, and 68.5%
respectively, showing its effectiveness in classifying tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building Efficient and Effective OpenQA Systems for Low-Resource
  Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03590v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03590v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emrah Budur, Rıza Özçelik, Dilara Soylu, Omar Khattab, Tunga Güngör, Christopher Potts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question answering (QA) is the task of answering questions posed in natural
language with free-form natural language answers extracted from a given
passage. In the OpenQA variant, only a question text is given, and the system
must retrieve relevant passages from an unstructured knowledge source and use
them to provide answers, which is the case in the mainstream QA systems on the
Web. QA systems currently are mostly limited to the English language due to the
lack of large-scale labeled QA datasets in non-English languages. In this
paper, we show that effective, low-cost OpenQA systems can be developed for
low-resource languages. The key ingredients are (1) weak supervision using
machine-translated labeled datasets and (2) a relevant unstructured knowledge
source in the target language. Furthermore, we show that only a few hundred
gold assessment examples are needed to reliably evaluate these systems. We
apply our method to Turkish as a challenging case study, since English and
Turkish are typologically very distinct. We present SQuAD-TR, a machine
translation of SQuAD2.0, and we build our OpenQA system by adapting ColBERT-QA
for Turkish. We obtain a performance improvement of 9-34% in the EM score and
13-33% in the F1 score compared to the BM25-based and DPR-based baseline QA
reader models by using two versions of Wikipedia dumps spanning two years. Our
results show that SQuAD-TR makes OpenQA feasible for Turkish, which we hope
encourages researchers to build OpenQA systems in other low-resource languages.
We make all the code, models, and the dataset publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-CUBE: Data Curriculum for Instruction-based Sentence Representation
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingqian Min, Kun Zhou, Dawei Gao, Wayne Xin Zhao, He Hu, Yaliang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, multi-task instruction tuning has been applied into sentence
representation learning, which endows the capability of generating specific
representations with the guidance of task instruction, exhibiting strong
generalization ability on new tasks. However, these methods mostly neglect the
potential interference problems across different tasks and instances, which may
affect the training and convergence of the model. To address it, we propose a
data curriculum method, namely Data-CUBE, that arranges the orders of all the
multi-task data for training, to minimize the interference risks from the two
views. In the task level, we aim to find the optimal task order to minimize the
total cross-task interference risk, which is exactly the traveling salesman
problem, hence we utilize a simulated annealing algorithm to find its solution.
In the instance level, we measure the difficulty of all instances per task,
then divide them into the easy-to-difficult mini-batches for training.
Experiments on MTEB sentence representation evaluation tasks show that our
approach can boost the performance of state-of-the-art methods. Our code and
data are publicly available at the link:
\url{https://github.com/RUCAIBox/Data-CUBE}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, working in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAPTAIN at COLIEE 2023: Efficient Methods for Legal Information
  Retrieval and Entailment Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chau Nguyen, Phuong Nguyen, Thanh Tran, Dat Nguyen, An Trieu, Tin Pham, Anh Dang, Le-Minh Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Competition on Legal Information Extraction/Entailment (COLIEE) is held
annually to encourage advancements in the automatic processing of legal texts.
Processing legal documents is challenging due to the intricate structure and
meaning of legal language. In this paper, we outline our strategies for
tackling Task 2, Task 3, and Task 4 in the COLIEE 2023 competition. Our
approach involved utilizing appropriate state-of-the-art deep learning methods,
designing methods based on domain characteristics observation, and applying
meticulous engineering practices and methodologies to the competition. As a
result, our performance in these tasks has been outstanding, with first places
in Task 2 and Task 3, and promising results in Task 4. Our source code is
available at https://github.com/Nguyen2015/CAPTAIN-COLIEE2023/tree/coliee2023.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is there really a Citation Age Bias in NLP? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoa Nguyen, Steffen Eger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Citations are a key ingredient of scientific research to relate a paper to
others published in the community. Recently, it has been noted that there is a
citation age bias in the Natural Language Processing (NLP) community, one of
the currently fastest growing AI subfields, in that the mean age of the
bibliography of NLP papers has become ever younger in the last few years,
leading to `citation amnesia' in which older knowledge is increasingly
forgotten. In this work, we put such claims into perspective by analyzing the
bibliography of $\sim$300k papers across 15 different scientific fields
submitted to the popular preprint server Arxiv in the time period from 2013 to
2022. We find that all AI subfields (in particular: cs.AI, cs.CL, cs.CV, cs.LG)
have similar trends of citation amnesia, in which the age of the bibliography
has roughly halved in the last 10 years (from above 12 in 2013 to below 7 in
2022), on average. Rather than diagnosing this as a citation age bias in the
NLP community, we believe this pattern is an artefact of the dynamics of these
research fields, in which new knowledge is produced in ever shorter time
intervals.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transfer the linguistic representations from TTS to accent conversion
  with non-parallel data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Chen, Jiakun Pei, Liumeng Xue, Mingyang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accent conversion aims to convert the accent of a source speech to a target
accent, meanwhile preserving the speaker's identity. This paper introduces a
novel non-autoregressive framework for accent conversion that learns
accent-agnostic linguistic representations and employs them to convert the
accent in the source speech. Specifically, the proposed system aligns speech
representations with linguistic representations obtained from Text-to-Speech
(TTS) systems, enabling training of the accent voice conversion model on
non-parallel data. Furthermore, we investigate the effectiveness of a
pretraining strategy on native data and different acoustic features within our
proposed framework. We conduct a comprehensive evaluation using both subjective
and objective metrics to assess the performance of our approach. The evaluation
results highlight the benefits of the pretraining strategy and the
incorporation of richer semantic features, resulting in significantly enhanced
audio quality and intelligibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RoBERTurk: Adjusting RoBERTa for Turkish 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03515v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03515v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nuri Tas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We pretrain RoBERTa on a Turkish corpora using BPE tokenizer. Our model
outperforms BERTurk family models on the BOUN dataset for the POS task while
resulting in underperformance on the IMST dataset for the same task and
achieving competitive scores on the Turkish split of the XTREME dataset for the
NER task - all while being pretrained on smaller data than its competitors. We
release our pretrained model and tokenizer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ROIC-DM: Robust Text Inference and Classification via Diffusion Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shilong Yuan, Wei Yuan, Tieke HE
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While language models have made many milestones in text inference and
classification tasks, they remain susceptible to adversarial attacks that can
lead to unforeseen outcomes. Existing works alleviate this problem by equipping
language models with defense patches. However, these defense strategies often
rely on impractical assumptions or entail substantial sacrifices in model
performance. Consequently, enhancing the resilience of the target model using
such defense mechanisms is a formidable challenge. This paper introduces an
innovative model for robust text inference and classification, built upon
diffusion models (ROIC-DM). Benefiting from its training involving denoising
stages, ROIC-DM inherently exhibits greater robustness compared to conventional
language models. Moreover, ROIC-DM can attain comparable, and in some cases,
superior performance to language models, by effectively incorporating them as
advisory components. Extensive experiments conducted with several strong
textual adversarial attacks on three datasets demonstrate that (1) ROIC-DM
outperforms traditional language models in robustness, even when the latter are
fortified with advanced defense mechanisms; (2) ROIC-DM can achieve comparable
and even better performance than traditional language models by using them as
advisors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>aaai2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token-free LLMs Can Generate Chinese Classical Poetry with More Accurate
  Format 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyue Yu, Lei Zang, Jiaotuan Wang, Chenyi Zhuang, Jinjie Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finetuned large language models (such as ChatGPT and Qwen-chat) can generate
Chinese classical poetry following human's instructions. LLMs perform well in
content, but are usually lacking in format, with occasionally excess or
insufficient number of characters in each line. Since most SOTA LLMs are
token-based, we assume that the format inaccuracy is due to the difficulty of
the "token planning" task, which means that the LLM need to know exactly how
much characters are contained in each token and do length-control planning
based on that knowledge. In this paper, we first confirm our assumption by
showing that existing token-based large language models has limited knowledge
on token-character relationship. We use a spelling bee probing procedure, and
find that Qwen-chat failed in nearly 15% Chinese spelling test. We then show
that a token-based model can be easily tailored into a token-free model (in
terms of Chinese), which can largely solve the format accuracy problem. Our
tailoring procedure removes long-token from vocabulary and keeps only
character-level or byte-level tokens. As part of our contribution, we release
the finetuned token-free model (which is based on Qwen-chat-7B), which can
generate chinese classical poetry following complex instructions like LLMs
(such as story paraphrasing), and also perform well in format. On the test set,
our token-free model achives an format accuracy of 0.96, compared to 0.84 for
token-based counterparts and 0.38 for GPT-4.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EAT: Self-Supervised Pre-Training with Efficient Audio Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxi Chen, Yuzhe Liang, Ziyang Ma, Zhisheng Zheng, Xie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio self-supervised learning (SSL) pre-training, which aims to learn good
representations from unlabeled audio, has made remarkable progress. However,
the extensive computational demands during pre-training pose a significant
barrier to the potential application and optimization of audio SSL models. In
this paper, inspired by the success of data2vec 2.0 in image modality and
Audio-MAE in audio modality, we introduce Efficient Audio Transformer (EAT) to
further improve the effectiveness and efficiency in audio SSL. The proposed EAT
adopts the bootstrap self-supervised training paradigm to the audio domain. A
novel Utterance-Frame Objective (UFO) is designed to enhance the modeling
capability of acoustic events. Furthermore, we reveal that the masking strategy
is critical in audio SSL pre-training, and superior audio representations can
be obtained with large inverse block masks. Experiment results demonstrate that
EAT achieves state-of-the-art (SOTA) performance on a range of audio-related
tasks, including AudioSet (AS-2M, AS-20K), ESC-50, and SPC-2, along with a
significant pre-training speedup up to ~15x compared to existing audio SSL
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PEneo: Unifying Line Extraction, Line Grouping, and Entity Linking for
  End-to-end Document Pair Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zening Lin, Jiapeng Wang, Teng Li, Wenhui Liao, Dayi Huang, Longfei Xiong, Lianwen Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document pair extraction aims to identify key and value entities as well as
their relationships from visually-rich documents. Most existing methods divide
it into two separate tasks: semantic entity recognition (SER) and relation
extraction (RE). However, simply concatenating SER and RE serially can lead to
severe error propagation, and it fails to handle cases like multi-line entities
in real scenarios. To address these issues, this paper introduces a novel
framework, PEneo (Pair Extraction new decoder option), which performs document
pair extraction in a unified pipeline, incorporating three concurrent
sub-tasks: line extraction, line grouping, and entity linking. This approach
alleviates the error accumulation problem and can handle the case of multi-line
entities. Furthermore, to better evaluate the model's performance and to
facilitate future research on pair extraction, we introduce RFUND, a
re-annotated version of the commonly used FUNSD and XFUND datasets, to make
them more accurate and cover realistic situations. Experiments on various
benchmarks demonstrate PEneo's superiority over previous pipelines, boosting
the performance by a large margin (e.g., 19.89%-22.91% F1 score on RFUND-EN)
when combined with various backbones like LiLT and LayoutLMv3, showing its
effectiveness and generality. Codes and the new annotations will be open to the
public.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Maintaining Journalistic Integrity in the Digital Age: A Comprehensive
  NLP Framework for Evaluating Online News Content 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03467v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03467v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ljubisa Bojic, Nikola Prodanovic, Agariadne Dwinggo Samala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid growth of online news platforms has led to an increased need for
reliable methods to evaluate the quality and credibility of news articles. This
paper proposes a comprehensive framework to analyze online news texts using
natural language processing (NLP) techniques, particularly a language model
specifically trained for this purpose, alongside other well-established NLP
methods. The framework incorporates ten journalism standards-objectivity,
balance and fairness, readability and clarity, sensationalism and clickbait,
ethical considerations, public interest and value, source credibility,
relevance and timeliness, factual accuracy, and attribution and transparency-to
assess the quality of news articles. By establishing these standards,
researchers, media organizations, and readers can better evaluate and
understand the content they consume and produce. The proposed method has some
limitations, such as potential difficulty in detecting subtle biases and the
need for continuous updating of the language model to keep pace with evolving
language patterns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03462v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03462v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peitian Zhang, Zheng Liu, Shitao Xiao, Ninglu Shao, Qiwei Ye, Zhicheng Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The utilization of long contexts poses a big challenge for large language
models due to their limited context window length. Although the context window
can be extended through fine-tuning, it will result in a considerable cost at
both training and inference time, and exert an unfavorable impact to the LLM's
original capabilities. In this work, we propose Activation Beacon, which
condenses LLM's raw activations into more compact forms such that it can
perceive a much longer context with a limited context window. Activation Beacon
is introduced as a plug-and-play module for the LLM. It fully preserves the
LLM's original capability on short contexts while extending the new capability
on processing longer contexts. Besides, it works with short sliding windows to
process the long context, which achieves a competitive memory and time
efficiency in both training and inference. Activation Beacon is learned by the
auto-regression task conditioned on a mixture of beacons with diversified
condensing ratios. Thanks to such a treatment, it can be efficiently trained
purely with short-sequence data in just 10K steps, which consumes less than 9
hours on a single 8xA800 GPU machine. The experimental studies show that
Activation Beacon is able to extend Llama-2-7B's context length by $\times100$
times (from 4K to 400K), meanwhile achieving a superior result on both
long-context generation and understanding tasks. Our model and code will be
available at the BGE repository.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Leveraging Large Language Models for Enhancing Entity Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03426v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03426v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huahang Li, Longyu Feng, Shuangyin Li, Fei Hao, Chen Jason Zhang, Yuanfeng Song, Lei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity resolution, the task of identifying and consolidating records that
pertain to the same real-world entity, plays a pivotal role in various sectors
such as e-commerce, healthcare, and law enforcement. The emergence of Large
Language Models (LLMs) like GPT-4 has introduced a new dimension to this task,
leveraging their advanced linguistic capabilities. This paper explores the
potential of LLMs in the entity resolution process, shedding light on both
their advantages and the computational complexities associated with large-scale
matching. We introduce strategies for the efficient utilization of LLMs,
including the selection of an optimal set of matching questions, namely MQsSP,
which is proved to be a NP-hard problem. Our approach optimally chooses the
most effective matching questions while keep consumption limited to your budget
. Additionally, we propose a method to adjust the distribution of possible
partitions after receiving responses from LLMs, with the goal of reducing the
uncertainty of entity resolution. We evaluate the effectiveness of our approach
using entropy as a metric, and our experimental results demonstrate the
efficiency and effectiveness of our proposed methods, offering promising
prospects for real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages,6 figures, ICDE 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GRAM: Global Reasoning for Multi-Page VQA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tsachi Blau, Sharon Fogel, Roi Ronen, Alona Golts, Roy Ganz, Elad Ben Avraham, Aviad Aberdam, Shahar Tsiper, Ron Litman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing use of transformer-based large language models brings forward
the challenge of processing long sequences. In document visual question
answering (DocVQA), leading methods focus on the single-page setting, while
documents can span hundreds of pages. We present GRAM, a method that seamlessly
extends pre-trained single-page models to the multi-page setting, without
requiring computationally-heavy pretraining. To do so, we leverage a
single-page encoder for local page-level understanding, and enhance it with
document-level designated layers and learnable tokens, facilitating the flow of
information across pages for global reasoning. To enforce our model to utilize
the newly introduced document-level tokens, we propose a tailored bias
adaptation method. For additional computational savings during decoding, we
introduce an optional compression stage using our C-Former model, which reduces
the encoded sequence length, thereby allowing a tradeoff between quality and
latency. Extensive experiments showcase GRAM's state-of-the-art performance on
the benchmarks for multi-page DocVQA, demonstrating the effectiveness of our
approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Escalation Risks from Language Models in Military and Diplomatic
  Decision-Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan-Pablo Rivera, Gabriel Mukobi, Anka Reuel, Max Lamparth, Chandler Smith, Jacquelyn Schneider
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Governments are increasingly considering integrating autonomous AI agents in
high-stakes military and foreign-policy decision-making, especially with the
emergence of advanced generative AI models like GPT-4. Our work aims to
scrutinize the behavior of multiple AI agents in simulated wargames,
specifically focusing on their predilection to take escalatory actions that may
exacerbate multilateral conflicts. Drawing on political science and
international relations literature about escalation dynamics, we design a novel
wargame simulation and scoring framework to assess the escalation risks of
actions taken by these agents in different scenarios. Contrary to prior
studies, our research provides both qualitative and quantitative insights and
focuses on large language models (LLMs). We find that all five studied
off-the-shelf LLMs show forms of escalation and difficult-to-predict escalation
patterns. We observe that models tend to develop arms-race dynamics, leading to
greater conflict, and in rare cases, even to the deployment of nuclear weapons.
Qualitatively, we also collect the models' reported reasonings for chosen
actions and observe worrying justifications based on deterrence and
first-strike tactics. Given the high stakes of military and foreign-policy
contexts, we recommend further examination and cautious consideration before
deploying autonomous language model agents for strategic military or diplomatic
decision-making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages body, 57 pages appendix, 46 figures, 11 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Empirical Study of Large Language Models as Automated Essay Scoring
  Tools in English Composition__Taking TOEFL Independent Writing Task for
  Example 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03401v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03401v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Xia, Shaoguang Mao, Chanjing Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have demonstrated exceptional capabilities in tasks
involving natural language generation, reasoning, and comprehension. This study
aims to construct prompts and comments grounded in the diverse scoring criteria
delineated within the official TOEFL guide. The primary objective is to assess
the capabilities and constraints of ChatGPT, a prominent representative of
large language models, within the context of automated essay scoring. The
prevailing methodologies for automated essay scoring involve the utilization of
deep neural networks, statistical machine learning techniques, and fine-tuning
pre-trained models. However, these techniques face challenges when applied to
different contexts or subjects, primarily due to their substantial data
requirements and limited adaptability to small sample sizes. In contrast, this
study employs ChatGPT to conduct an automated evaluation of English essays,
even with a small sample size, employing an experimental approach. The
empirical findings indicate that ChatGPT can provide operational functionality
for automated essay scoring, although the results exhibit a regression effect.
It is imperative to underscore that the effective design and implementation of
ChatGPT prompts necessitate a profound domain expertise and technical
proficiency, as these prompts are subject to specific threshold criteria.
Keywords: ChatGPT, Automated Essay Scoring, Prompt Learning, TOEFL Independent
Writing Task
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMs for Robotic Object Disambiguation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Connie Jiang, Yiqing Xu, David Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advantages of pre-trained large language models (LLMs) are apparent in a
variety of language processing tasks. But can a language model's knowledge be
further harnessed to effectively disambiguate objects and navigate
decision-making challenges within the realm of robotics? Our study reveals the
LLM's aptitude for solving complex decision making challenges that are often
previously modeled by Partially Observable Markov Decision Processes (POMDPs).
A pivotal focus of our research is the object disambiguation capability of
LLMs. We detail the integration of an LLM into a tabletop environment
disambiguation task, a decision making problem where the robot's task is to
discern and retrieve a user's desired object from an arbitrarily large and
complex cluster of objects. Despite multiple query attempts with zero-shot
prompt engineering (details can be found in the Appendix), the LLM struggled to
inquire about features not explicitly provided in the scene description. In
response, we have developed a few-shot prompt engineering system to improve the
LLM's ability to pose disambiguating queries. The result is a model capable of
both using given features when they are available and inferring new relevant
features when necessary, to successfully generate and navigate down a precise
decision tree to the correct object--even when faced with identical options.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Grimoire is All You Need for Enhancing Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ding Chen, Shichao Song, Qingchen Yu, Zhiyu Li, Wenjin Wang, Feiyu Xiong, Bo Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) is one of the key methods for enhancing the
performance of large language models on specific tasks by providing a set of
few-shot question and answer examples. However, the ICL capability of different
types of models shows significant variation due to factors such as model
architecture, volume of learning data, and the size of parameters. Generally,
the larger the model's parameter size and the more extensive the learning data,
the stronger its ICL capability. In this paper, we propose a method SLEICL
(Strong LLM Enhanced ICL) that involves learning from examples using strong
language models and then summarizing and transferring these learned skills to
weak language models for inference and application. This ensures the stability
and effectiveness of ICL. Compared to directly enabling weak language models to
learn from prompt examples, SLEICL reduces the difficulty of ICL for these
models. Our experiments, conducted on up to eight datasets with five language
models, demonstrate that weak language models achieve consistent improvement
over their own zero-shot or few-shot capabilities using the SLEICL method. Some
weak language models even surpass the performance of GPT4-1106-preview
(zero-shot) with the aid of SLEICL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Investigation of Large Language Models for Real-World Hate Speech
  Detection <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03346v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03346v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keyan Guo, Alexander Hu, Jaden Mu, Ziheng Shi, Ziming Zhao, Nishant Vishwamitra, Hongxin Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hate speech has emerged as a major problem plaguing our social spaces today.
While there have been significant efforts to address this problem, existing
methods are still significantly limited in effectively detecting hate speech
online. A major limitation of existing methods is that hate speech detection is
a highly contextual problem, and these methods cannot fully capture the context
of hate speech to make accurate predictions. Recently, large language models
(LLMs) have demonstrated state-of-the-art performance in several natural
language tasks. LLMs have undergone extensive training using vast amounts of
natural language data, enabling them to grasp intricate contextual details.
Hence, they could be used as knowledge bases for context-aware hate speech
detection. However, a fundamental problem with using LLMs to detect hate speech
is that there are no studies on effectively prompting LLMs for context-aware
hate speech detection. In this study, we conduct a large-scale study of hate
speech detection, employing five established hate speech datasets. We discover
that LLMs not only match but often surpass the performance of current benchmark
machine learning models in identifying hate speech. By proposing four diverse
prompting strategies that optimize the use of LLMs in detecting hate speech.
Our study reveals that a meticulously crafted reasoning prompt can effectively
capture the context of hate speech by fully utilizing the knowledge base in
LLMs, significantly outperforming existing techniques. Furthermore, although
LLMs can provide a rich knowledge base for the contextual detection of hate
speech, suitable prompting strategies play a crucial role in effectively
leveraging this knowledge base for efficient detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication on 22nd International Conference of Machine
  Learning and Applications, ICMLA 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pragmatic Evaluation of Clarifying Questions with Fact-Level Masking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.11571v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.11571v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Toles, Yukun Huang, Zhou Yu, Luis Gravano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to derive useful information by asking clarifying questions (ACQ)
is an important element of real life collaboration on reasoning tasks, such as
question answering (QA). Existing natural language ACQ challenges, however,
evaluate generations based on word overlap rather than the value of the
information itself. Word overlap is often an inappropriate metric for question
generation since many different questions could be useful in a given situation,
and a single question can be phrased many different ways. Instead, we propose
evaluating questions pragmatically based on the value of the information they
retrieve. Here we present a definition and framework for natural language
pragmatic asking of clarifying questions (PACQ), the problem of generating
questions that result in answers useful for a reasoning task. We also present
fact-level masking (FLM), a procedure for converting natural language datasets
into self-supervised PACQ datasets by omitting particular critical facts.
Finally, we generate a PACQ dataset from the HotpotQA dataset using FLM and
evaluate several zero-shot language models on it. Our experiments show that
current zero-shot models struggle to ask questions that retrieve useful
information, as compared to human annotators. These results demonstrate an
opportunity to use FLM datasets and the PACQ framework to objectively evaluate
and improve question generation and other language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive Survey on Instruction Following 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10475v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10475v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renze Lou, Kai Zhang, Wenpeng Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task semantics can be expressed by a set of input-output examples or a piece
of textual instruction. Conventional machine learning approaches for natural
language processing (NLP) mainly rely on the availability of large-scale sets
of task-specific examples. Two issues arise: first, collecting task-specific
labeled examples does not apply to scenarios where tasks may be too complicated
or costly to annotate, or the system is required to handle a new task
immediately; second, this is not user-friendly since end-users are probably
more willing to provide task description rather than a set of examples before
using the system. Therefore, the community is paying increasing interest in a
new supervision-seeking paradigm for NLP: learning to follow task instructions,
i.e., instruction following. Despite its impressive progress, there are some
common issues that the community struggles with. This survey paper tries to
summarize and provide insights to the current research on instruction
following, particularly, by answering the following questions: (i) What is task
instruction, and what instruction types exist? (ii) How to model instructions?
(iii) What are popular instruction following datasets and evaluation metrics?
(iv) What factors influence and explain the instructions' performance? (v) What
challenges remain in instruction following? To our knowledge, this is the first
comprehensive survey about instruction following.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. The paper list is available at
  https://github.com/RenzeLou/awesome-instruction-learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automated Evaluation of Classroom Instructional Support with LLMs and
  BoWs: Connecting Global Predictions to Specific Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01132v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01132v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Whitehill, Jennifer LoCasale-Crouch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the aim to provide teachers with more specific, frequent, and actionable
feedback about their teaching, we explore how Large Language Models (LLMs) can
be used to estimate ``Instructional Support'' domain scores of the CLassroom
Assessment Scoring System (CLASS), a widely used observation protocol. We
design a machine learning architecture that uses either zero-shot prompting of
Meta's Llama2, and/or a classic Bag of Words (BoW) model, to classify
individual utterances of teachers' speech (transcribed automatically using
OpenAI's Whisper) for the presence of Instructional Support. Then, these
utterance-level judgments are aggregated over an entire 15-min observation
session to estimate a global CLASS score. Experiments on two CLASS-coded
datasets of toddler and pre-kindergarten classrooms indicate that (1) automatic
CLASS Instructional Support estimation accuracy using the proposed method
(Pearson $R$ up to $0.47$) approaches human inter-rater reliability (up to
$R=0.55$); (2) LLMs yield slightly greater accuracy than BoW for this task,
though the best models often combined features extracted from both LLM and BoW;
and (3) for classifying individual utterances, there is still room for
improvement of automated methods compared to human-level judgments. Finally,
(4) we illustrate how the model's outputs can be visualized at the utterance
level to provide teachers with explainable feedback on which utterances were
most positively or negatively correlated with specific CLASS dimensions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Building a Non-native Speech Corpus Featuring Chinese-English Bilingual
  Children: Compilation and Rationale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.00446v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.00446v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hiuchung Hung, Andreas Maier, Thorsten Piske
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a non-native speech corpus consisting of narratives
from fifty 5- to 6-year-old Chinese-English children. Transcripts totaling 6.5
hours of children taking a narrative comprehension test in English (L2) are
presented, along with human-rated scores and annotations of grammatical and
pronunciation errors. The children also completed the parallel MAIN tests in
Chinese (L1) for reference purposes. For all tests we recorded audio and video
with our innovative self-developed remote collection methods. The video
recordings serve to mitigate the challenge of low intelligibility in L2
narratives produced by young children during the transcription process. This
corpus offers valuable resources for second language teaching and has the
potential to enhance the overall performance of automatic speech recognition
(ASR).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhance Multi-domain Sentiment Analysis of Review Texts through
  Prompting Strategies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.02045v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.02045v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yajing Wang, Zongwei Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have made significant strides in both scientific
research and practical applications. Existing studies have demonstrated the
state-of-the-art (SOTA) performance of LLMs in various natural language
processing tasks. However, the question of how to further enhance LLMs'
performance in specific task using prompting strategies remains a pivotal
concern. This paper explores the enhancement of LLMs' performance in sentiment
analysis through the application of prompting strategies. We formulate the
process of prompting for sentiment analysis tasks and introduce two novel
strategies tailored for sentiment analysis: RolePlaying (RP) prompting and
Chain-of-thought (CoT) prompting. Specifically, we also propose the RP-CoT
prompting strategy which is a combination of RP prompting and CoT prompting. We
conduct comparative experiments on three distinct domain datasets to evaluate
the effectiveness of the proposed sentiment analysis strategies. The results
demonstrate that the adoption of the proposed prompting strategies leads to a
increasing enhancement in sentiment analysis accuracy. Further, the CoT
prompting strategy exhibits a notable impact on implicit sentiment analysis,
with the RP-CoT prompting strategy delivering the most superior performance
among all strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating Semi-Supervised Learning Algorithms in Text <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01843v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01843v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Himmet Toprak Kesgin, Mehmet Fatih Amasyali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using large training datasets enhances the generalization capabilities of
neural networks. Semi-supervised learning (SSL) is useful when there are few
labeled data and a lot of unlabeled data. SSL methods that use data
augmentation are most successful for image datasets. In contrast, texts do not
have consistent augmentation methods as images. Consequently, methods that use
augmentation are not as effective in text data as they are in image data. In
this study, we compared SSL algorithms that do not require augmentation; these
are self-training, co-training, tri-training, and tri-training with
disagreement. In the experiments, we used 4 different text datasets for
different tasks. We examined the algorithms from a variety of perspectives by
asking experiment questions and suggested several improvements. Among the
algorithms, tri-training with disagreement showed the closest performance to
the Oracle; however, performance gap shows that new semi-supervised algorithms
or improvements in existing methods are needed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Innovations in Intelligent Systems and Applications Conference (ASYU)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ "Paraphrasing The Original Text" Makes High Accuracy Long-Context QA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11193v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11193v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijiong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most open-source generative language models currently have a context window
of no more than 4k, limiting their ability when facing long text. Even models
with longer context windows cannot guarantee satisfactory accuracy on
long-context problems. To tackle this issue, we explore from the perspective of
training data and theoretically demonstrate that improving the capability to
handle long contexts requires "effective" rather than simply "long" data. Based
on this insight, we propose using the "original text paraphrasing" task and
successfully extend the context window of existing models to 32k through a
low-cost and effective method. Our fine-tuned model achieves state-of-the-art
accuracy in multi-document-QA among models of comparable scale. The model and
training data have been made available on
HuggingFace(https://huggingface.co/yuyijiong/Qwen-14b-chat-yarn-32k) and
WiseModel(https://wisemodel.cn/models/yuyijiong/Qwen-14b-chat-yarn-32k).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Chinese version of this paper can be downloaded from
  (https://cloud.tsinghua.edu.cn/d/5894ec4442e54a6aac96/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RJUA-QA: A Comprehensive QA <span class="highlight-title">Dataset</span> for Urology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.09785v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.09785v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiwei Lyu, Chenfei Chi, Hongbo Cai, Lei Shi, Xiaoyan Yang, Lei Liu, Xiang Chen, Deng Zhao, Zhiqiang Zhang, Xianguo Lyu, Ming Zhang, Fangzhou Li, Xiaowei Ma, Yue Shen, Jinjie Gu, Wei Xue, Yiran Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce RJUA-QA, a novel medical dataset for question answering (QA) and
reasoning with clinical evidence, contributing to bridge the gap between
general large language models (LLMs) and medical-specific LLM applications.
RJUA-QA is derived from realistic clinical scenarios and aims to facilitate
LLMs in generating reliable diagnostic and advice. The dataset contains 2,132
curated Question-Context-Answer pairs, corresponding about 25,000 diagnostic
records and clinical cases. The dataset covers 67 common urological disease
categories, where the disease coverage exceeds 97.6\% of the population seeking
medical services in urology. Each data instance in RJUA-QA comprises: (1) a
question mirroring real patient to inquiry about clinical symptoms and medical
conditions, (2) a context including comprehensive expert knowledge, serving as
a reference for medical examination and diagnosis, (3) a doctor response
offering the diagnostic conclusion and suggested examination guidance, (4) a
diagnosed clinical disease as the recommended diagnostic outcome, and (5)
clinical advice providing recommendations for medical examination. RJUA-QA is
the first medical QA dataset for clinical reasoning over the patient inquiries,
where expert-level knowledge and experience are required for yielding
diagnostic conclusions and medical examination advice. A comprehensive
evaluation is conducted to evaluate the performance of both medical-specific
and general LLMs on the RJUA-QA dataset. Our data is are publicly available at
\url{https://github.com/alipay/RJU_Ant_QA}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>An initial version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Beginner to Expert: Modeling Medical Knowledge into General LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01040v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01040v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Li, Xiaoyan Yang, Haowen Wang, Qin Wang, Lei Liu, Junjie Wang, Yang Zhang, Mingyuan Chu, Sen Hu, Yicheng Chen, Yue Shen, Cong Fan, Wangshu Zhang, Teng Xu, Jinjie Gu, Jing Zheng, Guannan Zhang Ant Group
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, large language model (LLM) based artificial intelligence (AI)
systems have demonstrated remarkable capabilities in natural language
understanding and generation. However, these models face a significant
challenge when it comes to sensitive applications, such as reasoning over
medical knowledge and answering medical questions in a physician-like manner.
Prior studies attempted to overcome this challenge by increasing the model size
(>100B) to learn more general medical knowledge, while there is still room for
improvement in LLMs with smaller-scale model sizes (<100B). In this work, we
start from a pre-trained general LLM model (AntGLM-10B) and fine-tune it from a
medical beginner towards a medical expert (called AntGLM-Med-10B), which
leverages a 3-stage optimization procedure, i.e., general medical knowledge
injection, medical domain instruction tuning, and specific medical task
adaptation. Our contributions are threefold: (1) We specifically investigate
how to adapt a pre-trained general LLM in medical domain, especially for a
specific medical task. (2) We collect and construct large-scale medical
datasets for each stage of the optimization process. These datasets encompass
various data types and tasks, such as question-answering, medical reasoning,
multi-choice questions, and medical conversations. (3) Specifically for
multi-choice questions in the medical domain, we propose a novel
Verification-of-Choice approach for prompting engineering, which significantly
enhances the reasoning ability of LLMs. Remarkably, by combining the above
approaches, our AntGLM-Med-10B model can outperform the most of LLMs on
PubMedQA, including both general and medical LLMs, even when these LLMs have
larger model size.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Developed by Ant Group for PubMedQA leaderboard</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Token-Modification Adversarial Attacks for Natural Language Processing:
  A Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.00676v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.00676v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Roth, Yansong Gao, Alsharif Abuadbba, Surya Nepal, Wei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many adversarial attacks target natural language processing systems, most of
which succeed through modifying the individual tokens of a document. Despite
the apparent uniqueness of each of these attacks, fundamentally they are simply
a distinct configuration of four components: a goal function, allowable
transformations, a search method, and constraints. In this survey, we
systematically present the different components used throughout the literature,
using an attack-independent framework which allows for easy comparison and
categorisation of components. Our work aims to serve as a comprehensive guide
for newcomers to the field and to spark targeted research into refining the
individual attack components.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Version 3: edited and expanded</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GOAT-Bench: Safety Insights to Large Multimodal Models through
  Meme-Based Social Abuse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01523v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01523v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongzhan Lin, Ziyang Luo, Bo Wang, Ruichao Yang, Jing Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exponential growth of social media has profoundly transformed how
information is created, disseminated, and absorbed, exceeding any precedent in
the digital age. Regrettably, this explosion has also spawned a significant
increase in the online abuse of memes. Evaluating the negative impact of memes
is notably challenging, owing to their often subtle and implicit meanings,
which are not directly conveyed through the overt text and imagery. In light of
this, large multimodal models (LMMs) have emerged as a focal point of interest
due to their remarkable capabilities in handling diverse multimodal tasks. In
response to this development, our paper aims to thoroughly examine the capacity
of various LMMs (e.g. GPT-4V) to discern and respond to the nuanced aspects of
social abuse manifested in memes. We introduce the comprehensive meme
benchmark, GOAT-Bench, comprising over 6K varied memes encapsulating themes
such as implicit hate speech, sexism, and cyberbullying, etc. Utilizing
GOAT-Bench, we delve into the ability of LMMs to accurately assess hatefulness,
misogyny, offensiveness, sarcasm, and harmful content. Our extensive
experiments across a range of LMMs reveal that current models still exhibit a
deficiency in safety awareness, showing insensitivity to various forms of
implicit abuse. We posit that this shortfall represents a critical impediment
to the realization of safe artificial intelligence. The GOAT-Bench and
accompanying resources are publicly accessible at https://goatlmm.github.io/,
contributing to ongoing research in this vital field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WavMark: Watermarking for Audio Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12770v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12770v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyu Chen, Yu Wu, Shujie Liu, Tao Liu, Xiaoyong Du, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent breakthroughs in zero-shot voice synthesis have enabled imitating a
speaker's voice using just a few seconds of recording while maintaining a high
level of realism. Alongside its potential benefits, this powerful technology
introduces notable risks, including voice fraud and speaker impersonation.
Unlike the conventional approach of solely relying on passive methods for
detecting synthetic data, watermarking presents a proactive and robust defence
mechanism against these looming risks. This paper introduces an innovative
audio watermarking framework that encodes up to 32 bits of watermark within a
mere 1-second audio snippet. The watermark is imperceptible to human senses and
exhibits strong resilience against various attacks. It can serve as an
effective identifier for synthesized voices and holds potential for broader
applications in audio copyright protection. Moreover, this framework boasts
high flexibility, allowing for the combination of multiple watermark segments
to achieve heightened robustness and expanded capacity. Utilizing 10 to
20-second audio as the host, our approach demonstrates an average Bit Error
Rate (BER) of 0.48\% across ten common attacks, a remarkable reduction of over
2800\% in BER compared to the state-of-the-art watermarking tool. See
https://aka.ms/wavmark for demos of our work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Visual Grounding by Encouraging Consistent Gradient-based
  Explanations <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.15462v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.15462v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyan Yang, Kushal Kafle, Franck Dernoncourt, Vicente Ordonez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a margin-based loss for tuning joint vision-language models so
that their gradient-based explanations are consistent with region-level
annotations provided by humans for relatively smaller grounding datasets. We
refer to this objective as Attention Mask Consistency (AMC) and demonstrate
that it produces superior visual grounding results than previous methods that
rely on using vision-language models to score the outputs of object detectors.
Particularly, a model trained with AMC on top of standard vision-language
modeling objectives obtains a state-of-the-art accuracy of 86.49% in the
Flickr30k visual grounding benchmark, an absolute improvement of 5.38% when
compared to the best previous model trained under the same level of
supervision. Our approach also performs exceedingly well on established
benchmarks for referring expression comprehension where it obtains 80.34%
accuracy in the easy test of RefCOCO+, and 64.55% in the difficult split. AMC
is effective, easy to implement, and is general as it can be adopted by any
vision-language model, and can use any type of region annotations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023. Fix ReferIt results. Code:
  https://github.com/uvavision/AMC-grounding Project Webpage:
  https://vislang.ai/amc</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">37</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Amirkabir campus <span class="highlight-title">dataset</span>: Real-world challenges and scenarios of Visual
  Inertial Odometry (VIO) for visually impaired people 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03604v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03604v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Samadzadeh, Mohammad Hassan Mojab, Heydar Soudani, Seyed Hesamoddin Mireshghollah, Ahmad Nickabadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Inertial Odometry (VIO) algorithms estimate the accurate camera
trajectory by using camera and Inertial Measurement Unit (IMU) sensors. The
applications of VIO span a diverse range, including augmented reality and
indoor navigation. VIO algorithms hold the potential to facilitate navigation
for visually impaired individuals in both indoor and outdoor settings.
Nevertheless, state-of-the-art VIO algorithms encounter substantial challenges
in dynamic environments, particularly in densely populated corridors. Existing
VIO datasets, e.g., ADVIO, typically fail to effectively exploit these
challenges. In this paper, we introduce the Amirkabir campus dataset (AUT-VI)
to address the mentioned problem and improve the navigation systems. AUT-VI is
a novel and super-challenging dataset with 126 diverse sequences in 17
different locations. This dataset contains dynamic objects, challenging
loop-closure/map-reuse, different lighting conditions, reflections, and sudden
camera movements to cover all extreme navigation scenarios. Moreover, in
support of ongoing development efforts, we have released the Android
application for data capture to the public. This allows fellow researchers to
easily capture their customized VIO dataset variations. In addition, we
evaluate state-of-the-art Visual Inertial Odometry (VIO) and Visual Odometry
(VO) methods on our dataset, emphasizing the essential need for this
challenging dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Big Data and Deep Learning in Smart Cities: A Comprehensive <span class="highlight-title">Dataset</span> for
  AI-Driven Traffic Accident Detection and Computer Vision Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Adewopo, Nelly Elsayed, Zag Elsayed, Murat Ozer, Constantinos Zekios, Ahmed Abdelgawad, Magdy Bayoumi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the dynamic urban landscape, where the interplay of vehicles and
pedestrians defines the rhythm of life, integrating advanced technology for
safety and efficiency is increasingly crucial. This study delves into the
application of cutting-edge technological methods in smart cities, focusing on
enhancing public safety through improved traffic accident detection. Action
recognition plays a pivotal role in interpreting visual data and tracking
object motion such as human pose estimation in video sequences. The challenges
of action recognition include variability in rapid actions, limited dataset,
and environmental factors such as (Weather, Illumination, and Occlusions). In
this paper, we present a novel comprehensive dataset for traffic accident
detection. This datasets is specifically designed to bolster computer vision
and action recognition systems in predicting and detecting road traffic
accidents. We integrated datasets from wide variety of data sources, road
networks, weather conditions, and regions across the globe. This approach is
underpinned by empirical studies, aiming to contribute to the discourse on how
technology can enhance the quality of life in densely populated areas. This
research aims to bridge existing research gaps by introducing benchmark
datasets that leverage state-of-the-art algorithms tailored for traffic
accident detection in smart cities. These dataset is expected to advance
academic research and also enhance real-time accident detection applications,
contributing significantly to the evolution of smart urban environments. Our
study marks a pivotal step towards safer, more efficient smart cities,
harnessing the power of AI and machine learning to transform urban living.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Invisible Reflections: Leveraging Infrared Laser Reflections to Target
  Traffic Sign Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takami Sato, Sri Hrushikesh Varma Bhupathiraju, Michael Clifford, Takeshi Sugawara, Qi Alfred Chen, Sara Rampazzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  All vehicles must follow the rules that govern traffic behavior, regardless
of whether the vehicles are human-driven or Connected Autonomous Vehicles
(CAVs). Road signs indicate locally active rules, such as speed limits and
requirements to yield or stop. Recent research has demonstrated attacks, such
as adding stickers or projected colored patches to signs, that cause CAV
misinterpretation, resulting in potential safety issues. Humans can see and
potentially defend against these attacks. But humans can not detect what they
can not observe. We have developed an effective physical-world attack that
leverages the sensitivity of filterless image sensors and the properties of
Infrared Laser Reflections (ILRs), which are invisible to humans. The attack is
designed to affect CAV cameras and perception, undermining traffic sign
recognition by inducing misclassification. In this work, we formulate the
threat model and requirements for an ILR-based traffic sign perception attack
to succeed. We evaluate the effectiveness of the ILR attack with real-world
experiments against two major traffic sign recognition architectures on four
IR-sensitive cameras. Our black-box optimization methodology allows the attack
to achieve up to a 100% attack success rate in indoor, static scenarios and a
>80.5% attack success rate in our outdoor, moving vehicle scenarios. We find
the latest state-of-the-art certifiable defense is ineffective against ILR
attacks as it mis-certifies >33.5% of cases. To address this, we propose a
detection strategy based on the physical properties of IR laser reflections
which can detect 96% of ILR attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors are co-first. Accepted to NDSS '24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Involution Fused ConvNet for Classifying Eye-Tracking Patterns of
  Children with Autism Spectrum Disorder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03575v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03575v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md. Farhadul Islam, Meem Arafat Manab, Joyanta Jyoti Mondal, Sarah Zabeen, Fardin Bin Rahman, Md. Zahidul Hasan, Farig Sadeque, Jannatun Noor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autism Spectrum Disorder (ASD) is a complicated neurological condition which
is challenging to diagnose. Numerous studies demonstrate that children
diagnosed with autism struggle with maintaining attention spans and have less
focused vision. The eye-tracking technology has drawn special attention in the
context of ASD since anomalies in gaze have long been acknowledged as a
defining feature of autism in general. Deep Learning (DL) approaches coupled
with eye-tracking sensors are exploiting additional capabilities to advance the
diagnostic and its applications. By learning intricate nonlinear input-output
relations, DL can accurately recognize the various gaze and eye-tracking
patterns and adjust to the data. Convolutions alone are insufficient to capture
the important spatial information in gaze patterns or eye tracking. The dynamic
kernel-based process known as involutions can improve the efficiency of
classifying gaze patterns or eye tracking data. In this paper, we utilise two
different image-processing operations to see how these processes learn
eye-tracking patterns. Since these patterns are primarily based on spatial
information, we use involution with convolution making it a hybrid, which adds
location-specific capability to a deep learning model. Our proposed model is
implemented in a simple yet effective approach, which makes it easier for
applying in real life. We investigate the reasons why our approach works well
for classifying eye-tracking patterns. For comparative analysis, we experiment
with two separate datasets as well as a combined version of both. The results
show that IC with three involution layers outperforms the previous approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 13 figures, Submitted to Engineering Applications of
  Artificial Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SeTformer is What You Need for Vision and Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03540v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03540v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pourya Shamsolmoali, Masoumeh Zareapoor, Eric Granger, Michael Felsberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The dot product self-attention (DPSA) is a fundamental component of
transformers. However, scaling them to long sequences, like documents or
high-resolution images, becomes prohibitively expensive due to quadratic time
and memory complexities arising from the softmax operation. Kernel methods are
employed to simplify computations by approximating softmax but often lead to
performance drops compared to softmax attention. We propose SeTformer, a novel
transformer, where DPSA is purely replaced by Self-optimal Transport (SeT) for
achieving better performance and computational efficiency. SeT is based on two
essential softmax properties: maintaining a non-negative attention matrix and
using a nonlinear reweighting mechanism to emphasize important tokens in input
sequences. By introducing a kernel cost function for optimal transport,
SeTformer effectively satisfies these properties. In particular, with small and
basesized models, SeTformer achieves impressive top-1 accuracies of 84.7% and
86.2% on ImageNet-1K. In object detection, SeTformer-base outperforms the
FocalNet counterpart by +2.2 mAP, using 38% fewer parameters and 29% fewer
FLOPs. In semantic segmentation, our base-size model surpasses NAT by +3.5 mIoU
with 33% fewer parameters. SeTformer also achieves state-of-the-art results in
language modeling on the GLUE benchmark. These findings highlight SeTformer's
applicability in vision and language tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text-Driven Traffic Anomaly Detection with Temporal High-Frequency
  Modeling in Driving Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongqin Liang, Yuanman Li, Jiantao Zhou, Xia Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traffic anomaly detection (TAD) in driving videos is critical for ensuring
the safety of autonomous driving and advanced driver assistance systems.
Previous single-stage TAD methods primarily rely on frame prediction, making
them vulnerable to interference from dynamic backgrounds induced by the rapid
movement of the dashboard camera. While two-stage TAD methods appear to be a
natural solution to mitigate such interference by pre-extracting
background-independent features (such as bounding boxes and optical flow) using
perceptual algorithms, they are susceptible to the performance of first-stage
perceptual algorithms and may result in error propagation. In this paper, we
introduce TTHF, a novel single-stage method aligning video clips with text
prompts, offering a new perspective on traffic anomaly detection. Unlike
previous approaches, the supervised signal of our method is derived from
languages rather than orthogonal one-hot vectors, providing a more
comprehensive representation. Further, concerning visual representation, we
propose to model the high frequency of driving videos in the temporal domain.
This modeling captures the dynamic changes of driving scenes, enhances the
perception of driving behavior, and significantly improves the detection of
traffic anomalies. In addition, to better perceive various types of traffic
anomalies, we carefully design an attentive anomaly focusing mechanism that
visually and linguistically guides the model to adaptively focus on the visual
context of interest, thereby facilitating the detection of traffic anomalies.
It is shown that our proposed TTHF achieves promising performance,
outperforming state-of-the-art competitors by +5.4% AUC on the DoTA dataset and
achieving high generalization on the DADA dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Re:Draw -- Context Aware Translation as a Controllable Method for
  Artistic Production 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joao Liborio Cardoso, Francesco Banterle, Paolo Cignoni, Michael Wimmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce context-aware translation, a novel method that combines the
benefits of inpainting and image-to-image translation, respecting
simultaneously the original input and contextual relevance -- where existing
methods fall short. By doing so, our method opens new avenues for the
controllable use of AI within artistic creation, from animation to digital art.
  As an use case, we apply our method to redraw any hand-drawn animated
character eyes based on any design specifications - eyes serve as a focal point
that captures viewer attention and conveys a range of emotions, however, the
labor-intensive nature of traditional animation often leads to compromises in
the complexity and consistency of eye design. Furthermore, we remove the need
for production data for training and introduce a new character recognition
method that surpasses existing work by not requiring fine-tuning to specific
productions. This proposed use case could help maintain consistency throughout
production and unlock bolder and more detailed design choices without the
production cost drawbacks. A user study shows context-aware translation is
preferred over existing work 95.16% of the time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Segment Anything Model for Medical Image Segmentation: Current
  Applications and Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichi Zhang, Zhenrong Shen, Rushi Jiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the inherent flexibility of prompting, foundation models have emerged
as the predominant force in the fields of natural language processing and
computer vision. The recent introduction of the Segment Anything Model (SAM)
signifies a noteworthy expansion of the prompt-driven paradigm into the domain
of image segmentation, thereby introducing a plethora of previously unexplored
capabilities. However, the viability of its application to medical image
segmentation remains uncertain, given the substantial distinctions between
natural and medical images. In this work, we provide a comprehensive overview
of recent endeavors aimed at extending the efficacy of SAM to medical image
segmentation tasks, encompassing both empirical benchmarking and methodological
adaptations. Additionally, we explore potential avenues for future research
directions in SAM's role within medical image segmentation. While direct
application of SAM to medical image segmentation does not yield satisfactory
performance on multi-modal and multi-target medical datasets so far, numerous
insights gleaned from these efforts serve as valuable guidance for shaping the
trajectory of foundational models in the realm of medical image analysis. To
support ongoing research endeavors, we maintain an active repository that
contains an up-to-date paper list and a succinct summary of open-source
projects at https://github.com/YichiZhang98/SAM4MIS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FurniScene: A Large-scale 3D Room <span class="highlight-title">Dataset</span> with Intricate Furnishing
  Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Genghao Zhang, Yuxi Wang, Chuanchen Luo, Shibiao Xu, Junran Peng, Zhaoxiang Zhang, Man Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Indoor scene generation has attracted significant attention recently as it is
crucial for applications of gaming, virtual reality, and interior design.
Current indoor scene generation methods can produce reasonable room layouts but
often lack diversity and realism. This is primarily due to the limited coverage
of existing datasets, including only large furniture without tiny furnishings
in daily life. To address these challenges, we propose FurniScene, a
large-scale 3D room dataset with intricate furnishing scenes from interior
design professionals. Specifically, the FurniScene consists of 11,698 rooms and
39,691 unique furniture CAD models with 89 different types, covering things
from large beds to small teacups on the coffee table. To better suit
fine-grained indoor scene layout generation, we introduce a novel Two-Stage
Diffusion Scene Model (TSDSM) and conduct an evaluation benchmark for various
indoor scene generation based on FurniScene. Quantitative and qualitative
evaluations demonstrate the capability of our method to generate highly
realistic indoor scenes. Our dataset and code will be publicly available soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BCLNet: Bilateral Consensus Learning for Two-View Correspondence Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyang Miao, Guobao Xiao, Shiping Wang, Jun Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Correspondence pruning aims to establish reliable correspondences between two
related images and recover relative camera motion. Existing approaches often
employ a progressive strategy to handle the local and global contexts, with a
prominent emphasis on transitioning from local to global, resulting in the
neglect of interactions between different contexts. To tackle this issue, we
propose a parallel context learning strategy that involves acquiring bilateral
consensus for the two-view correspondence pruning task. In our approach, we
design a distinctive self-attention block to capture global context and
parallel process it with the established local context learning module, which
enables us to simultaneously capture both local and global consensuses. By
combining these local and global consensuses, we derive the required bilateral
consensus. We also design a recalibration block, reducing the influence of
erroneous consensus information and enhancing the robustness of the model. The
culmination of our efforts is the Bilateral Consensus Learning Network
(BCLNet), which efficiently estimates camera pose and identifies inliers (true
correspondences). Extensive experiments results demonstrate that our network
not only surpasses state-of-the-art methods on benchmark datasets but also
showcases robust generalization abilities across various feature extraction
techniques. Noteworthily, BCLNet obtains 3.98\% mAP5$^{\circ}$ gains over the
second best method on unknown outdoor dataset, and obviously accelerates model
training speed. The source code will be available at:
https://github.com/guobaoxiao/BCLNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Classification of Critical Configurations for any Number of Projective
  Views 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03450v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03450v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Bråtelund
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structure from motion is the process of recovering information about cameras
and 3D scene from a set of images. Generally, in a noise-free setting, all
information can be uniquely recovered if enough images and image points are
provided. There are, however, certain cases where unique recovery is
impossible, even in theory; these are called critical configurations. We use a
recently developed algebraic approach to classify all critical configurations
for any number of projective cameras. We show that they form well-known
algebraic varieties, such as quadric surfaces and curves of degree at most 4.
This paper also improves upon earlier results both by finding previously
unknown critical configurations and by showing that some configurations
previously believed to be critical are in fact not.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages, 10 figures, submitted to International Journal of Computer
  Vision. arXiv admin note: text overlap with arXiv:2112.05478,
  arXiv:2112.05074</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpecRef: A Fast Training-free Baseline of Specific Reference-Condition
  Real Image Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03433v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03433v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songyan Chen, Jiancheng Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-conditional image editing based on large diffusion generative model has
attracted the attention of both the industry and the research community. Most
existing methods are non-reference editing, with the user only able to provide
a source image and text prompt. However, it restricts user's control over the
characteristics of editing outcome. To increase user freedom, we propose a new
task called Specific Reference Condition Real Image Editing, which allows user
to provide a reference image to further control the outcome, such as replacing
an object with a particular one. To accomplish this, we propose a fast baseline
method named SpecRef. Specifically, we design a Specific Reference Attention
Controller to incorporate features from the reference image, and adopt a mask
mechanism to prevent interference between editing and non-editing regions. We
evaluate SpecRef on typical editing tasks and show that it can achieve
satisfactory performance. The source code is available on
https://github.com/jingjiqinggong/specp2p.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ See360: Novel Panoramic View Interpolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhi-Song Liu, Marie-Paule Cani, Wan-Chi Siu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present See360, which is a versatile and efficient framework for 360
panoramic view interpolation using latent space viewpoint estimation. Most of
the existing view rendering approaches only focus on indoor or synthetic 3D
environments and render new views of small objects. In contrast, we suggest to
tackle camera-centered view synthesis as a 2D affine transformation without
using point clouds or depth maps, which enables an effective 360? panoramic
scene exploration. Given a pair of reference images, the See360 model learns to
render novel views by a proposed novel Multi-Scale Affine Transformer (MSAT),
enabling the coarse-to-fine feature rendering. We also propose a Conditional
Latent space AutoEncoder (C-LAE) to achieve view interpolation at any arbitrary
angle. To show the versatility of our method, we introduce four training
datasets, namely UrbanCity360, Archinterior360, HungHom360 and Lab360, which
are collected from indoor and outdoor environments for both real and synthetic
rendering. Experimental results show that the proposed method is generic enough
to achieve real-time rendering of arbitrary views for all four datasets. In
addition, our See360 model can be applied to view synthesis in the wild: with
only a short extra training time (approximately 10 mins), and is able to render
unknown real-world scenes. The superior performance of See360 opens up a
promising direction for camera-centered view rendering and 360 panoramic view
interpolation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GRAM: Global Reasoning for Multi-Page VQA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tsachi Blau, Sharon Fogel, Roi Ronen, Alona Golts, Roy Ganz, Elad Ben Avraham, Aviad Aberdam, Shahar Tsiper, Ron Litman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing use of transformer-based large language models brings forward
the challenge of processing long sequences. In document visual question
answering (DocVQA), leading methods focus on the single-page setting, while
documents can span hundreds of pages. We present GRAM, a method that seamlessly
extends pre-trained single-page models to the multi-page setting, without
requiring computationally-heavy pretraining. To do so, we leverage a
single-page encoder for local page-level understanding, and enhance it with
document-level designated layers and learnable tokens, facilitating the flow of
information across pages for global reasoning. To enforce our model to utilize
the newly introduced document-level tokens, we propose a tailored bias
adaptation method. For additional computational savings during decoding, we
introduce an optional compression stage using our C-Former model, which reduces
the encoded sequence length, thereby allowing a tradeoff between quality and
latency. Extensive experiments showcase GRAM's state-of-the-art performance on
the benchmarks for multi-page DocVQA, demonstrating the effectiveness of our
approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bilateral Reference for High-Resolution Dichotomous Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03407v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03407v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Zheng, Dehong Gao, Deng-Ping Fan, Li Liu, Jorma Laaksonen, Wanli Ouyang, Nicu Sebe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel bilateral reference framework (***BiRefNet***) for
high-resolution dichotomous image segmentation (DIS). It comprises two
essential components: the localization module (LM) and the reconstruction
module (RM) with our proposed bilateral reference (BiRef). The LM aids in
object localization using global semantic information. Within the RM, we
utilize BiRef for the reconstruction process, where hierarchical patches of
images provide the source reference and gradient maps serve as the target
reference. These components collaborate to generate the final predicted maps.
We also introduce auxiliary gradient supervision to enhance focus on regions
with finer details. Furthermore, we outline practical training strategies
tailored for DIS to improve map quality and training process. To validate the
general applicability of our approach, we conduct extensive experiments on four
tasks to evince that *BiRefNet* exhibits remarkable performance, outperforming
task-specific cutting-edge methods across all benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning-based Image and Video Inpainting: A Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weize Quan, Jiaxi Chen, Yanli Liu, Dong-Ming Yan, Peter Wonka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image and video inpainting is a classic problem in computer vision and
computer graphics, aiming to fill in the plausible and realistic content in the
missing areas of images and videos. With the advance of deep learning, this
problem has achieved significant progress recently. The goal of this paper is
to comprehensively review the deep learning-based methods for image and video
inpainting. Specifically, we sort existing methods into different categories
from the perspective of their high-level inpainting pipeline, present different
deep learning architectures, including CNN, VAE, GAN, diffusion models, etc.,
and summarize techniques for module design. We review the training objectives
and the common benchmark datasets. We present evaluation metrics for low-level
pixel and high-level perceptional similarity, conduct a performance evaluation,
and discuss the strengths and weaknesses of representative inpainting methods.
We also discuss related real-world applications. Finally, we discuss open
challenges and suggest potential future research directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to IJCV</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ conv_einsum: A Framework for Representation and Fast Evaluation of
  Multilinear Operations in Convolutional Tensorial Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tahseen Rabbani, Jiahao Su, Xiaoyu Liu, David Chan, Geoffrey Sangston, Furong Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern ConvNets continue to achieve state-of-the-art results over a vast
array of vision and image classification tasks, but at the cost of increasing
parameters. One strategy for compactifying a network without sacrificing much
expressive power is to reshape it into a tensorial neural network (TNN), which
is a higher-order tensorization of its layers, followed by a factorization,
such as a CP-decomposition, which strips a weight down to its critical basis
components. Passes through TNNs can be represented as sequences of multilinear
operations (MLOs), where the evaluation path can greatly affect the number of
floating point operations (FLOPs) incurred. While functions such as the popular
einsum can evaluate simple MLOs such as contractions, existing implementations
cannot process multi-way convolutions, resulting in scant assessments of how
optimal evaluation paths through tensorized convolutional layers can improve
training speed. In this paper, we develop a unifying framework for representing
tensorial convolution layers as einsum-like strings and a meta-algorithm
conv_einsum which is able to evaluate these strings in a FLOPs-minimizing
manner. Comprehensive experiments, using our open-source implementation, over a
wide range of models, tensor decompositions, and diverse tasks, demonstrate
that conv_einsum significantly increases both computational and
memory-efficiency of convolutional TNNs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Effective Multiple-in-One Image Restoration: A Sequential and
  Prompt Learning Strategy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangtao Kong, Chao Dong, Lei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While single task image restoration (IR) has achieved significant successes,
it remains a challenging issue to train a single model which can tackle
multiple IR tasks. In this work, we investigate in-depth the multiple-in-one
(MiO) IR problem, which comprises seven popular IR tasks. We point out that MiO
IR faces two pivotal challenges: the optimization of diverse objectives and the
adaptation to multiple tasks. To tackle these challenges, we present two simple
yet effective strategies. The first strategy, referred to as sequential
learning, attempts to address how to optimize the diverse objectives, which
guides the network to incrementally learn individual IR tasks in a sequential
manner rather than mixing them together. The second strategy, i.e., prompt
learning, attempts to address how to adapt to the different IR tasks, which
assists the network to understand the specific task and improves the
generalization ability. By evaluating on 19 test sets, we demonstrate that the
sequential and prompt learning strategies can significantly enhance the MiO
performance of commonly used CNN and Transformer backbones. Our experiments
also reveal that the two strategies can supplement each other to learn better
degradation representations and enhance the model robustness. It is expected
that our proposed MiO IR formulation and strategies could facilitate the
research on how to train IR models with higher generalization capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Systematic comparison of semi-supervised and self-supervised learning
  for medical image classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.08919v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.08919v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Huang, Ruijie Jiang, Shuchin Aeron, Michael C. Hughes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many medical image classification problems, labeled data is scarce while
unlabeled data is more available. Semi-supervised learning and self-supervised
learning are two different research directions that can improve accuracy by
learning from extra unlabeled data. Recent methods from both directions have
reported significant gains on traditional benchmarks. Yet past benchmarks do
not focus on medical tasks and rarely compare self- and semi- methods together
on equal footing. Furthermore, past benchmarks often handle hyperparameter
tuning suboptimally. First, they may not tune hyperparameters at all, leading
to underfitting. Second, when tuning does occur, it often unrealistically uses
a labeled validation set much larger than the train set. Both cases make
previously published rankings of methods difficult to translate to practical
settings. This study contributes a systematic evaluation of self- and semi-
methods with a unified experimental protocol intended to guide a practitioner
with scarce overall labeled data and a limited compute budget. We answer two
key questions: Can hyperparameter tuning be effective with realistic-sized
validation sets? If so, when all methods are tuned well, which self- or
semi-supervised methods reach the best accuracy? Our study compares 13
representative semi- and self-supervised methods to strong labeled-set-only
baselines on 4 medical datasets. From 20000+ total GPU hours of computation, we
provide valuable best practices to resource-constrained, results-focused
practitioners.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Semi-supervised Learning; Self-supervised Learning; Medical Imaging</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive Review of YOLO Architectures in Computer Vision: From
  YOLOv1 to YOLOv8 and YOLO-NAS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.00501v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.00501v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Terven, Diana Cordova-Esparza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  YOLO has become a central real-time object detection system for robotics,
driverless cars, and video monitoring applications. We present a comprehensive
analysis of YOLO's evolution, examining the innovations and contributions in
each iteration from the original YOLO up to YOLOv8, YOLO-NAS, and YOLO with
Transformers. We start by describing the standard metrics and postprocessing;
then, we discuss the major changes in network architecture and training tricks
for each model. Finally, we summarize the essential lessons from YOLO's
development and provide a perspective on its future, highlighting potential
research directions to enhance real-time object detection systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 21 figures, 4 tables, published in Machine Learning and
  Knowledge Extraction. This version contains the last changes made to the
  published version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LIT-Former: Linking In-plane and Through-plane Transformers for
  Simultaneous CT Image Denoising and Deblurring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10630v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10630v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihao Chen, Chuang Niu, Qi Gao, Ge Wang, Hongming Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies 3D low-dose computed tomography (CT) imaging. Although
various deep learning methods were developed in this context, typically they
focus on 2D images and perform denoising due to low-dose and deblurring for
super-resolution separately. Up to date, little work was done for simultaneous
in-plane denoising and through-plane deblurring, which is important to obtain
high-quality 3D CT images with lower radiation and faster imaging speed. For
this task, a straightforward method is to directly train an end-to-end 3D
network. However, it demands much more training data and expensive
computational costs. Here, we propose to link in-plane and through-plane
transformers for simultaneous in-plane denoising and through-plane deblurring,
termed as LIT-Former, which can efficiently synergize in-plane and
through-plane sub-tasks for 3D CT imaging and enjoy the advantages of both
convolution and transformer networks. LIT-Former has two novel designs:
efficient multi-head self-attention modules (eMSM) and efficient convolutional
feedforward networks (eCFN). First, eMSM integrates in-plane 2D self-attention
and through-plane 1D self-attention to efficiently capture global interactions
of 3D self-attention, the core unit of transformer networks. Second, eCFN
integrates 2D convolution and 1D convolution to extract local information of 3D
convolution in the same fashion. As a result, the proposed LIT-Former synergize
these two subtasks, significantly reducing the computational complexity as
compared to 3D counterparts and enabling rapid convergence. Extensive
experimental results on simulated and clinical datasets demonstrate superior
performance over state-of-the-art models. The source code is made available at
https://github.com/hao1635/LIT-Former.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProS: Prompting-to-simulate Generalized knowledge for Universal
  Cross-Domain Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12478v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12478v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaipeng Fang, Jingkuan Song, Lianli Gao, Pengpeng Zeng, Zhi-Qi Cheng, Xiyao Li, Heng Tao Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of Universal Cross-Domain Retrieval (UCDR) is to achieve robust
performance in generalized test scenarios, wherein data may belong to strictly
unknown domains and categories during training. Recently, pre-trained models
with prompt tuning have shown strong generalization capabilities and attained
noteworthy achievements in various downstream tasks, such as few-shot learning
and video-text retrieval. However, applying them directly to UCDR may not
sufficiently to handle both domain shift (i.e., adapting to unfamiliar domains)
and semantic shift (i.e., transferring to unknown categories). To this end, we
propose Prompting-to-Simulate (ProS), the first method to apply prompt tuning
for UCDR. ProS employs a two-step process to simulate Content-aware Dynamic
Prompts (CaDP) which can impact models to produce generalized features for
UCDR. Concretely, in Prompt Units Learning stage, we introduce two Prompt Units
to individually capture domain and semantic knowledge in a mask-and-align way.
Then, in Context-aware Simulator Learning stage, we train a Content-aware
Prompt Simulator under a simulated test scenarios to produce the corresponding
CaDP. Extensive experiments conducted on three benchmark datasets show that our
method achieves new state-of-the-art performance without bringing excessive
parameters. Our method is publicly available at
https://anonymous.4open.science/r/ProS
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BadCLIP: Dual-Embedding Guided Backdoor Attack on Multimodal Contrastive
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12075v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12075v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Liang, Mingli Zhu, Aishan Liu, Baoyuan Wu, Xiaochun Cao, Ee-Chien Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Studying backdoor attacks is valuable for model copyright protection and
enhancing defenses. While existing backdoor attacks have successfully infected
multimodal contrastive learning models such as CLIP, they can be easily
countered by specialized backdoor defenses for MCL models. This paper reveals
the threats in this practical scenario that backdoor attacks can remain
effective even after defenses and introduces the \emph{\toolns} attack, which
is resistant to backdoor detection and model fine-tuning defenses. To achieve
this, we draw motivations from the perspective of the Bayesian rule and propose
a dual-embedding guided framework for backdoor attacks. Specifically, we ensure
that visual trigger patterns approximate the textual target semantics in the
embedding space, making it challenging to detect the subtle parameter
variations induced by backdoor learning on such natural trigger patterns.
Additionally, we optimize the visual trigger patterns to align the poisoned
samples with target vision features in order to hinder the backdoor unlearning
through clean fine-tuning. Extensive experiments demonstrate that our attack
significantly outperforms state-of-the-art baselines (+45.3% ASR) in the
presence of SoTA backdoor defenses, rendering these mitigation and detection
strategies virtually ineffective. Furthermore, our approach effectively attacks
some more rigorous scenarios like downstream tasks. We believe that this paper
raises awareness regarding the potential threats associated with the practical
application of multimodal contrastive learning and encourages the development
of more robust defense mechanisms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper lacks some work that needs to be cited</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Benchmark</span>ing the Robustness of LiDAR Semantic Segmentation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00970v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00970v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Yan, Chaoda Zheng, Ying Xue, Zhen Li, Shuguang Cui, Dengxin Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When using LiDAR semantic segmentation models for safety-critical
applications such as autonomous driving, it is essential to understand and
improve their robustness with respect to a large range of LiDAR corruptions. In
this paper, we aim to comprehensively analyze the robustness of LiDAR semantic
segmentation models under various corruptions. To rigorously evaluate the
robustness and generalizability of current approaches, we propose a new
benchmark called SemanticKITTI-C, which features 16 out-of-domain LiDAR
corruptions in three groups, namely adverse weather, measurement noise and
cross-device discrepancy. Then, we systematically investigate 11 LiDAR semantic
segmentation models, especially spanning different input representations (e.g.,
point clouds, voxels, projected images, and etc.), network architectures and
training schemes. Through this study, we obtain two insights: 1) We find out
that the input representation plays a crucial role in robustness. Specifically,
under specific corruptions, different representations perform variously. 2)
Although state-of-the-art methods on LiDAR semantic segmentation achieve
promising results on clean data, they are less robust when dealing with noisy
data. Finally, based on the above observations, we design a robust LiDAR
segmentation model (RLSeg) which greatly boosts the robustness with simple but
effective modifications. It is promising that our benchmark, comprehensive
analysis, and observations can boost future research in robust LiDAR semantic
segmentation for safety-critical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IJCV-2024. The benchmark will be made available at
  https://yanx27.github.io/RobustLidarSeg/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PGformer: Proxy-Bridged Game Transformer for Multi-Person Highly
  Interactive Extreme Motion Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03374v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03374v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanwen Fang, Jintai Chen, Peng-Tao Jiang, Chao Li, Yifeng Geng, Eddy K. F. Lam, Guodong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-person motion prediction is a challenging task, especially for
real-world scenarios of highly interacted persons. Most previous works have
been devoted to studying the case of weak interactions (e.g., walking
together), in which typically forecasting each human pose in isolation can
still achieve good performances. This paper focuses on collaborative motion
prediction for multiple persons with extreme motions and attempts to explore
the relationships between the highly interactive persons' pose trajectories.
Specifically, a novel cross-query attention (XQA) module is proposed to
bilaterally learn the cross-dependencies between the two pose sequences
tailored for this situation. A proxy unit is additionally introduced to bridge
the involved persons, which cooperates with our proposed XQA module and subtly
controls the bidirectional spatial information flows. These designs are then
integrated into a Transformer-based architecture and the resulting model is
called Proxy-bridged Game Transformer (PGformer) for multi-person interactive
motion prediction. Its effectiveness has been evaluated on the challenging ExPI
dataset, which involves highly interactive actions. Our PGformer consistently
outperforms the state-of-the-art methods in both short- and long-term
predictions by a large margin. Besides, our approach can also be compatible
with the weakly interacted CMU-Mocap and MuPoTS-3D datasets and extended to the
case of more than 2 individuals with encouraging results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Neighbor Layer Aggregation for Lightweight Self-Supervised
  Monocular Depth Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09272v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09272v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wang Boya, Wang Shuo, Ye Dong, Dou Ziwen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the frequent use of self-supervised monocular depth estimation in
robotics and autonomous driving, the model's efficiency is becoming
increasingly important. Most current approaches apply much larger and more
complex networks to improve the precision of depth estimation. Some researchers
incorporated Transformer into self-supervised monocular depth estimation to
achieve better performance. However, this method leads to high parameters and
high computation. We present a fully convolutional depth estimation network
using contextual feature fusion. Compared to UNet++ and HRNet, we use
high-resolution and low-resolution features to reserve information on small
targets and fast-moving objects instead of long-range fusion. We further
promote depth estimation results employing lightweight channel attention based
on convolution in the decoder stage. Our method reduces the parameters without
sacrificing accuracy. Experiments on the KITTI benchmark show that our method
can get better results than many large models, such as Monodepth2, with only 30
parameters. The source code is available at
https://github.com/boyagesmile/DNA-Depth.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training Convolutional Neural Networks with the Forward-Forward
  algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14924v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14924v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Scodellaro, Ajinkya Kulkarni, Frauke Alves, Matthias Schröter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent successes in analyzing images with deep neural networks are almost
exclusively achieved with Convolutional Neural Networks (CNNs). The training of
these CNNs, and in fact of all deep neural network architectures, uses the
backpropagation algorithm where the output of the network is compared with the
desired result and the difference is then used to tune the weights of the
network towards the desired outcome. In a 2022 preprint, Geoffrey Hinton
suggested an alternative way of training which passes the desired results
together with the images at the input of the network. This so called Forward
Forward (FF) algorithm has up to now only been used in fully connected
networks. In this paper, we show how the FF paradigm can be extended to CNNs.
Our FF-trained CNN, featuring a novel spatially-extended labeling technique,
achieves a classification accuracy of 99.16% on the MNIST hand-written digits
dataset. We show how different hyperparameters affect the performance of the
proposed algorithm and compare the results with CNN trained with the standard
backpropagation approach. Furthermore, we use Class Activation Maps to
investigate which type of features are learnt by the FF algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DarSwin: Distortion Aware Radial Swin Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.09691v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.09691v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshaya Athwale, Ichrak Shili, Émile Bergeron, Arman Afrasiyabi, Justin Lagüe, Ola Ahmad, Jean-François Lalonde
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wide-angle lenses are commonly used in perception tasks requiring a large
field of view. Unfortunately, these lenses produce significant distortions,
making conventional models that ignore the distortion effects unable to adapt
to wide-angle images. In this paper, we present a novel transformer-based model
that automatically adapts to the distortion produced by wide-angle lenses. Our
proposed image encoder architecture, dubbed DarSwin, leverages the physical
characteristics of such lenses analytically defined by the radial distortion
profile. In contrast to conventional transformer-based architectures, DarSwin
comprises a radial patch partitioning, a distortion-based sampling technique
for creating token embeddings, and an angular position encoding for radial
patch merging. Compared to other baselines, DarSwin achieves the best results
on different datasets with significant gains when trained on bounded levels of
distortions (very low, low, medium, and high) and tested on all, including
out-of-distribution distortions. While the base DarSwin architecture requires
knowledge of the radial distortion profile, we show it can be combined with a
self-calibration network that estimates such a profile from the input image
itself, resulting in a completely uncalibrated pipeline. Finally, we also
present DarSwin-Unet, which extends DarSwin, to an encoder-decoder architecture
suitable for pixel-level tasks. We demonstrate its performance on depth
estimation and show through extensive experiments that DarSwin-Unet can perform
zero-shot adaptation to unseen distortions of different wide-angle lenses. The
code and models are publicly available at https://lvsn.github.io/darswin/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is Complexity Required for Neural Network Pruning? A Case Study on
  Global Magnitude Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.14624v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.14624v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manas Gupta, Efe Camci, Vishandi Rudy Keneta, Abhishek Vaidyanathan, Ritwik Kanodia, Chuan-Sheng Foo, Wu Min, Lin Jie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pruning neural networks has become popular in the last decade when it was
shown that a large number of weights can be safely removed from modern neural
networks without compromising accuracy. Numerous pruning methods have been
proposed since, each claiming to be better than prior art, however, at the cost
of increasingly complex pruning methodologies. These methodologies include
utilizing importance scores, getting feedback through back-propagation or
having heuristics-based pruning rules amongst others. In this work, we question
whether this pattern of introducing complexity is really necessary to achieve
better pruning results. We benchmark these SOTA techniques against a simple
pruning baseline, namely, Global Magnitude Pruning (Global MP), that ranks
weights in order of their magnitudes and prunes the smallest ones.
Surprisingly, we find that vanilla Global MP performs very well against the
SOTA techniques. When considering sparsity-accuracy trade-off, Global MP
performs better than all SOTA techniques at all sparsity ratios. When
considering FLOPs-accuracy trade-off, some SOTA techniques outperform Global MP
at lower sparsity ratios, however, Global MP starts performing well at high
sparsity ratios and performs very well at extremely high sparsity ratios.
Moreover, we find that a common issue that many pruning algorithms run into at
high sparsity rates, namely, layer-collapse, can be easily fixed in Global MP.
We explore why layer collapse occurs in networks and how it can be mitigated in
Global MP by utilizing a technique called Minimum Threshold. We showcase the
above findings on various models (WRN-28-8, ResNet-32, ResNet-50, MobileNet-V1
and FastGRNN) and multiple datasets (CIFAR-10, ImageNet and HAR-2). Code is
available at https://github.com/manasgupta-1/GlobalMP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Region-Prompted Adapter Tuning for Visual Abductive Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10428v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10428v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Zhang, Yeo Keat Ee, Basura Fernando
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Abductive Reasoning is an emerging vision-language (VL) topic where
the model needs to retrieve/generate a likely textual hypothesis from a visual
input (image or its part) using backward reasoning based on commonsense. Unlike
in conventional VL retrieval or captioning tasks, where entities of texts
appear in the image, in abductive inferences, the relevant facts about
inferences are not readily apparent in the input images. Besides, these
inferences are causally linked to specific regional visual cues and would
change as cues change. Existing works highlight cues utilizing a specific
prompt (e.g., colorful prompt). Then, a full fine-tuning of a VL foundation
model is launched to tweak its function from perception to deduction. However,
the colorful prompt uniformly patchify ``regional hints'' and ``global
context'' at the same granularity level and may lose fine-grained visual
details crucial for VAR. Meanwhile, full fine-tuning of VLF on limited data
would easily be overfitted.
  To tackle this, we propose a simple yet effective Region-Prompted Adapter
(RPA), a hybrid parameter-efficient fine-tuning method that leverages the
strengths of detailed cues and efficient training for the VAR task.
RPA~consists of two novel modules: Regional Prompt Generator (RPG) and
Adapter$^\textbf{+}$. The prior encodes ``regional visual hints'' and ``global
contexts'' into visual prompts separately at fine and coarse-grained levels.
The latter extends the vanilla adapters with a new Map Adapter, which modifies
the attention map using a trainable low-dim query/key projection. Additionally,
we propose a new Dual-Contrastive Loss to regress the visual feature toward
features of factual description and plausible hypothesis. Experiments on the
Sherlock demonstrate that RPA outperforms previous SOTAs, achieving the 1st
rank on leaderboards (Comparison to Human Accuracy: RPA~31.74 vs CPT-CLIP
29.58).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 11 figures, Under Review of IEEE Transaction</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Masked Autoencoders by Learning Where to Mask 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06583v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06583v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haijian Chen, Wendong Zhang, Yunbo Wang, Xiaokang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Masked image modeling is a promising self-supervised learning method for
visual data. It is typically built upon image patches with random masks, which
largely ignores the variation of information density between them. The question
is: Is there a better masking strategy than random sampling and how can we
learn it? We empirically study this problem and initially find that introducing
object-centric priors in mask sampling can significantly improve the learned
representations. Inspired by this observation, we present AutoMAE, a fully
differentiable framework that uses Gumbel-Softmax to interlink an
adversarially-trained mask generator and a mask-guided image modeling process.
In this way, our approach can adaptively find patches with higher information
density for different images, and further strike a balance between the
information gain obtained from image reconstruction and its practical training
difficulty. In our experiments, AutoMAE is shown to provide effective
pretraining models on standard self-supervised benchmarks and downstream tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures. This paper has been accepted by PRCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GazeCLIP: Towards Enhancing Gaze Estimation via Text Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00260v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00260v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Wang, Hao Ruan, Mingjie Wang, Chuanghui Zhang, Huachun Li, Jun Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past decade, visual gaze estimation has garnered growing attention
within the research community, thanks to its wide-ranging application
scenarios. While existing estimation approaches have achieved remarkable
success in enhancing prediction accuracy, they primarily infer gaze directions
from single-image signals and discard the huge potentials of the currently
dominant text guidance. Notably, visual-language collaboration has been
extensively explored across a range of visual tasks, such as image synthesis
and manipulation, leveraging the remarkable transferability of large-scale
Contrastive Language-Image Pre-training (CLIP) model. Nevertheless, existing
gaze estimation approaches ignore the rich semantic cues conveyed by linguistic
signals and priors in CLIP feature space, thereby yielding performance
setbacks. In pursuit of making up this gap, we delve deeply into the text-eye
collaboration protocol and introduce a novel gaze estimation framework in this
paper, referred to as GazeCLIP. Specifically, we intricately design a
linguistic description generator to produce text signals with coarse
directional cues. Additionally, a CLIP-based backbone that excels in
characterizing text-eye pairs for gaze estimation is presented. This is
followed by the implementation of a fine-grained multi-modal fusion module
aimed at modeling the interrelationships between heterogeneous inputs.
Extensive experiments on three challenging datasets demonstrate the superiority
of the proposed GazeCLIP which surpasses the previous approaches and achieves
the state-of-the-art estimation accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AliFuse: Aligning and Fusing Multi-modal Medical Data for Computer-Aided
  Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01074v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01074v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiuhui Chen, Yi Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical data collected for making a diagnostic decision are typically
multi-modal and provide complementary perspectives of a subject. A
computer-aided diagnosis system welcomes multi-modal inputs; however, how to
effectively fuse such multi-modal data is a challenging task and attracts a lot
of attention in the medical research field. In this paper, we propose a
transformer-based framework, called Alifuse, for aligning and fusing
multi-modal medical data. Specifically, we convert images and unstructured and
structured texts into vision and language tokens, and use intramodal and
intermodal attention mechanisms to learn holistic representations of all
imaging and non-imaging data for classification. We apply Alifuse to classify
Alzheimer's disease and obtain state-of-the-art performance on five public
datasets, by outperforming eight baselines. The source code will be available
online later.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MLIC++: Linear Complexity Attention-based Multi-Reference Entropy
  Modeling for Learned Image Compression <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.15421v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.15421v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Jiang, Jiayu Yang, Yongqi Zhai, Feng Gao, Ronggang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, learned image compression has achieved impressive performance. The
entropy model, which estimates the distribution of the latent representation,
plays a crucial role in enhancing rate-distortion performance. However,
existing global context modules rely on computationally intensive quadratic
complexity computations to capture global correlations. This quadratic
complexity imposes limitations on the potential of high-resolution image
coding. Moreover, effectively capturing local, global, and channel-wise
contexts with acceptable even linear complexity within a single entropy model
remains a challenge. To address these limitations, we propose the Linear
Complexity Attention-based Multi-Reference Entropy Model (MEM++). MEM++
effectively captures the diverse range of correlations inherent in the latent
representation. Specifically, the latent representation is first divided into
multiple slices. When compressing a particular slice, the previously compressed
slices serve as its channel-wise contexts. To capture local contexts without
sacrificing performance, we introduce a novel checkerboard attention module.
Additionally, to capture global contexts, we propose the linear complexity
attention-based global correlations capturing by leveraging the decomposition
of the softmax operation. The attention map of the previously decoded slice is
implicitly computed and employed to predict global correlations in the current
slice. Based on MEM++, we propose image compression model MLIC++. Extensive
experimental evaluations demonstrate that our MLIC++ achieves state-of-the-art
performance, reducing BD-rate by 13.39% on the Kodak dataset compared to
VTM-17.0 in PSNR. Furthermore, MLIC++ exhibits linear GPU memory consumption
with resolution, making it highly suitable for high-resolution image coding.
Code and pre-trained models are available at
https://github.com/JiangWeibeta/MLIC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v5 is the journel version. Short version (v1, v2, v3) is accepted at
  ICML 2023 Neural Compression Workshop. MLIC++ is an extension of our ACMMM
  2023 paper MLIC: Multi-Reference Entropy Model for Learned Image Compression</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sports-QA: A Large-Scale Video Question Answering <span class="highlight-title">Benchmark</span> for Complex
  and Professional Sports 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01505v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01505v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haopeng Li, Andong Deng, Qiuhong Ke, Jun Liu, Hossein Rahmani, Yulan Guo, Bernt Schiele, Chen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning over sports videos for question answering is an important task with
numerous applications, such as player training and information retrieval.
However, this task has not been explored due to the lack of relevant datasets
and the challenging nature it presents. Most datasets for video question
answering (VideoQA) focus mainly on general and coarse-grained understanding of
daily-life videos, which is not applicable to sports scenarios requiring
professional action understanding and fine-grained motion analysis. In this
paper, we introduce the first dataset, named Sports-QA, specifically designed
for the sports VideoQA task. The Sports-QA dataset includes various types of
questions, such as descriptions, chronologies, causalities, and counterfactual
conditions, covering multiple sports. Furthermore, to address the
characteristics of the sports VideoQA task, we propose a new Auto-Focus
Transformer (AFT) capable of automatically focusing on particular scales of
temporal information for question answering. We conduct extensive experiments
on Sports-QA, including baseline studies and the evaluation of different
methods. The results demonstrate that our AFT achieves state-of-the-art
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhanced Breast Cancer Tumor Classification using MobileNetV2: A
  Detailed Exploration on Image Intensity, Error Mitigation, and
  Streamlit-driven Real-time Deployment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03020v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03020v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaditya Surya, Aditya Shah, Jarnell Kabore, Subash Sasikumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research introduces a sophisticated transfer learning model based on
Google's MobileNetV2 for breast cancer tumor classification into normal,
benign, and malignant categories, utilizing a dataset of 1576 ultrasound images
(265 normal, 891 benign, 420 malignant). The model achieves an accuracy of
0.82, precision of 0.83, recall of 0.81, ROC-AUC of 0.94, PR-AUC of 0.88, and
MCC of 0.74. It examines image intensity distributions and misclassification
errors, offering improvements for future applications. Addressing dataset
imbalances, the study ensures a generalizable model. This work, using a dataset
from Baheya Hospital, Cairo, Egypt, compiled by Walid Al-Dhabyani et al.,
emphasizes MobileNetV2's potential in medical imaging, aiming to improve
diagnostic precision in oncology. Additionally, the paper explores
Streamlit-based deployment for real-time tumor classification, demonstrating
MobileNetV2's applicability in medical imaging and setting a benchmark for
future research in oncology diagnostics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Visual Grounding by Encouraging Consistent Gradient-based
  Explanations <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.15462v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.15462v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyan Yang, Kushal Kafle, Franck Dernoncourt, Vicente Ordonez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a margin-based loss for tuning joint vision-language models so
that their gradient-based explanations are consistent with region-level
annotations provided by humans for relatively smaller grounding datasets. We
refer to this objective as Attention Mask Consistency (AMC) and demonstrate
that it produces superior visual grounding results than previous methods that
rely on using vision-language models to score the outputs of object detectors.
Particularly, a model trained with AMC on top of standard vision-language
modeling objectives obtains a state-of-the-art accuracy of 86.49% in the
Flickr30k visual grounding benchmark, an absolute improvement of 5.38% when
compared to the best previous model trained under the same level of
supervision. Our approach also performs exceedingly well on established
benchmarks for referring expression comprehension where it obtains 80.34%
accuracy in the easy test of RefCOCO+, and 64.55% in the difficult split. AMC
is effective, easy to implement, and is general as it can be adopted by any
vision-language model, and can use any type of region annotations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2023. Fix ReferIt results. Code:
  https://github.com/uvavision/AMC-grounding Project Webpage:
  https://vislang.ai/amc</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">13</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangled Neural Relational Inference for Interpretable Motion
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03599v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03599v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victoria M. Dax, Jiachen Li, Enna Sachdeva, Nakul Agarwal, Mykel J. Kochenderfer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective interaction modeling and behavior prediction of dynamic agents play
a significant role in interactive motion planning for autonomous robots.
Although existing methods have improved prediction accuracy, few research
efforts have been devoted to enhancing prediction model interpretability and
out-of-distribution (OOD) generalizability. This work addresses these two
challenging aspects by designing a variational auto-encoder framework that
integrates graph-based representations and time-sequence models to efficiently
capture spatio-temporal relations between interactive agents and predict their
dynamics. Our model infers dynamic interaction graphs in a latent space
augmented with interpretable edge features that characterize the interactions.
Moreover, we aim to enhance model interpretability and performance in OOD
scenarios by disentangling the latent space of edge features, thereby
strengthening model versatility and robustness. We validate our approach
through extensive experiments on both simulated and real-world datasets. The
results show superior performance compared to existing methods in modeling
spatio-temporal relations, motion prediction, and identifying time-invariant
latent features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating and Personalizing User-Perceived Quality of Text-to-Speech
  Voices for Delivering Mindfulness Meditation with Different Physical
  Embodiments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhonghao Shi, Han Chen, Anna-Maria Velentza, Siqi Liu, Nathaniel Dennler, Allison O'Connell, Maja Matarić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mindfulness-based therapies have been shown to be effective in improving
mental health, and technology-based methods have the potential to expand the
accessibility of these therapies. To enable real-time personalized content
generation for mindfulness practice in these methods, high-quality
computer-synthesized text-to-speech (TTS) voices are needed to provide verbal
guidance and respond to user performance and preferences. However, the
user-perceived quality of state-of-the-art TTS voices has not yet been
evaluated for administering mindfulness meditation, which requires emotional
expressiveness. In addition, work has not yet been done to study the effect of
physical embodiment and personalization on the user-perceived quality of TTS
voices for mindfulness. To that end, we designed a two-phase human subject
study. In Phase 1, an online Mechanical Turk between-subject study (N=471)
evaluated 3 (feminine, masculine, child-like) state-of-the-art TTS voices with
2 (feminine, masculine) human therapists' voices in 3 different physical
embodiment settings (no agent, conversational agent, socially assistive robot)
with remote participants. Building on findings from Phase 1, in Phase 2, an
in-person within-subject study (N=94), we used a novel framework we developed
for personalizing TTS voices based on user preferences, and evaluated
user-perceived quality compared to best-rated non-personalized voices from
Phase 1. We found that the best-rated human voice was perceived better than all
TTS voices; the emotional expressiveness and naturalness of TTS voices were
poorly rated, while users were satisfied with the clarity of TTS voices.
Surprisingly, by allowing users to fine-tune TTS voice features, the
user-personalized TTS voices could perform almost as well as human voices,
suggesting user personalization could be a simple and very effective tool to
improve user-perceived quality of TTS voice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Proceedings of the 2023 ACM/IEEE International
  Conference on Human-Robot Interaction, pp. 516-524. 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Overview of Dialogue Robot Competition 2023 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03547v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03547v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takashi Minato, Ryuichiro Higashinaka, Kurima Sakai, Tomo Funayama, Hiromitsu Nishizaki, Takayuki Naga
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We have held dialogue robot competitions in 2020 and 2022 to compare the
performances of interactive robots using an android that closely resembles a
human. In 2023, the third competition DRC2023 was held. The task of DRC2023 was
designed to be more challenging than the previous travel agent dialogue tasks.
Since anyone can now develop a dialogue system using LLMs, the participating
teams are required to develop a system that effectively uses information about
the situation on the spot (real-time information), which is not handled by
ChatGPT and other systems. DRC2023 has two rounds, a preliminary round and the
final round as well as the previous competitions. The preliminary round has
held on Oct.27 -- Nov.20, 2023 at real travel agency stores. The final round
will be held on December 23, 2023. This paper provides an overview of the task
settings and evaluation method of DRC2023 and the preliminary round results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of Dialogue Robot Competition 2023. arXiv admin note:
  text overlap with arXiv:2210.12863</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quadrotor Stabilization with Safety Guarantees: A Universal Formula
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03500v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03500v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Li, Zhiyong Sun, Siep Weiland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safe stabilization is a significant challenge for quadrotors, which involves
reaching a goal position while avoiding obstacles. Most of the existing
solutions for this problem rely on optimization-based methods, demanding
substantial onboard computational resources. This paper introduces a novel
approach to address this issue and provides a solution that offers fast
computational capabilities tailored for onboard execution. Drawing inspiration
from Sontag's universal formula, we propose an analytical control strategy that
incorporates the conditions of control Lyapunov functions (CLFs) and control
barrier functions (CBFs), effectively avoiding the need for solving
optimization problems onboard. Moreover, we extend our approach by
incorporating the concepts of input-to-state stability (ISS) and input-to-state
safety (ISSf), enhancing the universal formula's capacity to effectively manage
disturbances. Furthermore, we present a projection-based approach to ensure
that the universal formula remains effective even when faced with control input
constraints. The basic idea of this approach is to project the control input
derived from the universal formula onto the closest point within the control
input domain. Through comprehensive simulations and experimental results, we
validate the efficacy and highlight the advantages of our methodology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robots and Social Sustainability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bipin Indurkhya, Barbara Sienkiewicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sustainability is no longer a matter of choice but is invariably linked to
the survival of the entire ecosystem of our planet Earth. As robotics
technology is growing at an exponential rate, it is crucial to examine its
implications for sustainability. Our focus is on social sustainability,
specifically analyzing the role of robotics technology in this domain by
identifying six distinct ways robots influence social sustainability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Full version of "B.Indurkhya and B. Sienkiewicz, Robots and Social
  Sustainability, European Robotics Forum, 13-15 March, 2024, Rimini, Italy"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ N$^{3}$-Mapping: Normal Guided Neural Non-Projective Signed Distance
  Fields for Large-scale 3D Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuangfu Song, Junqiao Zhao, Kai Huang, Jiaye Lin, Chen Ye, Tiantian Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and dense mapping in large-scale environments is essential for
various robot applications. Recently, implicit neural signed distance fields
(SDFs) have shown promising advances in this task. However, most existing
approaches employ projective distances from range data as SDF supervision,
introducing approximation errors and thus degrading the mapping quality. To
address this problem, we introduce N3-Mapping, an implicit neural mapping
system featuring normal-guided neural non-projective signed distance fields.
Specifically, we directly sample points along the surface normal, instead of
the ray, to obtain more accurate non-projective distance values from range
data. Then these distance values are used as supervision to train the implicit
map. For large-scale mapping, we apply a voxel-oriented sliding window
mechanism to alleviate the forgetting issue with a bounded memory footprint.
Besides, considering the uneven distribution of measured point clouds, a
hierarchical sampling strategy is designed to improve training efficiency.
Experiments demonstrate that our method effectively mitigates SDF approximation
errors and achieves state-of-the-art mapping quality compared to existing
approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Engineering Features to Improve Pass Prediction in Soccer Simulation 2D
  Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03410v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03410v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nader Zare, Mahtab Sarvmaili, Aref Sayareh, Omid Amini, Stan Matwin Amilcar Soares
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soccer Simulation 2D (SS2D) is a simulation of a real soccer game in two
dimensions. In soccer, passing behavior is an essential action for keeping the
ball in possession of our team and creating goal opportunities. Similarly, for
SS2D, predicting the passing behaviors of both opponents and our teammates
helps manage resources and score more goals. Therefore, in this research, we
have tried to address the modeling of passing behavior of soccer 2D players
using Deep Neural Networks (DNN) and Random Forest (RF). We propose an embedded
data extraction module that can record the decision-making of agents in an
online format. Afterward, we apply four data sorting techniques for training
data preparation. After, we evaluate the trained models' performance playing
against 6 top teams of RoboCup 2019 that have distinctive playing strategies.
Finally, we examine the importance of different feature groups on the
prediction of a passing strategy. All results in each step of this work prove
our suggested methodology's effectiveness and improve the performance of the
pass prediction in Soccer Simulation 2D games ranging from 5\% (e.g., playing
against the same team) to 10\% (e.g., playing against Robocup top teams).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Dribbling, Passing, and Marking Actions in Soccer Simulation
  2D Games Using Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nader Zare, Omid Amini, Aref Sayareh, Mahtab Sarvmaili, Arad Firouzkouhi, Stan Matwin, Amilcar Soares
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The RoboCup competition was started in 1997, and is known as the oldest
RoboCup league. The RoboCup 2D Soccer Simulation League is a stochastic,
partially observable soccer environment in which 24 autonomous agents play on
two opposing teams. In this paper, we detail the main strategies and
functionalities of CYRUS, the RoboCup 2021 2D Soccer Simulation League
champions. The new functionalities presented and discussed in this work are (i)
Multi Action Dribble, (ii) Pass Prediction and (iii) Marking Decision. The
Multi Action Dribbling strategy enabled CYRUS to succeed more often and to be
safer when dribbling actions were performed during a game. The Pass Prediction
enhanced our gameplay by predicting our teammate's passing behavior,
anticipating and making our agents collaborate better towards scoring goals.
Finally, the Marking Decision addressed the multi-agent matching problem to
improve CYRUS defensive strategy by finding an optimal solution to mark
opponents' players.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Amplifying robotics capacities with a human touch: An immersive
  low-latency panoramic remote system 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Li, Jian Xu, Dewei Han, Kang Li, Zhaoyuan Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI and robotics technologies have witnessed remarkable advancements in the
past decade, revolutionizing work patterns and opportunities in various
domains. The application of these technologies has propelled society towards an
era of symbiosis between humans and machines. To facilitate efficient
communication between humans and intelligent robots, we propose the "Avatar"
system, an immersive low-latency panoramic human-robot interaction platform. We
have designed and tested a prototype of a rugged mobile platform integrated
with edge computing units, panoramic video capture devices, power batteries,
robot arms, and network communication equipment. Under favorable network
conditions, we achieved a low-latency high-definition panoramic visual
experience with a delay of 357ms. Operators can utilize VR headsets and
controllers for real-time immersive control of robots and devices. The system
enables remote control over vast physical distances, spanning campuses,
provinces, countries, and even continents (New York to Shenzhen). Additionally,
the system incorporates visual SLAM technology for map and trajectory
recording, providing autonomous navigation capabilities. We believe that this
intuitive system platform can enhance efficiency and situational experience in
human-robot collaboration, and with further advancements in related
technologies, it will become a versatile tool for efficient and symbiotic
cooperation between AI and humans.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMs for Robotic Object Disambiguation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Connie Jiang, Yiqing Xu, David Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advantages of pre-trained large language models (LLMs) are apparent in a
variety of language processing tasks. But can a language model's knowledge be
further harnessed to effectively disambiguate objects and navigate
decision-making challenges within the realm of robotics? Our study reveals the
LLM's aptitude for solving complex decision making challenges that are often
previously modeled by Partially Observable Markov Decision Processes (POMDPs).
A pivotal focus of our research is the object disambiguation capability of
LLMs. We detail the integration of an LLM into a tabletop environment
disambiguation task, a decision making problem where the robot's task is to
discern and retrieve a user's desired object from an arbitrarily large and
complex cluster of objects. Despite multiple query attempts with zero-shot
prompt engineering (details can be found in the Appendix), the LLM struggled to
inquire about features not explicitly provided in the scene description. In
response, we have developed a few-shot prompt engineering system to improve the
LLM's ability to pose disambiguating queries. The result is a model capable of
both using given features when they are available and inferring new relevant
features when necessary, to successfully generate and navigate down a precise
decision tree to the correct object--even when faced with identical options.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ STAMP: Differentiable Task and Motion Planning via Stein Variational
  Gradient Descent <span class="chip">CoRL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01775v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01775v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yewon Lee, Philip Huang, Krishna Murthy Jatavallabhula, Andrew Z. Li, Fabian Damken, Eric Heiden, Kevin Smith, Derek Nowrouzezahrai, Fabio Ramos, Florian Shkurti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Planning for many manipulation tasks, such as using tools or assembling
parts, often requires both symbolic and geometric reasoning. Task and Motion
Planning (TAMP) algorithms typically solve these problems by conducting a tree
search over high-level task sequences while checking for kinematic and dynamic
feasibility. This can be inefficient as the width of the tree can grow
exponentially with the number of possible actions and objects. In this paper,
we propose a novel approach to TAMP that relaxes discrete-and-continuous TAMP
problems into inference problems on a continuous domain. Our method, Stein Task
and Motion Planning (STAMP) subsequently solves this new problem using a
gradient-based variational inference algorithm called Stein Variational
Gradient Descent, by obtaining gradients from a parallelized differentiable
physics simulator. By introducing relaxations to the discrete variables,
leveraging parallelization, and approaching TAMP as an Bayesian inference
problem, our method is able to efficiently find multiple diverse plans in a
single optimization run. We demonstrate our method on two TAMP problems and
benchmark them against existing TAMP baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures, Learning Effective Abstractions for Planning
  (LEAP) Workshop at CoRL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Initialization Strategies for Range-Only Trajectory Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09011v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09011v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhishek Goudar, Frederike Dümbgen, Timothy D. Barfoot, Angela P. Schoellig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Range-only (RO) pose estimation involves determining a robot's pose over time
by measuring the distance between multiple devices on the robot, known as tags,
and devices installed in the environment, known as anchors. The nonconvex
nature of the range measurement model results in a cost function with possible
local minima. In the absence of a good initialization, commonly used iterative
solvers can get stuck in these local minima resulting in poor trajectory
estimation accuracy. In this work, we propose convex relaxations to the
original nonconvex problem based on semidefinite programs (SDPs). Specifically,
we formulate computationally tractable SDP relaxations to obtain accurate
initial pose and trajectory estimates for RO trajectory estimation under static
and dynamic (i.e., constant-velocity motion) conditions. Through simulation and
real experiments, we demonstrate that our proposed initialization strategies
estimate the initial state accurately compared to iterative local solvers.
Additionally, the proposed relaxations recover global minima under moderate
range measurement noise levels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bidirectional Temporal Plan Graph: Enabling Switchable Passing Orders
  for More Efficient Multi-Agent Path Finding Plan Execution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00315v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00315v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Su, Rishi Veerapaneni, Jiaoyang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Multi-Agent Path Finding (MAPF) problem involves planning collision-free
paths for multiple agents in a shared environment. The majority of MAPF solvers
rely on the assumption that an agent can arrive at a specific location at a
specific timestep. However, real-world execution uncertainties can cause agents
to deviate from this assumption, leading to collisions and deadlocks. Prior
research solves this problem by having agents follow a Temporal Plan Graph
(TPG), enforcing a consistent passing order at every location as defined in the
MAPF plan. However, we show that TPGs are overly strict because, in some
circumstances, satisfying the passing order requires agents to wait
unnecessarily, leading to longer execution time. To overcome this issue, we
introduce a new graphical representation called a Bidirectional Temporal Plan
Graph (BTPG), which allows switching passing orders during execution to avoid
unnecessary waiting time. We design two anytime algorithms for constructing a
BTPG: BTPG-na\"ive and BTPG-optimized. Experimental results show that following
BTPGs consistently outperforms following TPGs, reducing unnecessary waits by
8-20%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI-2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Re:Draw -- Context Aware Translation as a Controllable Method for
  Artistic Production 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joao Liborio Cardoso, Francesco Banterle, Paolo Cignoni, Michael Wimmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce context-aware translation, a novel method that combines the
benefits of inpainting and image-to-image translation, respecting
simultaneously the original input and contextual relevance -- where existing
methods fall short. By doing so, our method opens new avenues for the
controllable use of AI within artistic creation, from animation to digital art.
  As an use case, we apply our method to redraw any hand-drawn animated
character eyes based on any design specifications - eyes serve as a focal point
that captures viewer attention and conveys a range of emotions, however, the
labor-intensive nature of traditional animation often leads to compromises in
the complexity and consistency of eye design. Furthermore, we remove the need
for production data for training and introduce a new character recognition
method that surpasses existing work by not requiring fine-tuning to specific
productions. This proposed use case could help maintain consistency throughout
production and unlock bolder and more detailed design choices without the
production cost drawbacks. A user study shows context-aware translation is
preferred over existing work 95.16% of the time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Freetalker: Controllable Speech and Text-Driven Gesture Generation Based
  on Diffusion Models for Enhanced Speaker Naturalness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sicheng Yang, Zunnan Xu, Haiwei Xue, Yongkang Cheng, Shaoli Huang, Mingming Gong, Zhiyong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current talking avatars mostly generate co-speech gestures based on audio and
text of the utterance, without considering the non-speaking motion of the
speaker. Furthermore, previous works on co-speech gesture generation have
designed network structures based on individual gesture datasets, which results
in limited data volume, compromised generalizability, and restricted speaker
movements. To tackle these issues, we introduce FreeTalker, which, to the best
of our knowledge, is the first framework for the generation of both spontaneous
(e.g., co-speech gesture) and non-spontaneous (e.g., moving around the podium)
speaker motions. Specifically, we train a diffusion-based model for speaker
motion generation that employs unified representations of both speech-driven
gestures and text-driven motions, utilizing heterogeneous data sourced from
various motion datasets. During inference, we utilize classifier-free guidance
to highly control the style in the clips. Additionally, to create smooth
transitions between clips, we utilize DoubleTake, a method that leverages a
generative prior and ensures seamless motion blending. Extensive experiments
show that our method generates natural and controllable speaker movements. Our
code, model, and demo are are available at
\url{https://youngseng.github.io/FreeTalker/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-01-06T00:00:00Z">2024-01-06</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">22</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PIXAR: Auto-Regressive Language Modeling in Pixel Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yintao Tai, Xiyang Liao, Alessandro Suglia, Antonio Vergari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works showed the possibility of building open-vocabulary large
language models (LLMs) that directly operate on pixel representations and are
implemented as encoder-decoder models that reconstruct masked image patches of
rendered text. However, these pixel-based LLMs are limited to autoencoding
tasks and cannot generate new text as images. As such, they cannot be used for
open-answer or generative language tasks. In this work, we overcome this
limitation and introduce PIXAR, the first pixel-based autoregressive LLM that
does not rely on a pre-defined vocabulary for both input and output text.
Consisting of only a decoder, PIXAR can answer free-form generative tasks while
keeping the text representation learning performance on par with previous
encoder-decoder models. Furthermore, we highlight the challenges to
autoregressively generate non-blurred text as images and link this to the usual
maximum likelihood objective. We propose a simple adversarial pretraining that
significantly improves the readability and performance of PIXAR making it
comparable to GPT2 on short text generation tasks. This paves the way to
building open-vocabulary LLMs that are usable for free-form generative tasks
and questions the necessity of the usual symbolic input representation -- text
as tokens -- for these challenging tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Context Through Contrast 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kshitij Ambilduke, Aneesh Shetye, Diksha Bagade, Rishika Bhagwatkar, Khurshed Fitter, Prasad Vagdargi, Shital Chiddarwar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural machine translation benefits from semantically rich representations.
Considerable progress in learning such representations has been achieved by
language modelling and mutual information maximization objectives using
contrastive learning. The language-dependent nature of language modelling
introduces a trade-off between the universality of the learned representations
and the model's performance on the language modelling tasks. Although
contrastive learning improves performance, its success cannot be attributed to
mutual information alone. We propose a novel Context Enhancement step to
improve performance on neural machine translation by maximizing mutual
information using the Barlow Twins loss. Unlike other approaches, we do not
explicitly augment the data but view languages as implicit augmentations,
eradicating the risk of disrupting semantic information. Further, our method
does not learn embeddings from scratch and can be generalised to any set of
pre-trained embeddings. Finally, we evaluate the language-agnosticism of our
embeddings through language classification and use them for neural machine
translation to compare with state-of-the-art approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models as Visual Cross-Domain Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03253v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03253v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuhao Chen, Yulong Zhang, Weisen Jiang, Jiangang Lu, Yu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances achieved by deep learning models rely on the independent and
identically distributed assumption, hindering their applications in real-world
scenarios with domain shifts. To address the above issues, cross-domain
learning aims at extracting domain-invariant knowledge to reduce the domain
shift between training and testing data. However, in visual cross-domain
learning, traditional methods concentrate solely on the image modality,
neglecting the use of the text modality to alleviate the domain shift. In this
work, we propose Large Language models as Visual cross-dOmain learners (LLaVO).
LLaVO uses vision-language models to convert images into detailed textual
descriptions. A large language model is then finetuned on textual descriptions
of the source/target domain generated by a designed instruction template.
Extensive experimental results on various cross-domain tasks under the domain
generalization and unsupervised domain adaptation settings have demonstrated
the effectiveness of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reflections on Inductive Thematic Saturation as a potential metric for
  measuring the validity of an inductive Thematic Analysis with LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03239v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03239v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefano De Paoli, Walter Stan Mathis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a set of reflections on saturation and the use of Large
Language Models (LLMs) for performing Thematic Analysis (TA). The paper
suggests that initial thematic saturation (ITS) could be used as a metric to
assess part of the transactional validity of TA with LLM, focusing on the
initial coding. The paper presents the initial coding of two datasets of
different sizes, and it reflects on how the LLM reaches some form of analytical
saturation during the coding. The procedure proposed in this work leads to the
creation of two codebooks, one comprising the total cumulative initial codes
and the other the total unique codes. The paper proposes a metric to
synthetically measure ITS using a simple mathematical calculation employing the
ratio between slopes of cumulative codes and unique codes. The paper
contributes to the initial body of work exploring how to perform qualitative
analysis with LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Dawn After the Dark: An Empirical Study on Factuality Hallucination
  in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyi Li, Jie Chen, Ruiyang Ren, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of large language models (LLMs), hallucination (i.e., the tendency
to generate factually incorrect content) poses great challenge to trustworthy
and reliable deployment of LLMs in real-world applications. To tackle the LLM
hallucination, three key questions should be well studied: how to detect
hallucinations (detection), why do LLMs hallucinate (source), and what can be
done to mitigate them (mitigation). To address these challenges, this work
presents a systematic empirical study on LLM hallucination, focused on the the
three aspects of hallucination detection, source and mitigation. Specially, we
construct a new hallucination benchmark HaluEval 2.0, and designs a simple yet
effective detection method for LLM hallucination. Furthermore, we zoom into the
different training or utilization stages of LLMs and extensively analyze the
potential factors that lead to the LLM hallucination. Finally, we implement and
examine a series of widely used techniques to mitigate the hallucinations in
LLMs. Our work has led to several important findings to understand the
hallucination origin and mitigate the hallucinations in LLMs. Our code and data
can be accessed at https://github.com/RUCAIBox/HaluEval-2.0.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 8 figures, 13 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MPN: Leveraging Multilingual Patch Neuron for Cross-lingual Model
  Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03190v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03190v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nianwen Si, Hao Zhang, Weiqiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models are known for encoding a vast amount of factual
knowledge, but they often becomes outdated due to the ever-changing nature of
external information. A promising solution to this challenge is the utilization
of model editing methods to update the knowledge in an efficient manner.
However, the majority of existing model editing techniques are limited to
monolingual frameworks, thus failing to address the crucial issue of
cross-lingual knowledge synchronization for multilingual models. To tackle this
problem, we propose a simple yet effective method that trains multilingual
patch neuron to store cross-lingual knowledge. It can be easily adapted to
existing approaches to enhance their cross-lingual editing capabilities. To
evaluate our method, we conduct experiments using both the XNLI dataset and a
self-constructed XFEVER dataset. Experimental results demonstrate that our
proposed method achieves improved performance in cross-lingual editing tasks
without requiring excessive modifications to the original methodology, thereby
showcasing its user-friendly characteristics. Codes will be released soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ δ-CAUSAL: Exploring Defeasibility in Causal Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaobo Cui, Lazar Milikic, Yiyang Feng, Mete Ismayilzada, Debjit Paul, Antoine Bosselut, Boi Faltings
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Defeasibility in causal reasoning implies that the causal relationship
between cause and effect can be strengthened or weakened. Namely, the causal
strength between cause and effect should increase or decrease with the
incorporation of strengthening arguments (supporters) or weakening arguments
(defeaters), respectively. However, existing works ignore defeasibility in
causal reasoning and fail to evaluate existing causal strength metrics in
defeasible settings. In this work, we present {\delta}-CAUSAL, the first
benchmark dataset for studying defeasibility in causal reasoning.
{\delta}-CAUSAL includes around 11K events spanning ten domains, featuring
defeasible causality pairs, i.e., cause-effect pairs accompanied by supporters
and defeaters. We further show current causal strength metrics fail to reflect
the change of causal strength with the incorporation of supporters or defeaters
in {\delta}-CAUSAL. To this end, we propose CESAR (Causal Embedding aSsociation
with Attention Rating), a metric that measures causal strength based on
token-level causal relationships. CESAR achieves a significant 69.7% relative
improvement over existing metrics, increasing from 47.2% to 80.1% in capturing
the causal strength change brought by supporters and defeaters. We further
demonstrate even Large Language Models (LLMs) like GPT-3.5 still lag 4.5 and
10.7 points behind humans in generating supporters and defeaters, emphasizing
the challenge posed by {\delta}-CAUSAL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Joint-Reasoning based Disease Q&A System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prakash Chandra Sukhwal, Vaibhav Rajan, Atreyi Kankanhalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical question answer (QA) assistants respond to lay users' health-related
queries by synthesizing information from multiple sources using natural
language processing and related techniques. They can serve as vital tools to
alleviate issues of misinformation, information overload, and complexity of
medical language, thus addressing lay users' information needs while reducing
the burden on healthcare professionals. QA systems, the engines of such
assistants, have typically used either language models (LMs) or knowledge
graphs (KG), though the approaches could be complementary. LM-based QA systems
excel at understanding complex questions and providing well-formed answers, but
are prone to factual mistakes. KG-based QA systems, which represent facts well,
are mostly limited to answering short-answer questions with pre-created
templates. While a few studies have jointly used LM and KG approaches for
text-based QA, this was done to answer multiple-choice questions. Extant QA
systems also have limitations in terms of automation and performance. We
address these challenges by designing a novel, automated disease QA system
which effectively utilizes both LM and KG techniques through a joint-reasoning
approach to answer disease-related questions appropriate for lay users. Our
evaluation of the system using a range of quality metrics demonstrates its
efficacy over benchmark systems, including the popular ChatGPT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 6 figures, submitted to TMIS on 14 July 2023 (status: under
  review)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text-Video Retrieval via Variational Multi-Modal Hypergraph Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Li, Lixin Su, Jiashu Zhao, Long Xia, Hengyi Cai, Suqi Cheng, Hengzhu Tang, Junfeng Wang, Dawei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-video retrieval is a challenging task that aims to identify relevant
videos given textual queries. Compared to conventional textual retrieval, the
main obstacle for text-video retrieval is the semantic gap between the textual
nature of queries and the visual richness of video content. Previous works
primarily focus on aligning the query and the video by finely aggregating
word-frame matching signals. Inspired by the human cognitive process of
modularly judging the relevance between text and video, the judgment needs
high-order matching signal due to the consecutive and complex nature of video
contents. In this paper, we propose chunk-level text-video matching, where the
query chunks are extracted to describe a specific retrieval unit, and the video
chunks are segmented into distinct clips from videos. We formulate the
chunk-level matching as n-ary correlations modeling between words of the query
and frames of the video and introduce a multi-modal hypergraph for n-ary
correlation modeling. By representing textual units and video frames as nodes
and using hyperedges to depict their relationships, a multi-modal hypergraph is
constructed. In this way, the query and the video can be aligned in a
high-order semantic space. In addition, to enhance the model's generalization
ability, the extracted features are fed into a variational inference component
for computation, obtaining the variational representation under the Gaussian
distribution. The incorporation of hypergraphs and variational inference allows
our model to capture complex, n-ary interactions among textual and visual
contents. Experimental results demonstrate that our proposed method achieves
state-of-the-art performance on the text-video retrieval task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Part-of-Speech Tagger for Bodo Language using Deep Learning approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03175v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03175v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dhrubajyoti Pathak, Sanjib Narzary, Sukumar Nandi, Bidisha Som
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Processing systems such as Part-of-speech tagging, Named entity
recognition, Machine translation, Speech recognition, and Language modeling
(LM) are well-studied in high-resource languages. Nevertheless, research on
these systems for several low-resource languages, including Bodo, Mizo,
Nagamese, and others, is either yet to commence or is in its nascent stages.
Language model plays a vital role in the downstream tasks of modern NLP.
Extensive studies are carried out on LMs for high-resource languages.
Nevertheless, languages such as Bodo, Rabha, and Mising continue to lack
coverage. In this study, we first present BodoBERT, a language model for the
Bodo language. To the best of our knowledge, this work is the first such effort
to develop a language model for Bodo. Secondly, we present an ensemble DL-based
POS tagging model for Bodo. The POS tagging model is based on combinations of
BiLSTM with CRF and stacked embedding of BodoBERT with BytePairEmbeddings. We
cover several language models in the experiment to see how well they work in
POS tagging tasks. The best-performing model achieves an F1 score of 0.8041. A
comparative experiment was also conducted on Assamese POS taggers, considering
that the language is spoken in the same region as Bodo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Natural Language Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quartet Logic: A Four-Step Reasoning (QLFR) framework for advancing
  Short Text Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Wu, Yuanben Zhang, Zhonghe Han, Yingyan Hou, Lei Wang, Siye Liu, Qihang Gong, Yunping Ge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Short Text Classification (STC) is crucial for processing and comprehending
the brief but substantial content prevalent on contemporary digital platforms.
The STC encounters difficulties in grasping semantic and syntactic intricacies,
an issue that is apparent in traditional pre-trained language models. Although
Graph Convolutional Networks enhance performance by integrating external
knowledge bases, these methods are limited by the quality and extent of the
knowledge applied. Recently, the emergence of Large Language Models (LLMs) and
Chain-of-Thought (CoT) has significantly improved the performance of complex
reasoning tasks. However, some studies have highlighted the limitations of
their application in fundamental NLP tasks. Consequently, this study sought to
employ CoT to investigate the capabilities of LLMs in STC tasks. This study
introduces Quartet Logic: A Four-Step Reasoning (QLFR) framework. This
framework primarily incorporates Syntactic and Semantic Enrichment CoT,
effectively decomposing the STC task into four distinct steps: (i) essential
concept identification, (ii) common-sense knowledge retrieval, (iii) text
rewriting, and (iv) classification. This elicits the inherent knowledge and
abilities of LLMs to address the challenges in STC. Surprisingly, we found that
QLFR can also improve the performance of smaller models. Therefore, we
developed a CoT-Driven Multi-task learning (QLFR-CML) method to facilitate the
knowledge transfer from LLMs to smaller models. Extensive experimentation
across six short-text benchmarks validated the efficacy of the proposed
methods. Notably, QLFR achieved state-of-the-art performance on all datasets,
with significant improvements, particularly on the Ohsumed and TagMyNews
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Examining Forgetting in Continual Pre-training of Aligned Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen-An Li, Hung-Yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Language Models (LLMs) have exhibited remarkable
proficiency across various tasks. Given the potent applications of LLMs in
numerous fields, there has been a surge in LLM development. In developing LLMs,
a common practice involves continual pre-training on previously fine-tuned
models. However, this can lead to catastrophic forgetting. In our work, we
investigate the phenomenon of forgetting that occurs during continual
pre-training on an existing fine-tuned LLM. We evaluate the impact of
continuous pre-training on the fine-tuned LLM across various dimensions,
including output format, knowledge, and reliability. Experiment results
highlight the non-trivial challenge of addressing catastrophic forgetting
during continual pre-training, especially the repetition issue.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ResidualTransformer: Residual Low-Rank Learning with Weight-Sharing for
  Transformer Layers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.02489v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.02489v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Wang, Jinyu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Memory constraint of always-on devices is one of the major concerns when
deploying speech processing models on these devices. While larger models
trained with sufficiently large amount of data generally perform better, making
them fit in the device memory is a demanding challenge. In this paper, we aim
to reduce model size by reparameterizing model weights across Transformer
encoder layers and assuming a special weight composition and structure. More
specifically, inspired by ResNet and the more recent LoRA work, we propose an
approach named ResidualTransformer, where each weight matrix in a Transformer
layer comprises 1) a shared full-rank component with its adjacent layers, and
2) a unique low-rank component to itself. The low-rank matrices only account
for a small amount of model size increase. In addition, we add diagonal weight
matrices to improve modeling capacity of the low-rank matrices. Experiments of
our 10k-hour speech recognition and speech translation tasks show that the
Transformer encoder size can be reduced by ~3X with very slight performance
degradation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE ICASSP 2024. 5 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Analyzing the Impact of Fake News on the Anticipated Outcome of the 2024
  Election Ahead of Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.03750v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.03750v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaina Raza, Mizanur Rahman, Shardul Ghuge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite increasing awareness and research around fake news, there is still a
significant need for datasets that specifically target racial slurs and biases
within North American political speeches. This is particulary important in the
context of upcoming North American elections. This study introduces a
comprehensive dataset that illuminates these critical aspects of
misinformation. To develop this fake news dataset, we scraped and built a
corpus of 40,000 news articles about political discourses in North America. A
portion of this dataset (4000) was then carefully annotated, using a blend of
advanced language models and human verification methods. We have made both
these datasets openly available to the research community and have conducted
benchmarking on the annotated data to demonstrate its utility. We release the
best-performing language model along with data. We encourage researchers and
developers to make use of this dataset and contribute to this ongoing
initiative.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tailoring Personality Traits in Large Language Models via
  Unsupervisedly-Built Personalized Lexicons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16582v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16582v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianlong Li, Shihan Dou, Changze Lv, Wenhao Liu, Jianhan Xu, Muling Wu, Zixuan Ling, Xiaoqing Zheng, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personality plays a pivotal role in shaping human expression patterns, thus
regulating the personality of large language models (LLMs) holds significant
potential in enhancing the user experience of LLMs. Previous methods either
relied on fine-tuning LLMs on specific corpora or necessitated manually crafted
prompts to elicit specific personalities from LLMs. However, the former
approach is inefficient and costly, while the latter cannot precisely
manipulate personality traits at a fine-grained level. To address the above
challenges, we have employed a novel Unsupervisedly-Built Personalized Lexicons
(UBPL) in a pluggable manner during the decoding phase of LLMs to manipulate
their personality traits. UBPL is a lexicon built through an unsupervised
approach from a situational judgment test dataset (SJTs4LLM). Users can utilize
UBPL to adjust the probability vectors of predicted words in the decoding phase
of LLMs, thus influencing the personality expression of LLMs. Extensive
experimentation demonstrates the remarkable effectiveness and pluggability of
our method for fine-grained manipulation of LLM's personality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Action-Item-Driven Summarization of Long Meeting Transcripts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17581v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17581v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Logan Golia, Jugal Kalita
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increased prevalence of online meetings has significantly enhanced the
practicality of a model that can automatically generate the summary of a given
meeting. This paper introduces a novel and effective approach to automate the
generation of meeting summaries. Current approaches to this problem generate
general and basic summaries, considering the meeting simply as a long dialogue.
However, our novel algorithms can generate abstractive meeting summaries that
are driven by the action items contained in the meeting transcript. This is
done by recursively generating summaries and employing our action-item
extraction algorithm for each section of the meeting in parallel. All of these
sectional summaries are then combined and summarized together to create a
coherent and action-item-driven summary. In addition, this paper introduces
three novel methods for dividing up long transcripts into topic-based sections
to improve the time efficiency of our algorithm, as well as to resolve the
issue of large language models (LLMs) forgetting long-term dependencies. Our
pipeline achieved a BERTScore of 64.98 across the AMI corpus, which is an
approximately 4.98% increase from the current state-of-the-art result produced
by a fine-tuned BART (Bidirectional and Auto-Regressive Transformers) model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted into the 7th International Conference on Natural Language
  Processing and Information Retrieval (NLPIR 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explore Spurious Correlations at the Concept Level in Language Models
  for Text Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.08648v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.08648v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Zhou, Paiheng Xu, Xiaoyu Liu, Bang An, Wei Ai, Furong Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) have achieved notable success in numerous NLP tasks,
employing both fine-tuning and in-context learning (ICL) methods. While
language models demonstrate exceptional performance, they face robustness
challenges due to spurious correlations arising from imbalanced label
distributions in training data or ICL exemplars. Previous research has
primarily concentrated on word, phrase, and syntax features, neglecting the
concept level, often due to the absence of concept labels and difficulty in
identifying conceptual content in input texts. This paper introduces two main
contributions. First, we employ ChatGPT to assign concept labels to texts,
assessing concept bias in models during fine-tuning or ICL on test data. We
find that LMs, when encountering spurious correlations between a concept and a
label in training or prompts, resort to shortcuts for predictions. Second, we
introduce a data rebalancing technique that incorporates ChatGPT-generated
counterfactual data, thereby balancing label distribution and mitigating
spurious correlations. Our method's efficacy, surpassing traditional token
removal approaches, is validated through extensive testing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 page appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-Lingual Transfer Learning for Low-Resource Speech Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00789v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00789v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sameer Khurana, Nauman Dawalatabad, Antoine Laurent, Luis Vicente, Pablo Gimeno, Victoria Mingote, James Glass
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper presents a novel three-step transfer learning framework for
enhancing cross-lingual transfer from high- to low-resource languages in the
downstream application of Automatic Speech Translation. The approach integrates
a semantic knowledge-distillation step into the existing two-step cross-lingual
transfer learning framework XLS-R. This extra step aims to encode semantic
knowledge in the multilingual speech encoder pre-trained via Self-Supervised
Learning using unlabeled speech. Our proposed three-step cross-lingual transfer
learning framework addresses the large cross-lingual transfer gap (TRFGap)
observed in the XLS-R framework between high-resource and low-resource
languages. We validate our proposal through extensive experiments and
comparisons on the CoVoST-2 benchmark, showing significant improvements in
translation performance, especially for low-resource languages, and a notable
reduction in the TRFGap.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00793v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00793v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinglong Luo, Yehong Zhang, Jiaqi Zhang, Xin Mu, Hui Wang, Yue Yu, Zenglin Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing use of large language models hosted on cloud platforms to
offer inference services, privacy concerns are escalating, especially
concerning sensitive data like investment plans and bank account details.
Secure Multi-Party Computing (SMPC) emerges as a promising solution to protect
the privacy of inference data and model parameters. However, the application of
SMPC in Privacy-Preserving Inference (PPI) for large language models,
particularly those based on the Transformer architecture, often leads to
considerable slowdowns or declines in performance. This is largely due to the
multitude of nonlinear operations in the Transformer architecture, which are
not well-suited to SMPC and difficult to circumvent or optimize effectively. To
address this concern, we introduce an advanced optimization framework called
SecFormer, to achieve fast and accurate PPI for Transformer models. By
implementing model design optimization, we successfully eliminate the high-cost
exponential and maximum operations in PPI without sacrificing model
performance. Additionally, we have developed a suite of efficient SMPC
protocols that utilize segmented polynomials, Fourier series and Goldschmidt's
method to handle other complex nonlinear functions within PPI, such as GeLU,
LayerNorm, and Softmax. Our extensive experiments reveal that SecFormer
outperforms MPCFormer in performance, showing improvements of $5.6\%$ and
$24.2\%$ for BERT$_{\text{BASE}}$ and BERT$_{\text{LARGE}}$, respectively. In
terms of efficiency, SecFormer is 3.56 and 3.58 times faster than Puma for
BERT$_{\text{BASE}}$ and BERT$_{\text{LARGE}}$, demonstrating its effectiveness
and speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12pages, 9figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalist embedding models are better at short-context clinical
  semantic search than specialized embedding models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01943v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01943v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean-Baptiste Excoffier, Tom Roehr, Alexei Figueroa, Jens-Michalis Papaioannou, Keno Bressem, Matthieu Ortala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing use of tools and solutions based on Large Language Models
(LLMs) for various tasks in the medical domain has become a prominent trend.
Their use in this highly critical and sensitive domain has thus raised
important questions about their robustness, especially in response to
variations in input, and the reliability of the generated outputs. This study
addresses these questions by constructing a textual dataset based on the
ICD-10-CM code descriptions, widely used in US hospitals and containing many
clinical terms, and their easily reproducible rephrasing. We then benchmarked
existing embedding models, either generalist or specialized in the clinical
domain, in a semantic search task where the goal was to correctly match the
rephrased text to the original description. Our results showed that generalist
models performed better than clinical models, suggesting that existing clinical
specialized models are more sensitive to small changes in input that confuse
them. The highlighted problem of specialized models may be due to the fact that
they have not been trained on sufficient data, and in particular on datasets
that are not diverse enough to have a reliable global language understanding,
which is still necessary for accurate handling of medical documents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 1 figure, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Long-Term Ad Memorability: Understanding and Generating Memorable Ads 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.00378v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.00378v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harini S I, Somesh Singh, Yaman K Singla, Aanisha Bhattacharyya, Veeky Baths, Changyou Chen, Rajiv Ratn Shah, Balaji Krishnamurthy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Marketers spend billions of dollars on advertisements but to what end? At the
time of purchase, if customers cannot recognize the brand for which they saw an
ad, the money spent on the ad is essentially wasted. Despite its importance in
marketing, until now, there has been no study on the memorability of ads in the
ML literature. Most studies have been conducted on short-term recall (<5 mins)
on specific content types like object and action videos. On the other hand, the
advertising industry only cares about long-term memorability, and ads are
almost always highly multimodal, depicting a story through its different
modalities. With this motivation, we release the first large-scale memorability
dataset, LAMDBA, consisting of 1749 participants and 2205 ads covering 276
brands. Running statistical tests over different participant subpopulations and
ad types, we find many interesting insights into what makes an ad memorable.
For e.g., we find that brands that use commercials with fast-moving scenes are
more memorable than those with slower scenes (p=8e-10) and that people who use
ad-blockers remember fewer ads than those who don't (p=5e-3). Next, to simulate
the memorability of marketing materials for a particular audience, we present a
novel model, Henry, trained to leverage real-world knowledge of LLMs and visual
knowledge to predict the memorability. We test Henry on all the prominent
memorability datasets in literature (both images and videos) and achieve
state-of-the-art performance across all of them. Henry shows strong
generalization showing better results in 0-shot on unseen datasets. Next, we
propose the task of memorable ad generation and release a large-scale ad
dataset, UltraLAMBDA, consisting of 4 million ads with their Henry-assigned
memorability scores. We show that aligning Henry to generate memorable content
improves memorability scores by more than 25%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cheetah: Natural Language Generation for 517 African Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01053v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01053v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ife Adebara, AbdelRahim Elmadany, Muhammad Abdul-Mageed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-resource African languages pose unique challenges for natural language
processing (NLP) tasks, including natural language generation (NLG). In this
paper, we develop Cheetah, a massively multilingual NLG language model for
African languages. Cheetah supports 517 African languages and language
varieties, allowing us to address the scarcity of NLG resources and provide a
solution to foster linguistic diversity. We demonstrate the effectiveness of
Cheetah through comprehensive evaluations across seven generation downstream
tasks. In five of the seven tasks, Cheetah significantly outperforms other
models, showcasing its remarkable performance for generating coherent and
contextually appropriate text in a wide range of African languages. We
additionally conduct a detailed human evaluation to delve deeper into the
linguistic capabilities of Cheetah. The introduction of Cheetah has
far-reaching benefits for linguistic diversity. By leveraging pretrained models
and adapting them to specific languages, our approach facilitates the
development of practical NLG applications for African communities. The findings
of this study contribute to advancing NLP research in low-resource settings,
enabling greater accessibility and inclusion for African languages in a rapidly
expanding digital landscape. We publicly release our models for research.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">24</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spatiotemporally adaptive compression for scientific <span class="highlight-title">dataset</span> with
  feature preservation -- a case study on simulation data with extreme climate
  events analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Gong, Chengzhu Zhang, Xin Liang, Viktor Reshniak, Jieyang Chen, Anand Rangarajan, Sanjay Ranka, Nicolas Vidal, Lipeng Wan, Paul Ullrich, Norbert Podhorszki, Robert Jacob, Scott Klasky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific discoveries are increasingly constrained by limited storage space
and I/O capacities. For time-series simulations and experiments, their data
often need to be decimated over timesteps to accommodate storage and I/O
limitations. In this paper, we propose a technique that addresses storage costs
while improving post-analysis accuracy through spatiotemporal adaptive,
error-controlled lossy compression. We investigate the trade-off between data
precision and temporal output rates, revealing that reducing data precision and
increasing timestep frequency lead to more accurate analysis outcomes.
Additionally, we integrate spatiotemporal feature detection with data
compression and demonstrate that performing adaptive error-bounded compression
in higher dimensional space enables greater compression ratios, leveraging the
error propagation theory of a transformation-based compressor.
  To evaluate our approach, we conduct experiments using the well-known E3SM
climate simulation code and apply our method to compress variables used for
cyclone tracking. Our results show a significant reduction in storage size
while enhancing the quality of cyclone tracking analysis, both quantitatively
and qualitatively, in comparison to the prevalent timestep decimation approach.
Compared to three state-of-the-art lossy compressors lacking feature
preservation capabilities, our adaptive compression framework improves
perfectly matched cases in TC tracking by 26.4-51.3% at medium compression
ratios and by 77.3-571.1% at large compression ratios, with a merely 5-11%
computational overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 13 figures, 2023 IEEE International Conference on e-Science
  and Grid Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting Data Hierarchy as a New Modality for Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03312v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03312v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arjun Bhalla, Daniel Levenson, Jan Bernhard, Anton Abilov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work investigates how hierarchically structured data can help neural
networks learn conceptual representations of cathedrals. The underlying
WikiScenes dataset provides a spatially organized hierarchical structure of
cathedral components. We propose a novel hierarchical contrastive training
approach that leverages a triplet margin loss to represent the data's spatial
hierarchy in the encoder's latent space. As such, the proposed approach
investigates if the dataset structure provides valuable information for
self-supervised learning. We apply t-SNE to visualize the resultant latent
space and evaluate the proposed approach by comparing it with other
dataset-specific contrastive learning methods using a common downstream
classification task. The proposed method outperforms the comparable
weakly-supervised and baseline methods. Our findings suggest that dataset
structure is a valuable modality for weakly-supervised learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Realism in Action: Anomaly-Aware Diagnosis of Brain Tumors from Medical
  Images Using YOLOv8 and DeiT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyed Mohammad Hossein Hashemi, Leila Safari, Amirhossein Dadashzade Taromi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of medical sciences, reliable detection and classification of
brain tumors from images remains a formidable challenge due to the rarity of
tumors within the population of patients. Therefore, the ability to detect
tumors in anomaly scenarios is paramount for ensuring timely interventions and
improved patient outcomes. This study addresses the issue by leveraging deep
learning (DL) techniques to detect and classify brain tumors in challenging
situations. The curated data set from the National Brain Mapping Lab (NBML)
comprises 81 patients, including 30 Tumor cases and 51 Normal cases. The
detection and classification pipelines are separated into two consecutive
tasks. The detection phase involved comprehensive data analysis and
pre-processing to modify the number of image samples and the number of patients
of each class to anomaly distribution (9 Normal per 1 Tumor) to comply with
real world scenarios. Next, in addition to common evaluation metrics for the
testing, we employed a novel performance evaluation method called Patient to
Patient (PTP), focusing on the realistic evaluation of the model. In the
detection phase, we fine-tuned a YOLOv8n detection model to detect the tumor
region. Subsequent testing and evaluation yielded competitive performance both
in Common Evaluation Metrics and PTP metrics. Furthermore, using the Data
Efficient Image Transformer (DeiT) module, we distilled a Vision Transformer
(ViT) model from a fine-tuned ResNet152 as a teacher in the classification
phase. This approach demonstrates promising strides in reliable tumor detection
and classification, offering potential advancements in tumor diagnosis for
real-world medical imaging scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-View 3D Instance Segmentation of Structural Anomalies for Enhanced
  Structural Inspection of Concrete Bridges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03298v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03298v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Benz, Volker Rodehorst
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For effective structural damage assessment, the instances of damages need to
be localized in the world of a 3D model. Due to a lack of data, the detection
of structural anomalies can currently not be directly learned and performed in
3D space. In this work, a three-stage approach is presented, which uses the
good performance of detection models on image level to segment instances of
anomalies in the 3D space. In the detection stage, semantic segmentation
predictions are produced on image level. The mapping stage transfers the
image-level prediction onto the respective point cloud. In the extraction
stage, 3D anomaly instances are extracted from the segmented point cloud. Cloud
contraction is used to transform cracks into their medial axis representation.
For areal anomalies the bounding polygon is extracted by means of alpha shapes.
The approach covers the classes crack, spalling, and corrosion and the three
image-level segmentation models TopoCrack, nnU-Net, and DetectionHMA are
compared. Granted a localization tolerance of 4cm, IoUs of over 90% can be
achieved for crack and corrosion and 41% for spalling, which appears to be a
specifically challenging class. Detection on instance-level measured in AP is
about 45% for crack and spalling and 73% for corrosion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real Time Human Detection by Unmanned Aerial Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Walid Guettala, Ali Sayah, Laid Kahloul, Ahmed Tibermacine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the most important problems in computer vision and remote sensing is
object detection, which identifies particular categories of diverse things in
pictures. Two crucial data sources for public security are the thermal infrared
(TIR) remote sensing multi-scenario photos and videos produced by unmanned
aerial vehicles (UAVs). Due to the small scale of the target, complex scene
information, low resolution relative to the viewable videos, and dearth of
publicly available labeled datasets and training models, their object detection
procedure is still difficult. A UAV TIR object detection framework for pictures
and videos is suggested in this study. The Forward-looking Infrared (FLIR)
cameras used to gather ground-based TIR photos and videos are used to create
the ``You Only Look Once'' (YOLO) model, which is based on CNN architecture.
Results indicated that in the validating task, detecting human object had an
average precision at IOU (Intersection over Union) = 0.5, which was 72.5\%,
using YOLOv7 (YOLO version 7) state of the art model \cite{1}, while the
detection speed around 161 frames per second (FPS/second). The usefulness of
the YOLO architecture is demonstrated in the application, which evaluates the
cross-detection performance of people in UAV TIR videos under a YOLOv7 model in
terms of the various UAVs' observation angles. The qualitative and quantitative
evaluation of object detection from TIR pictures and videos using deep-learning
models is supported favorably by this work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analysis and Validation of Image Search Engines in Histopathology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03271v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03271v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isaiah Lahr, Saghir Alfasly, Peyman Nejat, Jibran Khan, Luke Kottom, Vaishnavi Kumbhar, Areej Alsaafin, Abubakr Shafique, Sobhan Hemati, Ghazal Alabtah, Nneka Comfere, Dennis Murphee, Aaron Mangold, Saba Yasir, Chady Meroueh, Lisa Boardman, Vijay H. Shah, Joaquin J. Garcia, H. R. Tizhoosh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Searching for similar images in archives of histology and histopathology
images is a crucial task that may aid in patient matching for various purposes,
ranging from triaging and diagnosis to prognosis and prediction. Whole slide
images (WSIs) are highly detailed digital representations of tissue specimens
mounted on glass slides. Matching WSI to WSI can serve as the critical method
for patient matching. In this paper, we report extensive analysis and
validation of four search methods bag of visual words (BoVW), Yottixel, SISH,
RetCCL, and some of their potential variants. We analyze their algorithms and
structures and assess their performance. For this evaluation, we utilized four
internal datasets ($1269$ patients) and three public datasets ($1207$
patients), totaling more than $200,000$ patches from $38$ different
classes/subtypes across five primary sites. Certain search engines, for
example, BoVW, exhibit notable efficiency and speed but suffer from low
accuracy. Conversely, search engines like Yottixel demonstrate efficiency and
speed, providing moderately accurate results. Recent proposals, including SISH,
display inefficiency and yield inconsistent outcomes, while alternatives like
RetCCL prove inadequate in both accuracy and efficiency. Further research is
imperative to address the dual aspects of accuracy and minimal storage
requirements in histopathological image search.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Autonomous <span class="highlight-title">Navigation</span> in Complex Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Gerstenslager, Jomol Lewis, Liam McKenna, Poorva Patel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the application of CNN-DNN network fusion to construct a
robot navigation controller within a simulated environment. The simulated
environment is constructed to model a subterranean rescue situation, such that
an autonomous agent is tasked with finding a goal within an unknown cavernous
system. Imitation learning is used to train the control algorithm to use LiDAR
and camera data to navigate the space and find the goal. The trained model is
then tested for robustness using Monte-Carlo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures, independent paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Group Activity Recognition using Unreliable Tracked Pose 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haritha Thilakarathne, Aiden Nibali, Zhen He, Stuart Morgan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Group activity recognition in video is a complex task due to the need for a
model to recognise the actions of all individuals in the video and their
complex interactions. Recent studies propose that optimal performance is
achieved by individually tracking each person and subsequently inputting the
sequence of poses or cropped images/optical flow into a model. This helps the
model to recognise what actions each person is performing before they are
merged to arrive at the group action class. However, all previous models are
highly reliant on high quality tracking and have only been evaluated using
ground truth tracking information. In practice it is almost impossible to
achieve highly reliable tracking information for all individuals in a group
activity video. We introduce an innovative deep learning-based group activity
recognition approach called Rendered Pose based Group Activity Recognition
System (RePGARS) which is designed to be tolerant of unreliable tracking and
pose information. Experimental results confirm that RePGARS outperforms all
existing group activity recognition algorithms tested which do not use ground
truth detection and tracking information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RustNeRF: Robust Neural Radiance Field with Low-Quality Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03257v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03257v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengfei Li, Ming Lu, Xiaofang Li, Shanghang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work on Neural Radiance Fields (NeRF) exploits multi-view 3D
consistency, achieving impressive results in 3D scene modeling and
high-fidelity novel-view synthesis. However, there are limitations. First,
existing methods assume enough high-quality images are available for training
the NeRF model, ignoring real-world image degradation. Second, previous methods
struggle with ambiguity in the training set due to unmodeled inconsistencies
among different views. In this work, we present RustNeRF for real-world
high-quality NeRF. To improve NeRF's robustness under real-world inputs, we
train a 3D-aware preprocessing network that incorporates real-world degradation
modeling. We propose a novel implicit multi-view guidance to address
information loss during image degradation and restoration. Extensive
experiments demonstrate RustNeRF's advantages over existing approaches under
real-world degradation. The code will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models as Visual Cross-Domain Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03253v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03253v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuhao Chen, Yulong Zhang, Weisen Jiang, Jiangang Lu, Yu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances achieved by deep learning models rely on the independent and
identically distributed assumption, hindering their applications in real-world
scenarios with domain shifts. To address the above issues, cross-domain
learning aims at extracting domain-invariant knowledge to reduce the domain
shift between training and testing data. However, in visual cross-domain
learning, traditional methods concentrate solely on the image modality,
neglecting the use of the text modality to alleviate the domain shift. In this
work, we propose Large Language models as Visual cross-dOmain learners (LLaVO).
LLaVO uses vision-language models to convert images into detailed textual
descriptions. A large language model is then finetuned on textual descriptions
of the source/target domain generated by a designed instruction template.
Extensive experimental results on various cross-domain tasks under the domain
generalization and unsupervised domain adaptation settings have demonstrated
the effectiveness of the proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpersonal Relationship Analysis with Dyadic EEG Signals via Learning
  Spatial-Temporal Patterns 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03250v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03250v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Ji, Fang liu, Xinxin Du, Niqi Liu, Chao Zhou, Mingjin Yu, Guozhen Zhao, Yong-Jin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interpersonal relationship quality is pivotal in social and occupational
contexts. Existing analysis of interpersonal relationships mostly rely on
subjective self-reports, whereas objective quantification remains challenging.
In this paper, we propose a novel social relationship analysis framework using
spatio-temporal patterns derived from dyadic EEG signals, which can be applied
to quantitatively measure team cooperation in corporate team building, and
evaluate interpersonal dynamics between therapists and patients in psychiatric
therapy. First, we constructed a dyadic-EEG dataset from 72 pairs of
participants with two relationships (stranger or friend) when watching
emotional videos simultaneously. Then we proposed a deep neural network on
dyadic-subject EEG signals, in which we combine the dynamic graph convolutional
neural network for characterizing the interpersonal relationships among the EEG
channels and 1-dimension convolution for extracting the information from the
time sequence. To obtain the feature vectors from two EEG recordings that well
represent the relationship of two subjects, we integrate deep canonical
correlation analysis and triplet loss for training the network. Experimental
results show that the social relationship type (stranger or friend) between two
individuals can be effectively identified through their EEG data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MirrorDiffusion: Stabilizing Diffusion Process in Zero-shot Image
  Translation by Prompts Redescription and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03221v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03221v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yupei Lin, Xiaoyu Xian, Yukai Shi, Liang Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, text-to-image diffusion models become a new paradigm in image
processing fields, including content generation, image restoration and
image-to-image translation. Given a target prompt, Denoising Diffusion
Probabilistic Models (DDPM) are able to generate realistic yet eligible images.
With this appealing property, the image translation task has the potential to
be free from target image samples for supervision. By using a target text
prompt for domain adaption, the diffusion model is able to implement zero-shot
image-to-image translation advantageously. However, the sampling and inversion
processes of DDPM are stochastic, and thus the inversion process often fail to
reconstruct the input content. Specifically, the displacement effect will
gradually accumulated during the diffusion and inversion processes, which led
to the reconstructed results deviating from the source domain. To make
reconstruction explicit, we propose a prompt redescription strategy to realize
a mirror effect between the source and reconstructed image in the diffusion
model (MirrorDiffusion). More specifically, a prompt redescription mechanism is
investigated to align the text prompts with latent code at each time step of
the Denoising Diffusion Implicit Models (DDIM) inversion to pursue a
structure-preserving reconstruction. With the revised DDIM inversion,
MirrorDiffusion is able to realize accurate zero-shot image translation by
editing optimized text prompts and latent code. Extensive experiments
demonstrate that MirrorDiffusion achieves superior performance over the
state-of-the-art methods on zero-shot image translation benchmarks by clear
margins and practical model stability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A prompt re-description strategy is proposed for stabilizing the
  diffusion model in image-to-image translation. Code and dataset page:
  https://mirrordiffusion.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MetaISP -- Exploiting Global Scene Structure for Accurate Multi-Device
  Color Rendition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03220v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03220v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matheus Souza, Wolfgang Heidrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image signal processors (ISPs) are historically grown legacy software systems
for reconstructing color images from noisy raw sensor measurements. Each
smartphone manufacturer has developed its ISPs with its own characteristic
heuristics for improving the color rendition, for example, skin tones and other
visually essential colors. The recent interest in replacing the historically
grown ISP systems with deep-learned pipelines to match DSLR's image quality
improves structural features in the image. However, these works ignore the
superior color processing based on semantic scene analysis that distinguishes
mobile phone ISPs from DSLRs. Here, we present MetaISP, a single model designed
to learn how to translate between the color and local contrast characteristics
of different devices. MetaISP takes the RAW image from device A as input and
translates it to RGB images that inherit the appearance characteristics of
devices A, B, and C. We achieve this result by employing a lightweight deep
learning technique that conditions its output appearance based on the device of
interest. In this approach, we leverage novel attention mechanisms inspired by
cross-covariance to learn global scene semantics. Additionally, we use the
metadata that typically accompanies RAW images and estimate scene illuminants
when they are unavailable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>VMV 2023, Project page: https://www.github.com/vccimaging/MetaISP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hi-Map: Hierarchical Factorized Radiance Field for High-Fidelity
  Monocular Dense Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongyan Hua, Haotian Bai, Zidong Cao, Ming Liu, Dacheng Tao, Lin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce Hi-Map, a novel monocular dense mapping approach
based on Neural Radiance Field (NeRF). Hi-Map is exceptional in its capacity to
achieve efficient and high-fidelity mapping using only posed RGB inputs. Our
method eliminates the need for external depth priors derived from e.g., a depth
estimation model. Our key idea is to represent the scene as a hierarchical
feature grid that encodes the radiance and then factorizes it into feature
planes and vectors. As such, the scene representation becomes simpler and more
generalizable for fast and smooth convergence on new observations. This allows
for efficient computation while alleviating noise patterns by reducing the
complexity of the scene representation. Buttressed by the hierarchical
factorized representation, we leverage the Sign Distance Field (SDF) as a proxy
of rendering for inferring the volume density, demonstrating high mapping
fidelity. Moreover, we introduce a dual-path encoding strategy to strengthen
the photometric cues and further boost the mapping quality, especially for the
distant and textureless regions. Extensive experiments demonstrate our method's
superiority in geometric and textural accuracy over the state-of-the-art
NeRF-based monocular mapping methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3DMIT: 3D Multi-modal Instruction Tuning for Scene Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03201v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03201v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeju Li, Chao Zhang, Xiaoyan Wang, Ruilong Ren, Yifan Xu, Ruifei Ma, Xiangde Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable potential of multi-modal large language models (MLLMs) in
comprehending both vision and language information has been widely
acknowledged. However, the scarcity of 3D scenes-language pairs in comparison
to their 2D counterparts, coupled with the inadequacy of existing approaches in
understanding of 3D scenes by LLMs, poses a significant challenge. In response,
we collect and construct an extensive dataset comprising 75K
instruction-response pairs tailored for 3D scenes. This dataset addresses tasks
related to 3D VQA, 3D grounding, and 3D conversation. To further enhance the
integration of 3D spatial information into LLMs, we introduce a novel and
efficient prompt tuning paradigm, 3DMIT. This paradigm eliminates the alignment
stage between 3D scenes and language and extends the instruction prompt with
the 3D modality information including the entire scene and segmented objects.
We evaluate the effectiveness of our method across diverse tasks in the 3D
scene domain and find that our approach serves as a strategic means to enrich
LLMs' comprehension of the 3D world. Our code is available at
https://github.com/staymylove/3DMIT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Bitrate Ladder Construction using Transfer Learning and
  Spatio-Temporal Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Falahati, Mohammad Karim Safavi, Ardavan Elahi, Farhad Pakdaman, Moncef Gabbouj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Providing high-quality video with efficient bitrate is a main challenge in
video industry. The traditional one-size-fits-all scheme for bitrate ladders is
inefficient and reaching the best content-aware decision computationally
impractical due to extensive encodings required. To mitigate this, we propose a
bitrate and complexity efficient bitrate ladder prediction method using
transfer learning and spatio-temporal features. We propose: (1) using feature
maps from well-known pre-trained DNNs to predict rate-quality behavior with
limited training data; and (2) improving highest quality rung efficiency by
predicting minimum bitrate for top quality and using it for the top rung. The
method tested on 102 video scenes demonstrates 94.1% reduction in complexity
versus brute-force at 1.71% BD-Rate expense. Additionally, transfer learning
was thoroughly studied through four networks and ablation studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 9 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DistFormer: Enhancing Local and Global Features for Monocular Per-Object
  Distance Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03191v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03191v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aniello Panariello, Gianluca Mancusi, Fedy Haj Ali, Angelo Porrello, Simone Calderara, Rita Cucchiara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate per-object distance estimation is crucial in safety-critical
applications such as autonomous driving, surveillance, and robotics. Existing
approaches rely on two scales: local information (i.e., the bounding box
proportions) or global information, which encodes the semantics of the scene as
well as the spatial relations with neighboring objects. However, these
approaches may struggle with long-range objects and in the presence of strong
occlusions or unusual visual patterns. In this respect, our work aims to
strengthen both local and global cues. Our architecture -- named DistFormer --
builds upon three major components acting jointly: i) a robust context encoder
extracting fine-grained per-object representations; ii) a masked
encoder-decoder module exploiting self-supervision to promote the learning of
useful per-object features; iii) a global refinement module that aggregates
object representations and computes a joint, spatially-consistent estimation.
To evaluate the effectiveness of DistFormer, we conduct experiments on the
standard KITTI dataset and the large-scale NuScenes and MOTSynth datasets. Such
datasets cover various indoor/outdoor environments, changing weather
conditions, appearances, and camera viewpoints. Our comprehensive analysis
shows that DistFormer outperforms existing methods. Moreover, we further delve
into its generalization capabilities, showing its regularization benefits in
zero-shot synth-to-real transfer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MPN: Leveraging Multilingual Patch Neuron for Cross-lingual Model
  Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03190v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03190v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nianwen Si, Hao Zhang, Weiqiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models are known for encoding a vast amount of factual
knowledge, but they often becomes outdated due to the ever-changing nature of
external information. A promising solution to this challenge is the utilization
of model editing methods to update the knowledge in an efficient manner.
However, the majority of existing model editing techniques are limited to
monolingual frameworks, thus failing to address the crucial issue of
cross-lingual knowledge synchronization for multilingual models. To tackle this
problem, we propose a simple yet effective method that trains multilingual
patch neuron to store cross-lingual knowledge. It can be easily adapted to
existing approaches to enhance their cross-lingual editing capabilities. To
evaluate our method, we conduct experiments using both the XNLI dataset and a
self-constructed XFEVER dataset. Experimental results demonstrate that our
proposed method achieves improved performance in cross-lingual editing tasks
without requiring excessive modifications to the original methodology, thereby
showcasing its user-friendly characteristics. Codes will be released soon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distribution-aware Interactive Attention Network and Large-scale Cloud
  Recognition <span class="highlight-title">Benchmark</span> on FY-4A Satellite Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqing Zhang, Jie Lei, Weiying Xie, Kai Jiang, Mingxiang Cao, Yunsong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate cloud recognition and warning are crucial for various applications,
including in-flight support, weather forecasting, and climate research.
However, recent deep learning algorithms have predominantly focused on
detecting cloud regions in satellite imagery, with insufficient attention to
the specificity required for accurate cloud recognition. This limitation
inspired us to develop the novel FY-4A-Himawari-8 (FYH) dataset, which includes
nine distinct cloud categories and uses precise domain adaptation methods to
align 70,419 image-label pairs in terms of projection, temporal resolution, and
spatial resolution, thereby facilitating the training of supervised deep
learning networks. Given the complexity and diversity of cloud formations, we
have thoroughly analyzed the challenges inherent to cloud recognition tasks,
examining the intricate characteristics and distribution of the data. To
effectively address these challenges, we designed a Distribution-aware
Interactive-Attention Network (DIAnet), which preserves pixel-level details
through a high-resolution branch and a parallel multi-resolution cross-branch.
We also integrated a distribution-aware loss (DAL) to mitigate the imbalance
across cloud categories. An Interactive Attention Module (IAM) further enhances
the robustness of feature extraction combined with spatial and channel
information. Empirical evaluations on the FYH dataset demonstrate that our
method outperforms other cloud recognition networks, achieving superior
performance in terms of mean Intersection over Union (mIoU). The code for
implementing DIAnet is available at https://github.com/icey-zhang/DIAnet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Informative ViT: Information Aggregation and Distribution for
  Hyperspectral and LiDAR Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqing Zhang, Jie Lei, Weiying Xie, Geng Yang, Daixun Li, Yunsong Li, Karim Seghouane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In multimodal land cover classification (MLCC), a common challenge is the
redundancy in data distribution, where irrelevant information from multiple
modalities can hinder the effective integration of their unique features. To
tackle this, we introduce the Multimodal Informative Vit (MIVit), a system with
an innovative information aggregate-distributing mechanism. This approach
redefines redundancy levels and integrates performance-aware elements into the
fused representation, facilitating the learning of semantics in both forward
and backward directions. MIVit stands out by significantly reducing redundancy
in the empirical distribution of each modality's separate and fused features.
It employs oriented attention fusion (OAF) for extracting shallow local
features across modalities in horizontal and vertical dimensions, and a
Transformer feature extractor for extracting deep global features through
long-range attention. We also propose an information aggregation constraint
(IAC) based on mutual information, designed to remove redundant information and
preserve complementary information within embedded features. Additionally, the
information distribution flow (IDF) in MIVit enhances performance-awareness by
distributing global classification information across different modalities'
feature maps. This architecture also addresses missing modality challenges with
lightweight independent modality classifiers, reducing the computational load
typically associated with Transformers. Our results show that MIVit's
bidirectional aggregate-distributing mechanism between modalities is highly
effective, achieving an average overall accuracy of 95.56% across three
multimodal datasets. This performance surpasses current state-of-the-art
methods in MLCC. The code for MIVit is accessible at
https://github.com/icey-zhang/MIViT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representation Recycling for Streaming Video Analysis <span class="chip">ECCV2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.13492v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.13492v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Can Ufuk Ertenli, Ramazan Gokberk Cinbis, Emre Akbas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present StreamDEQ, a method that aims to infer frame-wise representations
on videos with minimal per-frame computation. Conventional deep networks do
feature extraction from scratch at each frame in the absence of ad-hoc
solutions. We instead aim to build streaming recognition models that can
natively exploit temporal smoothness between consecutive video frames. We
observe that the recently emerging implicit layer models provide a convenient
foundation to construct such models, as they define representations as the
fixed-points of shallow networks, which need to be estimated using iterative
methods. Our main insight is to distribute the inference iterations over the
temporal axis by using the most recent representation as a starting point at
each frame. This scheme effectively recycles the recent inference computations
and greatly reduces the needed processing time. Through extensive experimental
analysis, we show that StreamDEQ is able to recover near-optimal
representations in a few frames' time and maintain an up-to-date representation
throughout the video duration. Our experiments on video semantic segmentation,
video object detection, and human pose estimation in videos show that StreamDEQ
achieves on-par accuracy with the baseline while being more than 2-4x faster.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v3: ECCV2022 paper. This version: extended version under review at
  TPAMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Implicit Shape Modeling for Anatomical Structure Refinement of
  Volumetric Medical Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06164v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06164v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghui Zhang, Hanxiao Zhang, Xin You, Guang-Zhong Yang, Yun Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shape modeling of volumetric data is essential for medical image analysis and
computer-aided intervention. In practice, automated shape reconstruction cannot
always achieve satisfactory results due to limited image resolution and a lack
of sufficiently detailed shape priors used as constraints. In this paper, a
unified framework is proposed for 3D shape modelling and segmentation
refinement based on implicit neural networks. To learn a sharable shape prior
from different instances within the same category during training, physical
details of volumetric data are firstly used to construct Physical-Informed
Continuous Coordinate Transform (PICCT) for implicit shape modeling. For
improved shape representation, implicit shape constraints based on Signed
Distance Function (SDF) are used for both instances and latent templates. For
inference, a Template Interaction Module (TIM) is proposed to refine 3D shapes
produced by Convolutional Neural Networks (CNNs) via deforming deep implicit
templates with latent codes. Experimental results on validation datasets
involving liver, pancreas and lung segmentation demonstrate the superiority of
our approach in shape refinement and reconstruction. The Chamfer Distance/Earth
Mover's Distance achieved by the proposed method are 0.232/0.087 for the Liver
dataset, 0.128/0.069 for the Pancreas dataset, and 0.417/0.100 for the Lung
Lobe dataset, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assessing Neural Network Robustness via Adversarial Pivotal Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09782v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09782v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter Ebert Christensen, Vésteinn Snæbjarnarson, Andrea Dittadi, Serge Belongie, Sagie Benaim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The robustness of image classifiers is essential to their deployment in the
real world. The ability to assess this resilience to manipulations or
deviations from the training data is thus crucial. These modifications have
traditionally consisted of minimal changes that still manage to fool
classifiers, and modern approaches are increasingly robust to them. Semantic
manipulations that modify elements of an image in meaningful ways have thus
gained traction for this purpose. However, they have primarily been limited to
style, color, or attribute changes. While expressive, these manipulations do
not make use of the full capabilities of a pretrained generative model. In this
work, we aim to bridge this gap. We show how a pretrained image generator can
be used to semantically manipulate images in a detailed, diverse, and
photorealistic way while still preserving the class of the original image.
Inspired by recent GAN-based image inversion methods, we propose a method
called Adversarial Pivotal Tuning (APT). Given an image, APT first finds a
pivot latent space input that reconstructs the image using a pretrained
generator. It then adjusts the generator's weights to create small yet semantic
manipulations in order to fool a pretrained classifier. APT preserves the full
expressive editing capabilities of the generative model. We demonstrate that
APT is capable of a wide range of class-preserving semantic image manipulations
that fool a variety of pretrained classifiers. Finally, we show that
classifiers that are robust to other benchmarks are not robust to APT
manipulations and suggest a method to improve them. Code available at:
https://captaine.github.io/apt/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Major changes include new experiments in Table 1 on page 5 and Table
  2-4 on page 6, new figure 5 on page 8. Paper accepted at WACV (oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transformer-based stereo-aware 3D object detection from binocular images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.11906v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.11906v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanqing Sun, Yanwei Pang, Jiale Cao, Jin Xie, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have shown promising progress in various visual object detection
tasks, including monocular 2D/3D detection and surround-view 3D detection. More
importantly, the attention mechanism in the Transformer model and the image
correspondence in binocular stereo are both similarity-based. However, directly
applying existing Transformer-based detectors to binocular stereo 3D object
detection leads to slow convergence and significant precision drops. We argue
that a key cause of this defect is that existing Transformers ignore the
stereo-specific image correspondence information. In this paper, we explore the
model design of Transformers in binocular 3D object detection, focusing
particularly on extracting and encoding the task-specific image correspondence
information. To achieve this goal, we present TS3D, a Transformer-based
Stereo-aware 3D object detector. In the TS3D, a Disparity-Aware Positional
Encoding (DAPE) module is proposed to embed the image correspondence
information into stereo features. The correspondence is encoded as normalized
sub-pixel-level disparity and is used in conjunction with sinusoidal 2D
positional encoding to provide the 3D location information of the scene. To
extract enriched multi-scale stereo features, we propose a Stereo Preserving
Feature Pyramid Network (SPFPN). The SPFPN is designed to preserve the
correspondence information while fusing intra-scale and aggregating cross-scale
stereo features. Our proposed TS3D achieves a 41.29% Moderate Car detection
average precision on the KITTI test set and takes 88 ms to detect objects from
each binocular image pair. It is competitive with advanced counterparts in
terms of both precision and inference speed.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">9</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Designing a Socially Assistive Robot to Support Older Adults with Low
  Vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emily Zhou, Zhonghao Shi, Xiaoyang Qiao, Maja J Matarić, Ava K Bittner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Socially assistive robots (SARs) have shown great promise in supplementing
and augmenting interventions to support the physical and mental well-being of
older adults. However, past work has not yet explored the potential of applying
SAR to lower the barriers of long-term low vision rehabilitation (LVR)
interventions for older adults. In this work, we present a user-informed design
process to validate the motivation and identify major design principles for
developing SAR for long-term LVR. To evaluate user-perceived usefulness and
acceptance of SAR in this novel domain, we performed a two-phase study through
user surveys. First, a group (n=38) of older adults with LV completed a
mailed-in survey. Next, a new group (n=13) of older adults with LV saw an
in-clinic SAR demo and then completed the survey. The study participants
reported that SARs would be useful, trustworthy, easy to use, and enjoyable
while providing socio-emotional support to augment LVR interventions. The
in-clinic demo group reported significantly more positive opinions of the SAR's
capabilities than did the baseline survey group that used mailed-in forms
without the SAR demo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Social Robotics: 13th International Conference, ICSR
  2021. Springer International Publishing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MOTO: Offline Pre-training to Online Fine-tuning for Model-based Robot
  Learning <span class="chip">CoRL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Rafailov, Kyle Hatch, Victor Kolev, John D. Martin, Mariano Phielipp, Chelsea Finn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of offline pre-training and online fine-tuning for
reinforcement learning from high-dimensional observations in the context of
realistic robot tasks. Recent offline model-free approaches successfully use
online fine-tuning to either improve the performance of the agent over the data
collection policy or adapt to novel tasks. At the same time, model-based RL
algorithms have achieved significant progress in sample efficiency and the
complexity of the tasks they can solve, yet remain under-utilized in the
fine-tuning setting. In this work, we argue that existing model-based offline
RL methods are not suitable for offline-to-online fine-tuning in
high-dimensional domains due to issues with distribution shifts, off-dynamics
data, and non-stationary rewards. We propose an on-policy model-based method
that can efficiently reuse prior data through model-based value expansion and
policy regularization, while preventing model exploitation by controlling
epistemic uncertainty. We find that our approach successfully solves tasks from
the MetaWorld benchmark, as well as the Franka Kitchen robot manipulation
environment completely from images. To the best of our knowledge, MOTO is the
first method to solve this environment from pixels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is an updated version of a manuscript that originally appeared
  at CoRL 2023. The project website is here https://sites.google.com/view/mo2o</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Theoretical Framework for the Optimization of Microphone Array
  Configuration for Humanoid Robot Audition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vladimir Tourbabin, Boaz Rafaely
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An important aspect of a humanoid robot is audition. Previous work has
presented robot systems capable of sound localization and source segregation
based on microphone arrays with various configurations. However, no theoretical
framework for the design of these arrays has been presented. In the current
paper, a design framework is proposed based on a novel array quality measure.
The measure is based on the effective rank of a matrix composed of the
generalized head related transfer functions (GHRTFs) that account for
microphone positions other than the ears. The measure is shown to be
theoretically related to standard array performance measures such as
beamforming robustness and DOA estimation accuracy. Then, the measure is
applied to produce sample designs of microphone arrays. Their performance is
investigated numerically, verifying the advantages of array design based on the
proposed theoretical framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Autonomous <span class="highlight-title">Navigation</span> in Complex Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Gerstenslager, Jomol Lewis, Liam McKenna, Poorva Patel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the application of CNN-DNN network fusion to construct a
robot navigation controller within a simulated environment. The simulated
environment is constructed to model a subterranean rescue situation, such that
an autonomous agent is tasked with finding a goal within an unknown cavernous
system. Imitation learning is used to train the control algorithm to use LiDAR
and camera data to navigate the space and find the goal. The trained model is
then tested for robustness using Monte-Carlo.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures, independent paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Challenges of Data-Driven Simulation of Diverse and Consistent Human
  Driving Behaviors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03236v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03236v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kalle Kujanpää, Daulet Baimukashev, Shibei Zhu, Shoaib Azam, Farzeen Munir, Gokhan Alcan, Ville Kyrki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building simulation environments for developing and testing autonomous
vehicles necessitates that the simulators accurately model the statistical
realism of the real-world environment, including the interaction with other
vehicles driven by human drivers. To address this requirement, an accurate
human behavior model is essential to incorporate the diversity and consistency
of human driving behavior. We propose a mathematical framework for designing a
data-driven simulation model that simulates human driving behavior more
realistically than the currently used physics-based simulation models.
Experiments conducted using the NGSIM dataset validate our hypothesis regarding
the necessity of considering the complexity, diversity, and consistency of
human driving behavior when aiming to develop realistic simulators.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Large-Language Model (LLM)-powered Human-Robot Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03217v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03217v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Callie Y. Kim, Christine P. Lee, Bilge Mutlu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-language models (LLMs) hold significant promise in improving
human-robot interaction, offering advanced conversational skills and
versatility in managing diverse, open-ended user requests in various tasks and
domains. Despite the potential to transform human-robot interaction, very
little is known about the distinctive design requirements for utilizing LLMs in
robots, which may differ from text and voice interaction and vary by task and
context. To better understand these requirements, we conducted a user study (n
= 32) comparing an LLM-powered social robot against text- and voice-based
agents, analyzing task-based requirements in conversational tasks, including
choose, generate, execute, and negotiate. Our findings show that LLM-powered
robots elevate expectations for sophisticated non-verbal cues and excel in
connection-building and deliberation, but fall short in logical communication
and may induce anxiety. We provide design implications both for robots
integrating LLMs and for fine-tuning LLMs for use with robots.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures. Callie Y. Kim and Christine P. Lee contributed
  equally to the work. To be published in Proceedings of the 2024 ACM/IEEE
  International Conference on Human-Robot Interaction (HRI '24), March 11--14,
  2024, Boulder, CO, USA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human as AI Mentor: Enhanced Human-in-the-loop Reinforcement Learning
  for Safe and Efficient Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zilin Huang, Zihao Sheng, Chengyuan Ma, Sikai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant progress in autonomous vehicles (AVs), the development of
driving policies that ensure both the safety of AVs and traffic flow efficiency
has not yet been fully explored. In this paper, we propose an enhanced
human-in-the-loop reinforcement learning method, termed the Human as AI
mentor-based deep reinforcement learning (HAIM-DRL) framework, which
facilitates safe and efficient autonomous driving in mixed traffic platoon.
Drawing inspiration from the human learning process, we first introduce an
innovative learning paradigm that effectively injects human intelligence into
AI, termed Human as AI mentor (HAIM). In this paradigm, the human expert serves
as a mentor to the AI agent. While allowing the agent to sufficiently explore
uncertain environments, the human expert can take control in dangerous
situations and demonstrate correct actions to avoid potential accidents. On the
other hand, the agent could be guided to minimize traffic flow disturbance,
thereby optimizing traffic flow efficiency. In detail, HAIM-DRL leverages data
collected from free exploration and partial human demonstrations as its two
training sources. Remarkably, we circumvent the intricate process of manually
designing reward functions; instead, we directly derive proxy state-action
values from partial human demonstrations to guide the agents' policy learning.
Additionally, we employ a minimal intervention technique to reduce the human
mentor's cognitive load. Comparative results show that HAIM-DRL outperforms
traditional methods in driving safety, sampling efficiency, mitigation of
traffic flow disturbance, and generalizability to unseen traffic scenarios. The
code and demo videos for this paper can be accessed at:
https://zilin-huang.github.io/HAIM-DRL-website/}{https://zilin-huang.github.io/HAIM-DRL-website/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decentralized Multi-Agent Active Search and Tracking when Targets
  Outnumber Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03154v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03154v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arundhati Banerjee, Jeff Schneider
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent multi-target tracking has a wide range of applications, including
wildlife patrolling, security surveillance or environment monitoring. Such
algorithms often make restrictive assumptions: the number of targets and/or
their initial locations may be assumed known, or agents may be pre-assigned to
monitor disjoint partitions of the environment, reducing the burden of
exploration. This also limits applicability when there are fewer agents than
targets, since agents are unable to continuously follow the targets in their
fields of view. Multi-agent tracking algorithms additionally assume inter-agent
synchronization of observations, or the presence of a central controller to
coordinate joint actions. Instead, we focus on the setting of decentralized
multi-agent, multi-target, simultaneous active search-and-tracking with
asynchronous inter-agent communication. Our proposed algorithm DecSTER uses a
sequential monte carlo implementation of the probability hypothesis density
filter for posterior inference combined with Thompson sampling for
decentralized multi-agent decision making. We compare different action
selection policies, focusing on scenarios where targets outnumber agents. In
simulation, we demonstrate that DecSTER is robust to unreliable inter-agent
communication and outperforms information-greedy baselines in terms of the
Optimal Sub-Pattern Assignment (OSPA) metric for different numbers of targets
and varying teamsizes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimating the Lateral Motion States of an Underwater Robot by Propeller
  Wake Sensing Using an Artificial Lateral Line 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03141v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03141v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Wang, Dexin Zhao, Youxi Zhao, Feitian Zhang, Tongsheng Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An artificial lateral line (ALL) is a bioinspired flow sensing system of an
underwater robot that consists of distributed flow sensors. The ALL has
achieved great success in sensing the motion states of bioinspired underwater
robots, e.g., robotic fish, that are driven by body undulation and/or tail
flapping. However, the ALL has not been systematically tested and studied in
the sensing of underwater robots driven by rotating propellers due to the
highly dynamic and complex flow field therein. This paper makes a bold
hypothesis that the distributed flow measurements sampled from the propeller
wake flow, although infeasible to represent the entire flow dynamics, provides
sufficient information for estimating the lateral motion states of the leader
underwater robot. An experimental testbed is constructed to investigate the
feasibility of such a state estimator which comprises a cylindrical ALL sensory
system, a rotating leader propeller, and a water tank with a planar sliding
guide. Specifically, a hybrid network that consists of a one-dimensional
convolution network (1DCNN) and a bidirectional long short-term memory network
(BiLSTM) is designed to extract the spatiotemporal features of the time series
of distributed pressure measurements. A multi-output deep learning network is
adopted to estimate the lateral motion states of the leader propeller. In
addition, the state estimator is optimized using the whale optimization
algorithm (WOA) considering the comprehensive estimation performance. Extensive
experiments are conducted the results of which validate the proposed
data-driven algorithm in estimating the motion states of the leader underwater
robot by propeller wake sensing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3DMIT: 3D Multi-modal Instruction Tuning for Scene Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03201v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03201v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeju Li, Chao Zhang, Xiaoyan Wang, Ruilong Ren, Yifan Xu, Ruifei Ma, Xiangde Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable potential of multi-modal large language models (MLLMs) in
comprehending both vision and language information has been widely
acknowledged. However, the scarcity of 3D scenes-language pairs in comparison
to their 2D counterparts, coupled with the inadequacy of existing approaches in
understanding of 3D scenes by LLMs, poses a significant challenge. In response,
we collect and construct an extensive dataset comprising 75K
instruction-response pairs tailored for 3D scenes. This dataset addresses tasks
related to 3D VQA, 3D grounding, and 3D conversation. To further enhance the
integration of 3D spatial information into LLMs, we introduce a novel and
efficient prompt tuning paradigm, 3DMIT. This paradigm eliminates the alignment
stage between 3D scenes and language and extends the instruction prompt with
the 3D modality information including the entire scene and segmented objects.
We evaluate the effectiveness of our method across diverse tasks in the 3D
scene domain and find that our approach serves as a strategic means to enrich
LLMs' comprehension of the 3D world. Our code is available at
https://github.com/staymylove/3DMIT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Bitrate Ladder Construction using Transfer Learning and
  Spatio-Temporal Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Falahati, Mohammad Karim Safavi, Ardavan Elahi, Farhad Pakdaman, Moncef Gabbouj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Providing high-quality video with efficient bitrate is a main challenge in
video industry. The traditional one-size-fits-all scheme for bitrate ladders is
inefficient and reaching the best content-aware decision computationally
impractical due to extensive encodings required. To mitigate this, we propose a
bitrate and complexity efficient bitrate ladder prediction method using
transfer learning and spatio-temporal features. We propose: (1) using feature
maps from well-known pre-trained DNNs to predict rate-quality behavior with
limited training data; and (2) improving highest quality rung efficiency by
predicting minimum bitrate for top quality and using it for the top rung. The
method tested on 102 video scenes demonstrates 94.1% reduction in complexity
versus brute-force at 1.71% BD-Rate expense. Additionally, transfer learning
was thoroughly studied through four networks and ablation studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 9 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transferable Learned Image Compression-Resistant Adversarial
  Perturbations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03115v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03115v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Sui, Zhuohang Li, Ding Ding, Xiang Pan, Xiaozhong Xu, Shan Liu, Zhenzhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial attacks can readily disrupt the image classification system,
revealing the vulnerability of DNN-based recognition tasks. While existing
adversarial perturbations are primarily applied to uncompressed images or
compressed images by the traditional image compression method, i.e., JPEG,
limited studies have investigated the robustness of models for image
classification in the context of DNN-based image compression. With the rapid
evolution of advanced image compression, DNN-based learned image compression
has emerged as the promising approach for transmitting images in many
security-critical applications, such as cloud-based face recognition and
autonomous driving, due to its superior performance over traditional
compression. Therefore, there is a pressing need to fully investigate the
robustness of a classification system post-processed by learned image
compression. To bridge this research gap, we explore the adversarial attack on
a new pipeline that targets image classification models that utilize learned
image compressors as pre-processing modules. Furthermore, to enhance the
transferability of perturbations across various quality levels and
architectures of learned image compression models, we introduce a saliency
score-based sampling method to enable the fast generation of transferable
perturbation. Extensive experiments with popular attack methods demonstrate the
enhanced transferability of our proposed method when attacking images that have
been post-processed with different learned image compression models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as poster at Data Compression Conference 2024 (DCC 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incorporating Visual Experts to Resolve the Information Loss in
  Multimodal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin He, Longhui Wei, Lingxi Xie, Qi Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) are experiencing rapid growth,
yielding a plethora of noteworthy contributions in recent months. The
prevailing trend involves adopting data-driven methodologies, wherein diverse
instruction-following datasets are collected. However, a prevailing challenge
persists in these approaches, specifically in relation to the limited visual
perception ability, as CLIP-like encoders employed for extracting visual
information from inputs. Though these encoders are pre-trained on billions of
image-text pairs, they still grapple with the information loss dilemma, given
that textual captions only partially capture the contents depicted in images.
To address this limitation, this paper proposes to improve the visual
perception ability of MLLMs through a mixture-of-experts knowledge enhancement
mechanism. Specifically, we introduce a novel method that incorporates
multi-task encoders and visual tools into the existing MLLMs training and
inference pipeline, aiming to provide a more comprehensive and accurate
summarization of visual inputs. Extensive experiments have evaluated its
effectiveness of advancing MLLMs, showcasing improved visual perception
achieved through the integration of visual experts.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-01-05T00:00:00Z">2024-01-05</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">28</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepSeek LLM: Scaling Open-Source Language Models with Longtermism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         DeepSeek-AI,  :, Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, Huazuo Gao, Kaige Gao, Wenjun Gao, Ruiqi Ge, Kang Guan, Daya Guo, Jianzhong Guo, Guangbo Hao, Zhewen Hao, Ying He, Wenjie Hu, Panpan Huang, Erhang Li, Guowei Li, Jiashi Li, Yao Li, Y. K. Li, Wenfeng Liang, Fangyun Lin, A. X. Liu, Bo Liu, Wen Liu, Xiaodong Liu, Xin Liu, Yiyuan Liu, Haoyu Lu, Shanghao Lu, Fuli Luo, Shirong Ma, Xiaotao Nie, Tian Pei, Yishi Piao, Junjie Qiu, Hui Qu, Tongzheng Ren, Zehui Ren, Chong Ruan, Zhangli Sha, Zhihong Shao, Junxiao Song, Xuecheng Su, Jingxiang Sun, Yaofeng Sun, Minghui Tang, Bingxuan Wang, Peiyi Wang, Shiyu Wang, Yaohui Wang, Yongji Wang, Tong Wu, Y. Wu, Xin Xie, Zhenda Xie, Ziwei Xie, Yiliang Xiong, Hanwei Xu, R. X. Xu, Yanhong Xu, Dejian Yang, Yuxiang You, Shuiping Yu, Xingkai Yu, B. Zhang, Haowei Zhang, Lecong Zhang, Liyue Zhang, Mingchuan Zhang, Minghua Zhang, Wentao Zhang, Yichao Zhang, Chenggang Zhao, Yao Zhao, Shangyan Zhou, Shunfeng Zhou, Qihao Zhu, Yuheng Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of open-source large language models (LLMs) has been
truly remarkable. However, the scaling law described in previous literature
presents varying conclusions, which casts a dark cloud over scaling LLMs. We
delve into the study of scaling laws and present our distinctive findings that
facilitate scaling of large scale models in two commonly used open-source
configurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek
LLM, a project dedicated to advancing open-source language models with a
long-term perspective. To support the pre-training phase, we have developed a
dataset that currently consists of 2 trillion tokens and is continuously
expanding. We further conduct supervised fine-tuning (SFT) and Direct
Preference Optimization (DPO) on DeepSeek LLM Base models, resulting in the
creation of DeepSeek Chat models. Our evaluation results demonstrate that
DeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in
the domains of code, mathematics, and reasoning. Furthermore, open-ended
evaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance
compared to GPT-3.5.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards ASR Robust Spoken Language Understanding Through In-Context
  Learning With Word Confusion Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Everson, Yile Gu, Huck Yang, Prashanth Gurunath Shivakumar, Guan-Ting Lin, Jari Kolehmainen, Ivan Bulyko, Ankur Gandhe, Shalini Ghosh, Wael Hamza, Hung-yi Lee, Ariya Rastrow, Andreas Stolcke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of spoken language understanding (SLU), numerous natural
language understanding (NLU) methodologies have been adapted by supplying large
language models (LLMs) with transcribed speech instead of conventional written
text. In real-world scenarios, prior to input into an LLM, an automated speech
recognition (ASR) system generates an output transcript hypothesis, where
inherent errors can degrade subsequent SLU tasks. Here we introduce a method
that utilizes the ASR system's lattice output instead of relying solely on the
top hypothesis, aiming to encapsulate speech ambiguities and enhance SLU
outcomes. Our in-context learning experiments, covering spoken question
answering and intent classification, underline the LLM's resilience to noisy
speech transcripts with the help of word confusion networks from lattices,
bridging the SLU performance gap between using the top ASR hypothesis and an
oracle upper bound. Additionally, we delve into the LLM's robustness to varying
ASR performance conditions and scrutinize the aspects of in-context learning
which prove the most influential.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Introducing Bode: A Fine-Tuned Large Language Model for Portuguese
  Prompt-Based Task 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02909v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02909v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Lino Garcia, Pedro Henrique Paiola, Luis Henrique Morelli, Giovani Candido, Arnaldo Cândido Júnior, Danilo Samuel Jodas, Luis C. S. Afonso, Ivan Rizzo Guilherme, Bruno Elias Penteado, João Paulo Papa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are increasingly bringing advances to Natural
Language Processing. However, low-resource languages, those lacking extensive
prominence in datasets for various NLP tasks, or where existing datasets are
not as substantial, such as Portuguese, already obtain several benefits from
LLMs, but not to the same extent. LLMs trained on multilingual datasets
normally struggle to respond to prompts in Portuguese satisfactorily,
presenting, for example, code switching in their responses. This work proposes
a fine-tuned LLaMA 2-based model for Portuguese prompts named Bode in two
versions: 7B and 13B. We evaluate the performance of this model in
classification tasks using the zero-shot approach with in-context learning, and
compare it with other LLMs. Our main contribution is to bring an LLM with
satisfactory results in the Portuguese language, as well as to provide a model
that is free for research or commercial purposes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02906v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02906v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renjie Pi, Tianyang Han, Yueqi Xie, Rui Pan, Qing Lian, Hanze Dong, Jipeng Zhang, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The deployment of multimodal large language models (MLLMs) has brought forth
a unique vulnerability: susceptibility to malicious attacks through visual
inputs. We delve into the novel challenge of defending MLLMs against such
attacks. We discovered that images act as a "foreign language" that is not
considered during alignment, which can make MLLMs prone to producing harmful
responses. Unfortunately, unlike the discrete tokens considered in text-based
LLMs, the continuous nature of image signals presents significant alignment
challenges, which poses difficulty to thoroughly cover the possible scenarios.
This vulnerability is exacerbated by the fact that open-source MLLMs are
predominantly fine-tuned on limited image-text pairs that is much less than the
extensive text-based pretraining corpus, which makes the MLLMs more prone to
catastrophic forgetting of their original abilities during explicit alignment
tuning. To tackle these challenges, we introduce MLLM-Protector, a
plug-and-play strategy combining a lightweight harm detector and a response
detoxifier. The harm detector's role is to identify potentially harmful outputs
from the MLLM, while the detoxifier corrects these outputs to ensure the
response stipulates to the safety standards. This approach effectively
mitigates the risks posed by malicious visual inputs without compromising the
model's overall performance. Our results demonstrate that MLLM-Protector offers
a robust solution to a previously unaddressed aspect of MLLM security.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AFSPP: Agent Framework for Shaping Preference and Personality with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihong He, Changwang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of Large Language Models (LLMs) has introduced a new paradigm
for investigating human behavior emulation. Recent research has employed
LLM-based Agents to create a sociological research environment, in which agents
exhibit behavior based on the unfiltered characteristics of large language
models. However, these studies overlook the iterative development within a
human-like setting - Human preferences and personalities are complex, shaped by
various factors and subject to ongoing change as a result of environmental and
subjective influences. In light of this observation, we propose Agent Framework
for Shaping Preference and Personality (AFSPP), exploring the multifaceted
impact of social networks and subjective consciousness on LLM-based Agents'
preference and personality formation. With AFSPP, we have, for the first time,
successfully replicated several key findings from human personality
experiments. And other AFSPP-based experimental results indicate that plan
making, sensory perceptions and social networking with subjective information,
wield the most pronounced influence on preference shaping. AFSPP can
significantly enhance the efficiency and scope of psychological experiments,
while yielding valuable insights for Trustworthy Artificial Intelligence
research for strategies to prevent undesirable preference and personality
development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pheme: Efficient and Conversational Speech Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02839v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02839v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paweł Budzianowski, Taras Sereda, Tomasz Cichy, Ivan Vulić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, speech generation has seen remarkable progress, now
achieving one-shot generation capability that is often virtually
indistinguishable from real human voice. Integrating such advancements in
speech generation with large language models might revolutionize a wide range
of applications. However, certain applications, such as assistive
conversational systems, require natural and conversational speech generation
tools that also operate efficiently in real time. Current state-of-the-art
models like VALL-E and SoundStorm, powered by hierarchical neural audio codecs,
require large neural components and extensive training data to work well. In
contrast, MQTTS aims to build more compact conversational TTS models while
capitalizing on smaller-scale real-life conversational speech data. However,
its autoregressive nature yields high inference latency and thus limits its
real-time usage. In order to mitigate the current limitations of the
state-of-the-art TTS models while capitalizing on their strengths, in this work
we introduce the Pheme model series that 1) offers compact yet high-performing
models, 2) allows for parallel speech generation of 3) natural conversational
speech, and 4) it can be trained efficiently on smaller-scale conversational
data, cutting data demands by more than 10x but still matching the quality of
the autoregressive TTS models. We also show that through simple teacher-student
distillation we can meet significant improvements in voice quality for
single-speaker setups on top of pretrained Pheme checkpoints, relying solely on
synthetic speech generated by much larger teacher models. Audio samples and
pretrained models are available online.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DocGraphLM: Documental Graph Language Model for Information Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongsheng Wang, Zhiqiang Ma, Armineh Nourbakhsh, Kang Gu, Sameena Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in Visually Rich Document Understanding (VrDU) have enabled
information extraction and question answering over documents with complex
layouts. Two tropes of architectures have emerged -- transformer-based models
inspired by LLMs, and Graph Neural Networks. In this paper, we introduce
DocGraphLM, a novel framework that combines pre-trained language models with
graph semantics. To achieve this, we propose 1) a joint encoder architecture to
represent documents, and 2) a novel link prediction approach to reconstruct
document graphs. DocGraphLM predicts both directions and distances between
nodes using a convergent joint loss function that prioritizes neighborhood
restoration and downweighs distant node detection. Our experiments on three
SotA datasets show consistent improvement on IE and QA tasks with the adoption
of graph features. Moreover, we report that adopting the graph features
accelerates convergence in the learning process during training, despite being
solely constructed through link prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at SIGIR'23 (repost for easier access)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PeFoMed: Parameter Efficient Fine-tuning on Multimodal Large Language
  Models for Medical Visual Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinlong He, Pengfei Li, Gang Liu, Zixu Zhao, Shenjun Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) represent an evolutionary expansion
in the capabilities of traditional large language models, enabling them to
tackle challenges that surpass the scope of purely text-based applications. It
leverages the knowledge previously encoded within these language models,
thereby enhancing their applicability and functionality in the reign of
multimodal contexts. Recent works investigate the adaptation of MLLMs to
predict free-form answers as a generative task to solve medical visual question
answering (Med-VQA) tasks. In this paper, we propose a parameter efficient
framework for fine-tuning MLLM specifically tailored to Med-VQA applications,
and empirically validate it on a public benchmark dataset. To accurately
measure the performance, we employ human evaluation and the results reveal that
our model achieves an overall accuracy of 81.9%, and outperforms the GPT-4v
model by a significant margin of 26% absolute accuracy on closed-ended
questions. The code will be available here: https://github.com/jinlHe/PeFoMed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models in Plant Biology 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02789v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02789v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hilbert Yuen In Lam, Xing Er Ong, Marek Mutwil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs), such as ChatGPT, have taken the world by storm
and have passed certain forms of the Turing test. However, LLMs are not limited
to human language and analyze sequential data, such as DNA, protein, and gene
expression. The resulting foundation models can be repurposed to identify the
complex patterns within the data, resulting in powerful, multi-purpose
prediction tools able to explain cellular systems. This review outlines the
different types of LLMs and showcases their recent uses in biology. Since LLMs
have not yet been embraced by the plant community, we also cover how these
models can be deployed for the plant kingdom.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From LLM to Conversational Agent: A Memory Enhanced Architecture with
  Fine-Tuning of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Na Liu, Liangyu Chen, Xiaoyu Tian, Wei Zou, Kaijiang Chen, Ming Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces RAISE (Reasoning and Acting through Scratchpad and
Examples), an advanced architecture enhancing the integration of Large Language
Models (LLMs) like GPT-4 into conversational agents. RAISE, an enhancement of
the ReAct framework, incorporates a dual-component memory system, mirroring
human short-term and long-term memory, to maintain context and continuity in
conversations. It entails a comprehensive agent construction scenario,
including phases like Conversation Selection, Scene Extraction, CoT Completion,
and Scene Augmentation, leading to the LLMs Training phase. This approach
appears to enhance agent controllability and adaptability in complex,
multi-turn dialogues. Our preliminary evaluations in a real estate sales
context suggest that RAISE has some advantages over traditional agents,
indicating its potential for broader applications. This work contributes to the
AI field by providing a robust framework for developing more context-aware and
versatile conversational agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Complex systems approach to natural language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02772v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02772v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomasz Stanisz, Stanisław Drożdż, Jarosław Kwapień
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The review summarizes the main methodological concepts used in studying
natural language from the perspective of complexity science and documents their
applicability in identifying both universal and system-specific features of
language in its written representation. Three main complexity-related research
trends in quantitative linguistics are covered. The first part addresses the
issue of word frequencies in texts and demonstrates that taking punctuation
into consideration restores scaling whose violation in the Zipf's law is often
observed for the most frequent words. The second part introduces methods
inspired by time series analysis, used in studying various kinds of
correlations in written texts. The related time series are generated on the
basis of text partition into sentences or into phrases between consecutive
punctuation marks. It turns out that these series develop features often found
in signals generated by complex systems, like long-range correlations or
(multi)fractal structures. Moreover, it appears that the distances between
punctuation marks comply with the discrete variant of the Weibull distribution.
In the third part, the application of the network formalism to natural language
is reviewed, particularly in the context of the so-called word-adjacency
networks. Parameters characterizing topology of such networks can be used for
classification of texts, for example, from a stylometric perspective. Network
approach can also be applied to represent the organization of word
associations. Structure of word-association networks turns out to be
significantly different from that observed in random networks, revealing
genuine properties of language. Finally, punctuation seems to have a
significant impact not only on the language's information-carrying ability but
also on its key statistical properties, hence it is recommended to consider
punctuation marks on a par with words.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>113 pages, 49 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hyperparameter-Free Approach for Faster Minimum Bayes Risk Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuu Jinnai, Kaito Ariu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Minimum Bayes-Risk (MBR) decoding is shown to be a powerful alternative to
beam search decoding for a wide range of text generation tasks. However, MBR
requires a huge amount of time for inference to compute the MBR objective,
which makes the method infeasible in many situations where response time is
critical. Confidence-based pruning (CBP) (Cheng and Vlachos, 2023) has recently
been proposed to reduce the inference time in machine translation tasks.
Although it is shown to significantly reduce the amount of computation, it
requires hyperparameter tuning using a development set to be effective. To this
end, we propose Approximate Minimum Bayes-Risk (AMBR) decoding, a
hyperparameter-free method to run MBR decoding approximately. AMBR is derived
from the observation that the problem of computing the sample-based MBR
objective is the medoid identification problem. AMBR uses the Correlated
Sequential Halving (CSH) algorithm (Baharav and Tse, 2019), the best
approximation algorithm to date for the medoid identification problem, to
compute the sample-based MBR objective. We evaluate AMBR on machine
translation, text summarization, and image captioning tasks. The results show
that AMBR achieves on par with CBP, with CBP selecting hyperparameters through
an Oracle for each given computation budget.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAMI: Multi-Attentional Mutual-Information for Long Sequence Neuron
  Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alfirsa Damasyifa Fauzulhaq, Wahyu Parwitayasa, Joseph Ananda Sugihdharma, M. Fadli Ridhani, Novanto Yudistira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neuron labeling is an approach to visualize the behaviour and respond of a
certain neuron to a certain pattern that activates the neuron. Neuron labeling
extract information about the features captured by certain neurons in a deep
neural network, one of which uses the encoder-decoder image captioning
approach. The encoder used can be a pretrained CNN-based model and the decoder
is an RNN-based model for text generation. Previous work, namely MILAN (Mutual
Information-guided Linguistic Annotation of Neuron), has tried to visualize the
neuron behaviour using modified Show, Attend, and Tell (SAT) model in the
encoder, and LSTM added with Bahdanau attention in the decoder. MILAN can show
great result on short sequence neuron captioning, but it does not show great
result on long sequence neuron captioning, so in this work, we would like to
improve the performance of MILAN even more by utilizing different kind of
attention mechanism and additionally adding several attention result into one,
in order to combine all the advantages from several attention mechanism. Using
our compound dataset, we obtained higher BLEU and F1-Score on our proposed
model, achieving 17.742 and 0.4811 respectively. At some point where the model
converges at the peak, our model obtained BLEU of 21.2262 and BERTScore
F1-Score of 0.4870.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ German Text Embedding Clustering <span class="highlight-title">Benchmark</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02709v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02709v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Silvan Wehrli, Bert Arnrich, Christopher Irrgang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces a benchmark assessing the performance of clustering
German text embeddings in different domains. This benchmark is driven by the
increasing use of clustering neural text embeddings in tasks that require the
grouping of texts (such as topic modeling) and the need for German resources in
existing benchmarks. We provide an initial analysis for a range of pre-trained
mono- and multilingual models evaluated on the outcome of different clustering
algorithms. Results include strong performing mono- and multilingual models.
Reducing the dimensions of embeddings can further improve clustering.
Additionally, we conduct experiments with continued pre-training for German
BERT models to estimate the benefits of this additional training. Our
experiments suggest that significant performance improvements are possible for
short text. All code and datasets are publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised hard Negative Augmentation for contrastive learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Shu, Vasileios Lampos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Unsupervised hard Negative Augmentation (UNA), a method that
generates synthetic negative instances based on the term frequency-inverse
document frequency (TF-IDF) retrieval model. UNA uses TF-IDF scores to
ascertain the perceived importance of terms in a sentence and then produces
negative samples by replacing terms with respect to that. Our experiments
demonstrate that models trained with UNA improve the overall performance in
semantic textual similarity tasks. Additional performance gains are obtained
when combining UNA with the paraphrasing augmentation. Further results show
that our method is compatible with different backbone models. Ablation studies
also support the choice of having a TF-IDF-driven control on negative
augmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code and pre-trained models are available at
  https://github.com/ClaudiaShu/UNA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Analyzing Wrap-Up Effects through an Information-Theoretic Lens <span class="chip">ACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.17213v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.17213v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clara Meister, Tiago Pimentel, Thomas Hikaru Clark, Ryan Cotterell, Roger Levy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerous analyses of reading time (RT) data have been implemented -- all in
an effort to better understand the cognitive processes driving reading
comprehension. However, data measured on words at the end of a sentence -- or
even at the end of a clause -- is often omitted due to the confounding factors
introduced by so-called "wrap-up effects," which manifests as a skewed
distribution of RTs for these words. Consequently, the understanding of the
cognitive processes that might be involved in these wrap-up effects is limited.
In this work, we attempt to learn more about these processes by examining the
relationship between wrap-up effects and information-theoretic quantities, such
as word and context surprisals. We find that the distribution of information in
prior contexts is often predictive of sentence- and clause-final RTs (while not
of sentence-medial RTs). This lends support to several prior hypotheses about
the processes involved in wrap-up effects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2022 (main conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Efficacy of Sampling Adapters <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03749v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03749v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clara Meister, Tiago Pimentel, Luca Malagutti, Ethan G. Wilcox, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sampling is a common strategy for generating text from probabilistic models,
yet standard ancestral sampling often results in text that is incoherent or
ungrammatical. To alleviate this issue, various modifications to a model's
sampling distribution, such as nucleus or top-k sampling, have been introduced
and are now ubiquitously used in language generation systems. We propose a
unified framework for understanding these techniques, which we term sampling
adapters. Sampling adapters often lead to qualitatively better text, which
raises the question: From a formal perspective, how are they changing the
(sub)word-level distributions of language generation models? And why do these
local changes lead to higher-quality text? We argue that the shift they enforce
can be viewed as a trade-off between precision and recall: while the model
loses its ability to produce certain strings, its precision rate on desirable
text increases. While this trade-off is not reflected in standard metrics of
distribution quality (such as perplexity), we find that several
precision-emphasizing measures indeed indicate that sampling adapters can lead
to probability distributions more aligned with the true distribution. Further,
these measures correlate with higher sequence-level quality scores,
specifically, Mauve.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023 Main Conference Proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Patterns of Persistence and Diffusibility across the World's Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01698v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01698v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyi Chen, Johannes Bjerva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language similarities can be caused by genetic relatedness, areal contact,
universality, or chance. Colexification, i.e. a type of similarity where a
single lexical form is used to convey multiple meanings, is underexplored. In
our work, we shed light on the linguistic causes of cross-lingual similarity in
colexification and phonology, by exploring genealogical stability (persistence)
and contact-induced change (diffusibility). We construct large-scale graphs
incorporating semantic, genealogical, phonological and geographical data for
1,966 languages. We then show the potential of this resource, by investigating
several established hypotheses from previous work in linguistics, while
proposing new ones. Our results strongly support a previously established
hypothesis in the linguistic literature, while offering contradicting evidence
to another. Our large scale resource opens for further research across
disciplines, e.g.~in multilingual NLP and comparative linguistics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PromptBench: A Unified Library for Evaluation of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07910v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07910v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaijie Zhu, Qinlin Zhao, Hao Chen, Jindong Wang, Xing Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evaluation of large language models (LLMs) is crucial to assess their
performance and mitigate potential security risks. In this paper, we introduce
PromptBench, a unified library to evaluate LLMs. It consists of several key
components that are easily used and extended by researchers: prompt
construction, prompt engineering, dataset and model loading, adversarial prompt
attack, dynamic evaluation protocols, and analysis tools. PromptBench is
designed to be an open, general, and flexible codebase for research purposes
that can facilitate original study in creating new benchmarks, deploying
downstream applications, and designing new evaluation protocols. The code is
available at: https://github.com/microsoft/promptbench and will be continuously
supported.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>An extension to PromptBench (arXiv:2306.04528) for unified evaluation
  of LLMs using the same name; code: https://github.com/microsoft/promptbench</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Annotation Sensitivity: Training Data Collection Methods Affect Model
  Performance <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.14212v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.14212v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christoph Kern, Stephanie Eckman, Jacob Beck, Rob Chew, Bolei Ma, Frauke Kreuter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When training data are collected from human annotators, the design of the
annotation instrument, the instructions given to annotators, the
characteristics of the annotators, and their interactions can impact training
data. This study demonstrates that design choices made when creating an
annotation instrument also impact the models trained on the resulting
annotations. We introduce the term annotation sensitivity to refer to the
impact of annotation data collection methods on the annotations themselves and
on downstream model performance and predictions. We collect annotations of hate
speech and offensive language in five experimental conditions of an annotation
instrument, randomly assigning annotators to conditions. We then fine-tune BERT
models on each of the five resulting datasets and evaluate model performance on
a holdout portion of each condition. We find considerable differences between
the conditions for 1) the share of hate speech/offensive language annotations,
2) model performance, 3) model predictions, and 4) model learning curves. Our
results emphasize the crucial role played by the annotation instrument which
has received little attention in the machine learning literature. We call for
additional research into how and why the instrument impacts the annotations to
inform the development of best practices in instrument design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2023 Findings:
  https://aclanthology.org/2023.findings-emnlp.992/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FlashDecoding++: Faster Large Language Model Inference on GPUs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.01282v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.01282v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Hong, Guohao Dai, Jiaming Xu, Qiuli Mao, Xiuhong Li, Jun Liu, Kangdi Chen, Yuhan Dong, Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the Large Language Model (LLM) becomes increasingly important in various
domains. However, the following challenges still remain unsolved in
accelerating LLM inference: (1) Synchronized partial softmax update. The
softmax operation requires a synchronized update operation among each partial
softmax result, leading to ~20% overheads for the attention computation in
LLMs. (2) Under-utilized computation of flat GEMM. The shape of matrices
performing GEMM in LLM inference is flat, leading to under-utilized computation
and >50% performance loss after padding zeros in previous designs. (3)
Performance loss due to static dataflow. Kernel performance in LLM depends on
varied input data features, hardware configurations, etc. A single and static
dataflow may lead to a 50.25% performance loss for GEMMs of different shapes in
LLM inference.
  We present FlashDecoding++, a fast LLM inference engine supporting mainstream
LLMs and hardware back-ends. To tackle the above challenges, FlashDecoding++
creatively proposes: (1) Asynchronized softmax with unified max value.
FlashDecoding++ introduces a unified max value technique for different partial
softmax computations to avoid synchronization. (2) Flat GEMM optimization with
double buffering. FlashDecoding++ points out that flat GEMMs with different
shapes face varied bottlenecks. Then, techniques like double buffering are
introduced. (3) Heuristic dataflow with hardware resource adaptation.
FlashDecoding++ heuristically optimizes dataflow using different hardware
resource considering input dynamics. Due to the versatility of optimizations in
FlashDecoding++, FlashDecoding++ can achieve up to 4.86x and 2.18x speedup on
both NVIDIA and AMD GPUs compared to Hugging Face implementations.
FlashDecoding++ also achieves an average speedup of 1.37x compared to
state-of-the-art LLM inference engines on mainstream LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ mFACE: Multilingual Summarization with Factual Consistency Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.10622v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.10622v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roee Aharoni, Shashi Narayan, Joshua Maynez, Jonathan Herzig, Elizabeth Clark, Mirella Lapata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Abstractive summarization has enjoyed renewed interest in recent years,
thanks to pre-trained language models and the availability of large-scale
datasets. Despite promising results, current models still suffer from
generating factually inconsistent summaries, reducing their utility for
real-world application. Several recent efforts attempt to address this by
devising models that automatically detect factual inconsistencies in machine
generated summaries. However, they focus exclusively on English, a language
with abundant resources. In this work, we leverage factual consistency
evaluation models to improve multilingual summarization. We explore two
intuitive approaches to mitigate hallucinations based on the signal provided by
a multilingual NLI model, namely data filtering and controlled generation.
Experimental results in the 45 languages from the XLSum dataset show gains over
strong baselines in both automatic and human evaluation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages with links to released data</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Code-Style In-Context Learning for Knowledge-Based Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.04695v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.04695v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijie Nie, Richong Zhang, Zhongyuan Wang, Xudong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current methods for Knowledge-Based Question Answering (KBQA) usually rely on
complex training techniques and model frameworks, leading to many limitations
in practical applications. Recently, the emergence of In-Context Learning (ICL)
capabilities in Large Language Models (LLMs) provides a simple and
training-free semantic parsing paradigm for KBQA: Given a small number of
questions and their labeled logical forms as demo examples, LLMs can understand
the task intent and generate the logic form for a new question. However,
current powerful LLMs have little exposure to logic forms during pre-training,
resulting in a high format error rate. To solve this problem, we propose a
code-style in-context learning method for KBQA, which converts the generation
process of unfamiliar logical form into the more familiar code generation
process for LLMs. Experimental results on three mainstream datasets show that
our method dramatically mitigated the formatting error problem in generating
logic forms while realizing a new SOTA on WebQSP, GrailQA, and GraphQ under the
few-shot setting. The code and supplementary files are released at
https://github.com/Arthurizijar/KB-Coder .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI2024 Camera Ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can AI Be as Creative as Humans? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01623v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01623v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Wang, James Zou, Michael Mozer, Anirudh Goyal, Alex Lamb, Linjun Zhang, Weijie J Su, Zhun Deng, Michael Qizhe Xie, Hannah Brown, Kenji Kawaguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creativity serves as a cornerstone for societal progress and innovation, but
its assessment remains a complex and often subjective endeavor. With the rise
of advanced generative AI models capable of tasks once reserved for human
creativity, the study of AI's creative potential becomes imperative for its
responsible development and application. This paper addresses the complexities
in defining and evaluating creativity by introducing a new concept called
Relative Creativity. Instead of trying to define creativity universally, we
shift the focus to whether AI can match the creative abilities of a
hypothetical human. This perspective draws inspiration from the Turing Test,
expanding upon it to address the challenges and subjectivities inherent in
evaluating creativity. This methodological shift facilitates a statistically
quantifiable evaluation of AI's creativity, which we term Statistical
Creativity. This approach allows for direct comparisons of AI's creative
abilities with those of specific human groups. Building on this foundation, we
discuss the application of statistical creativity in contemporary
prompt-conditioned autoregressive models. In addition to defining and analyzing
a measure of creativity, we introduce an actionable training guideline,
effectively bridging the gap between theoretical quantification of creativity
and practical model training. Through these multifaceted contributions, the
paper establishes a cohesive, continuously evolving, and transformative
framework for assessing and fostering statistical creativity in AI models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper introduce the notion of "Relative Creativity", presents
  measurable assessment, and provides AI training guidelines to foster AI's
  creative capabilities Project Page: https://ai-relative-creativity.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AstroLLaMA-Chat: Scaling AstroLLaMA with Conversational and Diverse
  <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01916v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01916v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ernest Perkowski, Rui Pan, Tuan Dung Nguyen, Yuan-Sen Ting, Sandor Kruk, Tong Zhang, Charlie O'Neill, Maja Jablonska, Zechang Sun, Michael J. Smith, Huiling Liu, Kevin Schawinski, Kartheik Iyer, Ioana Ciucă for UniverseTBD
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the potential of enhancing LLM performance in astronomy-focused
question-answering through targeted, continual pre-training. By employing a
compact 7B-parameter LLaMA-2 model and focusing exclusively on a curated set of
astronomy corpora -- comprising abstracts, introductions, and conclusions -- we
achieve notable improvements in specialized topic comprehension. While general
LLMs like GPT-4 excel in broader question-answering scenarios due to superior
reasoning capabilities, our findings suggest that continual pre-training with
limited resources can still enhance model performance on specialized topics.
Additionally, we present an extension of AstroLLaMA: the fine-tuning of the 7B
LLaMA model on a domain-specific conversational dataset, culminating in the
release of the chat-enabled AstroLLaMA for community use. Comprehensive
quantitative benchmarking is currently in progress and will be detailed in an
upcoming full paper. The model, AstroLLaMA-Chat, is now available at
https://huggingface.co/universeTBD, providing the first open-source
conversational AI tool tailored for the astronomy community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 1 figure, model is available at
  https://huggingface.co/universeTBD, published in RNAAS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT
  Models <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11698v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11698v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T. Truong, Simran Arora, Mantas Mazeika, Dan Hendrycks, Zinan Lin, Yu Cheng, Sanmi Koyejo, Dawn Song, Bo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Pre-trained Transformer (GPT) models have exhibited exciting
progress in their capabilities, capturing the interest of practitioners and the
public alike. Yet, while the literature on the trustworthiness of GPT models
remains limited, practitioners have proposed employing capable GPT models for
sensitive applications such as healthcare and finance -- where mistakes can be
costly. To this end, this work proposes a comprehensive trustworthiness
evaluation for large language models with a focus on GPT-4 and GPT-3.5,
considering diverse perspectives -- including toxicity, stereotype bias,
adversarial robustness, out-of-distribution robustness, robustness on
adversarial demonstrations, privacy, machine ethics, and fairness. Based on our
evaluations, we discover previously unpublished vulnerabilities to
trustworthiness threats. For instance, we find that GPT models can be easily
misled to generate toxic and biased outputs and leak private information in
both training data and conversation history. We also find that although GPT-4
is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more
vulnerable given jailbreaking system or user prompts, potentially because GPT-4
follows (misleading) instructions more precisely. Our work illustrates a
comprehensive trustworthiness evaluation of GPT models and sheds light on the
trustworthiness gaps. Our benchmark is publicly available at
https://decodingtrust.github.io/; our dataset can be previewed at
https://huggingface.co/datasets/AI-Secure/DecodingTrust; a concise version of
this work is at https://openreview.net/pdf?id=kaHpo8OZw2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023 Outstanding Paper (Datasets and Benchmarks Track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KwaiAgents: Generalized Information-seeking Agent System with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04889v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04889v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haojie Pan, Zepeng Zhai, Hao Yuan, Yaojia Lv, Ruiji Fu, Ming Liu, Zhongyuan Wang, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Driven by curiosity, humans have continually sought to explore and understand
the world around them, leading to the invention of various tools to satiate
this inquisitiveness. Despite not having the capacity to process and memorize
vast amounts of information in their brains, humans excel in critical thinking,
planning, reflection, and harnessing available tools to interact with and
interpret the world, enabling them to find answers efficiently. The recent
advancements in large language models (LLMs) suggest that machines might also
possess the aforementioned human-like capabilities, allowing them to exhibit
powerful abilities even with a constrained parameter count. In this paper, we
introduce KwaiAgents, a generalized information-seeking agent system based on
LLMs. Within KwaiAgents, we propose an agent system that employs LLMs as its
cognitive core, which is capable of understanding a user's query, behavior
guidelines, and referencing external documents. The agent can also update and
retrieve information from its internal memory, plan and execute actions using a
time-aware search-browse toolkit, and ultimately provide a comprehensive
response. We further investigate the system's performance when powered by LLMs
less advanced than GPT-4, and introduce the Meta-Agent Tuning (MAT) framework,
designed to ensure even an open-sourced 7B or 13B model performs well among
many agent systems. We exploit both benchmark and human evaluations to
systematically validate these capabilities. Extensive experiments show the
superiority of our agent system compared to other autonomous agents and
highlight the enhanced generalized agent-abilities of our fine-tuned LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieval-Augmented Generation for Large Language Models: A Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10997v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10997v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, Haofen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) demonstrate significant capabilities but face
challenges such as hallucination, outdated knowledge, and non-transparent,
untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has
emerged as a promising solution by incorporating knowledge from external
databases. This enhances the accuracy and credibility of the models,
particularly for knowledge-intensive tasks, and allows for continuous knowledge
updates and integration of domain-specific information. RAG synergistically
merges LLMs' intrinsic knowledge with the vast, dynamic repositories of
external databases. This comprehensive review paper offers a detailed
examination of the progression of RAG paradigms, encompassing the Naive RAG,
the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the
tripartite foundation of RAG frameworks, which includes the retrieval , the
generation and the augmentation techniques. The paper highlights the
state-of-the-art technologies embedded in each of these critical components,
providing a profound understanding of the advancements in RAG systems.
Furthermore, this paper introduces the metrics and benchmarks for assessing RAG
models, along with the most up-to-date evaluation framework. In conclusion, the
paper delineates prospective avenues for research, including the identification
of challenges, the expansion of multi-modalities, and the progression of the
RAG infrastructure and its ecosystem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing Work</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">82</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Denoising Vision Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Yang, Katie Z Luo, Jiefeng Li, Kilian Q Weinberger, Yonglong Tian, Yue Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We delve into a nuanced but significant challenge inherent to Vision
Transformers (ViTs): feature maps of these models exhibit grid-like artifacts,
which detrimentally hurt the performance of ViTs in downstream tasks. Our
investigations trace this fundamental issue down to the positional embeddings
at the input stage. To address this, we propose a novel noise model, which is
universally applicable to all ViTs. Specifically, the noise model dissects ViT
outputs into three components: a semantics term free from noise artifacts and
two artifact-related terms that are conditioned on pixel locations. Such a
decomposition is achieved by enforcing cross-view feature consistency with
neural fields in a per-image basis. This per-image optimization process
extracts artifact-free features from raw ViT outputs, providing clean features
for offline applications. Expanding the scope of our solution to support online
functionality, we introduce a learnable denoiser to predict artifact-free
features directly from unprocessed ViT outputs, which shows remarkable
generalization capabilities to novel data without the need for per-image
optimization. Our two-stage approach, termed Denoising Vision Transformers
(DVT), does not require re-training existing pre-trained ViTs and is
immediately applicable to any Transformer-based architecture. We evaluate our
method on a variety of representative ViTs (DINO, MAE, DeiT-III, EVA02, CLIP,
DINOv2, DINOv2-reg). Extensive evaluations demonstrate that our DVT
consistently and significantly improves existing state-of-the-art
general-purpose models in semantic and geometric tasks across multiple datasets
(e.g., +3.84 mIoU). We hope our study will encourage a re-evaluation of ViT
design, especially regarding the naive use of positional embeddings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://jiawei-yang.github.io/DenoisingViT/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open-Vocabulary SAM: Segment and Recognize Twenty-thousand Classes
  Interactively 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02955v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02955v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haobo Yuan, Xiangtai Li, Chong Zhou, Yining Li, Kai Chen, Chen Change Loy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The CLIP and Segment Anything Model (SAM) are remarkable vision foundation
models (VFMs). SAM excels in segmentation tasks across diverse domains, while
CLIP is renowned for its zero-shot recognition capabilities. This paper
presents an in-depth exploration of integrating these two models into a unified
framework. Specifically, we introduce the Open-Vocabulary SAM, a SAM-inspired
model designed for simultaneous interactive segmentation and recognition,
leveraging two unique knowledge transfer modules: SAM2CLIP and CLIP2SAM. The
former adapts SAM's knowledge into the CLIP via distillation and learnable
transformer adapters, while the latter transfers CLIP knowledge into SAM,
enhancing its recognition capabilities. Extensive experiments on various
datasets and detectors show the effectiveness of Open-Vocabulary SAM in both
segmentation and recognition tasks, significantly outperforming the naive
baselines of simply combining SAM and CLIP. Furthermore, aided with image
classification data training, our method can segment and recognize
approximately 22,000 classes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://www.mmlab-ntu.com/project/ovsam</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Locally Adaptive Neural 3D Morphable Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michail Tarasiou, Rolandos Alexandros Potamias, Eimear O'Sullivan, Stylianos Ploumpis, Stefanos Zafeiriou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the Locally Adaptive Morphable Model (LAMM), a highly flexible
Auto-Encoder (AE) framework for learning to generate and manipulate 3D meshes.
We train our architecture following a simple self-supervised training scheme in
which input displacements over a set of sparse control vertices are used to
overwrite the encoded geometry in order to transform one training sample into
another. During inference, our model produces a dense output that adheres
locally to the specified sparse geometry while maintaining the overall
appearance of the encoded object. This approach results in state-of-the-art
performance in both disentangling manipulated geometry and 3D mesh
reconstruction. To the best of our knowledge LAMM is the first end-to-end
framework that enables direct local control of 3D vertex geometry in a single
forward pass. A very efficient computational graph allows our network to train
with only a fraction of the memory required by previous methods and run faster
during inference, generating 12k vertex meshes at $>$60fps on a single CPU
thread. We further leverage local geometry control as a primitive for higher
level editing operations and present a set of derivative capabilities such as
swapping and sampling object parts. Code and pretrained models can be found at
https://github.com/michaeltrs/LAMM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPFormer: Enhancing Vision Transformer with Superpixel Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02931v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02931v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jieru Mei, Liang-Chieh Chen, Alan Yuille, Cihang Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce SPFormer, a novel Vision Transformer enhanced by
superpixel representation. Addressing the limitations of traditional Vision
Transformers' fixed-size, non-adaptive patch partitioning, SPFormer employs
superpixels that adapt to the image's content. This approach divides the image
into irregular, semantically coherent regions, effectively capturing intricate
details and applicable at both initial and intermediate feature levels.
  SPFormer, trainable end-to-end, exhibits superior performance across various
benchmarks. Notably, it exhibits significant improvements on the challenging
ImageNet benchmark, achieving a 1.4% increase over DeiT-T and 1.1% over DeiT-S
respectively. A standout feature of SPFormer is its inherent explainability.
The superpixel structure offers a window into the model's internal processes,
providing valuable insights that enhance the model's interpretability. This
level of clarity significantly improves SPFormer's robustness, particularly in
challenging scenarios such as image rotations and occlusions, demonstrating its
adaptability and resilience.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncovering the human motion pattern: Pattern Memory-based Diffusion
  Model for Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Yang, Pengfei Zhu, Mengshi Qi, Huadong Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human trajectory forecasting is a critical challenge in fields such as
robotics and autonomous driving. Due to the inherent uncertainty of human
actions and intentions in real-world scenarios, various unexpected occurrences
may arise. To uncover latent motion patterns in human behavior, we introduce a
novel memory-based method, named Motion Pattern Priors Memory Network. Our
method involves constructing a memory bank derived from clustered prior
knowledge of motion patterns observed in the training set trajectories. We
introduce an addressing mechanism to retrieve the matched pattern and the
potential target distributions for each prediction from the memory bank, which
enables the identification and retrieval of natural motion patterns exhibited
by agents, subsequently using the target priors memory token to guide the
diffusion model to generate predictions. Extensive experiments validate the
effectiveness of our approach, achieving state-of-the-art trajectory prediction
accuracy. The code will be made publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MLLM-Protector: Ensuring MLLM's Safety without Hurting Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02906v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02906v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renjie Pi, Tianyang Han, Yueqi Xie, Rui Pan, Qing Lian, Hanze Dong, Jipeng Zhang, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The deployment of multimodal large language models (MLLMs) has brought forth
a unique vulnerability: susceptibility to malicious attacks through visual
inputs. We delve into the novel challenge of defending MLLMs against such
attacks. We discovered that images act as a "foreign language" that is not
considered during alignment, which can make MLLMs prone to producing harmful
responses. Unfortunately, unlike the discrete tokens considered in text-based
LLMs, the continuous nature of image signals presents significant alignment
challenges, which poses difficulty to thoroughly cover the possible scenarios.
This vulnerability is exacerbated by the fact that open-source MLLMs are
predominantly fine-tuned on limited image-text pairs that is much less than the
extensive text-based pretraining corpus, which makes the MLLMs more prone to
catastrophic forgetting of their original abilities during explicit alignment
tuning. To tackle these challenges, we introduce MLLM-Protector, a
plug-and-play strategy combining a lightweight harm detector and a response
detoxifier. The harm detector's role is to identify potentially harmful outputs
from the MLLM, while the detoxifier corrects these outputs to ensure the
response stipulates to the safety standards. This approach effectively
mitigates the risks posed by malicious visual inputs without compromising the
model's overall performance. Our results demonstrate that MLLM-Protector offers
a robust solution to a previously unaddressed aspect of MLLM security.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reversing the Irreversible: A Survey on Inverse Biometrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02861v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02861v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marta Gomez-Barrero, Javier Galbally
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the widespread use of biometric recognition, several issues related to
the privacy and security provided by this technology have been recently raised
and analysed. As a result, the early common belief among the biometrics
community of templates irreversibility has been proven wrong. It is now an
accepted fact that it is possible to reconstruct from an unprotected template a
synthetic sample that matches the bona fide one. This reverse engineering
process, commonly referred to as \textit{inverse biometrics}, constitutes a
severe threat for biometric systems from two different angles: on the one hand,
sensitive personal data (i.e., biometric data) can be derived from compromised
unprotected templates; on the other hand, other powerful attacks can be
launched building upon these reconstructed samples. Given its important
implications, biometric stakeholders have produced over the last fifteen years
numerous works analysing the different aspects related to inverse biometrics:
development of reconstruction algorithms for different characteristics;
proposal of methodologies to assess the vulnerabilities of biometric systems to
the aforementioned algorithms; development of countermeasures to reduce the
possible effects of attacks. The present article is an effort to condense all
this information in one comprehensive review of: the problem itself, the
evaluation of the problem, and the mitigation of the problem. The present
article is an effort to condense all this information in one comprehensive
review of: the problem itself, the evaluation of the problem, and the
mitigation of the problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, journal, survey</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generating Non-Stationary Textures using Self-Rectification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Zhou, Rongjun Xiao, Dani Lischinski, Daniel Cohen-Or, Hui Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the challenge of example-based non-stationary texture
synthesis. We introduce a novel twostep approach wherein users first modify a
reference texture using standard image editing tools, yielding an initial rough
target for the synthesis. Subsequently, our proposed method, termed
"self-rectification", automatically refines this target into a coherent,
seamless texture, while faithfully preserving the distinct visual
characteristics of the reference exemplar. Our method leverages a pre-trained
diffusion network, and uses self-attention mechanisms, to gradually align the
synthesized texture with the reference, ensuring the retention of the
structures in the provided target. Through experimental validation, our
approach exhibits exceptional proficiency in handling non-stationary textures,
demonstrating significant advancements in texture synthesis when compared to
existing state-of-the-art techniques. Code is available at
https://github.com/xiaorongjun000/Self-Rectification
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/xiaorongjun000/Self-Rectification</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Stage Contrastive Regression for Action Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi An, Mengshi Qi, Huadong Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, there has been growing interest in the video-based action
quality assessment (AQA). Most existing methods typically solve AQA problem by
considering the entire video yet overlooking the inherent stage-level
characteristics of actions. To address this issue, we design a novel
Multi-stage Contrastive Regression (MCoRe) framework for the AQA task. This
approach allows us to efficiently extract spatial-temporal information, while
simultaneously reducing computational costs by segmenting the input video into
multiple stages or procedures. Inspired by the graph contrastive learning, we
propose a new stage-wise contrastive learning loss function to enhance
performance. As a result, MCoRe demonstrates the state-of-the-art result so far
on the widely-adopted fine-grained AQA dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CrisisViT: A Robust Vision Transformer for Crisis Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02838v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02838v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijun Long, Richard McCreadie, Muhammad Imran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In times of emergency, crisis response agencies need to quickly and
accurately assess the situation on the ground in order to deploy relevant
services and resources. However, authorities often have to make decisions based
on limited information, as data on affected regions can be scarce until local
response services can provide first-hand reports. Fortunately, the widespread
availability of smartphones with high-quality cameras has made citizen
journalism through social media a valuable source of information for crisis
responders. However, analyzing the large volume of images posted by citizens
requires more time and effort than is typically available. To address this
issue, this paper proposes the use of state-of-the-art deep neural models for
automatic image classification/tagging, specifically by adapting
transformer-based architectures for crisis image classification (CrisisViT). We
leverage the new Incidents1M crisis image dataset to develop a range of new
transformer-based image classification models. Through experimentation over the
standard Crisis image benchmark dataset, we demonstrate that the CrisisViT
models significantly outperform previous approaches in emergency type, image
relevance, humanitarian category, and damage severity classification.
Additionally, we show that the new Incidents1M dataset can further augment the
CrisisViT models resulting in an additional 1.25% absolute accuracy gain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Two-stage Progressive Residual Dense Attention Network for Image
  Denoising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wencong Wu, An Ge, Guannan Lv, Yuelong Xia, Yungang Zhang, Wen Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep convolutional neural networks (CNNs) for image denoising can effectively
exploit rich hierarchical features and have achieved great success. However,
many deep CNN-based denoising models equally utilize the hierarchical features
of noisy images without paying attention to the more important and useful
features, leading to relatively low performance. To address the issue, we
design a new Two-stage Progressive Residual Dense Attention Network
(TSP-RDANet) for image denoising, which divides the whole process of denoising
into two sub-tasks to remove noise progressively. Two different attention
mechanism-based denoising networks are designed for the two sequential
sub-tasks: the residual dense attention module (RDAM) is designed for the first
stage, and the hybrid dilated residual dense attention module (HDRDAM) is
proposed for the second stage. The proposed attention modules are able to learn
appropriate local features through dense connection between different
convolutional layers, and the irrelevant features can also be suppressed. The
two sub-networks are then connected by a long skip connection to retain the
shallow feature to enhance the denoising performance. The experiments on seven
benchmark datasets have verified that compared with many state-of-the-art
methods, the proposed TSP-RDANet can obtain favorable results both on synthetic
and real noisy image denoising. The code of our TSP-RDANet is available at
https://github.com/WenCongWu/TSP-RDANet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CRSOT: Cross-Resolution Object Tracking using Unaligned Frame and Event
  Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02826v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02826v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yabin Zhu, Xiao Wang, Chenglong Li, Bo Jiang, Lin Zhu, Zhixiang Huang, Yonghong Tian, Jin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing datasets for RGB-DVS tracking are collected with DVS346 camera and
their resolution ($346 \times 260$) is low for practical applications.
Actually, only visible cameras are deployed in many practical systems, and the
newly designed neuromorphic cameras may have different resolutions. The latest
neuromorphic sensors can output high-definition event streams, but it is very
difficult to achieve strict alignment between events and frames on both spatial
and temporal views. Therefore, how to achieve accurate tracking with unaligned
neuromorphic and visible sensors is a valuable but unresearched problem. In
this work, we formally propose the task of object tracking using unaligned
neuromorphic and visible cameras. We build the first unaligned frame-event
dataset CRSOT collected with a specially built data acquisition system, which
contains 1,030 high-definition RGB-Event video pairs, 304,974 video frames. In
addition, we propose a novel unaligned object tracking framework that can
realize robust tracking even using the loosely aligned RGB-Event data.
Specifically, we extract the template and search regions of RGB and Event data
and feed them into a unified ViT backbone for feature embedding. Then, we
propose uncertainty perception modules to encode the RGB and Event features,
respectively, then, we propose a modality uncertainty fusion module to
aggregate the two modalities. These three branches are jointly optimized in the
training phase. Extensive experiments demonstrate that our tracker can
collaborate the dual modalities for high-performance tracking even without
strictly temporal and spatial alignment. The source code, dataset, and
pre-trained models will be released at
https://github.com/Event-AHU/Cross_Resolution_SOT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Peer Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Object-Centric Instruction Augmentation for Robotic Manipulation <span class="chip">ICRA2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Wen, Yichen Zhu, Minjie Zhu, Jinming Li, Zhiyuan Xu, Zhengping Che, Chaomin Shen, Yaxin Peng, Dong Liu, Feifei Feng, Jian Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans interpret scenes by recognizing both the identities and positions of
objects in their observations. For a robot to perform tasks such as
\enquote{pick and place}, understanding both what the objects are and where
they are located is crucial. While the former has been extensively discussed in
the literature that uses the large language model to enrich the text
descriptions, the latter remains underexplored. In this work, we introduce the
\textit{Object-Centric Instruction Augmentation (OCI)} framework to augment
highly semantic and information-dense language instruction with position cues.
We utilize a Multi-modal Large Language Model (MLLM) to weave knowledge of
object locations into natural language instruction, thus aiding the policy
network in mastering actions for versatile manipulation. Additionally, we
present a feature reuse mechanism to integrate the vision-language features
from off-the-shelf pre-trained MLLM into policy networks. Through a series of
simulated and real-world robotic tasks, we demonstrate that robotic manipulator
imitation policies trained with our enhanced instructions outperform those
relying solely on traditional language instructions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICRA2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffbody: Diffusion-based Pose and Shape Editing of Human Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02804v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02804v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuta Okuyama, Yuki Endo, Yoshihiro Kanamori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pose and body shape editing in a human image has received increasing
attention. However, current methods often struggle with dataset biases and
deteriorate realism and the person's identity when users make large edits. We
propose a one-shot approach that enables large edits with identity
preservation. To enable large edits, we fit a 3D body model, project the input
image onto the 3D model, and change the body's pose and shape. Because this
initial textured body model has artifacts due to occlusion and the inaccurate
body shape, the rendered image undergoes a diffusion-based refinement, in which
strong noise destroys body structure and identity whereas insufficient noise
does not help. We thus propose an iterative refinement with weak noise, applied
first for the whole body and then for the face. We further enhance the realism
by fine-tuning text embeddings via self-supervised learning. Our quantitative
and qualitative evaluations demonstrate that our method outperforms other
existing methods across various datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WACV 2024, project page:
  https://www.cgg.cs.tsukuba.ac.jp/~okuyama/pub/diffbody/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Subjective and Objective Analysis of Indian Social Media Video Quality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02794v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02794v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sandeep Mishra, Mukul Jha, Alan C. Bovik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We conducted a large-scale subjective study of the perceptual quality of
User-Generated Mobile Video Content on a set of mobile-originated videos
obtained from the Indian social media platform ShareChat. The content viewed by
volunteer human subjects under controlled laboratory conditions has the benefit
of culturally diversifying the existing corpus of User-Generated Content (UGC)
video quality datasets. There is a great need for large and diverse UGC-VQA
datasets, given the explosive global growth of the visual internet and social
media platforms. This is particularly true in regard to videos obtained by
smartphones, especially in rapidly emerging economies like India. ShareChat
provides a safe and cultural community oriented space for users to generate and
share content in their preferred Indian languages and dialects. Our subjective
quality study, which is based on this data, offers a boost of cultural, visual,
and language diversification to the video quality research community. We expect
that this new data resource will also allow for the development of systems that
can predict the perceived visual quality of Indian social media videos, to
control scaling and compression protocols for streaming, provide better user
recommendations, and guide content analysis and processing. We demonstrate the
value of the new data resource by conducting a study of leading blind video
quality models on it, including a new model, called MoEVA, which deploys a
mixture of experts to predict video quality. Both the new LIVE-ShareChat
dataset and sample source code for MoEVA are being made freely available to the
research community at https://github.com/sandeep-sm/LIVE-SC
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to the IEEE Transactions on Image Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weakly Semi-supervised Tool Detection in Minimally Invasive Surgery
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02791v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02791v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryo Fujii, Ryo Hachiuma, Hideo Saito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surgical tool detection is essential for analyzing and evaluating minimally
invasive surgery videos. Current approaches are mostly based on supervised
methods that require large, fully instance-level labels (i.e., bounding boxes).
However, large image datasets with instance-level labels are often limited
because of the burden of annotation. Thus, surgical tool detection is important
when providing image-level labels instead of instance-level labels since
image-level annotations are considerably more time-efficient than
instance-level annotations. In this work, we propose to strike a balance
between the extremely costly annotation burden and detection performance. We
further propose a co-occurrence loss, which considers a characteristic that
some tool pairs often co-occur together in an image to leverage image-level
labels. Encapsulating the knowledge of co-occurrence using the co-occurrence
loss helps to overcome the difficulty in classification that originates from
the fact that some tools have similar shapes and textures. Extensive
experiments conducted on the Endovis2018 dataset in various data settings show
the effectiveness of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fus-MAE: A cross-attention-based data fusion approach for Masked
  Autoencoders in remote sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02764v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02764v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Chan-To-Hing, Bharadwaj Veeravalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised frameworks for representation learning have recently stirred
up interest among the remote sensing community, given their potential to
mitigate the high labeling costs associated with curating large satellite image
datasets. In the realm of multimodal data fusion, while the often used
contrastive learning methods can help bridging the domain gap between different
sensor types, they rely on data augmentations techniques that require expertise
and careful design, especially for multispectral remote sensing data. A
possible but rather scarcely studied way to circumvent these limitations is to
use a masked image modelling based pretraining strategy. In this paper, we
introduce Fus-MAE, a self-supervised learning framework based on masked
autoencoders that uses cross-attention to perform early and feature-level data
fusion between synthetic aperture radar and multispectral optical data - two
modalities with a significant domain gap. Our empirical findings demonstrate
that Fus-MAE can effectively compete with contrastive learning strategies
tailored for SAR-optical data fusion and outperforms other masked-autoencoders
frameworks trained on a larger corpus.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detection and Classification of Diabetic Retinopathy using Deep Learning
  Algorithms for Segmentation to Facilitate Referral Recommendation for Test
  and Treatment Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manoj S H, Arya A Bosale
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research paper addresses the critical challenge of diabetic retinopathy
(DR), a severe complication of diabetes leading to potential blindness. The
proposed methodology leverages transfer learning with convolutional neural
networks (CNNs) for automatic DR detection using a single fundus photograph,
demonstrating high effectiveness with a quadratic weighted kappa score of
0.92546 in the APTOS 2019 Blindness Detection Competition. The paper reviews
existing literature on DR detection, spanning classical computer vision methods
to deep learning approaches, particularly focusing on CNNs. It identifies gaps
in the research, emphasizing the lack of exploration in integrating pretrained
large language models with segmented image inputs for generating
recommendations and understanding dynamic interactions within a web application
context.Objectives include developing a comprehensive DR detection methodology,
exploring model integration, evaluating performance through competition
ranking, contributing significantly to DR detection methodologies, and
identifying research gaps.The methodology involves data preprocessing, data
augmentation, and the use of a U-Net neural network architecture for
segmentation. The U-Net model efficiently segments retinal structures,
including blood vessels, hard and soft exudates, haemorrhages, microaneurysms,
and the optical disc. High evaluation scores in Jaccard, F1, recall, precision,
and accuracy underscore the model's potential for enhancing diagnostic
capabilities in retinal pathology assessment.The outcomes of this research hold
promise for improving patient outcomes through timely diagnosis and
intervention in the fight against diabetic retinopathy, marking a significant
contribution to the field of medical image analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Systematic review of image segmentation using complex networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02758v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02758v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amin Rezaei, Fatemeh Asadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This review presents various image segmentation methods using complex
networks.
  Image segmentation is one of the important steps in image analysis as it
helps analyze and understand complex images. At first, it has been tried to
classify complex networks based on how it being used in image segmentation.
  In computer vision and image processing applications, image segmentation is
essential for analyzing complex images with irregular shapes, textures, or
overlapping boundaries. Advanced algorithms make use of machine learning,
clustering, edge detection, and region-growing techniques. Graph theory
principles combined with community detection-based methods allow for more
precise analysis and interpretation of complex images. Hybrid approaches
combine multiple techniques for comprehensive, robust segmentation, improving
results in computer vision and image processing tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reading Between the Frames: Multi-Modal Depression Detection in Videos
  from Non-Verbal Cues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Gimeno-Gómez, Ana-Maria Bucur, Adrian Cosma, Carlos-David Martínez-Hinarejos, Paolo Rosso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depression, a prominent contributor to global disability, affects a
substantial portion of the population. Efforts to detect depression from social
media texts have been prevalent, yet only a few works explored depression
detection from user-generated video content. In this work, we address this
research gap by proposing a simple and flexible multi-modal temporal model
capable of discerning non-verbal depression cues from diverse modalities in
noisy, real-world videos. We show that, for in-the-wild videos, using
additional high-level non-verbal cues is crucial to achieving good performance,
and we extracted and processed audio speech embeddings, face emotion
embeddings, face, body and hand landmarks, and gaze and blinking information.
Through extensive experiments, we show that our model achieves state-of-the-art
results on three key benchmark datasets for depression detection from video by
a substantial margin. Our code is publicly available on GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 46th European Conference on Information Retrieval (ECIR
  2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MAMI: Multi-Attentional Mutual-Information for Long Sequence Neuron
  Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alfirsa Damasyifa Fauzulhaq, Wahyu Parwitayasa, Joseph Ananda Sugihdharma, M. Fadli Ridhani, Novanto Yudistira
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neuron labeling is an approach to visualize the behaviour and respond of a
certain neuron to a certain pattern that activates the neuron. Neuron labeling
extract information about the features captured by certain neurons in a deep
neural network, one of which uses the encoder-decoder image captioning
approach. The encoder used can be a pretrained CNN-based model and the decoder
is an RNN-based model for text generation. Previous work, namely MILAN (Mutual
Information-guided Linguistic Annotation of Neuron), has tried to visualize the
neuron behaviour using modified Show, Attend, and Tell (SAT) model in the
encoder, and LSTM added with Bahdanau attention in the decoder. MILAN can show
great result on short sequence neuron captioning, but it does not show great
result on long sequence neuron captioning, so in this work, we would like to
improve the performance of MILAN even more by utilizing different kind of
attention mechanism and additionally adding several attention result into one,
in order to combine all the advantages from several attention mechanism. Using
our compound dataset, we obtained higher BLEU and F1-Score on our proposed
model, achieving 17.742 and 0.4811 respectively. At some point where the model
converges at the peak, our model obtained BLEU of 21.2262 and BERTScore
F1-Score of 0.4870.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing targeted transferability via feature space fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Zeng, Biwei Chen, Anjie Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial examples (AEs) have been extensively studied due to their
potential for privacy protection and inspiring robust neural networks. However,
making a targeted AE transferable across unknown models remains challenging. In
this paper, to alleviate the overfitting dilemma common in an AE crafted by
existing simple iterative attacks, we propose fine-tuning it in the feature
space. Specifically, starting with an AE generated by a baseline attack, we
encourage the features that contribute to the target class and discourage the
features that contribute to the original class in a middle layer of the source
model. Extensive experiments demonstrate that only a few iterations of
fine-tuning can boost existing attacks in terms of targeted transferability
nontrivially and universally. Our results also verify that the simple iterative
attacks can yield comparable or even better transferability than the
resource-intensive methods, which rely on training target-specific classifiers
or generators with additional data. The code is available at:
github.com/zengh5/TA_feature_FT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 10 figures, accepted by 2024ICASSP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting Traffic Flow with Federated Learning and Graph Neural with
  Asynchronous Computations Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02723v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02723v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Yaqub, Shahzad Ahmad, Malik Abdul Manan, Imran Shabir Chuhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time traffic flow prediction holds significant importance within the
domain of Intelligent Transportation Systems (ITS). The task of achieving a
balance between prediction precision and computational efficiency presents a
significant challenge. In this article, we present a novel deep-learning method
called Federated Learning and Asynchronous Graph Convolutional Network
(FLAGCN). Our framework incorporates the principles of asynchronous graph
convolutional networks with federated learning to enhance the accuracy and
efficiency of real-time traffic flow prediction. The FLAGCN model employs a
spatial-temporal graph convolution technique to asynchronously address
spatio-temporal dependencies within traffic data effectively. To efficiently
handle the computational requirements associated with this deep learning model,
this study used a graph federated learning technique known as GraphFL. This
approach is designed to facilitate the training process. The experimental
results obtained from conducting tests on two distinct traffic datasets
demonstrate that the utilization of FLAGCN leads to the optimization of both
training and inference durations while maintaining a high level of prediction
accuracy. FLAGCN outperforms existing models with significant improvements by
achieving up to approximately 6.85% reduction in RMSE, 20.45% reduction in
MAPE, compared to the best-performing existing models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 Figures, 1 Table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Image Demoireing from Unpaired Real Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02719v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02719v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunshan Zhong, Yuyao Zhou, Yuxin Zhang, Fei Chao, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on addressing the issue of image demoireing. Unlike the
large volume of existing studies that rely on learning from paired real data,
we attempt to learn a demoireing model from unpaired real data, i.e., moire
images associated with irrelevant clean images. The proposed method, referred
to as Unpaired Demoireing (UnDeM), synthesizes pseudo moire images from
unpaired datasets, generating pairs with clean images for training demoireing
models. To achieve this, we divide real moire images into patches and group
them in compliance with their moire complexity. We introduce a novel moire
generation framework to synthesize moire images with diverse moire features,
resembling real moire patches, and details akin to real moire-free images.
Additionally, we introduce an adaptive denoise method to eliminate the
low-quality pseudo moire images that adversely impact the learning of
demoireing models. We conduct extensive experiments on the commonly-used FHDMi
and UHDM datasets. Results manifest that our UnDeM performs better than
existing methods when using existing demoireing models such as MBCNN and
ESDNet-L. Code: https://github.com/zysxmu/UnDeM
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Complementary Information Mutual Learning for Multimodality Medical
  Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuyun Shen, Wenhao Li, Haoqing Chen, Xiaoling Wang, Fengping Zhu, Yuxin Li, Xiangfeng Wang, Bo Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiologists must utilize multiple modal images for tumor segmentation and
diagnosis due to the limitations of medical imaging and the diversity of tumor
signals. This leads to the development of multimodal learning in segmentation.
However, the redundancy among modalities creates challenges for existing
subtraction-based joint learning methods, such as misjudging the importance of
modalities, ignoring specific modal information, and increasing cognitive load.
These thorny issues ultimately decrease segmentation accuracy and increase the
risk of overfitting. This paper presents the complementary information mutual
learning (CIML) framework, which can mathematically model and address the
negative impact of inter-modal redundant information. CIML adopts the idea of
addition and removes inter-modal redundant information through inductive
bias-driven task decomposition and message passing-based redundancy filtering.
CIML first decomposes the multimodal segmentation task into multiple subtasks
based on expert prior knowledge, minimizing the information dependence between
modalities. Furthermore, CIML introduces a scheme in which each modality can
extract information from other modalities additively through message passing.
To achieve non-redundancy of extracted information, the redundant filtering is
transformed into complementary information learning inspired by the variational
information bottleneck. The complementary information learning procedure can be
efficiently solved by variational inference and cross-modal spatial attention.
Numerical results from the verification task and standard benchmarks indicate
that CIML efficiently removes redundant information between modalities,
outperforming SOTA methods regarding validation accuracy and segmentation
effect.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VoxelNextFusion: A Simple, Unified and Effective Voxel Fusion Framework
  for Multi-Modal 3D Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziying Song, Guoxin Zhang, Jun Xie, Lin Liu, Caiyan Jia, Shaoqing Xu, Zhepeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR-camera fusion can enhance the performance of 3D object detection by
utilizing complementary information between depth-aware LiDAR points and
semantically rich images. Existing voxel-based methods face significant
challenges when fusing sparse voxel features with dense image features in a
one-to-one manner, resulting in the loss of the advantages of images, including
semantic and continuity information, leading to sub-optimal detection
performance, especially at long distances. In this paper, we present
VoxelNextFusion, a multi-modal 3D object detection framework specifically
designed for voxel-based methods, which effectively bridges the gap between
sparse point clouds and dense images. In particular, we propose a voxel-based
image pipeline that involves projecting point clouds onto images to obtain both
pixel- and patch-level features. These features are then fused using a
self-attention to obtain a combined representation. Moreover, to address the
issue of background features present in patches, we propose a feature
importance module that effectively distinguishes between foreground and
background features, thus minimizing the impact of the background features.
Extensive experiments were conducted on the widely used KITTI and nuScenes 3D
object detection benchmarks. Notably, our VoxelNextFusion achieved around
+3.20% in AP@0.7 improvement for car detection in hard level compared to the
Voxel R-CNN baseline on the KITTI test dataset
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VoroNav: Voronoi-based Zero-shot Object <span class="highlight-title">Navigation</span> with Large Language
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengying Wu, Yao Mu, Bingxian Wu, Yi Hou, Ji Ma, Shanghang Zhang, Chang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of household robotics, the Zero-Shot Object Navigation (ZSON)
task empowers agents to adeptly traverse unfamiliar environments and locate
objects from novel categories without prior explicit training. This paper
introduces VoroNav, a novel semantic exploration framework that proposes the
Reduced Voronoi Graph to extract exploratory paths and planning nodes from a
semantic map constructed in real time. By harnessing topological and semantic
information, VoroNav designs text-based descriptions of paths and images that
are readily interpretable by a large language model (LLM). Our approach
presents a synergy of path and farsight descriptions to represent the
environmental context, enabling the LLM to apply commonsense reasoning to
ascertain the optimal waypoints for navigation. Extensive evaluation on the
HM3D and HSSD datasets validates that VoroNav surpasses existing ZSON
benchmarks in both success rates and exploration efficiency (+2.8% Success and
+3.7% SPL on HM3D, +2.6% Success and +3.8% SPL on HSSD). Additionally
introduced metrics that evaluate obstacle avoidance proficiency and perceptual
efficiency further corroborate the enhancements achieved by our method in ZSON
planning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PAHD: Perception-Action based Human Decision Making using Explainable
  Graph Neural Networks on SAR Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sasindu Wijeratne, Bingyi Zhang, Rajgopal Kannan, Viktor Prasanna, Carl Busart
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic Aperture Radar (SAR) images are commonly utilized in military
applications for automatic target recognition (ATR). Machine learning (ML)
methods, such as Convolutional Neural Networks (CNN) and Graph Neural Networks
(GNN), are frequently used to identify ground-based objects, including battle
tanks, personnel carriers, and missile launchers. Determining the vehicle
class, such as the BRDM2 tank, BMP2 tank, BTR60 tank, and BTR70 tank, is
crucial, as it can help determine whether the target object is an ally or an
enemy. While the ML algorithm provides feedback on the recognized target, the
final decision is left to the commanding officers. Therefore, providing
detailed information alongside the identified target can significantly impact
their actions. This detailed information includes the SAR image features that
contributed to the classification, the classification confidence, and the
probability of the identified object being classified as a different object
type or class. We propose a GNN-based ATR framework that provides the final
classified class and outputs the detailed information mentioned above. This is
the first study to provide a detailed analysis of the classification class,
making final decisions more straightforward. Moreover, our GNN framework
achieves an overall accuracy of 99.2\% when evaluated on the MSTAR dataset,
improving over previous state-of-the-art GNN methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Progressive Knowledge Distillation Of Stable Diffusion XL Using Layer
  Level Loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02677v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02677v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yatharth Gupta, Vishnu V. Jaddipal, Harish Prabhala, Sayak Paul, Patrick Von Platen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stable Diffusion XL (SDXL) has become the best open source text-to-image
model (T2I) for its versatility and top-notch image quality. Efficiently
addressing the computational demands of SDXL models is crucial for wider reach
and applicability. In this work, we introduce two scaled-down variants, Segmind
Stable Diffusion (SSD-1B) and Segmind-Vega, with 1.3B and 0.74B parameter
UNets, respectively, achieved through progressive removal using layer-level
losses focusing on reducing the model size while preserving generative quality.
We release these models weights at https://hf.co/Segmind. Our methodology
involves the elimination of residual networks and transformer blocks from the
U-Net structure of SDXL, resulting in significant reductions in parameters, and
latency. Our compact models effectively emulate the original SDXL by
capitalizing on transferred knowledge, achieving competitive results against
larger multi-billion parameter SDXL. Our work underscores the efficacy of
knowledge distillation coupled with layer-level losses in reducing model size
while preserving the high-quality generative capabilities of SDXL, thus
facilitating more accessible deployment in resource-constrained environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GTA: Guided Transfer of Spatial Attention from Object-Centric
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        SeokHyun Seo, Jinwoo Hong, JungWoo Chae, Kyungyul Kim, Sangheum Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utilizing well-trained representations in transfer learning often results in
superior performance and faster convergence compared to training from scratch.
However, even if such good representations are transferred, a model can easily
overfit the limited training dataset and lose the valuable properties of the
transferred representations. This phenomenon is more severe in ViT due to its
low inductive bias. Through experimental analysis using attention maps in ViT,
we observe that the rich representations deteriorate when trained on a small
dataset. Motivated by this finding, we propose a novel and simple
regularization method for ViT called Guided Transfer of spatial Attention
(GTA). Our proposed method regularizes the self-attention maps between the
source and target models. A target model can fully exploit the knowledge
related to object localization properties through this explicit regularization.
Our experimental results show that the proposed GTA consistently improves the
accuracy across five benchmark datasets especially when the number of training
data is small.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Benchmark</span>ing PathCLIP for Pathology Image Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunyi Zheng, Xiaonan Cui, Yuxuan Sun, Jingxiong Li, Honglin Li, Yunlong Zhang, Pingyi Chen, Xueping Jing, Zhaoxiang Ye, Lin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate image classification and retrieval are of importance for clinical
diagnosis and treatment decision-making. The recent contrastive language-image
pretraining (CLIP) model has shown remarkable proficiency in understanding
natural images. Drawing inspiration from CLIP, PathCLIP is specifically
designed for pathology image analysis, utilizing over 200,000 image and text
pairs in training. While the performance the PathCLIP is impressive, its
robustness under a wide range of image corruptions remains unknown. Therefore,
we conduct an extensive evaluation to analyze the performance of PathCLIP on
various corrupted images from the datasets of Osteosarcoma and WSSS4LUAD. In
our experiments, we introduce seven corruption types including brightness,
contrast, Gaussian blur, resolution, saturation, hue, and markup at four
severity levels. Through experiments, we find that PathCLIP is relatively
robustness to image corruptions and surpasses OpenAI-CLIP and PLIP in zero-shot
classification. Among the seven corruptions, blur and resolution can cause
server performance degradation of the PathCLIP. This indicates that ensuring
the quality of images is crucial before conducting a clinical test.
Additionally, we assess the robustness of PathCLIP in the task of image-image
retrieval, revealing that PathCLIP performs less effectively than PLIP on
Osteosarcoma but performs better on WSSS4LUAD under diverse corruptions.
Overall, PathCLIP presents impressive zero-shot classification and retrieval
performance for pathology images, but appropriate care needs to be taken when
using it. We hope this study provides a qualitative impression of PathCLIP and
helps understand its differences from other CLIP models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing 3D-Air Signature by Pen Tip Tail Trajectory Awareness: <span class="highlight-title">Dataset</span>
  and Featuring by Novel Spatio-temporal CNN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saurabh Atreya, Maheswar Bora, Aritra Mukherjee, Abhijit Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes a novel process of using pen tip and tail 3D trajectory
for air signature. To acquire the trajectories we developed a new pen tool and
a stereo camera was used. We proposed SliT-CNN, a novel 2D spatial-temporal
convolutional neural network (CNN) for better featuring of the air signature.
In addition, we also collected an air signature dataset from $45$ signers.
Skilled forgery signatures per user are also collected. A detailed benchmarking
of the proposed dataset using existing techniques and proposed CNN on existing
and proposed dataset exhibit the effectiveness of our methodology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted and presented in IJCB 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recent Advancement in 3D Biometrics using Monocular Camera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aritra Mukherjee, Abhijit Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent literature has witnessed significant interest towards 3D biometrics
employing monocular vision for robust authentication methods. Motivated by
this, in this work we seek to provide insight on recent development in the area
of 3D biometrics employing monocular vision. We present the similarity and
dissimilarity of 3D monocular biometrics and classical biometrics, listing the
strengths and challenges. Further, we provide an overview of recent techniques
in 3D biometrics with monocular vision, as well as application systems adopted
by the industry. Finally, we discuss open research problems in this area of
research
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted and presented in IJCB 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AG-ReID.v2: Bridging Aerial and Ground Views for Person
  Re-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huy Nguyen, Kien Nguyen, Sridha Sridharan, Clinton Fookes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aerial-ground person re-identification (Re-ID) presents unique challenges in
computer vision, stemming from the distinct differences in viewpoints, poses,
and resolutions between high-altitude aerial and ground-based cameras. Existing
research predominantly focuses on ground-to-ground matching, with aerial
matching less explored due to a dearth of comprehensive datasets. To address
this, we introduce AG-ReID.v2, a dataset specifically designed for person Re-ID
in mixed aerial and ground scenarios. This dataset comprises 100,502 images of
1,615 unique individuals, each annotated with matching IDs and 15 soft
attribute labels. Data were collected from diverse perspectives using a UAV,
stationary CCTV, and smart glasses-integrated camera, providing a rich variety
of intra-identity variations. Additionally, we have developed an explainable
attention network tailored for this dataset. This network features a
three-stream architecture that efficiently processes pairwise image distances,
emphasizes key top-down features, and adapts to variations in appearance due to
altitude differences. Comparative evaluations demonstrate the superiority of
our approach over existing baselines. We plan to release the dataset and
algorithm source code publicly, aiming to advance research in this specialized
field of computer vision. For access, please visit
https://github.com/huynguyen792/AG-ReID.v2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, Accepted by TIFS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Random Ensemble of Encrypted models for Enhancing Robustness against
  Adversarial Examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryota Iijima, Sayaka Shiota, Hitoshi Kiya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) are well known to be vulnerable to adversarial
examples (AEs). In addition, AEs have adversarial transferability, which means
AEs generated for a source model can fool another black-box model (target
model) with a non-trivial probability. In previous studies, it was confirmed
that the vision transformer (ViT) is more robust against the property of
adversarial transferability than convolutional neural network (CNN) models such
as ConvMixer, and moreover encrypted ViT is more robust than ViT without any
encryption. In this article, we propose a random ensemble of encrypted ViT
models to achieve much more robust models. In experiments, the proposed scheme
is verified to be more robust against not only black-box attacks but also
white-box ones than convention methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FED-NeRF: Achieve High 3D Consistency and Temporal Coherence for Face
  Video Editing on Dynamic NeRF 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Zhang, Yu-Wing Tai, Chi-Keung Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The success of the GAN-NeRF structure has enabled face editing on NeRF to
maintain 3D view consistency. However, achieving simultaneously multi-view
consistency and temporal coherence while editing video sequences remains a
formidable challenge. This paper proposes a novel face video editing
architecture built upon the dynamic face GAN-NeRF structure, which effectively
utilizes video sequences to restore the latent code and 3D face geometry. By
editing the latent code, multi-view consistent editing on the face can be
ensured, as validated by multiview stereo reconstruction on the resulting
edited images in our dynamic NeRF. As the estimation of face geometries occurs
on a frame-by-frame basis, this may introduce a jittering issue. We propose a
stabilizer that maintains temporal coherence by preserving smooth changes of
face expressions in consecutive frames. Quantitative and qualitative analyses
reveal that our method, as the pioneering 4D face video editor, achieves
state-of-the-art performance in comparison to existing 2D or 3D-based
approaches independently addressing identity and motion. Codes will be
released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Our code will be available at: https://github.com/ZHANG1023/FED-NeRF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling and Masking: A New Paradigm of Data Sampling for Image and Video
  Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongxu Liu, Yinghui Quan, Guoyao Xiao, Aobo Li, Jinjian Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quality assessment of images and videos emphasizes both local details and
global semantics, whereas general data sampling methods (e.g., resizing,
cropping or grid-based fragment) fail to catch them simultaneously. To address
the deficiency, current approaches have to adopt multi-branch models and take
as input the multi-resolution data, which burdens the model complexity. In this
work, instead of stacking up models, a more elegant data sampling method (named
as SAMA, scaling and masking) is explored, which compacts both the local and
global content in a regular input size. The basic idea is to scale the data
into a pyramid first, and reduce the pyramid into a regular data dimension with
a masking strategy. Benefiting from the spatial and temporal redundancy in
images and videos, the processed data maintains the multi-scale characteristics
with a regular input size, thus can be processed by a single-branch model. We
verify the sampling method in image and video quality assessment. Experiments
show that our sampling method can improve the performance of current
single-branch models significantly, and achieves competitive performance to the
multi-branch models without extra model complexity. The source code will be
available at https://github.com/Sissuire/SAMA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2024. Code has been released at
  https://github.com/Sissuire/SAMA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MOODv2: Masked Image Modeling for Out-of-Distribution Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02611v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02611v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyao Li, Pengguang Chen, Shaozuo Yu, Shu Liu, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The crux of effective out-of-distribution (OOD) detection lies in acquiring a
robust in-distribution (ID) representation, distinct from OOD samples. While
previous methods predominantly leaned on recognition-based techniques for this
purpose, they often resulted in shortcut learning, lacking comprehensive
representations. In our study, we conducted a comprehensive analysis, exploring
distinct pretraining tasks and employing various OOD score functions. The
results highlight that the feature representations pre-trained through
reconstruction yield a notable enhancement and narrow the performance gap among
various score functions. This suggests that even simple score functions can
rival complex ones when leveraging reconstruction-based pretext tasks.
Reconstruction-based pretext tasks adapt well to various score functions. As
such, it holds promising potential for further expansion. Our OOD detection
framework, MOODv2, employs the masked image modeling pretext task. Without
bells and whistles, MOODv2 impressively enhances 14.30% AUROC to 95.68% on
ImageNet and achieves 99.98% on CIFAR-10.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DHGCN: Dynamic Hop Graph Convolution Network for Self-supervised Point
  Cloud Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jincen Jiang, Lizhi Zhao, Xuequan Lu, Wei Hu, Imran Razzak, Meili Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works attempt to extend Graph Convolution Networks (GCNs) to point
clouds for classification and segmentation tasks. These works tend to sample
and group points to create smaller point sets locally and mainly focus on
extracting local features through GCNs, while ignoring the relationship between
point sets. In this paper, we propose the Dynamic Hop Graph Convolution Network
(DHGCN) for explicitly learning the contextual relationships between the
voxelized point parts, which are treated as graph nodes. Motivated by the
intuition that the contextual information between point parts lies in the
pairwise adjacent relationship, which can be depicted by the hop distance of
the graph quantitatively, we devise a novel self-supervised part-level hop
distance reconstruction task and design a novel loss function accordingly to
facilitate training. In addition, we propose the Hop Graph Attention (HGA),
which takes the learned hop distance as input for producing attention weights
to allow edge features to contribute distinctively in aggregation. Eventually,
the proposed DHGCN is a plug-and-play module that is compatible with
point-based backbone networks. Comprehensive experiments on different backbones
and tasks demonstrate that our self-supervised method achieves state-of-the-art
performance. Our source code is available at: https://github.com/Jinec98/DHGCN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Partition-based Nonrigid Registration for 3D Face Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuping Ye, Zhan Song, Juan Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a partition-based surface registration for 3D morphable
model(3DMM). In the 3DMM, it often requires to warp a handcrafted template
model into different captured models. The proposed method first utilizes the
landmarks to partition the template model then scale each part and finally
smooth the boundaries. This method is especially effective when the disparity
between the template model and the target model is huge. The experiment result
shows the method perform well than the traditional warp method and robust to
the local minima.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploiting Polarized Material Cues for Robust Car Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen Dong, Haiyang Mei, Ziqi Wei, Ao Jin, Sen Qiu, Qiang Zhang, Xin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Car detection is an important task that serves as a crucial prerequisite for
many automated driving functions. The large variations in lighting/weather
conditions and vehicle densities of the scenes pose significant challenges to
existing car detection algorithms to meet the highly accurate perception demand
for safety, due to the unstable/limited color information, which impedes the
extraction of meaningful/discriminative features of cars. In this work, we
present a novel learning-based car detection method that leverages trichromatic
linear polarization as an additional cue to disambiguate such challenging
cases. A key observation is that polarization, characteristic of the light
wave, can robustly describe intrinsic physical properties of the scene objects
in various imaging conditions and is strongly linked to the nature of materials
for cars (e.g., metal and glass) and their surrounding environment (e.g., soil
and trees), thereby providing reliable and discriminative features for robust
car detection in challenging scenes. To exploit polarization cues, we first
construct a pixel-aligned RGB-Polarization car detection dataset, which we
subsequently employ to train a novel multimodal fusion network. Our car
detection network dynamically integrates RGB and polarization features in a
request-and-complement manner and can explore the intrinsic material properties
of cars across all learning samples. We extensively validate our method and
demonstrate that it outperforms state-of-the-art detection methods.
Experimental results show that polarization is a powerful cue for car
detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Object-oriented backdoor attack against image captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meiling Li, Nan Zhong, Xinpeng Zhang, Zhenxing Qian, Sheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Backdoor attack against image classification task has been widely studied and
proven to be successful, while there exist little research on the backdoor
attack against vision-language models. In this paper, we explore backdoor
attack towards image captioning models by poisoning training data. Assuming the
attacker has total access to the training dataset, and cannot intervene in
model construction or training process. Specifically, a portion of benign
training samples is randomly selected to be poisoned. Afterwards, considering
that the captions are usually unfolded around objects in an image, we design an
object-oriented method to craft poisons, which aims to modify pixel values by a
slight range with the modification number proportional to the scale of the
current detected object region. After training with the poisoned data, the
attacked model behaves normally on benign images, but for poisoned images, the
model will generate some sentences irrelevant to the given image. The attack
controls the model behavior on specific test images without sacrificing the
generation performance on benign test images. Our method proves the weakness of
image captioning models to backdoor attack and we hope this work can raise the
awareness of defending against backdoor attack in the image captioning field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Characterizing Satellite Geometry via Accelerated 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Van Minh Nguyen, Emma Sandidge, Trupti Mahendrakar, Ryan T. White
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The accelerating deployment of spacecraft in orbit have generated interest in
on-orbit servicing (OOS), inspection of spacecraft, and active debris removal
(ADR). Such missions require precise rendezvous and proximity operations in the
vicinity of non-cooperative, possible unknown, resident space objects. Safety
concerns with manned missions and lag times with ground-based control
necessitate complete autonomy. This requires robust characterization of the
target's geometry. In this article, we present an approach for mapping
geometries of satellites on orbit based on 3D Gaussian Splatting that can run
on computing resources available on current spaceflight hardware. We
demonstrate model training and 3D rendering performance on a
hardware-in-the-loop satellite mock-up under several realistic lighting and
motion conditions. Our model is shown to be capable of training on-board and
rendering higher quality novel views of an unknown satellite nearly 2 orders of
magnitude faster than previous NeRF-based algorithms. Such on-board
capabilities are critical to enable downstream machine intelligence tasks
necessary for autonomous guidance, navigation, and control tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoCoT: Contrastive Chain-of-Thought Prompting for Large Multimodal
  Models with Multiple Image Inputs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daoan Zhang, Junming Yang, Hanjia Lyu, Zijian Jin, Yuan Yao, Mingkai Chen, Jiebo Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When exploring the development of Artificial General Intelligence (AGI), a
critical task for these models involves interpreting and processing information
from multiple image inputs. However, Large Multimodal Models (LMMs) encounter
two issues in such scenarios: (1) a lack of fine-grained perception, and (2) a
tendency to blend information across multiple images. We first extensively
investigate the capability of LMMs to perceive fine-grained visual details when
dealing with multiple input images. The research focuses on two aspects: first,
image-to-image matching (to evaluate whether LMMs can effectively reason and
pair relevant images), and second, multi-image-to-text matching (to assess
whether LMMs can accurately capture and summarize detailed image information).
We conduct evaluations on a range of both open-source and closed-source large
models, including GPT-4V, Gemini, OpenFlamingo, and MMICL. To enhance model
performance, we further develop a Contrastive Chain-of-Thought (CoCoT)
prompting approach based on multi-input multimodal models. This method requires
LMMs to compare the similarities and differences among multiple image inputs,
and then guide the models to answer detailed questions about multi-image inputs
based on the identified similarities and differences. Our experimental results
showcase CoCoT's proficiency in enhancing the multi-image comprehension
capabilities of large multimodal models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MC-ViViT: Multi-branch Classifier-ViViT to detect Mild Cognitive
  Impairment in older adults using facial videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.05292v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.05292v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Sun, Hiroko H. Dodge, Mohammad H. Mahoor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep machine learning models including Convolutional Neural Networks (CNN)
have been successful in the detection of Mild Cognitive Impairment (MCI) using
medical images, questionnaires, and videos. This paper proposes a novel
Multi-branch Classifier-Video Vision Transformer (MC-ViViT) model to
distinguish MCI from those with normal cognition by analyzing facial features.
The data comes from the I-CONECT, a behavioral intervention trial aimed at
improving cognitive function by providing frequent video chats. MC-ViViT
extracts spatiotemporal features of videos in one branch and augments
representations by the MC module. The I-CONECT dataset is challenging as the
dataset is imbalanced containing Hard-Easy and Positive-Negative samples, which
impedes the performance of MC-ViViT. We propose a loss function for Hard-Easy
and Positive-Negative Samples (HP Loss) by combining Focal loss and AD-CORRE
loss to address the imbalanced problem. Our experimental results on the
I-CONECT dataset show the great potential of MC-ViViT in predicting MCI with a
high accuracy of 90.63% accuracy on some of the interview videos.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 tables, 7 figures, 9 equations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TreeLearn: A Comprehensive Deep Learning Method for Segmenting
  Individual Trees from Ground-Based LiDAR Forest Point Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08471v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08471v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Henrich, Jan van Delden, Dominik Seidel, Thomas Kneib, Alexander Ecker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Laser-scanned point clouds of forests make it possible to extract valuable
information for forest management. To consider single trees, a forest point
cloud needs to be segmented into individual tree point clouds. Existing
segmentation methods are usually based on hand-crafted algorithms, such as
identifying trunks and growing trees from them, and face difficulties in dense
forests with overlapping tree crowns. In this study, we propose TreeLearn, a
deep learning-based approach for tree instance segmentation of forest point
clouds. Unlike previous methods, TreeLearn is trained on already segmented
point clouds in a data-driven manner, making it less reliant on predefined
features and algorithms. Furthermore, TreeLearn is implemented as a fully
automatic pipeline and does not rely on extensive hyperparameter tuning, which
makes it easy to use. Additionally, we introduce a new manually segmented
benchmark forest dataset containing 156 full trees, and 79 partial trees, that
have been cleanly segmented by hand. The data is generated by mobile laser
scanning and contributes to create a larger and more diverse data basis for
model development and fine-grained instance segmentation evaluation. We trained
TreeLearn on forest point clouds of 6665 trees, labeled using the Lidar360
software. An evaluation on the benchmark dataset shows that TreeLearn performs
equally well or better than the algorithm used to generate its training data.
Furthermore, the method's performance can be vastly improved by fine-tuning on
the cleanly labeled benchmark dataset. The TreeLearn code is available from
https://github.com/ecker-lab/TreeLearn. The data as well as trained models can
be found at https://doi.org/10.25625/VPMPID.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Network Initialization for Medical AI Models Using
  Large-Scale, Unlabeled Natural Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.07688v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.07688v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soroosh Tayebi Arasteh, Leo Misera, Jakob Nikolas Kather, Daniel Truhn, Sven Nebelung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training datasets, like ImageNet, have become the gold standard in
medical image analysis. However, the emergence of self-supervised learning
(SSL), which leverages unlabeled data to learn robust features, presents an
opportunity to bypass the intensive labeling process. In this study, we
explored if SSL for pre-training on non-medical images can be applied to chest
radiographs and how it compares to supervised pre-training on non-medical
images and on medical images. We utilized a vision transformer and initialized
its weights based on (i) SSL pre-training on natural images (DINOv2), (ii) SL
pre-training on natural images (ImageNet dataset), and (iii) SL pre-training on
chest radiographs from the MIMIC-CXR database. We tested our approach on over
800,000 chest radiographs from six large global datasets, diagnosing more than
20 different imaging findings. Our SSL pre-training on curated images not only
outperformed ImageNet-based pre-training (P<0.001 for all datasets) but, in
certain cases, also exceeded SL on the MIMIC-CXR dataset. Our findings suggest
that selecting the right pre-training strategy, especially with SSL, can be
pivotal for improving artificial intelligence (AI)'s diagnostic accuracy in
medical imaging. By demonstrating the promise of SSL in chest radiograph
analysis, we underline a transformative shift towards more efficient and
accurate AI models in medical imaging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in European Radiology Experimental</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language-free Compositional Action Generation via Decoupling Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.03538v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.03538v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Liu, Guangyi Chen, Yansong Tang, Guangrun Wang, Ser-Nam Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Composing simple elements into complex concepts is crucial yet challenging,
especially for 3D action generation. Existing methods largely rely on extensive
neural language annotations to discern composable latent semantics, a process
that is often costly and labor-intensive. In this study, we introduce a novel
framework to generate compositional actions without reliance on language
auxiliaries. Our approach consists of three main components: Action Coupling,
Conditional Action Generation, and Decoupling Refinement. Action Coupling
utilizes an energy model to extract the attention masks of each sub-action,
subsequently integrating two actions using these attentions to generate
pseudo-training examples. Then, we employ a conditional generative model, CVAE,
to learn a latent space, facilitating the diverse generation. Finally, we
propose Decoupling Refinement, which leverages a self-supervised pre-trained
model MAE to ensure semantic consistency between the sub-actions and
compositional actions. This refinement process involves rendering generated 3D
actions into 2D space, decoupling these images into two sub-segments, using the
MAE model to restore the complete image from sub-segments, and constraining the
recovered images to match images rendered from raw sub-actions. Due to the lack
of existing datasets containing both sub-actions and compositional actions, we
created two new datasets, named HumanAct-C and UESTC-C, and present a
corresponding evaluation metric. Both qualitative and quantitative assessments
are conducted to show our efficacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Surgical Aggregation: Federated Class-Heterogeneous Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06683v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06683v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Kulkarni, Adway Kanhere, Paul H. Yi, Vishwa S. Parekh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The release of numerous chest x-ray datasets has spearheaded the development
of deep learning models with expert-level performance. However, they have
limited interoperability due to class-heterogeneity -- a result of inconsistent
labeling schemes and partial annotations. Therefore, it is challenging to
leverage these datasets in aggregate to train models with a complete
representation of abnormalities that may occur within the thorax. In this work,
we propose surgical aggregation, a federated learning framework for aggregating
knowledge from class-heterogeneous datasets and learn a model that can
simultaneously predict the presence of all disease labels present across the
datasets. We evaluate our method using simulated and real-world
class-heterogeneous datasets across both independent and identically
distributed (iid) and non-iid settings. Our results show that surgical
aggregation outperforms current methods, has better generalizability, and is a
crucial first step towards tackling class-heterogeneity in federated learning
to facilitate the development of clinically-useful models using previously
non-interoperable chest x-ray datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancing Ischemic Stroke Diagnosis: A Novel Two-Stage Approach for
  Blood Clot Origin Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.13775v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.13775v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Koushik Sivarama Krishnan, P. J. Joe Nikesh, Swathi Gnanasekar, Karthik Sivarama Krishnan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An innovative two-stage methodology for categorizing blood clot origins is
presented in this paper, which is important for the diagnosis and treatment of
ischemic stroke. First, a background classifier based on MobileNetV3 segments
big whole-slide digital pathology images into numerous tiles to detect the
presence of cellular material. After that, different pre-trained image
classification algorithms are fine-tuned to determine the origin of blood
clots. Due to complex blood flow dynamics and limitations in conventional
imaging methods such as computed tomography (CT), magnetic resonance imaging
(MRI), and ultrasound, identifying the sources of blood clots is a challenging
task. Although these techniques are useful for identifying blood clots, they
are not very good at determining how they originated. To address these
challenges, our method makes use of robust computer vision models that have
been refined using information from whole-slide digital pathology images. Out
of all the models tested, the PoolFormer \cite{yu2022metaformer} performs
better than the others, with 93.4\% accuracy, 93.4\% precision, 93.4\% recall,
and 93.4\% F1-score. Moreover, it achieves the good weighted multi-class
logarithmic loss (WMCLL) of 0.4361, which emphasizes how effective it is in
this particular application. These encouraging findings suggest that our
approach can successfully identify the origin of blood clots in a variety of
vascular locations, potentially advancing ischemic stroke diagnosis and
treatment approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Randomized Smoothing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16221v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16221v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Scholten, Jan Schuchardt, Aleksandar Bojchevski, Stephan Günnemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world data is complex and often consists of objects that can be
decomposed into multiple entities (e.g. images into pixels, graphs into
interconnected nodes). Randomized smoothing is a powerful framework for making
models provably robust against small changes to their inputs - by guaranteeing
robustness of the majority vote when randomly adding noise before
classification. Yet, certifying robustness on such complex data via randomized
smoothing is challenging when adversaries do not arbitrarily perturb entire
objects (e.g. images) but only a subset of their entities (e.g. pixels). As a
solution, we introduce hierarchical randomized smoothing: We partially smooth
objects by adding random noise only on a randomly selected subset of their
entities. By adding noise in a more targeted manner than existing methods we
obtain stronger robustness guarantees while maintaining high accuracy. We
initialize hierarchical smoothing using different noising distributions,
yielding novel robustness certificates for discrete and continuous domains. We
experimentally demonstrate the importance of hierarchical smoothing in image
and node classification, where it yields superior robustness-accuracy
trade-offs. Overall, hierarchical smoothing is an important contribution
towards models that are both - certifiably robust to perturbations and
accurate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ConvNet vs Transformer, Supervised vs CLIP: Beyond ImageNet Accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09215v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09215v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kirill Vishniakov, Zhiqiang Shen, Zhuang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern computer vision offers a great variety of models to practitioners, and
selecting a model from multiple options for specific applications can be
challenging. Conventionally, competing model architectures and training
protocols are compared by their classification accuracy on ImageNet. However,
this single metric does not fully capture performance nuances critical for
specialized tasks. In this work, we conduct an in-depth comparative analysis of
model behaviors beyond ImageNet accuracy, for both ConvNet and Vision
Transformer architectures, each across supervised and CLIP training paradigms.
Although our selected models have similar ImageNet accuracies and compute
requirements, we find that they differ in many other aspects: types of
mistakes, output calibration, transferability, and feature invariance, among
others. This diversity in model characteristics, not captured by traditional
metrics, highlights the need for more nuanced analysis when choosing among
different models. Our code is available at
https://github.com/kirill-vish/Beyond-INet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://kirill-vish.github.io/beyond-imagenet-accuracy/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Brain tumor segmentation using synthetic MR images -- A comparison of
  GANs and diffusion models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02986v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02986v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Usman Akbar, Måns Larsson, Anders Eklund
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large annotated datasets are required for training deep learning models, but
in medical imaging data sharing is often complicated due to ethics,
anonymization and data protection legislation. Generative AI models, such as
generative adversarial networks (GANs) and diffusion models, can today produce
very realistic synthetic images, and can potentially facilitate data sharing.
However, in order to share synthetic medical images it must first be
demonstrated that they can be used for training different networks with
acceptable performance. Here, we therefore comprehensively evaluate four GANs
(progressive GAN, StyleGAN 1-3) and a diffusion model for the task of brain
tumor segmentation (using two segmentation networks, U-Net and a Swin
transformer). Our results show that segmentation networks trained on synthetic
images reach Dice scores that are 80% - 90% of Dice scores when training with
real images, but that memorization of the training images can be a problem for
diffusion models if the original dataset is too small. Our conclusion is that
sharing synthetic medical images is a viable option to sharing real images, but
that further work is required. The trained generative models and the generated
synthetic images are shared on AIDA data hub
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 Pages. 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep learning in computed tomography pulmonary angiography imaging: a
  dual-pronged approach for pulmonary embolism detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.05197v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.05197v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabiha Bushra, Muhammad E. H. Chowdhury, Rusab Sarmun, Saidul Kabir, Menatalla Said, Sohaib Bassam Zoghoul, Adam Mushtak, Israa Al-Hashimi, Abdulrahman Alqahtani, Anwarul Hasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing reliance on Computed Tomography Pulmonary Angiography (CTPA)
for Pulmonary Embolism (PE) diagnosis presents challenges and a pressing need
for improved diagnostic solutions. The primary objective of this study is to
leverage deep learning techniques to enhance the Computer Assisted Diagnosis
(CAD) of PE. With this aim, we propose a classifier-guided detection approach
that effectively leverages the classifier's probabilistic inference to direct
the detection predictions, marking a novel contribution in the domain of
automated PE diagnosis. Our classification system includes an Attention-Guided
Convolutional Neural Network (AG-CNN) that uses local context by employing an
attention mechanism. This approach emulates a human expert's attention by
looking at both global appearances and local lesion regions before making a
decision. The classifier demonstrates robust performance on the FUMPE dataset,
achieving an AUROC of 0.927, sensitivity of 0.862, specificity of 0.879, and an
F1-score of 0.805 with the Inception-v3 backbone architecture. Moreover, AG-CNN
outperforms the baseline DenseNet-121 model, achieving an 8.1% AUROC gain.
While previous research has mostly focused on finding PE in the main arteries,
our use of cutting-edge object detection models and ensembling techniques
greatly improves the accuracy of detecting small embolisms in the peripheral
arteries. Finally, our proposed classifier-guided detection approach further
refines the detection metrics, contributing new state-of-the-art to the
community: mAP$_{50}$, sensitivity, and F1-score of 0.846, 0.901, and 0.779,
respectively, outperforming the former benchmark with a significant 3.7%
improvement in mAP$_{50}$. Our research aims to elevate PE patient care by
integrating AI solutions into clinical workflows, highlighting the potential of
human-AI collaboration in medical diagnostics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Expert Systems With Applications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Visible-Infrared Person ReID by Collaborative Learning with
  Neighbor-Guided Label Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.12711v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.12711v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        De Cheng, Xiaojian Huang, Nannan Wang, Lingfeng He, Zhihui Li, Xinbo Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised learning visible-infrared person re-identification (USL-VI-ReID)
aims at learning modality-invariant features from unlabeled cross-modality
dataset, which is crucial for practical applications in video surveillance
systems. The key to essentially address the USL-VI-ReID task is to solve the
cross-modality data association problem for further heterogeneous joint
learning. To address this issue, we propose a Dual Optimal Transport Label
Assignment (DOTLA) framework to simultaneously assign the generated labels from
one modality to its counterpart modality. The proposed DOTLA mechanism
formulates a mutual reinforcement and efficient solution to cross-modality data
association, which could effectively reduce the side-effects of some
insufficient and noisy label associations. Besides, we further propose a
cross-modality neighbor consistency guided label refinement and regularization
module, to eliminate the negative effects brought by the inaccurate supervised
signals, under the assumption that the prediction or label distribution of each
example should be similar to its nearest neighbors. Extensive experimental
results on the public SYSU-MM01 and RegDB datasets demonstrate the
effectiveness of the proposed method, surpassing existing state-of-the-art
approach by a large margin of 7.76% mAP on average, which even surpasses some
supervised VI-ReID methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DRKF: Distilled Rotated Kernel Fusion for Efficient Rotation Invariant
  Descriptors in Local Feature Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.10907v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.10907v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ranran Huang, Jiancheng Cai, Chao Li, Zhuoyuan Wu, Xinmin Liu, Zhenhua Chai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of local feature descriptors degrades in the presence of
large rotation variations. To address this issue, we present an efficient
approach to learning rotation invariant descriptors. Specifically, we propose
Rotated Kernel Fusion (RKF) which imposes rotations on the convolution kernel
to improve the inherent nature of CNN. Since RKF can be processed by the
subsequent re-parameterization, no extra computational costs will be introduced
in the inference stage. Moreover, we present Multi-oriented Feature Aggregation
(MOFA) which aggregates features extracted from multiple rotated versions of
the input image and can provide auxiliary knowledge for the training of RKF by
leveraging the distillation strategy. We refer to the distilled RKF model as
DRKF. Besides the evaluation on a rotation-augmented version of the public
dataset HPatches, we also contribute a new dataset named DiverseBEV which is
collected during the drone's flight and consists of bird's eye view images with
large viewpoint changes and camera rotations. Extensive experiments show that
our method can outperform other state-of-the-art techniques when exposed to
large rotation variations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Managing the unknown: a survey on Open Set Recognition and tangential
  areas 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.08785v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.08785v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcos Barcina-Blanco, Jesus L. Lobo, Pablo Garcia-Bringas, Javier Del Ser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world scenarios classification models are often required to perform
robustly when predicting samples belonging to classes that have not appeared
during its training stage. Open Set Recognition addresses this issue by
devising models capable of detecting unknown classes from samples arriving
during the testing phase, while maintaining a good level of performance in the
classification of samples belonging to known classes. This review
comprehensively overviews the recent literature related to Open Set
Recognition, identifying common practices, limitations, and connections of this
field with other machine learning research areas, such as continual learning,
out-of-distribution detection, novelty detection, and uncertainty estimation.
Our work also uncovers open problems and suggests several research directions
that may motivate and articulate future efforts towards more safe Artificial
Intelligence methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 1 figure, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-Covariate Gait Recognition: A <span class="highlight-title">Benchmark</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14404v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14404v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shinan Zou, Chao Fan, Jianbo Xiong, Chuanfu Shen, Shiqi Yu, Jin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gait datasets are essential for gait research. However, this paper observes
that present benchmarks, whether conventional constrained or emerging
real-world datasets, fall short regarding covariate diversity. To bridge this
gap, we undertake an arduous 20-month effort to collect a cross-covariate gait
recognition (CCGR) dataset. The CCGR dataset has 970 subjects and about 1.6
million sequences; almost every subject has 33 views and 53 different
covariates. Compared to existing datasets, CCGR has both population and
individual-level diversity. In addition, the views and covariates are well
labeled, enabling the analysis of the effects of different factors. CCGR
provides multiple types of gait data, including RGB, parsing, silhouette, and
pose, offering researchers a comprehensive resource for exploration. In order
to delve deeper into addressing cross-covariate gait recognition, we propose
parsing-based gait recognition (ParsingGait) by utilizing the newly proposed
parsing data. We have conducted extensive experiments. Our main results show:
1) Cross-covariate emerges as a pivotal challenge for practical applications of
gait recognition. 2) ParsingGait demonstrates remarkable potential for further
advancement. 3) Alarmingly, existing SOTA methods achieve less than 43%
accuracy on the CCGR, highlighting the urgency of exploring cross-covariate
gait recognition. Link: https://github.com/ShinanZou/CCGR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by AAAI2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Context-Aware Iteration Policy Network for Efficient Optical Flow
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07180v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07180v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ri Cheng, Ruian He, Xuhao Jiang, Shili Zhou, Weimin Tan, Bo Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing recurrent optical flow estimation networks are computationally
expensive since they use a fixed large number of iterations to update the flow
field for each sample. An efficient network should skip iterations when the
flow improvement is limited. In this paper, we develop a Context-Aware
Iteration Policy Network for efficient optical flow estimation, which
determines the optimal number of iterations per sample. The policy network
achieves this by learning contextual information to realize whether flow
improvement is bottlenecked or minimal. On the one hand, we use iteration
embedding and historical hidden cell, which include previous iterations
information, to convey how flow has changed from previous iterations. On the
other hand, we use the incremental loss to make the policy network implicitly
perceive the magnitude of optical flow improvement in the subsequent iteration.
Furthermore, the computational complexity in our dynamic network is
controllable, allowing us to satisfy various resource preferences with a single
trained model. Our policy network can be easily integrated into
state-of-the-art optical flow networks. Extensive experiments show that our
method maintains performance while reducing FLOPs by about 40%/20% for the
Sintel/KITTI datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2024, Association for the Advancement of Artificial Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeepMerge: Deep-Learning-Based Region-Merging for Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.19787v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.19787v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianwei Lv, Claudio Persello, Wangbin Li, Xiao Huang, Dongping Ming, Alfred Stein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image segmentation aims to partition an image according to the objects in the
scene and is a fundamental step in analysing very high spatial-resolution (VHR)
remote sensing imagery. Current methods struggle to effectively consider land
objects with diverse shapes and sizes. Additionally, the determination of
segmentation scale parameters frequently adheres to a static and empirical
doctrine, posing limitations on the segmentation of large-scale remote sensing
images and yielding algorithms with limited interpretability. To address the
above challenges, we propose a deep-learning-based region merging method dubbed
DeepMerge to handle the segmentation of complete objects in large VHR images by
integrating deep learning and region adjacency graph (RAG). This is the first
method to use deep learning to learn the similarity and merge similar adjacent
super-pixels in RAG. We propose a modified binary tree sampling method to
generate shift-scale data, serving as inputs for transformer-based deep
learning networks, a shift-scale attention with 3-Dimension relative position
embedding to learn features across scales, and an embedding to fuse learned
features with hand-crafted features. DeepMerge can achieve high segmentation
accuracy in a supervised manner from large-scale remotely sensed images and
provides an interpretable optimal scale parameter, which is validated using a
remote sensing image of 0.55 m resolution covering an area of 5,660 km^2. The
experimental results show that DeepMerge achieves the highest F value (0.9550)
and the lowest total error TE (0.0895), correctly segmenting objects of
different sizes and outperforming all competing segmentation methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FENet: Focusing Enhanced Network for Lane Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17163v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17163v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liman Wang, Hanyang Zhong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by human driving focus, this research pioneers networks augmented
with Focusing Sampling, Partial Field of View Evaluation, Enhanced FPN
architecture and Directional IoU Loss - targeted innovations addressing
obstacles to precise lane detection for autonomous driving. Experiments
demonstrate our Focusing Sampling strategy, emphasizing vital distant details
unlike uniform approaches, significantly boosts both benchmark and practical
curved/distant lane recognition accuracy essential for safety. While FENetV1
achieves state-of-the-art conventional metric performance via enhancements
isolating perspective-aware contexts mimicking driver vision, FENetV2 proves
most reliable on the proposed Partial Field analysis. Hence we specifically
recommend V2 for practical lane navigation despite fractional degradation on
standard entire-image measures. Future directions include collecting on-road
data and integrating complementary dual frameworks to further breakthroughs
guided by human perception principles. The Code is available at
https://github.com/HanyangZhong/FENet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages including appendix. The Code is available at
  https://github.com/HanyangZhong/FENet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predicting Parkinson's disease evolution using deep learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17290v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17290v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Frasca, Davide La Torre, Gabriella Pravettoni, Ilaria Cutica
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parkinson's disease is a neurological condition that occurs in nearly 1% of
the world's population. The disease is manifested by a drop in dopamine
production, symptoms are cognitive and behavioural and include a wide range of
personality changes, depressive disorders, memory problems, and emotional
dysregulation, which can occur as the disease progresses. Early diagnosis and
accurate staging of the disease are essential to apply the appropriate
therapeutic approaches to slow cognitive and motor decline.
  Currently, there is not a single blood test or biomarker available to
diagnose Parkinson's disease. Magnetic resonance imaging has been used for the
past three decades to diagnose and distinguish between PD and other
neurological conditions. However, in recent years new possibilities have
arisen: several AI algorithms have been developed to increase the precision and
accuracy of differential diagnosis of PD at an early stage.
  To our knowledge, no AI tools have been designed to identify the stage of
progression. This paper aims to fill this gap. Using the "Parkinson's
Progression Markers Initiative" dataset, which reports the patient's MRI and an
indication of the disease stage, we developed a model to identify the level of
progression. The images and the associated scores were used for training and
assessing different deep-learning models. Our analysis distinguished four
distinct disease progression levels based on a standard scale (Hoehn and Yah
scale). The final architecture consists of the cascading of a 3DCNN network,
adopted to reduce and extract the spatial characteristics of the RMI for
efficient training of the successive LSTM layers, aiming at modelling the
temporal dependencies among the data.
  Our results show that the proposed 3DCNN + LSTM model achieves
state-of-the-art results by classifying the elements with 91.90\% as macro
averaged OVR AUC on four classes
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Research on Multilingual Natural Scene Text Detection Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11153v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11153v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural scene text detection is a significant challenge in computer vision,
with tremendous potential applications in multilingual, diverse, and complex
text scenarios. We propose a multilingual text detection model to address the
issues of low accuracy and high difficulty in detecting multilingual text in
natural scenes. In response to the challenges posed by multilingual text images
with multiple character sets and various font styles, we introduce the SFM Swin
Transformer feature extraction network to enhance the model's robustness in
detecting characters and fonts across different languages. Dealing with the
considerable variation in text scales and complex arrangements in natural scene
text images, we present the AS-HRFPN feature fusion network by incorporating an
Adaptive Spatial Feature Fusion module and a Spatial Pyramid Pooling module.
The feature fusion network improvements enhance the model's ability to detect
text sizes and orientations. Addressing diverse backgrounds and font variations
in multilingual scene text images is a challenge for existing methods. Limited
local receptive fields hinder detection performance. To overcome this, we
propose a Global Semantic Segmentation Branch, extracting and preserving global
features for more effective text detection, aligning with the need for
comprehensive information. In this study, we collected and built a real-world
multilingual natural scene text image dataset and conducted comprehensive
experiments and analyses. The experimental results demonstrate that the
proposed algorithm achieves an F-measure of 85.02\%, which is 4.71\% higher
than the baseline model. We also conducted extensive cross-dataset validation
on MSRA-TD500, ICDAR2017MLT, and ICDAR2015 datasets to verify the generality of
our approach. The code and dataset can be found at
https://github.com/wangmelon/CEMLT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Sorry, we discovered certain mistake and asked that the current
  version be removed in order to perform a thorough reanalysis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tissue Artifact Segmentation and Severity Analysis for Automated
  Diagnosis Using Whole Slide Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01386v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01386v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Galib Muhammad Shahriar Himel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditionally, pathological analysis and diagnosis are performed by manually
eyeballing glass slide specimens under a microscope by an expert. The whole
slide image is the digital specimen produced from the glass slide. Whole slide
image enabled specimens to be observed on a computer screen and led to
computational pathology where computer vision and artificial intelligence are
utilized for automated analysis and diagnosis. With the current computational
advancement, the entire whole slide image can be analyzed autonomously without
human supervision. However, the analysis could fail or lead to wrong diagnosis
if the whole slide image is affected by tissue artifacts such as tissue fold or
air bubbles depending on the severity. Existing artifact detection methods rely
on experts for severity assessment to eliminate artifact affected regions from
the analysis. This process is time consuming, exhausting and undermines the
goal of automated analysis or removal of artifacts without evaluating their
severity, which could result in the loss of diagnostically important data.
Therefore, it is necessary to detect artifacts and then assess their severity
automatically. In this paper, we propose a system that incorporates severity
evaluation with artifact detection utilizing convolutional neural networks. The
proposed system uses DoubleUNet to segment artifacts and an ensemble network of
six fine tuned convolutional neural network models to determine severity. This
method outperformed current state of the art in accuracy by 9 percent for
artifact segmentation and achieved a strong correlation of 97 percent with the
evaluation of pathologists for severity assessment. The robustness of the
system was demonstrated using our proposed heterogeneous dataset and practical
usability was ensured by integrating it with an automated analysis system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>60 pages, 21 figures, 16 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-aligned supervision for Real Image Dehazing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04940v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04940v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junkai Fan, Fei Guo, Jianjun Qian, Xiang Li, Jun Li, Jian Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Removing haze from real-world images is challenging due to unpredictable
weather conditions, resulting in the misalignment of hazy and clear image
pairs. In this paper, we propose an innovative dehazing framework that operates
under non-aligned supervision. This framework is grounded in the atmospheric
scattering model, and consists of three interconnected networks: dehazing,
airlight, and transmission networks. In particular, we explore a non-alignment
scenario that a clear reference image, unaligned with the input hazy image, is
utilized to supervise the dehazing network. To implement this, we present a
multi-scale reference loss that compares the feature representations between
the referred image and the dehazed output. Our scenario makes it easier to
collect hazy/clear image pairs in real-world environments, even under
conditions of misalignment and shift views. To showcase the effectiveness of
our scenario, we have collected a new hazy dataset including 415 image pairs
captured by mobile Phone in both rural and urban areas, called "Phone-Hazy".
Furthermore, we introduce a self-attention network based on mean and variance
for modeling real infinite airlight, using the dark channel prior as positional
guidance. Additionally, a channel attention network is employed to estimate the
three-channel transmission. Experimental results demonstrate the superior
performance of our framework over existing state-of-the-art techniques in the
real-world image dehazing task. Phone-Hazy and code will be available at
https://fanjunkai1.github.io/projectpage/NSDNet/index.html.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-attention learning enables real-time nonuniform rotational
  distortion correction in OCT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04512v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04512v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Zhang, Jianlong Yang, Jingqian Zhang, Shiqing Zhao, Aili Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nonuniform rotational distortion (NURD) correction is vital for endoscopic
optical coherence tomography (OCT) imaging and its functional extensions, such
as angiography and elastography. Current NURD correction methods require
time-consuming feature tracking or cross-correlation calculations and thus
sacrifice temporal resolution. Here we propose a cross-attention learning
method for the NURD correction in OCT. Our method is inspired by the recent
success of the self-attention mechanism in natural language processing and
computer vision. By leveraging its ability to model long-range dependencies, we
can directly obtain the correlation between OCT A-lines at any distance, thus
accelerating the NURD correction. We develop an end-to-end stacked
cross-attention network and design three types of optimization constraints. We
compare our method with two traditional feature-based methods and a CNN-based
method, on two publicly-available endoscopic OCT datasets and a private dataset
collected on our home-built endoscopic OCT system. Our method achieved a
$\sim3\times$ speedup to real time ($26\pm 3$ fps), and superior correction
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised CT Metal Artifact Reduction by Plugging Diffusion Priors in
  Dual Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.16742v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.16742v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Liu, Yaoqin Xie, Songhui Diao, Shan Tan, Xiaokun Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  During the process of computed tomography (CT), metallic implants often cause
disruptive artifacts in the reconstructed images, impeding accurate diagnosis.
Several supervised deep learning-based approaches have been proposed for
reducing metal artifacts (MAR). However, these methods heavily rely on training
with simulated data, as obtaining paired metal artifact CT and clean CT data in
clinical settings is challenging. This limitation can lead to decreased
performance when applying these methods in clinical practice. Existing
unsupervised MAR methods, whether based on learning or not, typically operate
within a single domain, either in the image domain or the sinogram domain. In
this paper, we propose an unsupervised MAR method based on the diffusion model,
a generative model with a high capacity to represent data distributions.
Specifically, we first train a diffusion model using CT images without metal
artifacts. Subsequently, we iteratively utilize the priors embedded within the
pre-trained diffusion model in both the sinogram and image domains to restore
the degraded portions caused by metal artifacts. This dual-domain processing
empowers our approach to outperform existing unsupervised MAR methods,
including another MAR method based on the diffusion model, which we have
qualitatively and quantitatively validated using synthetic datasets. Moreover,
our method demonstrates superior visual results compared to both supervised and
unsupervised methods on clinical datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.07728v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.07728v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seokhyeon Ha, Sunbeom Jung, Jungwoo Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning pre-trained neural network models has become a widely adopted
approach across various domains. However, it can lead to the distortion of
pre-trained feature extractors that already possess strong generalization
capabilities. Mitigating feature distortion during adaptation to new target
domains is crucial. Recent studies have shown promising results in handling
feature distortion by aligning the head layer on in-distribution datasets
before performing fine-tuning. Nonetheless, a significant limitation arises
from the treatment of batch normalization layers during fine-tuning, leading to
suboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning
(DAFT), a novel approach that incorporates batch normalization conversion and
the integration of linear probing and fine-tuning. Our batch normalization
conversion method effectively mitigates feature distortion by reducing
modifications to the neural network during fine-tuning. Additionally, we
introduce the integration of linear probing and fine-tuning to optimize the
head layer with gradual adaptation of the feature extractor. By leveraging
batch normalization layers and integrating linear probing and fine-tuning, our
DAFT significantly mitigates feature distortion and achieves improved model
performance on both in-distribution and out-of-distribution datasets. Extensive
experiments demonstrate that our method outperforms other baseline methods,
demonstrating its effectiveness in not only improving performance but also
mitigating feature distortion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Open and Comprehensive Pipeline for Unified Object Grounding and
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02361v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02361v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Zhao, Yicheng Chen, Shilin Xu, Xiangtai Li, Xinjiang Wang, Yining Li, Haian Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grounding-DINO is a state-of-the-art open-set detection model that tackles
multiple vision tasks including Open-Vocabulary Detection (OVD), Phrase
Grounding (PG), and Referring Expression Comprehension (REC). Its effectiveness
has led to its widespread adoption as a mainstream architecture for various
downstream applications. However, despite its significance, the original
Grounding-DINO model lacks comprehensive public technical details due to the
unavailability of its training code. To bridge this gap, we present
MM-Grounding-DINO, an open-source, comprehensive, and user-friendly baseline,
which is built with the MMDetection toolbox. It adopts abundant vision datasets
for pre-training and various detection and grounding datasets for fine-tuning.
We give a comprehensive analysis of each reported result and detailed settings
for reproduction. The extensive experiments on the benchmarks mentioned
demonstrate that our MM-Grounding-DINO-Tiny outperforms the Grounding-DINO-Tiny
baseline. We release all our models to the research community. Codes and
trained models are released at
https://github.com/open-mmlab/mmdetection/tree/main/configs/mm_grounding_dino.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11700v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11700v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi Yan, Delin Qu, Dong Wang, Dan Xu, Zhigang Wang, Bin Zhao, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce $\textbf{GS-SLAM}$ that first utilizes 3D
Gaussian representation in the Simultaneous Localization and Mapping (SLAM)
system. It facilitates a better balance between efficiency and accuracy.
Compared to recent SLAM methods employing neural implicit representations, our
method utilizes a real-time differentiable splatting rendering pipeline that
offers significant speedup to map optimization and RGB-D re-rendering.
Specifically, we propose an adaptive expansion strategy that adds new or
deletes noisy 3D Gaussian in order to efficiently reconstruct new observed
scene geometry and improve the mapping of previously observed areas. This
strategy is essential to extend 3D Gaussian representation to reconstruct the
whole scene rather than synthesize a static object in existing methods.
Moreover, in the pose tracking process, an effective coarse-to-fine technique
is designed to select reliable 3D Gaussian representations to optimize camera
pose, resulting in runtime reduction and robust estimation. Our method achieves
competitive performance compared with existing state-of-the-art real-time
methods on the Replica, TUM-RGBD datasets. The source code will be released
soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ControlDreamer: Stylized 3D Generation with Multi-View ControlNet 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01129v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01129v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeongtak Oh, Jooyoung Choi, Yongsung Kim, Minjun Park, Chaehun Shin, Sungroh Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in text-to-3D generation have significantly contributed
to the automation and democratization of 3D content creation. Building upon
these developments, we aim to address the limitations of current methods in
generating 3D models with creative geometry and styles. We introduce multi-view
ControlNet, a novel depth-aware multi-view diffusion model trained on generated
datasets from a carefully curated text corpus. Our multi-view ControlNet is
then integrated into our two-stage pipeline, ControlDreamer, enabling
text-guided generation of stylized 3D models. Additionally, we present a
comprehensive benchmark for 3D style editing, encompassing a broad range of
subjects, including objects, animals, and characters, to further facilitate
research on diverse 3D generation. Our comparative analysis reveals that this
new pipeline outperforms existing text-to-3D methods as evidenced by human
evaluations and CLIP score metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://controldreamer.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MatchDet: A Collaborative Framework for Image Matching and Object
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10983v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10983v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinxiang Lai, Wenlong Wu, Bin-Bin Gao, Jun Liu, Jiawei Zhan, Congchong Nie, Yi Zeng, Chengjie Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image matching and object detection are two fundamental and challenging
tasks, while many related applications consider them two individual tasks (i.e.
task-individual). In this paper, a collaborative framework called MatchDet
(i.e. task-collaborative) is proposed for image matching and object detection
to obtain mutual improvements. To achieve the collaborative learning of the two
tasks, we propose three novel modules, including a Weighted Spatial Attention
Module (WSAM) for Detector, and Weighted Attention Module (WAM) and Box Filter
for Matcher. Specifically, the WSAM highlights the foreground regions of target
image to benefit the subsequent detector, the WAM enhances the connection
between the foreground regions of pair images to ensure high-quality matches,
and Box Filter mitigates the impact of false matches. We evaluate the
approaches on a new benchmark with two datasets called Warp-COCO and
miniScanNet. Experimental results show our approaches are effective and achieve
competitive improvements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TR-DETR: Task-Reciprocal Transformer for Joint Moment Retrieval and
  Highlight Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02309v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02309v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Sun, Mingyao Zhou, Wenjing Chen, Wei Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video moment retrieval (MR) and highlight detection (HD) based on natural
language queries are two highly related tasks, which aim to obtain relevant
moments within videos and highlight scores of each video clip. Recently,
several methods have been devoted to building DETR-based networks to solve both
MR and HD jointly. These methods simply add two separate task heads after
multi-modal feature extraction and feature interaction, achieving good
performance. Nevertheless, these approaches underutilize the reciprocal
relationship between two tasks. In this paper, we propose a task-reciprocal
transformer based on DETR (TR-DETR) that focuses on exploring the inherent
reciprocity between MR and HD. Specifically, a local-global multi-modal
alignment module is first built to align features from diverse modalities into
a shared latent space. Subsequently, a visual feature refinement is designed to
eliminate query-irrelevant information from visual features for modal
interaction. Finally, a task cooperation module is constructed to refine the
retrieval pipeline and the highlight score prediction process by utilizing the
reciprocity between MR and HD. Comprehensive experiments on QVHighlights,
Charades-STA and TVSum datasets demonstrate that TR-DETR outperforms existing
state-of-the-art methods. Codes are available at
\url{https://github.com/mingyao1120/TR-DETR}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI-24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Incremental Rotation Averaging Revisited and More: A New Rotation
  Averaging <span class="highlight-title">Benchmark</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.16924v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.16924v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Gao, Hainan Cui, Shuhan Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In order to further advance the accuracy and robustness of the incremental
parameter estimation-based rotation averaging methods, in this paper, a new
member of the Incremental Rotation Averaging (IRA) family is introduced, which
is termed as IRAv4. As the most significant feature of the IRAv4, a
task-specific connected dominating set is extracted to serve as a more reliable
and accurate reference for rotation global alignment. In addition, to further
address the limitations of the existing rotation averaging benchmark of relying
on the slightly outdated Bundler camera calibration results as ground truths
and focusing solely on rotation estimation accuracy, this paper presents a new
COLMAP-based rotation averaging benchmark that incorporates a cross check
between COLMAP and Bundler, and employ the accuracy of both rotation and
downstream location estimation as evaluation metrics, which is desired to
provide a more reliable and comprehensive evaluation tool for the rotation
averaging research. Comprehensive comparisons between the proposed IRAv4 and
other mainstream rotation averaging methods on this new benchmark demonstrate
the effectiveness of our proposed approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Transactions</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast-iTPN: Integrally Pre-Trained Transformer Pyramid Network with Token
  Migration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12735v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12735v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunjie Tian, Lingxi Xie, Jihao Qiu, Jianbin Jiao, Yaowei Wang, Qi Tian, Qixiang Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose integrally pre-trained transformer pyramid network (iTPN), towards
jointly optimizing the network backbone and the neck, so that transfer gap
between representation models and downstream tasks is minimal. iTPN is born
with two elaborated designs: 1) The first pre-trained feature pyramid upon
vision transformer (ViT). 2) Multi-stage supervision to the feature pyramid
using masked feature modeling (MFM). iTPN is updated to Fast-iTPN, reducing
computational memory overhead and accelerating inference through two flexible
designs. 1) Token migration: dropping redundant tokens of the backbone while
replenishing them in the feature pyramid without attention operations. 2) Token
gathering: reducing computation cost caused by global attention by introducing
few gathering tokens. The base/large-level Fast-iTPN achieve 88.75%/89.5% top-1
accuracy on ImageNet-1K. With 1x training schedule using DINO, the
base/large-level Fast-iTPN achieves 58.4%/58.8% box AP on COCO object
detection, and a 57.5%/58.7% mIoU on ADE20K semantic segmentation using
MaskDINO. Fast-iTPN can accelerate the inference procedure by up to 70%, with
negligible performance loss, demonstrating the potential to be a powerful
backbone for downstream vision tasks. The code is available at:
github.com/sunsmarterjie/iTPN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The tiny/small/base-level models report new records on ImageNet-1K.
  Code: github.com/sunsmarterjie/iTPN</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning from History: Task-agnostic Model Contrastive Learning for
  Image Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.06023v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.06023v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gang Wu, Junjun Jiang, Kui Jiang, Xianming Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has emerged as a prevailing paradigm for high-level
vision tasks, which, by introducing properly negative samples, has also been
exploited for low-level vision tasks to achieve a compact optimization space to
account for their ill-posed nature. However, existing methods rely on manually
predefined and task-oriented negatives, which often exhibit pronounced
task-specific biases. To address this challenge, our paper introduces an
innovative method termed 'learning from history', which dynamically generates
negative samples from the target model itself. Our approach, named Model
Contrastive paradigm for Image Restoration (MCIR), rejuvenates latency models
as negative models, making it compatible with diverse image restoration tasks.
We propose the Self-Prior guided Negative loss (SPN) to enable it. This
approach significantly enhances existing models when retrained with the
proposed model contrastive paradigm. The results show significant improvements
in image restoration across various tasks and architectures. For example,
models retrained with SPN outperform the original FFANet and DehazeFormer by
3.41 dB and 0.57 dB on the RESIDE indoor dataset for image dehazing. Similarly,
they achieve notable improvements of 0.47 dB on SPA-Data over IDT for image
deraining and 0.12 dB on Manga109 for a 4x scale super-resolution over
lightweight SwinIR, respectively. Code and retrained models are available at
https://github.com/Aitical/MCIR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera Ready Version. Accepted to The 38th Annual AAAI Conference on
  Artificial Intelligence (AAAI 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploiting Spatial-Temporal Context for Interacting Hand Reconstruction
  on Monocular RGB Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.04074v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.04074v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weichao Zhao, Hezhen Hu, Wengang Zhou, Li li, Houqiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing interacting hands from monocular RGB data is a challenging
task, as it involves many interfering factors, e.g. self- and mutual occlusion
and similar textures. Previous works only leverage information from a single
RGB image without modeling their physically plausible relation, which leads to
inferior reconstruction results. In this work, we are dedicated to explicitly
exploiting spatial-temporal information to achieve better interacting hand
reconstruction. On one hand, we leverage temporal context to complement
insufficient information provided by the single frame, and design a novel
temporal framework with a temporal constraint for interacting hand motion
smoothness. On the other hand, we further propose an interpenetration detection
module to produce kinetically plausible interacting hands without physical
collisions. Extensive experiments are performed to validate the effectiveness
of our proposed framework, which achieves new state-of-the-art performance on
public benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by TOMM 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Applications of Large Scale Foundation Models for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12144v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12144v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Huang, Yue Chen, Zhu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since DARPA Grand Challenges (rural) in 2004/05 and Urban Challenges in 2007,
autonomous driving has been the most active field of AI applications. Recently
powered by large language models (LLMs), chat systems, such as chatGPT and
PaLM, emerge and rapidly become a promising direction to achieve artificial
general intelligence (AGI) in natural language processing (NLP). There comes a
natural thinking that we could employ these abilities to reformulate autonomous
driving. By combining LLM with foundation models, it is possible to utilize the
human knowledge, commonsense and reasoning to rebuild autonomous driving
systems from the current long-tailed AI dilemma. In this paper, we investigate
the techniques of foundation models and LLMs applied for autonomous driving,
categorized as simulation, world model, data annotation and planning or E2E
solutions etc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages. A survey paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GeoLocator: a location-integrated large multimodal model for inferring
  geo-privacy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13018v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13018v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Yang, Siqin Wang, Daoyang Li, Yixian Zhang, Shuju Sun, Junzhou He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geographic privacy or geo-privacy refers to the keeping private of one's
geographic location, especially the restriction of geographical data maintained
by personal electronic devices. Geo-privacy is a crucial aspect of personal
security; however, it often goes unnoticed in daily activities. With the surge
in the use of Large Multimodal Models (LMMs), such as GPT-4, for Open Source
Intelligence (OSINT), the potential risks associated with geo-privacy breaches
have intensified. This study develops a location-integrated GPT-4 based model
named GeoLocator and designs four-dimensional experiments to demonstrate its
capability in inferring the locational information of input imageries and/or
social media contents. Our experiments reveal that GeoLocator generates
specific geographic details with high accuracy and consequently embeds the risk
of the model users exposing geospatial information to the public
unintentionally, highlighting the thread of online data sharing, information
gathering technologies and LLMs on geo-privacy. We conclude with the broader
implications of GeoLocator and our findings for individuals and the community
at large, by emphasizing the urgency for enhanced awareness and protective
measures against geo-privacy leakage in the era of advanced AI and widespread
social media usage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large-scale data extraction from the UNOS organ donor documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15752v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15752v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marek Rychlik, Bekir Tanriover, Yan Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we focus on three major task: 1) discussing our methods: Our
method captures a portion of the data in DCD flowsheets, kidney perfusion data,
and Flowsheet data captured peri-organ recovery surgery. 2) demonstrating the
result: We built a comprehensive, analyzable database from 2022 OPTN data. This
dataset is by far larger than any previously available even in this preliminary
phase; and 3) proving that our methods can be extended to all the past OPTN
data and future data.
  The scope of our study is all Organ Procurement and Transplantation Network
(OPTN) data of the USA organ donors since 2008. The data was not analyzable in
a large scale in the past because it was captured in PDF documents known as
``Attachments'', whereby every donor's information was recorded into dozens of
PDF documents in heterogeneous formats. To make the data analyzable, one needs
to convert the content inside these PDFs to an analyzable data format, such as
a standard SQL database. In this paper we will focus on 2022 OPTN data, which
consists of $\approx 400,000$ PDF documents spanning millions of pages. The
entire OPTN data covers 15 years (2008--20022). This paper assumes that readers
are familiar with the content of the OPTN data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Single-subject Multi-contrast MRI Super-resolution via Implicit Neural
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.15065v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.15065v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian McGinnis, Suprosanna Shit, Hongwei Bran Li, Vasiliki Sideri-Lampretsa, Robert Graf, Maik Dannecker, Jiazhen Pan, Nil Stolt Ansó, Mark Mühlau, Jan S. Kirschke, Daniel Rueckert, Benedikt Wiestler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinical routine and retrospective cohorts commonly include multi-parametric
Magnetic Resonance Imaging; however, they are mostly acquired in different
anisotropic 2D views due to signal-to-noise-ratio and scan-time constraints.
Thus acquired views suffer from poor out-of-plane resolution and affect
downstream volumetric image analysis that typically requires isotropic 3D
scans. Combining different views of multi-contrast scans into high-resolution
isotropic 3D scans is challenging due to the lack of a large training cohort,
which calls for a subject-specific framework. This work proposes a novel
solution to this problem leveraging Implicit Neural Representations (INR). Our
proposed INR jointly learns two different contrasts of complementary views in a
continuous spatial function and benefits from exchanging anatomical information
between them. Trained within minutes on a single commodity GPU, our model
provides realistic super-resolution across different pairs of contrasts in our
experiments with three datasets. Using Mutual Information (MI) as a metric, we
find that our model converges to an optimum MI amongst sequences, achieving
anatomically faithful reconstruction. Code is available at:
https://github.com/jqmcginnis/multi_contrast_inr/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SCoRD: Subject-Conditional Relation Detection with Text-Augmented Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12910v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12910v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyan Yang, Kushal Kafle, Zhe Lin, Scott Cohen, Zhihong Ding, Vicente Ordonez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Subject-Conditional Relation Detection SCoRD, where conditioned on
an input subject, the goal is to predict all its relations to other objects in
a scene along with their locations. Based on the Open Images dataset, we
propose a challenging OIv6-SCoRD benchmark such that the training and testing
splits have a distribution shift in terms of the occurrence statistics of
$\langle$subject, relation, object$\rangle$ triplets. To solve this problem, we
propose an auto-regressive model that given a subject, it predicts its
relations, objects, and object locations by casting this output as a sequence
of tokens. First, we show that previous scene-graph prediction methods fail to
produce as exhaustive an enumeration of relation-object pairs when conditioned
on a subject on this benchmark. Particularly, we obtain a recall@3 of 83.8% for
our relation-object predictions compared to the 49.75% obtained by a recent
scene graph detector. Then, we show improved generalization on both
relation-object and object-box predictions by leveraging during training
relation-object pairs obtained automatically from textual captions and for
which no object-box annotations are available. Particularly, for
$\langle$subject, relation, object$\rangle$ triplets for which no object
locations are available during training, we are able to obtain a recall@3 of
33.80% for relation-object pairs and 26.75% for their box locations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">17</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Reinforcement Learning for Local Path Following of an Autonomous
  Formula SAE Vehicle 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02903v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02903v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harvey Merton, Thomas Delamore, Karl Stol, Henry Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the continued introduction of driverless events to Formula:Society of
Automotive Engineers (F:SAE) competitions around the world, teams are
investigating all aspects of the autonomous vehicle stack. This paper presents
the use of Deep Reinforcement Learning (DRL) and Inverse Reinforcement Learning
(IRL) to map locally-observed cone positions to a desired steering angle for
race track following. Two state-of-the-art algorithms not previously tested in
this context: soft actor critic (SAC) and adversarial inverse reinforcement
learning (AIRL), are used to train models in a representative simulation. Three
novel reward functions for use by RL algorithms in an autonomous racing context
are also discussed. Tests performed in simulation and the real world suggest
that both algorithms can successfully train models for local path following.
Suggestions for future work are presented to allow these models to scale to a
full F:SAE vehicle.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>As presented at the Australasian Conference on Robotics and
  Automation (ACRA 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model predictive altitude and velocity control in ergodic potential
  field directed multi-UAV search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02899v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02899v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luka Lanča, Karlo Jakac, Stefan Ivić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research addresses the challenge of executing multi-UAV survey missions
over diverse terrains characterized by varying elevations. The approach
integrates advanced two-dimensional ergodic search technique with model
predictive control of UAV altitude and velocity. Optimization of altitude and
velocity is performed along anticipated UAV ground routes, considering multiple
objectives and constraints. This yields a flight regimen tailored to the
terrain, as well as the motion and sensing characteristics of the UAVs. The
proposed UAV motion control strategy is assessed through simulations of
realistic search missions and actual terrain models. Results demonstrate the
successful integration of model predictive altitude and velocity control with a
two-dimensional potential field-guided ergodic search. Adjusting UAV altitudes
to near-ideal levels facilitates the utilization of sensing ranges, thereby
enhancing the effectiveness of the search. Furthermore, the control algorithm
is capable of real-time computation, encouraging its practical application in
real-world scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ iPolicy: Incremental Policy Algorithms for Feedback Motion Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02883v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02883v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoxiang Zhao, Devesh K. Jha, Yebin Wang, Minghui Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents policy-based motion planning for robotic systems. The
motion planning literature has been mostly focused on open-loop trajectory
planning which is followed by tracking online. In contrast, we solve the
problem of path planning and controller synthesis simultaneously by solving the
related feedback control problem. We present a novel incremental policy
(iPolicy) algorithm for motion planning, which integrates sampling-based
methods and set-valued optimal control methods to compute feedback controllers
for the robotic system. In particular, we use sampling to incrementally
construct the state space of the system. Asynchronous value iterations are
performed on the sampled state space to synthesize the incremental policy
feedback controller. We show the convergence of the estimates to the optimal
value function in continuous state space. Numerical results with various
different dynamical systems (including nonholonomic systems) verify the
optimality and effectiveness of iPolicy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Omnidirectional Multi-Rotor Aerial Vehicle Pose Optimization: A Novel
  Approach to Physical Layer Security 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Bonilla Licea, Giuseppe Silano, Mounir Ghogho, Martin Saska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of Multi-Rotor Aerial Vehicles (MRAVs) into 5G and 6G
networks enhances coverage, connectivity, and congestion management. This
fosters communication-aware robotics, exploring the interplay between robotics
and communications, but also makes the MRAVs susceptible to malicious attacks,
such as jamming. One traditional approach to counter these attacks is the use
of beamforming on the MRAVs to apply physical layer security techniques. In
this paper, we explore pose optimization as an alternative approach to
countering jamming attacks on MRAVs. This technique is intended for
omnidirectional MRAVs, which are drones capable of independently controlling
both their position and orientation, as opposed to the more common
underactuated MRAVs whose orientation cannot be controlled independently of
their position. In this paper, we consider an omnidirectional MRAV serving as a
Base Station (BS) for legitimate ground nodes, under attack by a malicious
jammer. We optimize the MRAV pose (i.e., position and orientation) to maximize
the minimum Signal-to-Interference-plus-Noise Ratio (SINR) over all legitimate
nodes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures, Accepted for presentation to the 2024 IEEE
  International Conference on Acoustics, Speech, and Signal Processing (ICASSP
  2024), Seoul, Korea. Copyright may be transferred without notice, after which
  this version may no longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating Flow Theory and Adaptive Robot Roles: A Conceptual Model of
  Dynamic Robot Role Adaptation for the Enhanced Flow Experience in Long-term
  Multi-person Human-Robot Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huili Chen, Sharifa Alghowinem, Cynthia Breazeal, Hae Won Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel conceptual model for a robot's behavioral
adaptation in its long-term interaction with humans, integrating dynamic robot
role adaptation with principles of flow experience from psychology. This
conceptualization introduces a hierarchical interaction objective grounded in
the flow experience, serving as the overarching adaptation goal for the robot.
This objective intertwines both cognitive and affective sub-objectives and
incorporates individual and group-level human factors. The dynamic role
adaptation approach is a cornerstone of our model, highlighting the robot's
ability to fluidly adapt its support roles - from leader to follower - with the
aim of maintaining equilibrium between activity challenge and user skill,
thereby fostering the user's optimal flow experiences. Moreover, this work
delves into a comprehensive exploration of the limitations and potential
applications of our proposed conceptualization. Our model places a particular
emphasis on the multi-person HRI paradigm, a dimension of HRI that is both
under-explored and challenging. In doing so, we aspire to extend the
applicability and relevance of our conceptualization within the HRI field,
contributing to the future development of adaptive social robots capable of
sustaining long-term interactions with humans.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparative Evaluation of RGB-D SLAM Methods for Humanoid Robot
  Localization and Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02816v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02816v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirhosein Vedadi, Aghil Yousefi-Koma, Parsa Yazdankhah, Amin Mozayyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we conducted a comparative evaluation of three RGB-D SLAM
(Simultaneous Localization and Mapping) algorithms: RTAB-Map, ORB-SLAM3, and
OpenVSLAM for SURENA-V humanoid robot localization and mapping. Our test
involves the robot to follow a full circular pattern, with an Intel RealSense
D435 RGB-D camera installed on its head. In assessing localization accuracy,
ORB-SLAM3 outperformed the others with an ATE of 0.1073, followed by RTAB-Map
at 0.1641 and OpenVSLAM at 0.1847. However, it should be noted that both
ORB-SLAM3 and OpenVSLAM faced challenges in maintaining accurate odometry when
the robot encountered a wall with limited feature points. Nevertheless,
OpenVSLAM demonstrated the ability to detect loop closures and successfully
relocalize itself within the map when the robot approached its initial
location. The investigation also extended to mapping capabilities, where
RTAB-Map excelled by offering diverse mapping outputs, including dense,
OctoMap, and occupancy grid maps. In contrast, both ORB-SLAM3 and OpenVSLAM
provided only sparse maps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 11th RSI International Conference on Robotics and
  Mechatronics (ICRoM 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Object-Centric Instruction Augmentation for Robotic Manipulation <span class="chip">ICRA2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Wen, Yichen Zhu, Minjie Zhu, Jinming Li, Zhiyuan Xu, Zhengping Che, Chaomin Shen, Yaxin Peng, Dong Liu, Feifei Feng, Jian Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans interpret scenes by recognizing both the identities and positions of
objects in their observations. For a robot to perform tasks such as
\enquote{pick and place}, understanding both what the objects are and where
they are located is crucial. While the former has been extensively discussed in
the literature that uses the large language model to enrich the text
descriptions, the latter remains underexplored. In this work, we introduce the
\textit{Object-Centric Instruction Augmentation (OCI)} framework to augment
highly semantic and information-dense language instruction with position cues.
We utilize a Multi-modal Large Language Model (MLLM) to weave knowledge of
object locations into natural language instruction, thus aiding the policy
network in mastering actions for versatile manipulation. Additionally, we
present a feature reuse mechanism to integrate the vision-language features
from off-the-shelf pre-trained MLLM into policy networks. Through a series of
simulated and real-world robotic tasks, we demonstrate that robotic manipulator
imitation policies trained with our enhanced instructions outperform those
relying solely on traditional language instructions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICRA2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Kinematic Base State Estimation for Humanoid using Invariant Extended
  Kalman Filter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirhosein Vedadi, Aghil Yousefi-Koma, Masoud Shariat-Panahi, Mahdi Nozari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the design and implementation of a Right Invariant
Extended Kalman Filter (RIEKF) for estimating the states of the kinematic base
of the Surena V humanoid robot. The state representation of the robot is
defined on the Lie group $SE_4(3)$, encompassing the position, velocity, and
orientation of the base, as well as the position of the left and right feet. In
addition, we incorporated IMU biases as concatenated states within the filter.
  The prediction step of the RIEKF utilizes IMU equations, while the update
step incorporates forward kinematics. To evaluate the performance of the RIEKF,
we conducted experiments using the Choreonoid dynamic simulation framework and
compared it against a Quaternion-based Extended Kalman Filter (QEKF). The
results of the analysis demonstrate that the RIEKF exhibits reduced drift in
localization and achieves estimation convergence in a shorter time compared to
the QEKF. These findings highlight the effectiveness of the proposed RIEKF for
accurate state estimation of the kinematic base in humanoid robotics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 11th RSI International Conference on Robotics and
  Mechatronics (ICRoM 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Design Optimization of Wire Arrangement with Variable Relay Points in
  Numerical Simulation for Tendon-driven Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kento Kawaharazuka, Shunnosuke Yoshimura, Temma Suzuki, Kei Okada, Masayuki Inaba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the most important features of tendon-driven robots is the ease of
wire arrangement and the degree of freedom it affords, enabling the
construction of a body that satisfies the desired characteristics by modifying
the wire arrangement. Various wire arrangement optimization methods have been
proposed, but they have simplified the configuration by assuming that the
moment arm of wires to joints are constant, or by disregarding wire
arrangements that span multiple joints and include relay points. In this study,
we formulate a more flexible wire arrangement optimization problem in which
each wire is represented by a start point, multiple relay points, and an end
point, and achieve the desired physical performance based on black-box
optimization. We consider a multi-objective optimization which simultaneously
takes into account both the feasible operational force space and velocity
space, and discuss the optimization results obtained from various
configurations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at IEEE Robotics and Automation Letters (RA-L), website -
  https://haraduka.github.io/muscle-arrange-optimization/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VoroNav: Voronoi-based Zero-shot Object <span class="highlight-title">Navigation</span> with Large Language
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengying Wu, Yao Mu, Bingxian Wu, Yi Hou, Ji Ma, Shanghang Zhang, Chang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of household robotics, the Zero-Shot Object Navigation (ZSON)
task empowers agents to adeptly traverse unfamiliar environments and locate
objects from novel categories without prior explicit training. This paper
introduces VoroNav, a novel semantic exploration framework that proposes the
Reduced Voronoi Graph to extract exploratory paths and planning nodes from a
semantic map constructed in real time. By harnessing topological and semantic
information, VoroNav designs text-based descriptions of paths and images that
are readily interpretable by a large language model (LLM). Our approach
presents a synergy of path and farsight descriptions to represent the
environmental context, enabling the LLM to apply commonsense reasoning to
ascertain the optimal waypoints for navigation. Extensive evaluation on the
HM3D and HSSD datasets validates that VoroNav surpasses existing ZSON
benchmarks in both success rates and exploration efficiency (+2.8% Success and
+3.7% SPL on HM3D, +2.6% Success and +3.8% SPL on HSSD). Additionally
introduced metrics that evaluate obstacle avoidance proficiency and perceptual
efficiency further corroborate the enhancements achieved by our method in ZSON
planning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robot Vulnerability and the Elicitation of User Empathy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02684v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02684v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Morten Roed Frederiksen, Katrin Fischer, Maja Matarić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes a between-subjects Amazon Mechanical Turk study (n =
220) that investigated how a robot's affective narrative influences its ability
to elicit empathy in human observers. We first conducted a pilot study to
develop and validate the robot's affective narratives. Then, in the full study,
the robot used one of three different affective narrative strategies (funny,
sad, neutral) while becoming less functional at its shopping task over the
course of the interaction. As the functionality of the robot degraded,
participants were repeatedly asked if they were willing to help the robot. The
results showed that conveying a sad narrative significantly influenced the
participants' willingness to help the robot throughout the interaction and
determined whether participants felt empathetic toward the robot throughout the
interaction. Furthermore, a higher amount of past experience with robots also
increased the participants' willingness to help the robot. This work suggests
that affective narratives can be useful in short-term interactions that benefit
from emotional connections between humans and robots.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published by and copyright protected by IEEE, 8 pages, 4 figures,
  31st IEEE International Conference on Robot & Human Interactive Communication
  (RO-MAN 2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The RoSiD Tool: Empowering Users to Design Multimodal Signals for
  Human-Robot Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathaniel Dennler, David Delgado, Daniel Zeng, Stefanos Nikolaidis, Maja Matarić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robots that cooperate with humans must be effective at communicating with
them. However, people have varied preferences for communication based on many
contextual factors, such as culture, environment, and past experience. To
communicate effectively, robots must take those factors into consideration. In
this work, we present the Robot Signal Design (RoSiD) tool to empower people to
easily self-specify communicative preferences for collaborative robots. We show
through a participatory design study that the RoSiD tool enables users to
create signals that align with their communicative preferences, and we
illuminate how this tool can be further improved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ISER 2023. 8 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating Open-World Shared Control in Immersive Avatars 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03079v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03079v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Naughton, James Seungbum Nam, Andrew Stratton, Kris Hauser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Teleoperated avatar robots allow people to transport their manipulation
skills to environments that may be difficult or dangerous to work in. Current
systems are able to give operators direct control of many components of the
robot to immerse them in the remote environment, but operators still struggle
to complete tasks as competently as they could in person. We present a
framework for incorporating open-world shared control into avatar robots to
combine the benefits of direct and shared control. This framework preserves the
fluency of our avatar interface by minimizing obstructions to the operator's
view and using the same interface for direct, shared, and fully autonomous
control. In a human subjects study (N=19), we find that operators using this
framework complete a range of tasks significantly more quickly and reliably
than those that do not.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Software Implementation of Digital Filtering via Tustin's Bilinear
  Transform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.03071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.03071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Connor W. Herron
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The purpose of this work is to provide some notes on a software
implementation for digital filtering via Tustins Bilinear Transform. The first
section discusses how to solve for the input and output coefficients by hand
using a generalized approach called Horners method. The second section presents
some results of this generalized digital filtering approach using the IHMC Open
Robotics Software stack and Simulation Construction Set 2. This generalized
approach can solve for the digital coefficients for any causal transfer
function.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian Heuristics for Robust Spatial Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.00344v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.00344v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aamir Hussain Chughtai, Muhammad Tahir, Momin Uppal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial perception is a key task in several machine intelligence applications
such as robotics and computer vision. In general, it involves the nonlinear
estimation of hidden variables that represent the system's state. However, in
the presence of measurement outliers, the standard nonlinear least squared
formulation results in poor estimates. Several methods have been considered in
the literature to improve the reliability of the estimation process. Most
methods are based on heuristics since guaranteed global robust estimation is
not generally practical due to high computational costs. Recently general
purpose robust estimation heuristics have been proposed that leverage existing
non-minimal solvers available for the outlier-free formulations without the
need for an initial guess. In this work, we propose three Bayesian heuristics
that have similar structures. We evaluate these heuristics in practical
scenarios to demonstrate their merits in different applications including 3D
point cloud registration, mesh registration and pose graph optimization. The
general computational advantages our proposals offer make them attractive
candidates for spatial perception tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PLE-SLAM: A Visual-Inertial SLAM Based on Point-Line Features and
  Efficient IMU Initialization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01081v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01081v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaming He, Mingrui Li, Yangyang Wang, Hongyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual-inertial SLAM is crucial in various fields, such as aerial vehicles,
industrial robots, and autonomous driving. The fusion of camera and inertial
measurement unit (IMU) makes up for the shortcomings of a signal sensor, which
significantly improves the accuracy and robustness of localization in
challenging environments. This article presents PLE-SLAM, an accurate and
real-time visual-inertial SLAM algorithm based on point-line features and
efficient IMU initialization. First, we use parallel computing methods to
extract features and compute descriptors to ensure real-time performance.
Adjacent short line segments are merged into long line segments, and isolated
short line segments are directly deleted. Second, a
rotation-translation-decoupled initialization method is extended to use both
points and lines. Gyroscope bias is optimized by tightly coupling IMU
measurements and image observations. Accelerometer bias and gravity direction
are solved by an analytical method for efficiency. To improve the system's
intelligence in handling complex environments, a scheme of leveraging semantic
information and geometric constraints to eliminate dynamic features and A
solution for loop detection and closed-loop frame pose estimation using CNN and
GNN are integrated into the system. All networks are accelerated to ensure
real-time performance. The experiment results on public datasets illustrate
that PLE-SLAM is one of the state-of-the-art visual-inertial SLAM systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transformers for Trajectory Optimization with Application to Spacecraft
  Rendezvous 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13831v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13831v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tommaso Guffanti, Daniele Gammelli, Simone D'Amico, Marco Pavone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable and efficient trajectory optimization methods are a fundamental need
for autonomous dynamical systems, effectively enabling applications including
rocket landing, hypersonic reentry, spacecraft rendezvous, and docking. Within
such safety-critical application areas, the complexity of the emerging
trajectory optimization problems has motivated the application of AI-based
techniques to enhance the performance of traditional approaches. However,
current AI-based methods either attempt to fully replace traditional control
algorithms, thus lacking constraint satisfaction guarantees and incurring in
expensive simulation, or aim to solely imitate the behavior of traditional
methods via supervised learning. To address these limitations, this paper
proposes the Autonomous Rendezvous Transformer (ART) and assesses the
capability of modern generative models to solve complex trajectory optimization
problems, both from a forecasting and control standpoint. Specifically, this
work assesses the capabilities of Transformers to (i) learn near-optimal
policies from previously collected data, and (ii) warm-start a sequential
optimizer for the solution of non-convex optimal control problems, thus
guaranteeing hard constraint satisfaction. From a forecasting perspective,
results highlight how ART outperforms other learning-based architectures at
predicting known fuel-optimal trajectories. From a control perspective,
empirical analyses show how policies learned through Transformers are able to
generate near-optimal warm-starts, achieving trajectories that are (i) more
fuel-efficient, (ii) obtained in fewer sequential optimizer iterations, and
(iii) computed with an overall runtime comparable to benchmarks based on convex
optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented in 2024 IEEE Aerospace Conference</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CrisisViT: A Robust Vision Transformer for Crisis Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02838v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02838v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijun Long, Richard McCreadie, Muhammad Imran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In times of emergency, crisis response agencies need to quickly and
accurately assess the situation on the ground in order to deploy relevant
services and resources. However, authorities often have to make decisions based
on limited information, as data on affected regions can be scarce until local
response services can provide first-hand reports. Fortunately, the widespread
availability of smartphones with high-quality cameras has made citizen
journalism through social media a valuable source of information for crisis
responders. However, analyzing the large volume of images posted by citizens
requires more time and effort than is typically available. To address this
issue, this paper proposes the use of state-of-the-art deep neural models for
automatic image classification/tagging, specifically by adapting
transformer-based architectures for crisis image classification (CrisisViT). We
leverage the new Incidents1M crisis image dataset to develop a range of new
transformer-based image classification models. Through experimentation over the
standard Crisis image benchmark dataset, we demonstrate that the CrisisViT
models significantly outperform previous approaches in emergency type, image
relevance, humanitarian category, and damage severity classification.
Additionally, we show that the new Incidents1M dataset can further augment the
CrisisViT models resulting in an additional 1.25% absolute accuracy gain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MusicAOG: an Energy-Based Model for Learning and Sampling a Hierarchical
  Representation of Symbolic Music 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yikai Qian, Tianle Wang, Xinyi Tong, Xin Jin, Duo Xu, Bo Zheng, Tiezheng Ge, Feng Yu, Song-Chun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In addressing the challenge of interpretability and generalizability of
artificial music intelligence, this paper introduces a novel symbolic
representation that amalgamates both explicit and implicit musical information
across diverse traditions and granularities. Utilizing a hierarchical and-or
graph representation, the model employs nodes and edges to encapsulate a broad
spectrum of musical elements, including structures, textures, rhythms, and
harmonies. This hierarchical approach expands the representability across
various scales of music. This representation serves as the foundation for an
energy-based model, uniquely tailored to learn musical concepts through a
flexible algorithm framework relying on the minimax entropy principle.
Utilizing an adapted Metropolis-Hastings sampling technique, the model enables
fine-grained control over music generation. A comprehensive empirical
evaluation, contrasting this novel approach with existing methodologies,
manifests considerable advancements in interpretability and controllability.
This study marks a substantial contribution to the fields of music analysis,
composition, and computational musicology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling and Masking: A New Paradigm of Data Sampling for Image and Video
  Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongxu Liu, Yinghui Quan, Guoyao Xiao, Aobo Li, Jinjian Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quality assessment of images and videos emphasizes both local details and
global semantics, whereas general data sampling methods (e.g., resizing,
cropping or grid-based fragment) fail to catch them simultaneously. To address
the deficiency, current approaches have to adopt multi-branch models and take
as input the multi-resolution data, which burdens the model complexity. In this
work, instead of stacking up models, a more elegant data sampling method (named
as SAMA, scaling and masking) is explored, which compacts both the local and
global content in a regular input size. The basic idea is to scale the data
into a pyramid first, and reduce the pyramid into a regular data dimension with
a masking strategy. Benefiting from the spatial and temporal redundancy in
images and videos, the processed data maintains the multi-scale characteristics
with a regular input size, thus can be processed by a single-branch model. We
verify the sampling method in image and video quality assessment. Experiments
show that our sampling method can improve the performance of current
single-branch models significantly, and achieves competitive performance to the
multi-branch models without extra model complexity. The source code will be
available at https://github.com/Sissuire/SAMA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2024. Code has been released at
  https://github.com/Sissuire/SAMA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieval-Augmented Text-to-Audio Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08051v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08051v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Yuan, Haohe Liu, Xubo Liu, Qiushi Huang, Mark D. Plumbley, Wenwu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent progress in text-to-audio (TTA) generation, we show that the
state-of-the-art models, such as AudioLDM, trained on datasets with an
imbalanced class distribution, such as AudioCaps, are biased in their
generation performance. Specifically, they excel in generating common audio
classes while underperforming in the rare ones, thus degrading the overall
generation performance. We refer to this problem as long-tailed text-to-audio
generation. To address this issue, we propose a simple retrieval-augmented
approach for TTA models. Specifically, given an input text prompt, we first
leverage a Contrastive Language Audio Pretraining (CLAP) model to retrieve
relevant text-audio pairs. The features of the retrieved audio-text data are
then used as additional conditions to guide the learning of TTA models. We
enhance AudioLDM with our proposed approach and denote the resulting augmented
system as Re-AudioLDM. On the AudioCaps dataset, Re-AudioLDM achieves a
state-of-the-art Frechet Audio Distance (FAD) of 1.37, outperforming the
existing approaches by a large margin. Furthermore, we show that Re-AudioLDM
can generate realistic audio for complex scenes, rare audio classes, and even
unseen audio types, indicating its potential in TTA tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TR-DETR: Task-Reciprocal Transformer for Joint Moment Retrieval and
  Highlight Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02309v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02309v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Sun, Mingyao Zhou, Wenjing Chen, Wei Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video moment retrieval (MR) and highlight detection (HD) based on natural
language queries are two highly related tasks, which aim to obtain relevant
moments within videos and highlight scores of each video clip. Recently,
several methods have been devoted to building DETR-based networks to solve both
MR and HD jointly. These methods simply add two separate task heads after
multi-modal feature extraction and feature interaction, achieving good
performance. Nevertheless, these approaches underutilize the reciprocal
relationship between two tasks. In this paper, we propose a task-reciprocal
transformer based on DETR (TR-DETR) that focuses on exploring the inherent
reciprocity between MR and HD. Specifically, a local-global multi-modal
alignment module is first built to align features from diverse modalities into
a shared latent space. Subsequently, a visual feature refinement is designed to
eliminate query-irrelevant information from visual features for modal
interaction. Finally, a task cooperation module is constructed to refine the
retrieval pipeline and the highlight score prediction process by utilizing the
reciprocity between MR and HD. Comprehensive experiments on QVHighlights,
Charades-STA and TVSum datasets demonstrate that TR-DETR outperforms existing
state-of-the-art methods. Codes are available at
\url{https://github.com/mingyao1120/TR-DETR}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI-24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ M$^{2}$UGen: Multi-modal Music Understanding and Generation with the
  Power of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11255v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11255v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atin Sakkeer Hussain, Shansong Liu, Chenshuo Sun, Ying Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current landscape of research leveraging large language models (LLMs) is
experiencing a surge. Many works harness the powerful reasoning capabilities of
these models to comprehend various modalities, such as text, speech, images,
videos, etc. They also utilize LLMs to understand human intention and generate
desired outputs like images, videos, and music. However, research that combines
both understanding and generation using LLMs is still limited and in its
nascent stage. To address this gap, we introduce a Multi-modal Music
Understanding and Generation (M$^{2}$UGen) framework that integrates LLM's
abilities to comprehend and generate music for different modalities. The
M$^{2}$UGen framework is purpose-built to unlock creative potential from
diverse sources of inspiration, encompassing music, image, and video through
the use of pretrained MERT, ViT, and ViViT models, respectively. To enable
music generation, we explore the use of AudioLDM 2 and MusicGen. Bridging
multi-modal understanding and music generation is accomplished through the
integration of the LLaMA 2 model. Furthermore, we make use of the MU-LLaMA
model to generate extensive datasets that support text/image/video-to-music
generation, facilitating the training of our M$^{2}$UGen framework. We conduct
a thorough evaluation of our proposed framework. The experimental results
demonstrate that our model achieves or surpasses the performance of the current
state-of-the-art models.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-01-04T00:00:00Z">2024-01-04</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">42</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Task Oriented Dialogue as a Catalyst for Self-Supervised Automatic
  Speech Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David M. Chan, Shalini Ghosh, Hitesh Tulsiani, Ariya Rastrow, Björn Hoffmeister
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While word error rates of automatic speech recognition (ASR) systems have
consistently fallen, natural language understanding (NLU) applications built on
top of ASR systems still attribute significant numbers of failures to
low-quality speech recognition results. Existing assistant systems collect
large numbers of these unsuccessful interactions, but these systems usually
fail to learn from these interactions, even in an offline fashion. In this
work, we introduce CLC: Contrastive Learning for Conversations, a family of
methods for contrastive fine-tuning of models in a self-supervised fashion,
making use of easily detectable artifacts in unsuccessful conversations with
assistants. We demonstrate that our CLC family of approaches can improve the
performance of ASR models on OD3, a new public large-scale semi-synthetic
meta-dataset of audio task-oriented dialogues, by up to 19.2%. These gains
transfer to real-world systems as well, where we show that CLC can help to
improve performance by up to 6.7% over baselines. We make OD3 publicly
available at https://github.com/amazon-science/amazon-od3 .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLaMA Pro: Progressive LLaMA with Block Expansion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02415v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02415v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyue Wu, Yukang Gan, Yixiao Ge, Zeyu Lu, Jiahao Wang, Ye Feng, Ping Luo, Ying Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans generally acquire new skills without compromising the old; however,
the opposite holds for Large Language Models (LLMs), e.g., from LLaMA to
CodeLLaMA. To this end, we propose a new post-pretraining method for LLMs with
an expansion of Transformer blocks. We tune the expanded blocks using only new
corpus, efficiently and effectively improving the model's knowledge without
catastrophic forgetting. In this paper, we experiment on the corpus of code and
math, yielding LLaMA Pro-8.3B, a versatile foundation model initialized from
LLaMA2-7B, excelling in general tasks, programming, and mathematics. LLaMA Pro
and its instruction-following counterpart (LLaMA Pro-Instruct) achieve advanced
performance among various benchmarks, demonstrating superiority over existing
open models in the LLaMA family and the immense potential of reasoning and
addressing diverse tasks as an intelligent agent. Our findings provide valuable
insights into integrating natural and programming languages, laying a solid
foundation for developing advanced language agents that operate effectively in
various environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM Augmented LLMs: Expanding Capabilities through Composition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rachit Bansal, Bidisha Samanta, Siddharth Dalmia, Nitish Gupta, Shikhar Vashishth, Sriram Ganapathy, Abhishek Bapna, Prateek Jain, Partha Talukdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundational models with billions of parameters which have been trained on
large corpora of data have demonstrated non-trivial skills in a variety of
domains. However, due to their monolithic structure, it is challenging and
expensive to augment them or impart new skills. On the other hand, due to their
adaptation abilities, several new instances of these models are being trained
towards new domains and tasks. In this work, we study the problem of efficient
and practical composition of existing foundation models with more specific
models to enable newer capabilities. To this end, we propose CALM --
Composition to Augment Language Models -- which introduces cross-attention
between models to compose their representations and enable new capabilities.
Salient features of CALM are: (i) Scales up LLMs on new tasks by 're-using'
existing LLMs along with a few additional parameters and data, (ii) Existing
model weights are kept intact, and hence preserves existing capabilities, and
(iii) Applies to diverse domains and settings. We illustrate that augmenting
PaLM2-S with a smaller model trained on low-resource languages results in an
absolute improvement of up to 13\% on tasks like translation into English and
arithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is
augmented with a code-specific model, we see a relative improvement of 40\%
over the base model for code generation and explanation tasks -- on-par with
fully fine-tuned counterparts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 2 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TinyLlama: An Open-Source Small Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02385v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02385v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiyuan Zhang, Guangtao Zeng, Tianduo Wang, Wei Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present TinyLlama, a compact 1.1B language model pretrained on around 1
trillion tokens for approximately 3 epochs. Building on the architecture and
tokenizer of Llama 2, TinyLlama leverages various advances contributed by the
open-source community (e.g., FlashAttention), achieving better computational
efficiency. Despite its relatively small size, TinyLlama demonstrates
remarkable performance in a series of downstream tasks. It significantly
outperforms existing open-source language models with comparable sizes. Our
model checkpoints and code are publicly available on GitHub at
https://github.com/jzhang38/TinyLlama.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPEER: Sentence-Level Planning of Long Clinical Summaries via Embedded
  Entity Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02369v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02369v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Griffin Adams, Jason Zucker, Noémie Elhadad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinician must write a lengthy summary each time a patient is discharged from
the hospital. This task is time-consuming due to the sheer number of unique
clinical concepts covered in the admission. Identifying and covering salient
entities is vital for the summary to be clinically useful. We fine-tune
open-source LLMs (Mistral-7B-Instruct and Zephyr-7B-\b{eta}) on the task and
find that they generate incomplete and unfaithful summaries. To increase entity
coverage, we train a smaller, encoder-only model to predict salient entities,
which are treated as content-plans to guide the LLM. To encourage the LLM to
focus on specific mentions in the source notes, we propose SPEER:
Sentence-level Planning via Embedded Entity Retrieval. Specifically, we mark
each salient entity span with special "{{ }}" boundary tags and instruct the
LLM to retrieve marked spans before generating each sentence. Sentence-level
planning acts as a form of state tracking in that the model is explicitly
recording the entities it uses. We fine-tune Mistral and Zephyr variants on a
large-scale, diverse dataset of ~167k in-patient hospital admissions and
evaluate on 3 datasets. SPEER shows gains in both coverage and faithfulness
metrics over non-guided and guided baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Extraction: Contextualising Tabular Data for Efficient
  Summarisation by Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uday Allu, Biddwan Ahmed, Vishesh Tripathi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The conventional use of the Retrieval-Augmented Generation (RAG) architecture
has proven effective for retrieving information from diverse documents.
However, challenges arise in handling complex table queries, especially within
PDF documents containing intricate tabular structures.This research introduces
an innovative approach to enhance the accuracy of complex table queries in
RAG-based systems. Our methodology involves storing PDFs in the retrieval
database and extracting tabular content separately. The extracted tables
undergo a process of context enrichment, concatenating headers with
corresponding values. To ensure a comprehensive understanding of the enriched
data, we employ a fine-tuned version of the Llama-2-chat language model for
summarisation within the RAG architecture. Furthermore, we augment the tabular
data with contextual sense using the ChatGPT 3.5 API through a one-shot prompt.
This enriched data is then fed into the retrieval database alongside other
PDFs. Our approach aims to significantly improve the precision of complex table
queries, offering a promising solution to a longstanding challenge in
information retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLaVA-$φ$: Efficient Multi-Modal Assistant with Small Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichen Zhu, Minjie Zhu, Ning Liu, Zhicai Ou, Xiaofeng Mou, Jian Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce LLaVA-$\phi$ (LLaVA-Phi), an efficient
multi-modal assistant that harnesses the power of the recently advanced small
language model, Phi-2, to facilitate multi-modal dialogues. LLaVA-Phi marks a
notable advancement in the realm of compact multi-modal models. It demonstrates
that even smaller language models, with as few as 2.7B parameters, can
effectively engage in intricate dialogues that integrate both textual and
visual elements, provided they are trained with high-quality corpora. Our model
delivers commendable performance on publicly available benchmarks that
encompass visual comprehension, reasoning, and knowledge-based perception.
Beyond its remarkable performance in multi-modal dialogue tasks, our model
opens new avenues for applications in time-sensitive environments and systems
that require real-time interaction, such as embodied agents. It highlights the
potential of smaller language models to achieve sophisticated levels of
understanding and interaction, while maintaining greater resource
efficiency.The project is available at {https://github.com/zhuyiche/llava-phi}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>technique report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are LLMs Robust for Spoken Dialogues? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02297v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02297v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyed Mahed Mousavi, Gabriel Roccabruna, Simone Alghisi, Massimo Rizzoli, Mirco Ravanelli, Giuseppe Riccardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Pre-Trained Language Models have demonstrated state-of-the-art
performance in different downstream tasks, including dialogue state tracking
and end-to-end response generation. Nevertheless, most of the publicly
available datasets and benchmarks on task-oriented dialogues focus on written
conversations. Consequently, the robustness of the developed models to spoken
interactions is unknown. In this work, we have evaluated the performance of
LLMs for spoken task-oriented dialogues on the DSTC11 test sets. Due to the
lack of proper spoken dialogue datasets, we have automatically transcribed a
development set of spoken dialogues with a state-of-the-art ASR engine. We have
characterized the ASR-error types and their distributions and simulated these
errors in a large dataset of dialogues. We report the intrinsic (perplexity)
and extrinsic (human evaluation) performance of fine-tuned GPT-2 and T5 models
in two subtasks of response generation and dialogue state tracking,
respectively. The results show that LLMs are not robust to spoken noise by
default, however, fine-tuning/training such models on a proper dataset of
spoken TODs can result in a more robust performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Response Evaluation from Interlocutor's Eye for Open-Domain
  Dialogue Systems <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuma Tsuta, Naoki Yoshinaga, Shoetsu Sato, Masashi Toyoda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-domain dialogue systems have started to engage in continuous
conversations with humans. Those dialogue systems are required to be adjusted
to the human interlocutor and evaluated in terms of their perspective. However,
it is questionable whether the current automatic evaluation methods can
approximate the interlocutor's judgments. In this study, we analyzed and
examined what features are needed in an automatic response evaluator from the
interlocutor's perspective. The first experiment on the Hazumi dataset revealed
that interlocutor awareness plays a critical role in making automatic response
evaluation correlate with the interlocutor's judgments. The second experiment
using massive conversations on X (formerly Twitter) confirmed that dialogue
continuity prediction can train an interlocutor-aware response evaluator
without human feedback while revealing the difficulty in evaluating generated
responses compared to human responses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures, 5 tables, Accepted by IJCNLP-AACL 2023 SRW</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ L3Cube-IndicNews: News-based Short Text and Long Document Classification
  <span class="highlight-title">Dataset</span>s in Indic Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aishwarya Mirashi, Srushti Sonavane, Purva Lingayat, Tejas Padhiyar, Raviraj Joshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce L3Cube-IndicNews, a multilingual text
classification corpus aimed at curating a high-quality dataset for Indian
regional languages, with a specific focus on news headlines and articles. We
have centered our work on 10 prominent Indic languages, including Hindi,
Bengali, Marathi, Telugu, Tamil, Gujarati, Kannada, Odia, Malayalam, and
Punjabi. Each of these news datasets comprises 10 or more classes of news
articles. L3Cube-IndicNews offers 3 distinct datasets tailored to handle
different document lengths that are classified as: Short Headlines
Classification (SHC) dataset containing the news headline and news category,
Long Document Classification (LDC) dataset containing the whole news article
and the news category, and Long Paragraph Classification (LPC) containing
sub-articles of the news and the news category. We maintain consistent labeling
across all 3 datasets for in-depth length-based analysis. We evaluate each of
these Indic language datasets using 4 different models including monolingual
BERT, multilingual Indic Sentence BERT (IndicSBERT), and IndicBERT. This
research contributes significantly to expanding the pool of available text
classification datasets and also makes it possible to develop topic
classification models for Indian regional languages. This also serves as an
excellent resource for cross-lingual analysis owing to the high overlap of
labels among languages. The datasets and models are shared publicly at
https://github.com/l3cube-pune/indic-nlp
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the International Conference on Natural Language
  Processing (ICON 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Multi-Facts Reasoning Network For Complex Temporal Question
  Answering Over Knowledge Graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rikui Huang, Wei Wei, Xiaoye Qu, Wenfeng Xie, Xianling Mao, Dangyang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal Knowledge Graph (TKG) is an extension of regular knowledge graph by
attaching the time scope. Existing temporal knowledge graph question answering
(TKGQA) models solely approach simple questions, owing to the prior assumption
that each question only contains a single temporal fact with explicit/implicit
temporal constraints. Hence, they perform poorly on questions which own
multiple temporal facts. In this paper, we propose \textbf{\underline{J}}oint
\textbf{\underline{M}}ulti \textbf{\underline{F}}acts
\textbf{\underline{R}}easoning \textbf{\underline{N}}etwork (JMFRN), to jointly
reasoning multiple temporal facts for accurately answering \emph{complex}
temporal questions. Specifically, JMFRN first retrieves question-related
temporal facts from TKG for each entity of the given complex question. For
joint reasoning, we design two different attention (\ie entity-aware and
time-aware) modules, which are suitable for universal settings, to aggregate
entities and timestamps information of retrieved facts. Moreover, to filter
incorrect type answers, we introduce an additional answer type discrimination
task. Extensive experiments demonstrate our proposed method significantly
outperforms the state-of-art on the well-known complex temporal question
benchmark TimeQuestions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DIALIGHT: Lightweight Multilingual Development and Evaluation of
  Task-Oriented Dialogue Systems with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songbo Hu, Xiaobin Wang, Zhangdie Yuan, Anna Korhonen, Ivan Vulić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present DIALIGHT, a toolkit for developing and evaluating multilingual
Task-Oriented Dialogue (ToD) systems which facilitates systematic evaluations
and comparisons between ToD systems using fine-tuning of Pretrained Language
Models (PLMs) and those utilising the zero-shot and in-context learning
capabilities of Large Language Models (LLMs). In addition to automatic
evaluation, this toolkit features (i) a secure, user-friendly web interface for
fine-grained human evaluation at both local utterance level and global dialogue
level, and (ii) a microservice-based backend, improving efficiency and
scalability. Our evaluations reveal that while PLM fine-tuning leads to higher
accuracy and coherence, LLM-based systems excel in producing diverse and
likeable responses. However, we also identify significant challenges of LLMs in
adherence to task-specific instructions and generating outputs in multiple
languages, highlighting areas for future research. We hope this open-sourced
toolkit will serve as a valuable resource for researchers aiming to develop and
properly evaluate multilingual ToD systems and will lower, currently still
high, entry barriers in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 tables, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Location Aware Modular Biencoder for Tourism Question Answering <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Li, Martin Tomko, Timothy Baldwin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Answering real-world tourism questions that seek Point-of-Interest (POI)
recommendations is challenging, as it requires both spatial and non-spatial
reasoning, over a large candidate pool. The traditional method of encoding each
pair of question and POI becomes inefficient when the number of candidates
increases, making it infeasible for real-world applications. To overcome this,
we propose treating the QA task as a dense vector retrieval problem, where we
encode questions and POIs separately and retrieve the most relevant POIs for a
question by utilizing embedding space similarity. We use pretrained language
models (PLMs) to encode textual information, and train a location encoder to
capture spatial information of POIs. Experiments on a real-world tourism QA
dataset demonstrate that our approach is effective, efficient, and outperforms
previous methods across all metrics. Enabled by the dense retrieval
architecture, we further build a global evaluation baseline, expanding the
search space by 20 times compared to previous work. We also explore several
factors that impact on the model's performance through follow-up experiments.
Our code and model are publicly available at https://github.com/haonan-li/LAMB.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shayona@SMM4H23: COVID-19 Self diagnosis classification using BERT and
  LightGBM models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rushi Chavda, Darshan Makwana, Vraj Patel, Anupam Shukla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes approaches and results for shared Task 1 and 4 of
SMMH4-23 by Team Shayona. Shared Task-1 was binary classification of english
tweets self-reporting a COVID-19 diagnosis, and Shared Task-4 was Binary
classification of English Reddit posts self-reporting a social anxiety disorder
diagnosis. Our team has achieved the highest f1-score 0.94 in Task-1 among all
participants. We have leveraged the Transformer model (BERT) in combination
with the LightGBM model for both tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Boundary of GPT-4V on Marine Analysis: A Preliminary Case
  Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02147v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02147v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqiang Zheng, Yiwei Chen, Jipeng Zhang, Tuan-Anh Vu, Huimin Zeng, Yue Him Wong Tim, Sai-Kit Yeung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated a powerful ability to answer
various queries as a general-purpose assistant. The continuous multi-modal
large language models (MLLM) empower LLMs with the ability to perceive visual
signals. The launch of GPT-4 (Generative Pre-trained Transformers) has
generated significant interest in the research communities. GPT-4V(ison) has
demonstrated significant power in both academia and industry fields, as a focal
point in a new artificial intelligence generation. Though significant success
was achieved by GPT-4V, exploring MLLMs in domain-specific analysis (e.g.,
marine analysis) that required domain-specific knowledge and expertise has
gained less attention. In this study, we carry out the preliminary and
comprehensive case study of utilizing GPT-4V for marine analysis. This report
conducts a systematic evaluation of existing GPT-4V, assessing the performance
of GPT-4V on marine research and also setting a new standard for future
developments in MLLMs. The experimental results of GPT-4V show that the
responses generated by GPT-4V are still far away from satisfying the
domain-specific requirements of the marine professions. All images and prompts
used in this study will be available at
https://github.com/hkust-vgd/Marine_GPT-4V_Eval
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>51 pages, 36 figures, Repository:
  https://github.com/hkust-vgd/Marine_GPT-4V_Eval</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DCR-Consistency: Divide-Conquer-Reasoning for Consistency Evaluation and
  Improvement of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02132v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02132v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wendi Cui, Jiaxin Zhang, Zhuohang Li, Lopez Damien, Kamalika Das, Bradley Malin, Sricharan Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating the quality and variability of text generated by Large Language
Models (LLMs) poses a significant, yet unresolved research challenge.
Traditional evaluation methods, such as ROUGE and BERTScore, which measure
token similarity, often fail to capture the holistic semantic equivalence. This
results in a low correlation with human judgments and intuition, which is
especially problematic in high-stakes applications like healthcare and finance
where reliability, safety, and robust decision-making are highly critical. This
work proposes DCR, an automated framework for evaluating and improving the
consistency of LLM-generated texts using a divide-conquer-reasoning approach.
Unlike existing LLM-based evaluators that operate at the paragraph level, our
method employs a divide-and-conquer evaluator (DCE) that breaks down the
paragraph-to-paragraph comparison between two generated responses into
individual sentence-to-paragraph comparisons, each evaluated based on
predefined criteria. To facilitate this approach, we introduce an automatic
metric converter (AMC) that translates the output from DCE into an
interpretable numeric score. Beyond the consistency evaluation, we further
present a reason-assisted improver (RAI) that leverages the analytical reasons
with explanations identified by DCE to generate new responses aimed at reducing
these inconsistencies. Through comprehensive and systematic empirical analysis,
we show that our approach outperforms state-of-the-art methods by a large
margin (e.g., +19.3% and +24.3% on the SummEval dataset) in evaluating the
consistency of LLM generation across multiple benchmarks in semantic, factual,
and summarization consistency tasks. Our approach also substantially reduces
nearly 90% of output inconsistencies, showing promise for effective
hallucination mitigation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PEFT for Speech: Unveiling Optimal Placement, Merging Strategies, and
  Ensemble Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02122v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02122v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tzu-Han Lin, How-Shing Wang, Hao-Yung Weng, Kuang-Chen Peng, Zih-Ching Chen, Hung-yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-Efficient Fine-Tuning (PEFT) is increasingly recognized as an
effective method in speech processing. However, the optimal approach and the
placement of PEFT methods remain inconclusive. Our study conducts extensive
experiments to compare different PEFT methods and their layer-wise placement
adapting Differentiable Architecture Search (DARTS). We also explore the use of
ensemble learning to leverage diverse PEFT strategies. The results reveal that
DARTS does not outperform the baseline approach, which involves inserting the
same PEFT method into all layers of a Self-Supervised Learning (SSL) model. In
contrast, an ensemble learning approach, particularly one employing majority
voting, demonstrates superior performance. Our statistical evidence indicates
that different PEFT methods learn in varied ways. This variation might explain
why the synergistic integration of various PEFT methods through ensemble
learning can harness their unique learning capabilities more effectively
compared to individual layer-wise optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICASSP 2024 Self-supervision in Audio, Speech and Beyond
  workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using LLM to select the right SQL Query from candidates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02115v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02115v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenwen Li, Tao Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-SQL models can generate a list of candidate SQL queries, and the best
query is often in the candidate list, but not at the top of the list. An
effective re-rank method can select the right SQL query from the candidate list
and improve the model's performance. Previous studies on code generation
automatically generate test cases and use them to re-rank candidate codes.
However, automatic test case generation for text-to-SQL is an understudied
field. We propose an automatic test case generation method that first generates
a database and then uses LLMs to predict the ground truth, which is the
expected execution results of the ground truth SQL query on this database. To
reduce the difficulty for LLMs to predict, we conduct experiments to search for
ways to generate easy databases for LLMs and design easy-to-understand prompts.
Based on our test case generation method, we propose a re-rank method to select
the right SQL query from the candidate list. Given a candidate list, our method
can generate test cases and re-rank the candidate list according to their pass
numbers on these test cases and their generation probabilities. The experiment
results on the validation dataset of Spider show that the performance of some
state-of-the-art models can get a 3.6\% improvement after applying our re-rank
method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Re-evaluating the Memory-balanced Pipeline Parallelism: BPipe 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mincong Huang, Chao Wang, Chi Ma, Yineng Zhang, Peng Zhang, Lei Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pipeline parallelism is an essential technique in the training of large-scale
Transformer models. However, it suffers from imbalanced memory consumption,
leading to insufficient memory utilization. The BPipe technique was proposed to
address this issue and has proven effective in the GPT-3 model. Nevertheless,
our experiments have not yielded similar benefits for LLaMA training.
Additionally, BPipe only yields negligible benefits for GPT-3 training when
applying flash attention. We analyze the underlying causes of the divergent
performance of BPipe on GPT-3 and LLaMA. Furthermore, we introduce a novel
method to estimate the performance of BPipe.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ICE-GRT: Instruction Context Enhancement by Generative Reinforcement
  based Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02072v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02072v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Zheng, Ke Sun, Da Tang, Yukun Ma, Yuyu Zhang, Chenguang Xi, Xun Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of Large Language Models (LLMs) such as ChatGPT and LLaMA
encounter limitations in domain-specific tasks, with these models often lacking
depth and accuracy in specialized areas, and exhibiting a decrease in general
capabilities when fine-tuned, particularly analysis ability in small sized
models. To address these gaps, we introduce ICE-GRT, utilizing Reinforcement
Learning from Human Feedback (RLHF) grounded in Proximal Policy Optimization
(PPO), demonstrating remarkable ability in in-domain scenarios without
compromising general task performance. Our exploration of ICE-GRT highlights
its understanding and reasoning ability to not only generate robust answers but
also to provide detailed analyses of the reasons behind the answer. This
capability marks a significant progression beyond the scope of Supervised
Fine-Tuning models. The success of ICE-GRT is dependent on several crucial
factors, including Appropriate Data, Reward Size Scaling, KL-Control, Advantage
Normalization, etc. The ICE-GRT model exhibits state-of-the-art performance in
domain-specific tasks and across 12 general Language tasks against equivalent
size and even larger size LLMs, highlighting the effectiveness of our approach.
We provide a comprehensive analysis of the ICE-GRT, underscoring the
significant advancements it brings to the field of LLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding LLMs: A Comprehensive Overview from Training to Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02038v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02038v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiheng Liu, Hao He, Tianle Han, Xu Zhang, Mengyuan Liu, Jiaming Tian, Yutong Zhang, Jiaqi Wang, Xiaohui Gao, Tianyang Zhong, Yi Pan, Shaochen Xu, Zihao Wu, Zhengliang Liu, Xin Zhang, Shu Zhang, Xintao Hu, Tuo Zhang, Ning Qiang, Tianming Liu, Bao Ge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The introduction of ChatGPT has led to a significant increase in the
utilization of Large Language Models (LLMs) for addressing downstream tasks.
There's an increasing focus on cost-efficient training and deployment within
this context. Low-cost training and deployment of LLMs represent the future
development trend. This paper reviews the evolution of large language model
training techniques and inference deployment technologies aligned with this
emerging trend. The discussion on training includes various aspects, including
data preprocessing, training architecture, pre-training tasks, parallel
training, and relevant content related to model fine-tuning. On the inference
side, the paper covers topics such as model compression, parallel computation,
memory scheduling, and structural optimization. It also explores LLMs'
utilization and provides insights into their future development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages,6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text2MDT: Extracting Medical Decision Trees from Medical Texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02034v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02034v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Zhu, Wenfeng Li, Xing Tian, Pengfei Wang, Xiaoling Wang, Jin Chen, Yuanbin Wu, Yuan Ni, Guotong Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge of the medical decision process, which can be modeled as medical
decision trees (MDTs), is critical to build clinical decision support systems.
However, the current MDT construction methods rely heavily on time-consuming
and laborious manual annotation. In this work, we propose a novel task,
Text2MDT, to explore the automatic extraction of MDTs from medical texts such
as medical guidelines and textbooks. We normalize the form of the MDT and
create an annotated Text-to-MDT dataset in Chinese with the participation of
medical experts. We investigate two different methods for the Text2MDT tasks:
(a) an end-to-end framework which only relies on a GPT style large language
models (LLM) instruction tuning to generate all the node information and tree
structures. (b) The pipeline framework which decomposes the Text2MDT task to
three subtasks. Experiments on our Text2MDT dataset demonstrate that: (a) the
end-to-end method basd on LLMs (7B parameters or larger) show promising
results, and successfully outperform the pipeline methods. (b) The
chain-of-thought (COT) prompting method \cite{Wei2022ChainOT} can improve the
performance of the fine-tuned LLMs on the Text2MDT test set. (c) the
lightweight pipelined method based on encoder-based pretrained models can
perform comparably with LLMs with model complexity two magnititudes smaller.
Our Text2MDT dataset is open-sourced at
\url{https://tianchi.aliyun.com/dataset/95414}, and the source codes are
open-sourced at \url{https://github.com/michael-wzhu/text2dt}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Contrast: Better Reflection Through Inconsistent Solving
  Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqi Zhang, Yongliang Shen, Linjuan Wu, Qiuying Peng, Jun Wang, Yueting Zhuang, Weiming Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reflection capacity of Large Language Model (LLM) has garnered extensive
attention. A post-hoc prompting strategy, e.g., reflexion and self-refine,
refines LLM's response based on self-evaluated or external feedback. However,
recent research indicates without external feedback, LLM's intrinsic reflection
is unstable. Our investigation unveils that the key bottleneck is the quality
of the self-evaluated feedback. We find LLMs often exhibit overconfidence or
high randomness when self-evaluate, offering stubborn or inconsistent feedback,
which causes poor reflection. To remedy this, we advocate Self-Contrast: It
adaptively explores diverse solving perspectives tailored to the request,
contrasts the differences, and summarizes these discrepancies into a checklist
which could be used to re-examine and eliminate discrepancies. Our method
endows LLM with diverse perspectives to alleviate stubborn biases. Moreover,
their discrepancies indicate potential errors or inherent uncertainties that
LLM often overlooks. Reflecting upon these can catalyze more accurate and
stable reflection. Experiments conducted on a series of reasoning and
translation tasks with different LLMs serve to underscore the effectiveness and
generality of our strategy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Memory, Consciousness and Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02509v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02509v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jitang Li, Jinzheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development in cognitive science and Large Language Models (LLMs),
increasing connections have come to light between these two distinct fields.
Building upon these connections, we propose a conjecture suggesting the
existence of a duality between LLMs and Tulving's theory of memory. We identify
a potential correspondence between Tulving's synergistic ecphory model (SEM) of
retrieval and the emergent abilities observed in LLMs, serving as supporting
evidence for our conjecture. Furthermore, we speculate that consciousness may
be considered a form of emergent ability based on this duality. We also discuss
how other theories of consciousness intersect with our research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Language-Model Agents on Realistic Autonomous Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11671v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11671v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Megan Kinniment, Lucas Jun Koba Sato, Haoxing Du, Brian Goodrich, Max Hasin, Lawrence Chan, Luke Harold Miles, Tao R. Lin, Hjalmar Wijk, Joel Burget, Aaron Ho, Elizabeth Barnes, Paul Christiano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this report, we explore the ability of language model agents to acquire
resources, create copies of themselves, and adapt to novel challenges they
encounter in the wild. We refer to this cluster of capabilities as "autonomous
replication and adaptation" or ARA. We believe that systems capable of ARA
could have wide-reaching and hard-to-anticipate consequences, and that
measuring and forecasting ARA may be useful for informing measures around
security, monitoring, and alignment. Additionally, once a system is capable of
ARA, placing bounds on a system's capabilities may become significantly more
difficult.
  We construct four simple example agents that combine language models with
tools that allow them to take actions in the world. We then evaluate these
agents on 12 tasks relevant to ARA. We find that these language model agents
can only complete the easiest tasks from this list, although they make some
progress on the more challenging tasks. Unfortunately, these evaluations are
not adequate to rule out the possibility that near-future agents will be
capable of ARA. In particular, we do not think that these evaluations provide
good assurance that the ``next generation'' of language models (e.g. 100x
effective compute scaleup on existing models) will not yield agents capable of
ARA, unless intermediate evaluations are performed during pretraining.
Relatedly, we expect that fine-tuning of the existing models could produce
substantially more competent agents, even if the fine-tuning is not directly
targeted at ARA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One Shot Learning as Instruction Data Prospector for Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10302v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10302v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunshui Li, Binyuan Hui, Xiaobo Xia, Jiaxi Yang, Min Yang, Lei Zhang, Shuzheng Si, Junhao Liu, Tongliang Liu, Fei Huang, Yongbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning large language models(LLMs) with human is a critical step in
effectively utilizing their pre-trained capabilities across a wide array of
language tasks. Current instruction tuning practices often rely on expanding
dataset size without a clear strategy for ensuring data quality, which can
inadvertently introduce noise and degrade model performance. To address this
challenge, we introduce Nuggets, a novel and efficient methodology that employs
one shot learning to select high-quality instruction data from expansive
datasets. Nuggets assesses the potential of individual instruction examples to
act as effective one shot examples, thereby identifying those that can
significantly enhance diverse task performance. Nuggets utilizes a scoring
system based on the impact of candidate examples on the perplexity of a diverse
anchor set, facilitating the selection of the most beneficial data for
instruction tuning. Through rigorous testing on two benchmarks, including
MT-Bench and Alpaca-Eval, we demonstrate that instruction tuning with the top
1% of Nuggets-curated examples substantially outperforms conventional methods
that use the full dataset. These findings advocate for a data selection
paradigm that prioritizes quality, offering a more efficient pathway to align
LLMs with humans.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vietnamese Poem Generation & The Prospect Of Cross-Language Poem-To-Poem
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01078v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01078v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Triet Minh Huynh, Quan Le Bao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Poetry generation has been a challenging task in the field of Natural
Language Processing, as it requires the model to understand the nuances of
language, sentiment, and style. In this paper, we propose using Large Language
Models to generate Vietnamese poems of various genres from natural language
prompts, thereby facilitating an intuitive process with enhanced content
control. Our most efficacious model, the GPT-3 Babbage variant, achieves a
custom evaluation score of 0.8, specifically tailored to the "luc bat" genre of
Vietnamese poetry. Furthermore, we also explore the idea of paraphrasing poems
into normal text prompts and yield a relatively high score of 0.781 in the "luc
bat" genre. This experiment presents the potential for cross-Language
poem-to-poem translation with translated poems as the inputs while concurrently
maintaining complete control over the generated content.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial Data Poisoning for Fake News Detection: How to Make a Model
  Misclassify a Target News without Modifying It 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15228v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15228v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Siciliano, Luca Maiano, Lorenzo Papa, Federica Baccini, Irene Amerini, Fabrizio Silvestri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fake news detection models are critical to countering disinformation but can
be manipulated through adversarial attacks. In this position paper, we analyze
how an attacker can compromise the performance of an online learning detector
on specific news content without being able to manipulate the original target
news. In some contexts, such as social networks, where the attacker cannot
exert complete control over all the information, this scenario can indeed be
quite plausible. Therefore, we show how an attacker could potentially introduce
poisoning data into the training data to manipulate the behavior of an online
learning method. Our initial findings reveal varying susceptibility of logistic
regression models based on complexity and attack type.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How do media talk about the Covid-19 pandemic? Metaphorical thematic
  clustering in Italian online newspapers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.02106v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.02106v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucia Busso, Ottavia Tordini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The contribution presents a study on figurative language of the first months
of the COVID-19 crisis in Italian online newspapers. Particularly, we contrast
topics and metaphorical language used by journalists in the first and second
phase of the government response to the pandemic in Spring 2020. The analysis
is conducted on a journalistic corpus collected between February 24th and June
3rd, 2020. The analysis is performed using both quantitative and qualitative
approaches, combining Structural Topic Modelling (Roberts et al. 2016),
Conceptual Metaphor Theory (Lakoff & Johnson, 1980), and qualitative-corpus
based metaphor analysis (Charteris-Black, 2004). We find a significant shift in
topics discussed across Phase 1 and Phase 2, and interesting overlaps in
topic-specific metaphors. Using qualitative corpus analysis, we present a more
in-depth case study discussing metaphorical collocations of the topics of
Economy and Society
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unleashing the Emergent Cognitive Synergy in Large Language Models: A
  Task-Solving Agent through Multi-Persona Self-Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05300v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05300v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, Heng Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human intelligence thrives on cognitive synergy, where collaboration among
different minds yield superior outcomes compared to isolated individuals. In
this work, we propose Solo Performance Prompting (SPP), which transforms a
single LLM into a cognitive synergist by engaging in multi-turn
self-collaboration with multiple personas. A cognitive synergist is an
intelligent agent that collaboratively combines multiple minds' strengths and
knowledge to enhance problem-solving in complex tasks. By dynamically
identifying and simulating different personas based on task inputs, SPP
unleashes the potential of cognitive synergy in LLMs. Our in-depth analysis
shows that assigning multiple fine-grained personas in LLMs improves
problem-solving abilities compared to using a single or fixed number of
personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing,
Codenames Collaborative, and Logic Grid Puzzle, encompassing both
knowledge-intensive and reasoning-intensive types. Unlike previous works, such
as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs,
experimental results demonstrate that SPP effectively reduces factual
hallucination, and maintains strong reasoning capabilities. Additionally,
comparative experiments show that cognitive synergy only emerges in GPT-4 and
does not appear in less capable models, such as GPT-3.5-turbo and
Llama2-13b-chat, which draws an interesting analogy to human development. Code,
data, and prompts can be found at:
https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emotionally Numb or Empathetic? Evaluating How LLMs Feel Using
  EmotionBench 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.03656v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.03656v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jen-tse Huang, Man Ho Lam, Eric John Li, Shujie Ren, Wenxuan Wang, Wenxiang Jiao, Zhaopeng Tu, Michael R. Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating Large Language Models' (LLMs) anthropomorphic capabilities has
become increasingly important in contemporary discourse. Utilizing the emotion
appraisal theory from psychology, we propose to evaluate the empathy ability of
LLMs, i.e., how their feelings change when presented with specific situations.
After a careful and comprehensive survey, we collect a dataset containing over
400 situations that have proven effective in eliciting the eight emotions
central to our study. Categorizing the situations into 36 factors, we conduct a
human evaluation involving more than 1,200 subjects worldwide. With the human
evaluation results as references, our evaluation includes five LLMs, covering
both commercial and open-source models, including variations in model sizes,
featuring the latest iterations, such as GPT-4 and LLaMA-2. We find that,
despite several misalignments, LLMs can generally respond appropriately to
certain situations. Nevertheless, they fall short in alignment with the
emotional behaviors of human beings and cannot establish connections between
similar situations. Our collected dataset of situations, the human evaluation
results, and the code of our testing framework, dubbed EmotionBench, is made
openly accessible via https://github.com/CUHK-ARISE/EmotionBench. We aspire to
contribute to the advancement of LLMs regarding better alignment with the
emotional behaviors of human beings, thereby enhancing their utility and
applicability as intelligent assistants.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages. Added demographic distribution of the user study. Added
  ethics statements and limitations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Aligned Multimodal Learning for NER on Tweet Posts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.08372v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.08372v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peipei Liu, Hong Li, Yimo Ren, Jie Liu, Shuaizong Si, Hongsong Zhu, Limin Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mining structured knowledge from tweets using named entity recognition (NER)
can be beneficial for many down stream applications such as recommendation and
intention understanding. With tweet posts tending to be multimodal, multimodal
named entity recognition (MNER) has attracted more attention. In this paper, we
propose a novel approach, which can dynamically align the image and text
sequence and achieve the multi-level cross-modal learning to augment textual
word representation for MNER improvement. To be specific, our framework can be
split into three main stages: the first stage focuses on intra-modality
representation learning to derive the implicit global and local knowledge of
each modality, the second evaluates the relevance between the text and its
accompanying image and integrates different grained visual information based on
the relevance, the third enforces semantic refinement via iterative cross-modal
interactions and co-attention. We conduct experiments on two open datasets, and
the results and detailed analysis demonstrate the advantage of our model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UstanceBR: a multimodal language resource for stance prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06374v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06374v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Camila Pereira, Matheus Pavan, Sungwon Yoon, Ricelli Ramos, Pablo Costa, Lais Cavalheiro, Ivandre Paraboni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces UstanceBR, a multimodal corpus in the Brazilian
Portuguese Twitter domain for target-based stance prediction. The corpus
comprises 86.8 k labelled stances towards selected target topics, and extensive
network information about the users who published these stances on social
media. In this article we describe the corpus multimodal data, and a number of
usage examples in both in-domain and zero-shot stance prediction based on text-
and network-related information, which are intended to provide initial baseline
results for future studies in the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.04589v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.04589v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Yang, Yingxue Zhang, Fandong Meng, Jie Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite Multi-modal Large Language Models (MM-LLMs) have made exciting
strides recently, they are still struggling to efficiently model the
interactions among multi-modal inputs and the generation in non-textual
modalities. In this work, we propose TEAL (Tokenize and Embed ALl)}, an
approach to treat the input from any modality as a token sequence and learn a
joint embedding space for all modalities. Specifically, for the input from any
modality, TEAL first discretizes it into a token sequence with the
off-the-shelf tokenizer and embeds the token sequence into a joint embedding
space with a learnable embedding matrix. MM-LLMs just need to predict the
multi-modal tokens autoregressively as the textual LLMs do. Finally, the
corresponding de-tokenizer is applied to generate the output in each modality
based on the predicted token sequence. With the joint embedding space, TEAL
enables the frozen LLMs to perform both understanding and generation tasks
involving non-textual modalities, such as image and audio. Thus, the textual
LLM can just work as an interface and maintain its high performance in textual
understanding and generation. Experiments show that TEAL achieves substantial
improvements in multi-modal understanding, and implements a simple scheme for
multi-modal generations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Multi-modal, Large Language Models, Tokenizer, Understanding and
  Generation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ T-Eval: Evaluating the Tool Utilization Capability Step by Step 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14033v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14033v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehui Chen, Weihua Du, Wenwei Zhang, Kuikun Liu, Jiangning Liu, Miao Zheng, Jingming Zhuo, Songyang Zhang, Dahua Lin, Kai Chen, Feng Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLM) have achieved remarkable performance on various
NLP tasks and are augmented by tools for broader applications. Yet, how to
evaluate and analyze the tool-utilization capability of LLMs is still
under-explored. In contrast to previous works that evaluate models
holistically, we comprehensively decompose the tool utilization into multiple
sub-processes, including instruction following, planning, reasoning, retrieval,
understanding, and review. Based on that, we further introduce T-Eval to
evaluate the tool utilization capability step by step. T-Eval disentangles the
tool utilization evaluation into several sub-domains along model capabilities,
facilitating the inner understanding of both holistic and isolated competency
of LLMs. We conduct extensive experiments on T-Eval and in-depth analysis of
various LLMs. T-Eval not only exhibits consistency with the outcome-oriented
evaluation but also provides a more fine-grained analysis of the capabilities
of LLMs, providing a new perspective in LLM evaluation on tool-utilization
ability. The benchmark will be available at
https://github.com/open-compass/T-Eval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code: https://github.com/open-compass/T-Eval; Website:
  https://open-compass.github.io/T-Eval</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video Understanding with Large Language Models: A Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17432v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17432v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang, Daoan Zhang, Jie An, Jingyang Lin, Rongyi Zhu, Ali Vosoughi, Chao Huang, Zeliang Zhang, Feng Zheng, Jianguo Zhang, Ping Luo, Jiebo Luo, Chenliang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the burgeoning growth of online video platforms and the escalating
volume of video content, the demand for proficient video understanding tools
has intensified markedly. Given the remarkable capabilities of Large Language
Models (LLMs) in language and multimodal tasks, this survey provides a detailed
overview of the recent advancements in video understanding harnessing the power
of LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly
advanced, particularly their ability for open-ended spatial-temporal reasoning
combined with commonsense knowledge, suggesting a promising path for future
video understanding. We examine the unique characteristics and capabilities of
Vid-LLMs, categorizing the approaches into four main types: LLM-based Video
Agents, Vid-LLMs Pretraining, Vid-LLMs Instruction Tuning, and Hybrid Methods.
Furthermore, this survey presents a comprehensive study of the tasks, datasets,
and evaluation methodologies for Vid-LLMs. Additionally, it explores the
expansive applications of Vid-LLMs across various domains, highlighting their
remarkable scalability and versatility in real-world video understanding
challenges. Finally, it summarizes the limitations of existing Vid-LLMs and
outlines directions for future research. For more information, readers are
recommended to visit the repository at
https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Theory of Hallucinations based on Equivariance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14504v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14504v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hisaichi Shibata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study aims to acquire knowledge for creating very large language models
that are immune to hallucinations. Hallucinations in contemporary large
language models are often attributed to a misunderstanding of real-world social
relationships. Therefore, I hypothesize that very large language models capable
of thoroughly grasping all these relationships will be free from
hallucinations. Additionally, I propose that certain types of equivariant
language models are adept at learning and understanding these relationships.
Building on this, I have developed a specialized cross-entropy error function
to create a hallucination scale for language models, which measures their
extent of equivariance acquisition. Utilizing this scale, I tested language
models for their ability to acquire character-level equivariance. In
particular, I introduce and employ a novel technique based on T5 (Text To Text
Transfer Transformer) that efficiently understands permuted input texts without
the need for explicit dictionaries to convert token IDs (integers) to texts
(strings). This T5 model demonstrated a moderate ability to acquire
character-level equivariance. Additionally, I discovered scale laws that can
aid in developing hallucination-free language models at the character level.
This methodology can be extended to assess equivariance acquisition at the word
level, paving the way for very large language models that can comprehensively
understand relationships and, consequently, avoid hallucinations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GIT-Mol: A Multi-modal Large Language Model for Molecular Science with
  Graph, Image, and Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.06911v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.06911v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Liu, Yiming Ren, Zhixiang Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have made significant strides in natural language
processing, enabling innovative applications in molecular science by processing
textual representations of molecules. However, most existing language models
cannot capture the rich information with complex molecular structures or
images. In this paper, we introduce GIT-Mol, a multi-modal large language model
that integrates the Graph, Image, and Text information. To facilitate the
integration of multi-modal molecular data, we propose GIT-Former, a novel
architecture that is capable of aligning all modalities into a unified latent
space. We achieve a 5%-10% accuracy increase in properties prediction and a
20.2% boost in molecule generation validity compared to the baselines. With the
any-to-language molecular translation strategy, our model has the potential to
perform more downstream tasks, such as compound name recognition and chemical
reaction prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VideoChat: Chat-Centric Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.06355v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.06355v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we initiate an attempt of developing an end-to-end
chat-centric video understanding system, coined as VideoChat. It integrates
video foundation models and large language models via a learnable neural
interface, excelling in spatiotemporal reasoning, event localization, and
causal relationship inference. To instructively tune this system, we build a
video-centric instruction dataset, composed of thousands of videos associated
with detailed descriptions and conversations. This dataset emphasizes
spatiotemporal reasoning and captures causal relationships, providing a
valuable asset for training our chat-centric video understanding system.
Preliminary qualitative experiments demonstrate the potential of our system
across a broad spectrum of video applications, which could serve as a simple
prototype system for future research on chat-centric video understanding.
Access our code and data at https://github.com/OpenGVLab/Ask-Anything
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-target Stance Detection by Exploiting Target Analytical
  Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01761v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01761v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daijun Ding, Rong Chen, Liwen Jing, Bowen Zhang, Xu Huang, Li Dong, Xiaowen Zhao, Ge Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-target stance detection (CTSD) is an important task, which infers the
attitude of the destination target by utilizing annotated data derived from the
source target. One important approach in CTSD is to extract domain-invariant
features to bridge the knowledge gap between multiple targets. However, the
analysis of informal and short text structure, and implicit expressions,
complicate the extraction of domain-invariant knowledge. In this paper, we
propose a Multi-Perspective Prompt-Tuning (MPPT) model for CTSD that uses the
analysis perspective as a bridge to transfer knowledge. First, we develop a
two-stage instruct-based chain-of-thought method (TsCoT) to elicit target
analysis perspectives and provide natural language explanations (NLEs) from
multiple viewpoints by formulating instructions based on large language model
(LLM). Second, we propose a multi-perspective prompt-tuning framework
(MultiPLN) to fuse the NLEs into the stance predictor. Extensive experiments
results demonstrate the superiority of MPPT against the state-of-the-art
baseline methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interactive Text-to-SQL Generation via Editable Step-by-Step
  Explanations <span class="chip">EMNLP 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.07372v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.07372v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Tian, Zheng Zhang, Zheng Ning, Toby Jia-Jun Li, Jonathan K. Kummerfeld, Tianyi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relational databases play an important role in business, science, and more.
However, many users cannot fully unleash the analytical power of relational
databases, because they are not familiar with database languages such as SQL.
Many techniques have been proposed to automatically generate SQL from natural
language, but they suffer from two issues: (1) they still make many mistakes,
particularly for complex queries, and (2) they do not provide a flexible way
for non-expert users to validate and refine incorrect queries. To address these
issues, we introduce a new interaction mechanism that allows users to directly
edit a step-by-step explanation of a query to fix errors. Our experiments on
multiple datasets, as well as a user study with 24 participants, demonstrate
that our approach can achieve better performance than multiple SOTA approaches.
Our code and datasets are available at https://github.com/magic-YuanTian/STEPS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to EMNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM in a flash: Efficient Large Language Model Inference with Limited
  Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11514v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11514v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keivan Alizadeh, Iman Mirzadeh, Dmitry Belenko, Karen Khatamifard, Minsik Cho, Carlo C Del Mundo, Mohammad Rastegari, Mehrdad Farajtabar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are central to modern natural language
processing, delivering exceptional performance in various tasks. However, their
substantial computational and memory requirements present challenges,
especially for devices with limited DRAM capacity. This paper tackles the
challenge of efficiently running LLMs that exceed the available DRAM capacity
by storing the model parameters in flash memory, but bringing them on demand to
DRAM. Our method involves constructing an inference cost model that takes into
account the characteristics of flash memory, guiding us to optimize in two
critical areas: reducing the volume of data transferred from flash and reading
data in larger, more contiguous chunks. Within this hardware-informed
framework, we introduce two principal techniques. First, "windowing"
strategically reduces data transfer by reusing previously activated neurons,
and second, "row-column bundling", tailored to the sequential data access
strengths of flash memory, increases the size of data chunks read from flash
memory. These methods collectively enable running models up to twice the size
of the available DRAM, with a 4-5x and 20-25x increase in inference speed
compared to naive loading approaches in CPU and GPU, respectively. Our
integration of sparsity awareness, context-adaptive loading, and a
hardware-oriented design paves the way for effective inference of LLMs on
devices with limited memory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">112</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Prompt with Text Only Supervision for Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02418v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02418v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Uzair Khattak, Muhammad Ferjad Naeem, Muzammal Naseer, Luc Van Gool, Federico Tombari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundational vision-language models such as CLIP are becoming a new paradigm
in vision, due to their excellent generalization abilities. However, adapting
these models for downstream tasks while maintaining their generalization
remains a challenge. In literature, one branch of methods adapts CLIP by
learning prompts using visual information. While effective, most of these works
require labeled data which is not practical, and often struggle to generalize
towards new datasets due to over-fitting on the source data. An alternative
approach resorts to training-free methods by generating class descriptions from
large language models (LLMs) and perform prompt ensembling. However, these
methods often generate class specific prompts that cannot be transferred to
other classes, which incur higher costs by generating LLM descriptions for each
class separately. In this work, we propose to combine the strengths of these
both streams of methods by learning prompts using only text data derived from
LLMs. As supervised training of prompts is not trivial due to absence of
images, we develop a training approach that allows prompts to extract rich
contextual knowledge from LLM data. Moreover, with LLM contextual data mapped
within the learned prompts, it enables zero-shot transfer of prompts to new
classes and datasets potentially cutting the LLM prompt engineering cost. To
the best of our knowledge, this is the first work that learns generalized
prompts using text only data. We perform extensive evaluations on 4 benchmarks
where our method improves over prior ensembling works while being competitive
to those utilizing labeled images. Our code and pre-trained models are
available at https://github.com/muzairkhattak/ProText.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://muzairkhattak.github.io/ProText/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ODIN: A Single Model for 2D and 3D Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02416v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02416v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayush Jain, Pushkal Katara, Nikolaos Gkanatsios, Adam W. Harley, Gabriel Sarch, Kriti Aggarwal, Vishrav Chaudhary, Katerina Fragkiadaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art models on contemporary 3D perception benchmarks like ScanNet
consume and label dataset-provided 3D point clouds, obtained through post
processing of sensed multiview RGB-D images. They are typically trained
in-domain, forego large-scale 2D pre-training and outperform alternatives that
featurize the posed RGB-D multiview images instead. The gap in performance
between methods that consume posed images versus post-processed 3D point clouds
has fueled the belief that 2D and 3D perception require distinct model
architectures. In this paper, we challenge this view and propose ODIN
(Omni-Dimensional INstance segmentation), a model that can segment and label
both 2D RGB images and 3D point clouds, using a transformer architecture that
alternates between 2D within-view and 3D cross-view information fusion. Our
model differentiates 2D and 3D feature operations through the positional
encodings of the tokens involved, which capture pixel coordinates for 2D patch
tokens and 3D coordinates for 3D feature tokens. ODIN achieves state-of-the-art
performance on ScanNet200, Matterport3D and AI2THOR 3D instance segmentation
benchmarks, and competitive performance on ScanNet, S3DIS and COCO. It
outperforms all previous works by a wide margin when the sensed 3D point cloud
is used in place of the point cloud sampled from 3D mesh. When used as the 3D
perception engine in an instructable embodied agent architecture, it sets a new
state-of-the-art on the TEACh action-from-dialogue benchmark. Our code and
checkpoints can be found at the project website: https://odin-seg.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bring Metric Functions into Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie An, Zhengyuan Yang, Jianfeng Wang, Linjie Li, Zicheng Liu, Lijuan Wang, Jiebo Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a Cascaded Diffusion Model (Cas-DM) that improves a Denoising
Diffusion Probabilistic Model (DDPM) by effectively incorporating additional
metric functions in training. Metric functions such as the LPIPS loss have been
proven highly effective in consistency models derived from the score matching.
However, for the diffusion counterparts, the methodology and efficacy of adding
extra metric functions remain unclear. One major challenge is the mismatch
between the noise predicted by a DDPM at each step and the desired clean image
that the metric function works well on. To address this problem, we propose
Cas-DM, a network architecture that cascades two network modules to effectively
apply metric functions to the diffusion model training. The first module,
similar to a standard DDPM, learns to predict the added noise and is unaffected
by the metric function. The second cascaded module learns to predict the clean
image, thereby facilitating the metric function computation. Experiment results
show that the proposed diffusion model backbone enables the effective use of
the LPIPS loss, leading to state-of-the-art image quality (FID, sFID, IS) on
various established benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM Augmented LLMs: Expanding Capabilities through Composition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rachit Bansal, Bidisha Samanta, Siddharth Dalmia, Nitish Gupta, Shikhar Vashishth, Sriram Ganapathy, Abhishek Bapna, Prateek Jain, Partha Talukdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundational models with billions of parameters which have been trained on
large corpora of data have demonstrated non-trivial skills in a variety of
domains. However, due to their monolithic structure, it is challenging and
expensive to augment them or impart new skills. On the other hand, due to their
adaptation abilities, several new instances of these models are being trained
towards new domains and tasks. In this work, we study the problem of efficient
and practical composition of existing foundation models with more specific
models to enable newer capabilities. To this end, we propose CALM --
Composition to Augment Language Models -- which introduces cross-attention
between models to compose their representations and enable new capabilities.
Salient features of CALM are: (i) Scales up LLMs on new tasks by 're-using'
existing LLMs along with a few additional parameters and data, (ii) Existing
model weights are kept intact, and hence preserves existing capabilities, and
(iii) Applies to diverse domains and settings. We illustrate that augmenting
PaLM2-S with a smaller model trained on low-resource languages results in an
absolute improvement of up to 13\% on tasks like translation into English and
arithmetic reasoning for low-resource languages. Similarly, when PaLM2-S is
augmented with a code-specific model, we see a relative improvement of 40\%
over the base model for code generation and explanation tasks -- on-par with
fully fine-tuned counterparts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 2 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What You See is What You GAN: Rendering Every Pixel for High-Fidelity
  Geometry in 3D GANs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex Trevithick, Matthew Chan, Towaki Takikawa, Umar Iqbal, Shalini De Mello, Manmohan Chandraker, Ravi Ramamoorthi, Koki Nagano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D-aware Generative Adversarial Networks (GANs) have shown remarkable
progress in learning to generate multi-view-consistent images and 3D geometries
of scenes from collections of 2D images via neural volume rendering. Yet, the
significant memory and computational costs of dense sampling in volume
rendering have forced 3D GANs to adopt patch-based training or employ
low-resolution rendering with post-processing 2D super resolution, which
sacrifices multiview consistency and the quality of resolved geometry.
Consequently, 3D GANs have not yet been able to fully resolve the rich 3D
geometry present in 2D images. In this work, we propose techniques to scale
neural volume rendering to the much higher resolution of native 2D images,
thereby resolving fine-grained 3D geometry with unprecedented detail. Our
approach employs learning-based samplers for accelerating neural rendering for
3D GAN training using up to 5 times fewer depth samples. This enables us to
explicitly "render every pixel" of the full-resolution image during training
and inference without post-processing superresolution in 2D. Together with our
strategy to learn high-quality surface geometry, our method synthesizes
high-resolution 3D geometry and strictly view-consistent images while
maintaining image quality on par with baselines relying on post-processing
super resolution. We demonstrate state-of-the-art 3D gemetric quality on FFHQ
and AFHQ, setting a new standard for unsupervised learning of 3D shapes in 3D
GANs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>See our project page: https://research.nvidia.com/labs/nxp/wysiwyg/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3D Open-Vocabulary Panoptic Segmentation with 2D-3D Vision-Language
  Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02402v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02402v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Xiao, Longlong Jing, Shangxuan Wu, Alex Zihao Zhu, Jingwei Ji, Chiyu Max Jiang, Wei-Chih Hung, Thomas Funkhouser, Weicheng Kuo, Anelia Angelova, Yin Zhou, Shiwei Sheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D panoptic segmentation is a challenging perception task, which aims to
predict both semantic and instance annotations for 3D points in a scene.
Although prior 3D panoptic segmentation approaches have achieved great
performance on closed-set benchmarks, generalizing to novel categories remains
an open problem. For unseen object categories, 2D open-vocabulary segmentation
has achieved promising results that solely rely on frozen CLIP backbones and
ensembling multiple classification outputs. However, we find that simply
extending these 2D models to 3D does not achieve good performance due to poor
per-mask classification quality on novel categories. In this paper, we propose
the first method to tackle 3D open-vocabulary panoptic segmentation. Our model
takes advantage of the fusion between learnable LiDAR features and dense frozen
vision CLIP features, using a single classification head to make predictions
for both base and novel classes. To further improve the classification
performance on novel classes and leverage the CLIP model, we propose two novel
loss functions: object-level distillation loss and voxel-level distillation
loss. Our experiments on the nuScenes and SemanticKITTI datasets show that our
method outperforms strong baselines by a large margin.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning the 3D Fauna of the Web 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02400v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02400v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zizhang Li, Dor Litvak, Ruining Li, Yunzhi Zhang, Tomas Jakab, Christian Rupprecht, Shangzhe Wu, Andrea Vedaldi, Jiajun Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning 3D models of all animals on the Earth requires massively scaling up
existing solutions. With this ultimate goal in mind, we develop 3D-Fauna, an
approach that learns a pan-category deformable 3D animal model for more than
100 animal species jointly. One crucial bottleneck of modeling animals is the
limited availability of training data, which we overcome by simply learning
from 2D Internet images. We show that prior category-specific attempts fail to
generalize to rare species with limited training images. We address this
challenge by introducing the Semantic Bank of Skinned Models (SBSM), which
automatically discovers a small set of base animal shapes by combining
geometric inductive priors with semantic knowledge implicitly captured by an
off-the-shelf self-supervised feature extractor. To train such a model, we also
contribute a new large-scale dataset of diverse animal species. At inference
time, given a single image of any quadruped animal, our model reconstructs an
articulated 3D mesh in a feed-forward fashion within seconds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally to this work. The last
  three authors contributed equally. Project page:
  https://kyleleey.github.io/3DFauna/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChartAssisstant: A Universal Chart Multimodal Language Model via
  Chart-to-Table Pre-training and Multitask Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanqing Meng, Wenqi Shao, Quanfeng Lu, Peng Gao, Kaipeng Zhang, Yu Qiao, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Charts play a vital role in data visualization, understanding data patterns,
and informed decision-making. However, their unique combination of graphical
elements (e.g., bars, lines) and textual components (e.g., labels, legends)
poses challenges for general-purpose multimodal models. While vision-language
models trained on chart data excel in comprehension, they struggle with
generalization and require task-specific fine-tuning. To address these
challenges, we propose ChartAssistant, a chart-based vision-language model for
universal chart comprehension and reasoning. ChartAssistant leverages ChartSFT,
a comprehensive dataset covering diverse chart-related tasks with basic and
specialized chart types. It undergoes a two-stage training process, starting
with pre-training on chart-to-table parsing to align chart and text, followed
by multitask instruction-following fine-tuning. This approach enables
ChartAssistant to achieve competitive performance across various chart tasks
without task-specific fine-tuning. Experimental results demonstrate significant
performance gains over the state-of-the-art UniChart method, outperforming
OpenAI's GPT-4V(ision) on real-world chart data. The code and data are
available at https://github.com/OpenGVLab/ChartAst.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Survey of 3D Human Body Pose and Shape Estimation Methods for
  Contemporary Dance Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Darshan Venkatrayappa, Alain Tremeau, Damien Muselet, Philippe Colantoni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D human body shape and pose estimation from RGB images is a challenging
problem with potential applications in augmented/virtual reality, healthcare
and fitness technology and virtual retail. Recent solutions have focused on
three types of inputs: i) single images, ii) multi-view images and iii) videos.
In this study, we surveyed and compared 3D body shape and pose estimation
methods for contemporary dance and performing arts, with a special focus on
human body pose and dressing, camera viewpoint, illumination conditions and
background conditions. We demonstrated that multi-frame methods, such as PHALP,
provide better results than single-frame method for pose estimation when
dancers are performing contemporary dances.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2008.09062 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Open and Comprehensive Pipeline for Unified Object Grounding and
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Zhao, Yicheng Chen, Shilin Xu, Xiangtai Li, Xinjiang Wang, Yining Li, Haian Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Grounding-DINO is a state-of-the-art open-set detection model that tackles
multiple vision tasks including Open-Vocabulary Detection (OVD), Phrase
Grounding (PG), and Referring Expression Comprehension (REC). Its effectiveness
has led to its widespread adoption as a mainstream architecture for various
downstream applications. However, despite its significance, the original
Grounding-DINO model lacks comprehensive public technical details due to the
unavailability of its training code. To bridge this gap, we present
MM-Grounding-DINO, an open-source, comprehensive, and user-friendly baseline,
which is built with the MMDetection toolbox. It adopts abundant vision datasets
for pre-training and various detection and grounding datasets for fine-tuning.
We give a comprehensive analysis of each reported result and detailed settings
for reproduction. The extensive experiments on the benchmarks mentioned
demonstrate that our MM-Grounding-DINO-Tiny outperforms the Grounding-DINO-Tiny
baseline. We release all our models to the research community. Codes and
trained models are released at
https://github.com/open-mmlab/mmdetection/configs/mm_grounding_dino.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A novel method to enhance pneumonia detection via a model-level
  ensembling of CNN and vision transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02358v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02358v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sandeep Angara, Nishith Reddy Mannuru, Aashrith Mannuru, Sharath Thirunagaru
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pneumonia remains a leading cause of morbidity and mortality worldwide. Chest
X-ray (CXR) imaging is a fundamental diagnostic tool, but traditional analysis
relies on time-intensive expert evaluation. Recently, deep learning has shown
immense potential for automating pneumonia detection from CXRs. This paper
explores applying neural networks to improve CXR-based pneumonia diagnosis. We
developed a novel model fusing Convolution Neural networks (CNN) and Vision
Transformer networks via model-level ensembling. Our fusion architecture
combines a ResNet34 variant and a Multi-Axis Vision Transformer small model.
Both base models are initialized with ImageNet pre-trained weights. The output
layers are removed, and features are combined using a flattening layer before
final classification. Experiments used the Kaggle pediatric pneumonia dataset
containing 1,341 normal and 3,875 pneumonia CXR images. We compared our model
against standalone ResNet34, Vision Transformer, and Swin Transformer Tiny
baseline models using identical training procedures. Extensive data
augmentation, Adam optimization, learning rate warmup, and decay were employed.
The fusion model achieved a state-of-the-art accuracy of 94.87%, surpassing the
baselines. We also attained excellent sensitivity, specificity, kappa score,
and positive predictive value. Confusion matrix analysis confirms fewer
misclassifications. The ResNet34 and Vision Transformer combination enables
jointly learning robust features from CNNs and Transformer paradigms. This
model-level ensemble technique effectively integrates their complementary
strengths for enhanced pneumonia classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fit-NGP: Fitting Object Models to Neural Graphics Primitives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02357v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02357v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marwan Taher, Ignacio Alzugaray, Andrew J. Davison
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate 3D object pose estimation is key to enabling many robotic
applications that involve challenging object interactions. In this work, we
show that the density field created by a state-of-the-art efficient radiance
field reconstruction method is suitable for highly accurate and robust pose
estimation for objects with known 3D models, even when they are very small and
with challenging reflective surfaces. We present a fully automatic object pose
estimation system based on a robot arm with a single wrist-mounted camera,
which can scan a scene from scratch, detect and estimate the 6-Degrees of
Freedom (DoF) poses of multiple objects within a couple of minutes of
operation. Small objects such as bolts and nuts are estimated with accuracy on
order of 1mm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mining Fine-Grained Image-Text Alignment for Zero-Shot Captioning via
  Text-Only Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longtian Qiu, Shan Ning, Xuming He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image captioning aims at generating descriptive and meaningful textual
descriptions of images, enabling a broad range of vision-language applications.
Prior works have demonstrated that harnessing the power of Contrastive Image
Language Pre-training (CLIP) offers a promising approach to achieving zero-shot
captioning, eliminating the need for expensive caption annotations. However,
the widely observed modality gap in the latent space of CLIP harms the
performance of zero-shot captioning by breaking the alignment between paired
image-text features. To address this issue, we conduct an analysis on the CLIP
latent space which leads to two findings. Firstly, we observe that the CLIP's
visual feature of image subregions can achieve closer proximity to the paired
caption due to the inherent information loss in text descriptions. In addition,
we show that the modality gap between a paired image-text can be empirically
modeled as a zero-mean Gaussian distribution. Motivated by the findings, we
propose a novel zero-shot image captioning framework with text-only training to
reduce the modality gap. In particular, we introduce a subregion feature
aggregation to leverage local region information, which produces a compact
visual representation for matching text representation. Moreover, we
incorporate a noise injection and CLIP reranking strategy to boost captioning
performance. We also extend our framework to build a zero-shot VQA pipeline,
demonstrating its generality. Through extensive experiments on common
captioning and VQA datasets such as MSCOCO, Flickr30k and VQAV2, we show that
our method achieves remarkable performance improvements. Code is available at
https://github.com/Artanic30/MacCap.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2024.Open sourced, Code and Model Available</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Linguistic Profiling of Deepfakes: An Open Database for Next-Generation
  Deepfake Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yabin Wang, Zhiwu Huang, Zhiheng Ma, Xiaopeng Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of text-to-image generative models has revolutionized the field
of deepfakes, enabling the creation of realistic and convincing visual content
directly from textual descriptions. However, this advancement presents
considerably greater challenges in detecting the authenticity of such content.
Existing deepfake detection datasets and methods often fall short in
effectively capturing the extensive range of emerging deepfakes and offering
satisfactory explanatory information for detection. To address the significant
issue, this paper introduces a deepfake database (DFLIP-3K) for the development
of convincing and explainable deepfake detection. It encompasses about 300K
diverse deepfake samples from approximately 3K generative models, which boasts
the largest number of deepfake models in the literature. Moreover, it collects
around 190K linguistic footprints of these deepfakes. The two distinguished
features enable DFLIP-3K to develop a benchmark that promotes progress in
linguistic profiling of deepfakes, which includes three sub-tasks namely
deepfake detection, model identification, and prompt prediction. The deepfake
model and prompt are two essential components of each deepfake, and thus
dissecting them linguistically allows for an invaluable exploration of
trustworthy and interpretable evidence in deepfake detection, which we believe
is the key for the next-generation deepfake detection. Furthermore, DFLIP-3K is
envisioned as an open database that fosters transparency and encourages
collaborative efforts to further enhance its growth. Our extensive experiments
on the developed benchmark verify that our DFLIP-3K database is capable of
serving as a standardized resource for evaluating and comparing
linguistic-based deepfake detection, identification, and prompt prediction
techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLaVA-$φ$: Efficient Multi-Modal Assistant with Small Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichen Zhu, Minjie Zhu, Ning Liu, Zhicai Ou, Xiaofeng Mou, Jian Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce LLaVA-$\phi$ (LLaVA-Phi), an efficient
multi-modal assistant that harnesses the power of the recently advanced small
language model, Phi-2, to facilitate multi-modal dialogues. LLaVA-Phi marks a
notable advancement in the realm of compact multi-modal models. It demonstrates
that even smaller language models, with as few as 2.7B parameters, can
effectively engage in intricate dialogues that integrate both textual and
visual elements, provided they are trained with high-quality corpora. Our model
delivers commendable performance on publicly available benchmarks that
encompass visual comprehension, reasoning, and knowledge-based perception.
Beyond its remarkable performance in multi-modal dialogue tasks, our model
opens new avenues for applications in time-sensitive environments and systems
that require real-time interaction, such as embodied agents. It highlights the
potential of smaller language models to achieve sophisticated levels of
understanding and interaction, while maintaining greater resource
efficiency.The project is available at {https://github.com/zhuyiche/llava-phi}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>technique report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ClassWise-SAM-Adapter: Parameter Efficient Fine-tuning Adapts Segment
  Anything to SAR Domain for Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyang Pu, Hecheng Jia, Linghao Zheng, Feng Wang, Feng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of artificial intelligence, the emergence of foundation models,
backed by high computing capabilities and extensive data, has been
revolutionary. Segment Anything Model (SAM), built on the Vision Transformer
(ViT) model with millions of parameters and vast training dataset SA-1B, excels
in various segmentation scenarios relying on its significance of semantic
information and generalization ability. Such achievement of visual foundation
model stimulates continuous researches on specific downstream tasks in computer
vision. The ClassWise-SAM-Adapter (CWSAM) is designed to adapt the
high-performing SAM for landcover classification on space-borne Synthetic
Aperture Radar (SAR) images. The proposed CWSAM freezes most of SAM's
parameters and incorporates lightweight adapters for parameter efficient
fine-tuning, and a classwise mask decoder is designed to achieve semantic
segmentation task. This adapt-tuning method allows for efficient landcover
classification of SAR images, balancing the accuracy with computational demand.
In addition, the task specific input module injects low frequency information
of SAR images by MLP-based layers to improve the model performance. Compared to
conventional state-of-the-art semantic segmentation algorithms by extensive
experiments, CWSAM showcases enhanced performance with fewer computing
resources, highlighting the potential of leveraging foundational models like
SAM for specific downstream tasks in the SAR domain. The source code is
available at: https://github.com/xypu98/CWSAM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BA-SAM: Scalable Bias-Mode Attention Mask for Segment Anything Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiran Song, Qianyu Zhou, Xiangtai Li, Deng-Ping Fan, Xuequan Lu, Lizhuang Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the challenge of image resolution variation for the
Segment Anything Model (SAM). SAM, known for its zero-shot generalizability,
exhibits a performance degradation when faced with datasets with varying image
sizes. Previous approaches tend to resize the image to a fixed size or adopt
structure modifications, hindering the preservation of SAM's rich prior
knowledge. Besides, such task-specific tuning necessitates a complete
retraining of the model, which is cost-expensive and unacceptable for
deployment in the downstream tasks. In this paper, we reformulate this issue as
a length extrapolation problem, where token sequence length varies while
maintaining a consistent patch size for images of different sizes. To this end,
we propose Scalable Bias-Mode Attention Mask (BA-SAM) to enhance SAM's
adaptability to varying image resolutions while eliminating the need for
structure modifications. Firstly, we introduce a new scaling factor to ensure
consistent magnitude in the attention layer's dot product values when the token
sequence length changes. Secondly, we present a bias-mode attention mask that
allows each token to prioritize neighboring information, mitigating the impact
of untrained distant information. Our BA-SAM demonstrates efficacy in two
scenarios: zero-shot and fine-tuning. Extensive evaluation on diverse datasets,
including DIS5K, DUTS, ISIC, COD10K, and COCO, reveals its ability to
significantly mitigate performance degradation in the zero-shot setting and
achieve state-of-the-art performance with minimal fine-tuning. Furthermore, we
propose a generalized model and benchmark, showcasing BA-SAM's generalizability
across all four datasets simultaneously.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SuperEdge: Towards a Generalization Model for Self-Supervised Edge
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leng Kai, Zhang Zhijie, Liu Jie, Zed Boukhers, Sui Wei, Cong Yang, Li Zhijun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Edge detection is a fundamental technique in various computer vision tasks.
Edges are indeed effectively delineated by pixel discontinuity and can offer
reliable structural information even in textureless areas. State-of-the-art
heavily relies on pixel-wise annotations, which are labor-intensive and subject
to inconsistencies when acquired manually. In this work, we propose a novel
self-supervised approach for edge detection that employs a multi-level,
multi-homography technique to transfer annotations from synthetic to real-world
datasets. To fully leverage the generated edge annotations, we developed
SuperEdge, a streamlined yet efficient model capable of concurrently extracting
edges at pixel-level and object-level granularity. Thanks to self-supervised
training, our method eliminates the dependency on manual annotated edge labels,
thereby enhancing its generalizability across diverse datasets. Comparative
evaluations reveal that SuperEdge advances edge detection, demonstrating
improvements of 4.9% in ODS and 3.3% in OIS over the existing STEdge method on
BIPEDv2.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TR-DETR: Task-Reciprocal Transformer for Joint Moment Retrieval and
  Highlight Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Sun, Mingyao Zhou, Wenjing Chen, Wei Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video moment retrieval (MR) and highlight detection (HD) based on natural
language queries are two highly related tasks, which aim to obtain relevant
moments within videos and highlight scores of each video clip. Recently,
several methods have been devoted to building DETR-based networks to solve both
MR and HD jointly. These methods simply add two separate task heads after
multi-modal feature extraction and feature interaction, achieving good
performance. Nevertheless, these approaches underutilize the reciprocal
relationship between two tasks. In this paper, we propose a task-reciprocal
transformer based on DETR (TR-DETR) that focuses on exploring the inherent
reciprocity between MR and HD. Specifically, a local-global multi-modal
alignment module is first built to align features from diverse modalities into
a shared latent space. Subsequently, a visual feature refinement is designed to
eliminate query-irrelevant information from visual features for modal
interaction. Finally, a task cooperation module is constructed to refine the
retrieval pipeline and the highlight score prediction process by utilizing the
reciprocity between MR and HD. Comprehensive experiments on QVHighlights,
Charades-STA and TVSum datasets demonstrate that TR-DETR outperforms existing
state-of-the-art methods. Codes are available at
\url{https://github.com/mingyao1120/TR-DETR}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI-24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GridFormer: Point-Grid Transformer for Surface Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengtao Li, Ge Gao, Yudong Liu, Yu-Shen Liu, Ming Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit neural networks have emerged as a crucial technology in 3D surface
reconstruction. To reconstruct continuous surfaces from discrete point clouds,
encoding the input points into regular grid features (plane or volume) has been
commonly employed in existing approaches. However, these methods typically use
the grid as an index for uniformly scattering point features. Compared with the
irregular point features, the regular grid features may sacrifice some
reconstruction details but improve efficiency. To take full advantage of these
two types of features, we introduce a novel and high-efficiency attention
mechanism between the grid and point features named Point-Grid Transformer
(GridFormer). This mechanism treats the grid as a transfer point connecting the
space and point cloud. Our method maximizes the spatial expressiveness of grid
features and maintains computational efficiency. Furthermore, optimizing
predictions over the entire space could potentially result in blurred
boundaries. To address this issue, we further propose a boundary optimization
strategy incorporating margin binary cross-entropy loss and boundary sampling.
This approach enables us to achieve a more precise representation of the object
structure. Our experiments validate that our method is effective and
outperforms the state-of-the-art approaches under widely used benchmarks by
producing more precise geometry reconstructions. The code is available at
https://github.com/list17/GridFormer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distillation-based fabric anomaly detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02287v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02287v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Thomine, Hichem Snoussi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised texture anomaly detection has been a concerning topic in a vast
amount of industrial processes. Patterned textures inspection, particularly in
the context of fabric defect detection, is indeed a widely encountered use
case. This task involves handling a diverse spectrum of colors and textile
types, encompassing a wide range of fabrics. Given the extensive variability in
colors, textures, and defect types, fabric defect detection poses a complex and
challenging problem in the field of patterned textures inspection. In this
article, we propose a knowledge distillation-based approach tailored
specifically for addressing the challenge of unsupervised anomaly detection in
textures resembling fabrics. Our method aims to redefine the recently
introduced reverse distillation approach, which advocates for an
encoder-decoder design to mitigate classifier bias and to prevent the student
from reconstructing anomalies. In this study, we present a new reverse
distillation technique for the specific task of fabric defect detection. Our
approach involves a meticulous design selection that strategically highlights
high-level features. To demonstrate the capabilities of our approach both in
terms of performance and inference speed, we conducted a series of experiments
on multiple texture datasets, including MVTEC AD, AITEX, and TILDA, alongside
conducting experiments on a dataset acquired from a textile manufacturing
facility. The main contributions of this paper are the following: a robust
texture anomaly detector utilizing a reverse knowledge-distillation technique
suitable for both anomaly detection and domain generalization and a novel
dataset encompassing a diverse range of fabrics and defects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Textile Research Journal. 2023;0(0)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PEGASUS: Physically Enhanced Gaussian Splatting Simulation System for
  6DOF Object Pose <span class="highlight-title">Dataset</span> Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02281v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02281v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Meyer, Floris Erich, Yusuke Yoshiyasu, Marc Stamminger, Noriaki Ando, Yukiyasu Domae
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Physically Enhanced Gaussian Splatting Simulation System
(PEGASUS) for 6DOF object pose dataset generation, a versatile dataset
generator based on 3D Gaussian Splatting. Environment and object
representations can be easily obtained using commodity cameras to reconstruct
with Gaussian Splatting. PEGASUS allows the composition of new scenes by
merging the respective underlying Gaussian Splatting point cloud of an
environment with one or multiple objects. Leveraging a physics engine enables
the simulation of natural object placement within a scene through interaction
between meshes extracted for the objects and the environment. Consequently, an
extensive amount of new scenes - static or dynamic - can be created by
combining different environments and objects. By rendering scenes from various
perspectives, diverse data points such as RGB images, depth maps, semantic
masks, and 6DoF object poses can be extracted. Our study demonstrates that
training on data generated by PEGASUS enables pose estimation networks to
successfully transfer from synthetic data to real-world data. Moreover, we
introduce the Ramen dataset, comprising 30 Japanese cup noodle items. This
dataset includes spherical scans that captures images from both object
hemisphere and the Gaussian Splatting reconstruction, making them compatible
with PEGASUS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://meyerls.github.io/pegasus_web</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lightweight Fish Classification Model for Sustainable Marine Management:
  Indonesian Case 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Febrian Kurniawan, Gandeva Bayu Satrya, Firuz Kamalov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The enormous demand for seafood products has led to exploitation of marine
resources and near-extinction of some species. In particular, overfishing is
one the main issues in sustainable marine development. In alignment with the
protection of marine resources and sustainable fishing, this study proposes to
advance fish classification techniques that support identifying protected fish
species using state-of-the-art machine learning. We use a custom modification
of the MobileNet model to design a lightweight classifier called M-MobileNet
that is capable of running on limited hardware. As part of the study, we
compiled a labeled dataset of 37,462 images of fish found in the waters of the
Indonesian archipelago. The proposed model is trained on the dataset to
classify images of the captured fish into their species and give
recommendations on whether they are consumable or not. Our modified MobileNet
model uses only 50\% of the top layer parameters with about 42% GTX 860M
utility and achieves up to 97% accuracy in fish classification and determining
its consumability. Given the limited computing capacity available on many
fishing vessels, the proposed model provides a practical solution to on-site
fish classification. In addition, synchronized implementation of the proposed
model on multiple vessels can supply valuable information about the movement
and location of different species of fish.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ShapeAug: Occlusion Augmentation for Event Camera Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katharina Bendig, René Schuster, Didier Stricker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Dynamic Vision Sensors (DVSs) sparked a lot of interest due to
their inherent advantages over conventional RGB cameras. These advantages
include a low latency, a high dynamic range and a low energy consumption.
Nevertheless, the processing of DVS data using Deep Learning (DL) methods
remains a challenge, particularly since the availability of event training data
is still limited. This leads to a need for event data augmentation techniques
in order to improve accuracy as well as to avoid over-fitting on the training
data. Another challenge especially in real world automotive applications is
occlusion, meaning one object is hindering the view onto the object behind it.
In this paper, we present a novel event data augmentation approach, which
addresses this problem by introducing synthetic events for randomly moving
objects in a scene. We test our method on multiple DVS classification datasets,
resulting in an relative improvement of up to 6.5 % in top1-accuracy. Moreover,
we apply our augmentation technique on the real world Gen1 Automotive Event
Dataset for object detection, where we especially improve the detection of
pedestrians by up to 5 %.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICPRAM 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Slot-guided Volumetric Object Radiance Fields <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Di Qi, Tong Yang, Xiangyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel framework for 3D object-centric representation learning.
Our approach effectively decomposes complex scenes into individual objects from
a single image in an unsupervised fashion. This method, called slot-guided
Volumetric Object Radiance Fields (sVORF), composes volumetric object radiance
fields with object slots as a guidance to implement unsupervised 3D scene
decomposition. Specifically, sVORF obtains object slots from a single image via
a transformer module, maps these slots to volumetric object radiance fields
with a hypernetwork and composes object radiance fields with the guidance of
object slots at a 3D location. Moreover, sVORF significantly reduces memory
requirement due to small-sized pixel rendering during training. We demonstrate
the effectiveness of our approach by showing top results in scene decomposition
and generation tasks of complex synthetic datasets (e.g., Room-Diverse).
Furthermore, we also confirm the potential of sVORF to segment objects in
real-world scenes (e.g., the LLFF dataset). We hope our approach can provide
preliminary understanding of the physical world and help ease future research
in 3D object-centric representation learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nodule detection and generation on chest X-rays: NODE21 Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ecem Sogancioglu, Bram van Ginneken, Finn Behrendt, Marcel Bengs, Alexander Schlaefer, Miron Radu, Di Xu, Ke Sheng, Fabien Scalzo, Eric Marcus, Samuele Papa, Jonas Teuwen, Ernst Th. Scholten, Steven Schalekamp, Nils Hendrix, Colin Jacobs, Ward Hendrix, Clara I Sánchez, Keelin Murphy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pulmonary nodules may be an early manifestation of lung cancer, the leading
cause of cancer-related deaths among both men and women. Numerous studies have
established that deep learning methods can yield high-performance levels in the
detection of lung nodules in chest X-rays. However, the lack of gold-standard
public datasets slows down the progression of the research and prevents
benchmarking of methods for this task. To address this, we organized a public
research challenge, NODE21, aimed at the detection and generation of lung
nodules in chest X-rays. While the detection track assesses state-of-the-art
nodule detection systems, the generation track determines the utility of nodule
generation algorithms to augment training data and hence improve the
performance of the detection systems. This paper summarizes the results of the
NODE21 challenge and performs extensive additional experiments to examine the
impact of the synthetically generated nodule training images on the detection
algorithm performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prompt Decoupling for Text-to-Image Person Re-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihao Li, Lei Tan, Pingyang Dai, Yan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image person re-identification (TIReID) aims to retrieve the target
person from an image gallery via a textual description query. Recently,
pre-trained vision-language models like CLIP have attracted significant
attention and have been widely utilized for this task due to their robust
capacity for semantic concept learning and rich multi-modal knowledge. However,
recent CLIP-based TIReID methods commonly rely on direct fine-tuning of the
entire network to adapt the CLIP model for the TIReID task. Although these
methods show competitive performance on this topic, they are suboptimal as they
necessitate simultaneous domain adaptation and task adaptation. To address this
issue, we attempt to decouple these two processes during the training stage.
Specifically, we introduce the prompt tuning strategy to enable domain
adaptation and propose a two-stage training approach to disentangle domain
adaptation from task adaptation. In the first stage, we freeze the two encoders
from CLIP and solely focus on optimizing the prompts to alleviate domain gap
between the original training data of CLIP and downstream tasks. In the second
stage, we maintain the fixed prompts and fine-tune the CLIP model to prioritize
capturing fine-grained information, which is more suitable for TIReID task.
Finally, we evaluate the effectiveness of our method on three widely used
datasets. Compared to the directly fine-tuned approach, our method achieves
significant improvements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Frequency Domain Nuances Mining for Visible-Infrared Person
  Re-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukang Zhang, Yang Lu, Yan Yan, Hanzi Wang, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The key of visible-infrared person re-identification (VIReID) lies in how to
minimize the modality discrepancy between visible and infrared images. Existing
methods mainly exploit the spatial information while ignoring the
discriminative frequency information. To address this issue, this paper aims to
reduce the modality discrepancy from the frequency domain perspective.
Specifically, we propose a novel Frequency Domain Nuances Mining (FDNM) method
to explore the cross-modality frequency domain information, which mainly
includes an amplitude guided phase (AGP) module and an amplitude nuances mining
(ANM) module. These two modules are mutually beneficial to jointly explore
frequency domain visible-infrared nuances, thereby effectively reducing the
modality discrepancy in the frequency domain. Besides, we propose a
center-guided nuances mining loss to encourage the ANM module to preserve
discriminative identity information while discovering diverse cross-modality
nuances. To the best of our knowledge, this is the first work that explores the
potential frequency information for VIReID research. Extensive experiments show
that the proposed FDNM has significant advantages in improving the performance
of VIReID. Specifically, our method outperforms the second-best method by 5.2\%
in Rank-1 accuracy and 5.8\% in mAP on the SYSU-MM01 dataset under the indoor
search mode, respectively. Besides, we also validate the effectiveness and
generalization of our method on the challenging visible-infrared face
recognition task. \textcolor{magenta}{The code will be available.}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing RAW-to-sRGB with Decoupled Style Structure in Fourier Domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02161v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02161v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanhua He, Tao Hu, Guoli Wang, Zejin Wang, Run Wang, Qian Zhang, Keyu Yan, Ziyi Chen, Rui Li, Chenjun Xie, Jie Zhang, Man Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  RAW to sRGB mapping, which aims to convert RAW images from smartphones into
RGB form equivalent to that of Digital Single-Lens Reflex (DSLR) cameras, has
become an important area of research. However, current methods often ignore the
difference between cell phone RAW images and DSLR camera RGB images, a
difference that goes beyond the color matrix and extends to spatial structure
due to resolution variations. Recent methods directly rebuild color mapping and
spatial structure via shared deep representation, limiting optimal performance.
Inspired by Image Signal Processing (ISP) pipeline, which distinguishes image
restoration and enhancement, we present a novel Neural ISP framework, named
FourierISP. This approach breaks the image down into style and structure within
the frequency domain, allowing for independent optimization. FourierISP is
comprised of three subnetworks: Phase Enhance Subnet for structural refinement,
Amplitude Refine Subnet for color learning, and Color Adaptation Subnet for
blending them in a smooth manner. This approach sharpens both color and
structure, and extensive evaluations across varied datasets confirm that our
approach realizes state-of-the-art results. Code will be available at
~\url{https://github.com/alexhe101/FourierISP}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Frequency-Adaptive Pan-Sharpening with Mixture of Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuanhua He, Keyu Yan, Rui Li, Chengjun Xie, Jie Zhang, Man Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pan-sharpening involves reconstructing missing high-frequency information in
multi-spectral images with low spatial resolution, using a higher-resolution
panchromatic image as guidance. Although the inborn connection with frequency
domain, existing pan-sharpening research has not almost investigated the
potential solution upon frequency domain. To this end, we propose a novel
Frequency Adaptive Mixture of Experts (FAME) learning framework for
pan-sharpening, which consists of three key components: the Adaptive Frequency
Separation Prediction Module, the Sub-Frequency Learning Expert Module, and the
Expert Mixture Module. In detail, the first leverages the discrete cosine
transform to perform frequency separation by predicting the frequency mask. On
the basis of generated mask, the second with low-frequency MOE and
high-frequency MOE takes account for enabling the effective low-frequency and
high-frequency information reconstruction. Followed by, the final fusion module
dynamically weights high-frequency and low-frequency MOE knowledge to adapt to
remote sensing images with significant content variations. Quantitative and
qualitative experiments over multiple datasets demonstrate that our method
performs the best against other state-of-the-art ones and comprises a strong
generalization ability for real-world scenes. Code will be made publicly at
\url{https://github.com/alexhe101/FAME-Net}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Marginal Debiased Network for Fair Visual Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mei Wang, Weihong Deng, Sen Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) are often prone to learn the spurious
correlations between target classes and bias attributes, like gender and race,
inherent in a major portion of training data (bias-aligned samples), thus
showing unfair behavior and arising controversy in the modern pluralistic and
egalitarian society. In this paper, we propose a novel marginal debiased
network (MDN) to learn debiased representations. More specifically, a marginal
softmax loss (MSL) is designed by introducing the idea of margin penalty into
the fairness problem, which assigns a larger margin for bias-conflicting
samples (data without spurious correlations) than for bias-aligned ones, so as
to deemphasize the spurious correlations and improve generalization on unbiased
test criteria. To determine the margins, our MDN is optimized through a meta
learning framework. We propose a meta equalized loss (MEL) to perceive the
model fairness, and adaptively update the margin parameters by metaoptimization
which requires the trained model guided by the optimal margins should minimize
MEL computed on an unbiased meta-validation set. Extensive experiments on
BiasedMNIST, Corrupted CIFAR-10, CelebA and UTK-Face datasets demonstrate that
our MDN can achieve a remarkable performance on under-represented samples and
obtain superior debiased results against the previous approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Boundary of GPT-4V on Marine Analysis: A Preliminary Case
  Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02147v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02147v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqiang Zheng, Yiwei Chen, Jipeng Zhang, Tuan-Anh Vu, Huimin Zeng, Yue Him Wong Tim, Sai-Kit Yeung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated a powerful ability to answer
various queries as a general-purpose assistant. The continuous multi-modal
large language models (MLLM) empower LLMs with the ability to perceive visual
signals. The launch of GPT-4 (Generative Pre-trained Transformers) has
generated significant interest in the research communities. GPT-4V(ison) has
demonstrated significant power in both academia and industry fields, as a focal
point in a new artificial intelligence generation. Though significant success
was achieved by GPT-4V, exploring MLLMs in domain-specific analysis (e.g.,
marine analysis) that required domain-specific knowledge and expertise has
gained less attention. In this study, we carry out the preliminary and
comprehensive case study of utilizing GPT-4V for marine analysis. This report
conducts a systematic evaluation of existing GPT-4V, assessing the performance
of GPT-4V on marine research and also setting a new standard for future
developments in MLLMs. The experimental results of GPT-4V show that the
responses generated by GPT-4V are still far away from satisfying the
domain-specific requirements of the marine professions. All images and prompts
used in this study will be available at
https://github.com/hkust-vgd/Marine_GPT-4V_Eval
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>51 pages, 36 figures, Repository:
  https://github.com/hkust-vgd/Marine_GPT-4V_Eval</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GUESS:GradUally Enriching SyntheSis for Text-Driven Human Motion
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02142v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02142v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuehao Gao, Yang Yang, Zhenyu Xie, Shaoyi Du, Zhongqian Sun, Yang Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel cascaded diffusion-based generative
framework for text-driven human motion synthesis, which exploits a strategy
named GradUally Enriching SyntheSis (GUESS as its abbreviation). The strategy
sets up generation objectives by grouping body joints of detailed skeletons in
close semantic proximity together and then replacing each of such joint group
with a single body-part node. Such an operation recursively abstracts a human
pose to coarser and coarser skeletons at multiple granularity levels. With
gradually increasing the abstraction level, human motion becomes more and more
concise and stable, significantly benefiting the cross-modal motion synthesis
task. The whole text-driven human motion synthesis problem is then divided into
multiple abstraction levels and solved with a multi-stage generation framework
with a cascaded latent diffusion model: an initial generator first generates
the coarsest human motion guess from a given text description; then, a series
of successive generators gradually enrich the motion details based on the
textual description and the previous synthesized results. Notably, we further
integrate GUESS with the proposed dynamic multi-condition fusion mechanism to
dynamically balance the cooperative effects of the given textual condition and
synthesized coarse motion prompt in different generation stages. Extensive
experiments on large-scale datasets verify that GUESS outperforms existing
state-of-the-art methods by large margins in terms of accuracy, realisticness,
and diversity. Code is available at https://github.com/Xuehao-Gao/GUESS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Visualization and Computer Graphics
  (2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian Intrinsic Groupwise Image Registration: Unsupervised
  Disentanglement of Anatomy and Geometry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02141v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02141v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinzhe Luo, Xin Wang, Linda Shapiro, Chun Yuan, Jianfeng Feng, Xiahai Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article presents a general Bayesian learning framework for multi-modal
groupwise registration on medical images. The method builds on probabilistic
modelling of the image generative process, where the underlying common anatomy
and geometric variations of the observed images are explicitly disentangled as
latent variables. Thus, groupwise registration is achieved through the solution
to Bayesian inference. We propose a novel hierarchical variational
auto-encoding architecture to realize the inference procedure of the latent
variables, where the registration parameters can be calculated in a
mathematically interpretable fashion. Remarkably, this new paradigm can learn
groupwise registration in an unsupervised closed-loop self-reconstruction
process, sparing the burden of designing complex intensity-based similarity
measures. The computationally efficient disentangled architecture is also
inherently scalable and flexible, allowing for groupwise registration on
large-scale image groups with variable sizes. Furthermore, the inferred
structural representations from disentanglement learning are capable of
capturing the latent anatomy of the observations with visual semantics.
Extensive experiments were conducted to validate the proposed framework,
including four datasets from cardiac, brain and abdominal medical images. The
results have demonstrated the superiority of our method over conventional
similarity-based approaches in terms of accuracy, efficiency, scalability and
interpretability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explore Human Parsing Modality for Action Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02138v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02138v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinfu Liu, Runwei Ding, Yuhang Wen, Nan Dai, Fanyang Meng, Shen Zhao, Mengyuan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal-based action recognition methods have achieved high success using
pose and RGB modality. However, skeletons sequences lack appearance depiction
and RGB images suffer irrelevant noise due to modality limitations. To address
this, we introduce human parsing feature map as a novel modality, since it can
selectively retain effective semantic features of the body parts, while
filtering out most irrelevant noise. We propose a new dual-branch framework
called Ensemble Human Parsing and Pose Network (EPP-Net), which is the first to
leverage both skeletons and human parsing modalities for action recognition.
The first human pose branch feeds robust skeletons in graph convolutional
network to model pose features, while the second human parsing branch also
leverages depictive parsing feature maps to model parsing festures via
convolutional backbones. The two high-level features will be effectively
combined through a late fusion strategy for better action recognition.
Extensive experiments on NTU RGB+D and NTU RGB+D 120 benchmarks consistently
verify the effectiveness of our proposed EPP-Net, which outperforms the
existing action recognition methods. Our code is available at:
https://github.com/liujf69/EPP-Net-Action.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2307.07977</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SyCoCa: Symmetrizing Contrastive Captioners with Attentive Masking for
  Multimodal Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziping Ma, Furong Xu, Jian Liu, Ming Yang, Qingpei Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal alignment between language and vision is the fundamental topic in
current vision-language model research. Contrastive Captioners (CoCa), as a
representative method, integrates Contrastive Language-Image Pretraining (CLIP)
and Image Caption (IC) into a unified framework, resulting in impressive
results. CLIP imposes a bidirectional constraints on global representation of
entire images and sentences. Although IC conducts an unidirectional
image-to-text generation on local representation, it lacks any constraint on
local text-to-image reconstruction, which limits the ability to understand
images at a fine-grained level when aligned with texts. To achieve multimodal
alignment from both global and local perspectives, this paper proposes
Symmetrizing Contrastive Captioners (SyCoCa), which introduces bidirectional
interactions on images and texts across the global and local representation
levels. Specifically, we expand a Text-Guided Masked Image Modeling (TG-MIM)
head based on ITC and IC heads. The improved SyCoCa can further leverage
textual cues to reconstruct contextual images and visual cues to predict
textual contents. When implementing bidirectional local interactions, the local
contents of images tend to be cluttered or unrelated to their textual
descriptions. Thus, we employ an attentive masking strategy to select effective
image patches for interaction. Extensive experiments on five vision-language
tasks, including image-text retrieval, image-captioning, visual question
answering, and zero-shot/finetuned image classification, validate the
effectiveness of our proposed method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unified Diffusion-Based Rigid and Non-Rigid Editing with Text and Image
  Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02126v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02126v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Wang, Ping Liu, Wei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing text-to-image editing methods tend to excel either in rigid or
non-rigid editing but encounter challenges when combining both, resulting in
misaligned outputs with the provided text prompts. In addition, integrating
reference images for control remains challenging. To address these issues, we
present a versatile image editing framework capable of executing both rigid and
non-rigid edits, guided by either textual prompts or reference images. We
leverage a dual-path injection scheme to handle diverse editing scenarios and
introduce an integrated self-attention mechanism for fusion of appearance and
structural information. To mitigate potential visual artifacts, we further
employ latent fusion techniques to adjust intermediate latents. Compared to
previous work, our approach represents a significant advance in achieving
precise and versatile image editing. Comprehensive experiments validate the
efficacy of our method, showcasing competitive or superior results in
text-based editing and appearance transfer tasks, encompassing both rigid and
non-rigid settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost
  Whole-Body Teleoperation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zipeng Fu, Tony Z. Zhao, Chelsea Finn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning from human demonstrations has shown impressive performance
in robotics. However, most results focus on table-top manipulation, lacking the
mobility and dexterity necessary for generally useful tasks. In this work, we
develop a system for imitating mobile manipulation tasks that are bimanual and
require whole-body control. We first present Mobile ALOHA, a low-cost and
whole-body teleoperation system for data collection. It augments the ALOHA
system with a mobile base, and a whole-body teleoperation interface. Using data
collected with Mobile ALOHA, we then perform supervised behavior cloning and
find that co-training with existing static ALOHA datasets boosts performance on
mobile manipulation tasks. With 50 demonstrations for each task, co-training
can increase success rates by up to 90%, allowing Mobile ALOHA to autonomously
complete complex mobile manipulation tasks such as sauteing and serving a piece
of shrimp, opening a two-door wall cabinet to store heavy cooking pots, calling
and entering an elevator, and lightly rinsing a used pan using a kitchen
faucet. Project website: https://mobile-aloha.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://mobile-aloha.github.io (Zipeng Fu and Tony
  Z. Zhao are project co-leads, Chelsea Finn is the advisor)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Source-Free Online Domain Adaptive Semantic Segmentation of Satellite
  Images under Image Degradation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02113v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02113v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fahim Faisal Niloy, Kishor Kumar Bhaumik, Simon S. Woo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online adaptation to distribution shifts in satellite image segmentation
stands as a crucial yet underexplored problem. In this paper, we address
source-free and online domain adaptation, i.e., test-time adaptation (TTA), for
satellite images, with the focus on mitigating distribution shifts caused by
various forms of image degradation. Towards achieving this goal, we propose a
novel TTA approach involving two effective strategies. First, we progressively
estimate the global Batch Normalization (BN) statistics of the target
distribution with incoming data stream. Leveraging these statistics during
inference has the ability to effectively reduce domain gap. Furthermore, we
enhance prediction quality by refining the predicted masks using global class
centers. Both strategies employ dynamic momentum for fast and stable
convergence. Notably, our method is backpropagation-free and hence fast and
lightweight, making it highly suitable for on-the-fly adaptation to new domain.
Through comprehensive experiments across various domain adaptation scenarios,
we demonstrate the robust performance of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Significance of Anatomical Constraints in Virtual Try-On 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02110v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02110v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Debapriya Roy, Sanchayan Santra, Diganta Mukherjee, Bhabatosh Chanda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The system of Virtual Try-ON (VTON) allows a user to try a product virtually.
In general, a VTON system takes a clothing source and a person's image to
predict the try-on output of the person in the given clothing. Although
existing methods perform well for simple poses, in case of bent or crossed arms
posture or when there is a significant difference between the alignment of the
source clothing and the pose of the target person, these methods fail by
generating inaccurate clothing deformations. In the VTON methods that employ
Thin Plate Spline (TPS) based clothing transformations, this mainly occurs for
two reasons - (1)~the second-order smoothness constraint of TPS that restricts
the bending of the object plane. (2)~Overlaps among different clothing parts
(e.g., sleeves and torso) can not be modeled by a single TPS transformation, as
it assumes the clothing as a single planar object; therefore, disregards the
independence of movement of different clothing parts. To this end, we make two
major contributions. Concerning the bending limitations of TPS, we propose a
human AnaTomy-Aware Geometric (ATAG) transformation. Regarding the overlap
issue, we propose a part-based warping approach that divides the clothing into
independently warpable parts to warp them separately and later combine them.
Extensive analysis shows the efficacy of this approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2208.08076</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLAPP: Contrastive Language-Audio Pre-training in Passive Underwater
  Vessel Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Li, Jingsheng Gao, Tong Yu, Suncheng Xiang, Jiacheng Ruan, Ting Liu, Yuzhuo Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing research on audio classification faces challenges in recognizing
attributes of passive underwater vessel scenarios and lacks well-annotated
datasets due to data privacy concerns. In this study, we introduce CLAPP
(Contrastive Language-Audio Pre-training in Passive Underwater Vessel
Classification), a novel model. Our aim is to train a neural network using a
wide range of vessel audio and vessel state text pairs obtained from an
oceanship dataset. CLAPP is capable of directly learning from raw vessel audio
data and, when available, from carefully curated labels, enabling improved
recognition of vessel attributes in passive underwater vessel scenarios.
Model's zero-shot capability allows predicting the most relevant vessel state
description for a given vessel audio, without directly optimizing for the task.
Our approach aims to solve 2 challenges: vessel audio-text classification and
passive underwater vessel audio attribute recognition. The proposed method
achieves new state-of-the-art results on both Deepship and Shipsear public
datasets, with a notable margin of about 7%-13% for accuracy compared to prior
methods on zero-shot task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preserving Image Properties Through Initializations in Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02097v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02097v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeffrey Zhang, Shao-Yu Chang, Kedan Li, David Forsyth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retail photography imposes specific requirements on images. For instance,
images may need uniform background colors, consistent model poses, centered
products, and consistent lighting. Minor deviations from these standards impact
a site's aesthetic appeal, making the images unsuitable for use. We show that
Stable Diffusion methods, as currently applied, do not respect these
requirements. The usual practice of training the denoiser with a very noisy
image and starting inference with a sample of pure noise leads to inconsistent
generated images during inference. This inconsistency occurs because it is easy
to tell the difference between samples of the training and inference
distributions. As a result, a network trained with centered retail product
images with uniform backgrounds generates images with erratic backgrounds. The
problem is easily fixed by initializing inference with samples from an
approximation of noisy images. However, in using such an approximation, the
joint distribution of text and noisy image at inference time still slightly
differs from that at training time. This discrepancy is corrected by training
the network with samples from the approximate noisy image distribution.
Extensive experiments on real application data show significant qualitative and
quantitative improvements in performance from adopting these procedures.
Finally, our procedure can interact well with other control-based methods to
further enhance the controllability of diffusion-based methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Class-Incremental Learning with Prototype Guided Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiyang Guo, Fei Zhu, Wenzhuo Liu, Xu-Yao Zhang, Cheng-Lin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing federated learning methods have effectively addressed decentralized
learning in scenarios involving data privacy and non-IID data. However, in
real-world situations, each client dynamically learns new classes, requiring
the global model to maintain discriminative capabilities for both new and old
classes. To effectively mitigate the effects of catastrophic forgetting and
data heterogeneity under low communication costs, we designed a simple and
effective method named PLoRA. On the one hand, we adopt prototype learning to
learn better feature representations and leverage the heuristic information
between prototypes and class features to design a prototype re-weight module to
solve the classifier bias caused by data heterogeneity without retraining the
classification layer. On the other hand, our approach utilizes a pre-trained
model as the backbone and utilizes LoRA to fine-tune with a tiny amount of
parameters when learning new classes. Moreover, PLoRA does not rely on
similarity-based module selection strategies, thereby further reducing
communication overhead. Experimental results on standard datasets indicate that
our method outperforms the state-of-the-art approaches significantly. More
importantly, our method exhibits strong robustness and superiority in various
scenarios and degrees of data heterogeneity. Our code will be publicly
available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging SAM for Single-Source Domain Generalization in Medical Image
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02076v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02076v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanhui Wang, Huaize Ye, Yi Xia, Xueyan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain Generalization (DG) aims to reduce domain shifts between domains to
achieve promising performance on the unseen target domain, which has been
widely practiced in medical image segmentation. Single-source domain
generalization (SDG) is the most challenging setting that trains on only one
source domain. Although existing methods have made considerable progress on SDG
of medical image segmentation, the performances are still far from the
applicable standards when faced with a relatively large domain shift. In this
paper, we leverage the Segment Anything Model (SAM) to SDG to greatly improve
the ability of generalization. Specifically, we introduce a parallel framework,
the source images are sent into the SAM module and normal segmentation module
respectively. To reduce the calculation resources, we apply a merging strategy
before sending images to the SAM module. We extract the bounding boxes from the
segmentation module and send the refined version as prompts to the SAM module.
We evaluate our model on a classic DG dataset and achieve competitive results
compared to other state-of-the-art DG methods. Furthermore, We conducted a
series of ablation experiments to prove the effectiveness of the proposed
method. The code is publicly available at https://github.com/SARIHUST/SAMMed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalizable vision-language pre-training for annotation-free pathology
  localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Yang, Hong-Yu Zhou, Cheng Li, Weijian Huang, Jiarun Liu, Shanshan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Locating pathologies automatically from medical images aids the understanding
of the emergence and progression of diseases, and such an ability can
significantly benefit clinical diagnostics. However, existing deep learning
models heavily rely on expert annotations and lack generalization capabilities
in open clinical environments. In this study, we present a generalizable
vision-language pre-training model for Annotation-Free pathology Localization
(AFLoc). The core strength of AFLoc lies in its image annotation-free
multi-level semantic structure-based contrastive learning, which
comprehensively aligns multi-granularity medical concepts from reports with
abundant image features, to adapt to the diverse expressions of observed and
emerging unseen pathologies. We conducted extensive experimental validation
across 4 distinct external datasets, encompassing 11 types of chest
pathologies, to verify its generalization ability. The results demonstrate that
AFLoc surpasses 6 state-of-the-art methods and even outperforms the human
benchmark in locating 5 different pathologies, underscoring its suitability for
complex clinical environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Cloud-edge Collaborative Inference for Object
  Re-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuanming Wang, Yuxin Yang, Mengshi Qi, Huadong Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current object re-identification (ReID) system follows the centralized
processing paradigm, i.e., all computations are conducted in the cloud server
and edge devices are only used to capture and send images. As the number of
videos experiences a rapid escalation, this paradigm has become impractical due
to the finite computational resources. In such a scenario, the ReID system
should be converted to fit in the cloud-edge collaborative processing paradigm,
which is crucial to boost the scalability and practicality of ReID systems.
However, current relevant work lacks research on this issue, making it
challenging for ReID methods to be adapted effectively. Therefore, we pioneer a
cloud-edge collaborative inference framework for ReID systems and particularly
propose a distribution-aware correlation modeling network (DaCM) to make the
desired image return to the cloud server as soon as possible via learning to
model the spatial-temporal correlations among instances. DaCM embeds the
spatial-temporal correlations implicitly included in the timestamps into a
graph structure, and it can be applied in the cloud to regulate the size of the
upload window and on the edge device to adjust the sequence of images,
respectively. Traditional ReID methods can be combined with DaCM seamlessly,
enabling their application within our proposed edge-cloud collaborative
framework. Extensive experiments demonstrate that our method obviously reduces
transmission overhead and significantly improves performance. We will release
our code and model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffusionEdge: Diffusion Probabilistic Model for Crisp Edge Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02032v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02032v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfan Ye, Kai Xu, Yuhang Huang, Renjiao Yi, Zhiping Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Limited by the encoder-decoder architecture, learning-based edge detectors
usually have difficulty predicting edge maps that satisfy both correctness and
crispness. With the recent success of the diffusion probabilistic model (DPM),
we found it is especially suitable for accurate and crisp edge detection since
the denoising process is directly applied to the original image size.
Therefore, we propose the first diffusion model for the task of general edge
detection, which we call DiffusionEdge. To avoid expensive computational
resources while retaining the final performance, we apply DPM in the latent
space and enable the classic cross-entropy loss which is uncertainty-aware in
pixel level to directly optimize the parameters in latent space in a
distillation manner. We also adopt a decoupled architecture to speed up the
denoising process and propose a corresponding adaptive Fourier filter to adjust
the latent features of specific frequencies. With all the technical designs,
DiffusionEdge can be stably trained with limited resources, predicting crisp
and accurate edge maps with much fewer augmentation strategies. Extensive
experiments on four edge detection benchmarks demonstrate the superiority of
DiffusionEdge both in correctness and crispness. On the NYUDv2 dataset,
compared to the second best, we increase the ODS, OIS (without post-processing)
and AC by 30.2%, 28.1% and 65.1%, respectively. Code:
https://github.com/GuHuangAI/DiffusionEdge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spy-Watermark: Robust Invisible Watermarking for Backdoor Attack 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02031v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02031v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruofei Wang, Renjie Wan, Zongyu Guo, Qing Guo, Rui Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Backdoor attack aims to deceive a victim model when facing backdoor instances
while maintaining its performance on benign data. Current methods use manual
patterns or special perturbations as triggers, while they often overlook the
robustness against data corruption, making backdoor attacks easy to defend in
practice. To address this issue, we propose a novel backdoor attack method
named Spy-Watermark, which remains effective when facing data collapse and
backdoor defense. Therein, we introduce a learnable watermark embedded in the
latent domain of images, serving as the trigger. Then, we search for a
watermark that can withstand collapse during image decoding, cooperating with
several anti-collapse operations to further enhance the resilience of our
trigger against data corruption. Extensive experiments are conducted on
CIFAR10, GTSRB, and ImageNet datasets, demonstrating that Spy-Watermark
overtakes ten state-of-the-art methods in terms of robustness and stealthiness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spikformer V2: Join the High Accuracy Club on ImageNet with an SNN
  Ticket 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02020v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02020v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaokun Zhou, Kaiwei Che, Wei Fang, Keyu Tian, Yuesheng Zhu, Shuicheng Yan, Yonghong Tian, Li Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking Neural Networks (SNNs), known for their biologically plausible
architecture, face the challenge of limited performance. The self-attention
mechanism, which is the cornerstone of the high-performance Transformer and
also a biologically inspired structure, is absent in existing SNNs. To this
end, we explore the potential of leveraging both self-attention capability and
biological properties of SNNs, and propose a novel Spiking Self-Attention (SSA)
and Spiking Transformer (Spikformer). The SSA mechanism eliminates the need for
softmax and captures the sparse visual feature employing spike-based Query,
Key, and Value. This sparse computation without multiplication makes SSA
efficient and energy-saving. Further, we develop a Spiking Convolutional Stem
(SCS) with supplementary convolutional layers to enhance the architecture of
Spikformer. The Spikformer enhanced with the SCS is referred to as Spikformer
V2. To train larger and deeper Spikformer V2, we introduce a pioneering
exploration of Self-Supervised Learning (SSL) within the SNN. Specifically, we
pre-train Spikformer V2 with masking and reconstruction style inspired by the
mainstream self-supervised Transformer, and then finetune the Spikformer V2 on
the image classification on ImageNet. Extensive experiments show that
Spikformer V2 outperforms other previous surrogate training and ANN2SNN
methods. An 8-layer Spikformer V2 achieves an accuracy of 80.38% using 4 time
steps, and after SSL, a 172M 16-layer Spikformer V2 reaches an accuracy of
81.10% with just 1 time step. To the best of our knowledge, this is the first
time that the SNN achieves 80+% accuracy on ImageNet. The code will be
available at Spikformer V2.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Diffusion-Based Image Synthesis with Context Prediction <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02015v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02015v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ling Yang, Jingwei Liu, Shenda Hong, Zhilong Zhang, Zhilin Huang, Zheming Cai, Wentao Zhang, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models are a new class of generative models, and have dramatically
promoted image generation with unprecedented quality and diversity. Existing
diffusion models mainly try to reconstruct input image from a corrupted one
with a pixel-wise or feature-wise constraint along spatial axes. However, such
point-based reconstruction may fail to make each predicted pixel/feature fully
preserve its neighborhood context, impairing diffusion-based image synthesis.
As a powerful source of automatic supervisory signal, context has been well
studied for learning representations. Inspired by this, we for the first time
propose ConPreDiff to improve diffusion-based image synthesis with context
prediction. We explicitly reinforce each point to predict its neighborhood
context (i.e., multi-stride features/tokens/pixels) with a context decoder at
the end of diffusion denoising blocks in training stage, and remove the decoder
for inference. In this way, each point can better reconstruct itself by
preserving its semantic connections with neighborhood context. This new
paradigm of ConPreDiff can generalize to arbitrary discrete and continuous
diffusion backbones without introducing extra parameters in sampling procedure.
Extensive experiments are conducted on unconditional image generation,
text-to-image generation and image inpainting tasks. Our ConPreDiff
consistently outperforms previous methods and achieves a new SOTA text-to-image
generation results on MS-COCO, with a zero-shot FID score of 6.21.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vulnerabilities Unveiled: Adversarially Attacking a Multimodal Vision
  Langauge Model for Pathology Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jai Prakash Veerla, Poojitha Thota, Partha Sai Guttikonda, Shirin Nilizadeh, Jacob M. Luber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the dynamic landscape of medical artificial intelligence, this study
explores the vulnerabilities of the Pathology Language-Image Pretraining (PLIP)
model, a Vision Language Foundation model, under targeted adversarial
conditions. Leveraging the Kather Colon dataset with 7,180 H&E images across
nine tissue types, our investigation employs Projected Gradient Descent (PGD)
adversarial attacks to intentionally induce misclassifications. The outcomes
reveal a 100% success rate in manipulating PLIP's predictions, underscoring its
susceptibility to adversarial perturbations. The qualitative analysis of
adversarial examples delves into the interpretability challenges, shedding
light on nuanced changes in predictions induced by adversarial manipulations.
These findings contribute crucial insights into the interpretability, domain
adaptation, and trustworthiness of Vision Language Models in medical imaging.
The study emphasizes the pressing need for robust defenses to ensure the
reliability of AI models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting Future States with Spatial Point Processes in Single Molecule
  Resolution Spatial Transcriptomics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parisa Boodaghi Malidarreh, Biraaj Rout, Mohammad Sadegh Nasr, Priyanshi Borad, Jillur Rahman Saurav, Jai Prakash Veerla, Kelli Fenelon, Theodora Koromila, Jacob M. Luber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a pipeline based on Random Forest Regression to
predict the future distribution of cells that are expressed by the Sog-D gene
(active cells) in both the Anterior to posterior (AP) and the Dorsal to Ventral
(DV) axis of the Drosophila in embryogenesis process. This method provides
insights about how cells and living organisms control gene expression in super
resolution whole embryo spatial transcriptomics imaging at sub cellular, single
molecule resolution. A Random Forest Regression model was used to predict the
next stage active distribution based on the previous one. To achieve this goal,
we leveraged temporally resolved, spatial point processes by including Ripley's
K-function in conjunction with the cell's state in each stage of embryogenesis,
and found average predictive accuracy of active cell distribution. This tool is
analogous to RNA Velocity for spatially resolved developmental biology, from
one data point we can predict future spatially resolved gene expression using
features from the spatial point processes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OptFlow: Fast Optimization-based Scene Flow Estimation without
  Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahul Ahuja, Chris Baker, Wilko Schwarting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene flow estimation is a crucial component in the development of autonomous
driving and 3D robotics, providing valuable information for environment
perception and navigation. Despite the advantages of learning-based scene flow
estimation techniques, their domain specificity and limited generalizability
across varied scenarios pose challenges. In contrast, non-learning
optimization-based methods, incorporating robust priors or regularization,
offer competitive scene flow estimation performance, require no training, and
show extensive applicability across datasets, but suffer from lengthy inference
times. In this paper, we present OptFlow, a fast optimization-based scene flow
estimation method. Without relying on learning or any labeled datasets, OptFlow
achieves state-of-the-art performance for scene flow estimation on popular
autonomous driving benchmarks. It integrates a local correlation weight matrix
for correspondence matching, an adaptive correspondence threshold limit for
nearest-neighbor search, and graph prior rigidity constraints, resulting in
expedited convergence and improved point correspondence identification.
Moreover, we demonstrate how integrating a point cloud registration function
within our objective function bolsters accuracy and differentiates between
static and dynamic points without relying on external odometry data.
Consequently, OptFlow outperforms the baseline graph-prior method by
approximately 20% and the Neural Scene Flow Prior method by 5%-7% in accuracy,
all while offering the fastest inference time among all non-learning scene flow
estimation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the proceedings of the IEEE/CVF Winter Conference on
  Applications of Computer Vision (WACV), 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robot-Assisted Deep Venous Thrombosis Ultrasound Examination using
  Virtual Fixture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dianye Huang, Chenguang Yang, Mingchuan Zhou, Angelos Karlas, Nassir Navab, Zhongliang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Venous Thrombosis (DVT) is a common vascular disease with blood clots
inside deep veins, which may block blood flow or even cause a life-threatening
pulmonary embolism. A typical exam for DVT using ultrasound (US) imaging is by
pressing the target vein until its lumen is fully compressed. However, the
compression exam is highly operator-dependent. To alleviate intra- and
inter-variations, we present a robotic US system with a novel hybrid force
motion control scheme ensuring position and force tracking accuracy, and soft
landing of the probe onto the target surface. In addition, a path-based virtual
fixture is proposed to realize easy human-robot interaction for repeat
compression operation at the lesion location. To ensure the biometric
measurements obtained in different examinations are comparable, the 6D scanning
path is determined in a coarse-to-fine manner using both an external RGBD
camera and US images. The RGBD camera is first used to extract a rough scanning
path on the object. Then, the segmented vascular lumen from US images are used
to optimize the scanning path to ensure the visibility of the target object. To
generate a continuous scan path for developing virtual fixtures, an arc-length
based path fitting model considering both position and orientation is proposed.
Finally, the whole system is evaluated on a human-like arm phantom with an
uneven surface.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted Paper IEEE T-ASE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Singular Value Decomposition in a Convolutional Neural Network to
  Improve Brain Tumor Segmentation Accuracy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pegah Ahadian, Maryam Babaei, Kourosh Parand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A brain tumor consists of cells showing abnormal brain growth. The area of
the brain tumor significantly affects choosing the type of treatment and
following the course of the disease during the treatment. At the same time,
pictures of Brain MRIs are accompanied by noise. Eliminating existing noises
can significantly impact the better segmentation and diagnosis of brain tumors.
In this work, we have tried using the analysis of eigenvalues. We have used the
MSVD algorithm, reducing the image noise and then using the deep neural network
to segment the tumor in the images. The proposed method's accuracy was
increased by 2.4% compared to using the original images. With Using the MSVD
method, convergence speed has also increased, showing the proposed method's
effectiveness
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Novel End-to-End Production-Ready Machine Learning Flow for
  Nanolithography Modeling and Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed S. E. Habib, Hossam A. H. Fahmy, Mohamed F. Abu-ElYazeed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optical lithography is the main enabler to semiconductor manufacturing. It
requires extensive processing to perform the Resolution Enhancement Techniques
(RETs) required to transfer the design data to a working Integrated Circuits
(ICs). The processing power and computational runtime for RETs tasks is ever
increasing due to the continuous reduction of the feature size and the
expansion of the chip area. State-of-the-art research sought Machine Learning
(ML) technologies to reduce runtime and computational power, however they are
still not used in production yet. In this study, we analyze the reasons holding
back ML computational lithography from being production ready and present a
novel highly scalable end-to-end flow that enables production ready ML-RET
correction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Branched Variational Autoencoder Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02526v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02526v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Salah, David Yevick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a modified variational autoencoder (VAEs) that contains
an additional neural network branch. The resulting branched VAE (BVAE)
contributes a classification component based on the class labels to the total
loss and therefore imparts categorical information to the latent
representation. As a result, the latent space distributions of the input
classes are separated and ordered, thereby enhancing the classification
accuracy. The degree of improvement is quantified by numerical calculations
employing the benchmark MNIST dataset for both unrotated and rotated digits.
The proposed technique is then compared to and then incorporated into a VAE
with fixed output distributions. This procedure is found to yield improved
performance for a wide range of output distributions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comprehensive Exploration of Synthetic Data Generation: A Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        André Bauer, Simon Trapp, Michael Stenger, Robert Leppich, Samuel Kounev, Mark Leznik, Kyle Chard, Ian Foster
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed a surge in the popularity of Machine Learning
(ML), applied across diverse domains. However, progress is impeded by the
scarcity of training data due to expensive acquisition and privacy legislation.
Synthetic data emerges as a solution, but the abundance of released models and
limited overview literature pose challenges for decision-making. This work
surveys 417 Synthetic Data Generation (SDG) models over the last decade,
providing a comprehensive overview of model types, functionality, and
improvements. Common attributes are identified, leading to a classification and
trend analysis. The findings reveal increased model performance and complexity,
with neural network-based approaches prevailing, except for privacy-preserving
data generation. Computer vision dominates, with GANs as primary generative
models, while diffusion models, transformers, and RNNs compete. Implications
from our performance evaluation highlight the scarcity of common metrics and
datasets, making comparisons challenging. Additionally, the neglect of training
and computational costs in literature necessitates attention in future
research. This work serves as a guide for SDG model selection and identifies
crucial areas for future exploration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image-based Deep Learning for Smart Digital Twins: a Review 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02523v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02523v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Ruman Islam, Mahadevan Subramaniam, Pei-Chi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Smart Digital twins (SDTs) are being increasingly used to virtually replicate
and predict the behaviors of complex physical systems through continual data
assimilation enabling the optimization of the performance of these systems by
controlling the actions of systems. Recently, deep learning (DL) models have
significantly enhanced the capabilities of SDTs, particularly for tasks such as
predictive maintenance, anomaly detection, and optimization. In many domains,
including medicine, engineering, and education, SDTs use image data
(image-based SDTs) to observe and learn system behaviors and control their
behaviors. This paper focuses on various approaches and associated challenges
in developing image-based SDTs by continually assimilating image data from
physical systems. The paper also discusses the challenges involved in designing
and implementing DL models for SDTs, including data acquisition, processing,
and interpretation. In addition, insights into the future directions and
opportunities for developing new image-based DL approaches to develop robust
SDTs are provided. This includes the potential for using generative models for
data augmentation, developing multi-modal DL models, and exploring the
integration of DL with other technologies, including 5G, edge computing, and
IoT. In this paper, we describe the image-based SDTs, which enable broader
adoption of the digital twin DT paradigms across a broad spectrum of areas and
the development of new methods to improve the abilities of SDTs in replicating,
predicting, and optimizing the behavior of complex systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 figures, and 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The cell signaling structure function 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Layton Aho, Mark Winter, Marc DeCarlo, Agne Frismantiene, Yannick Blum, Paolo Armando Gagliardi, Olivier Pertz, Andrew R. Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Live cell microscopy captures 5-D $(x,y,z,channel,time)$ movies that display
patterns of cellular motion and signaling dynamics. We present here an approach
to finding spatiotemporal patterns of cell signaling dynamics in 5-D live cell
microscopy movies unique in requiring no \emph{a priori} knowledge of expected
pattern dynamics, and no training data. The proposed cell signaling structure
function (SSF) is a Kolmogorov structure function that optimally measures cell
signaling state as nuclear intensity w.r.t. surrounding cytoplasm, a
significant improvement compared to the current state-of-the-art cytonuclear
ratio. SSF kymographs store at each spatiotemporal cell centroid the SSF value,
or a functional output such as velocity. Patterns of similarity are identified
via the metric normalized compression distance (NCD). The NCD is a reproducing
kernel for a Hilbert space that represents the input SSF kymographs as points
in a low dimensional embedding that optimally captures the pattern similarity
identified by the NCD throughout the space. The only parameter is the expected
cell radii ($\mu m$). A new formulation of the cluster structure function
optimally estimates how meaningful an embedding from the RKHS representation.
Results are presented quantifying the impact of ERK and AKT signaling between
different oncogenic mutations, and by the relation between ERK signaling and
cellular velocity patterns for movies of 2-D monolayers of human breast
epithelial (MCF10A) cells, 3-D MCF10A spheroids under optogenetic manipulation
of ERK, and human induced pluripotent stem cells .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VASE: Object-Centric Appearance and Shape Manipulation of Real Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02473v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02473v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elia Peruzzo, Vidit Goel, Dejia Xu, Xingqian Xu, Yifan Jiang, Zhangyang Wang, Humphrey Shi, Nicu Sebe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, several works tackled the video editing task fostered by the
success of large-scale text-to-image generative models. However, most of these
methods holistically edit the frame using the text, exploiting the prior given
by foundation diffusion models and focusing on improving the temporal
consistency across frames. In this work, we introduce a framework that is
object-centric and is designed to control both the object's appearance and,
notably, to execute precise and explicit structural modifications on the
object. We build our framework on a pre-trained image-conditioned diffusion
model, integrate layers to handle the temporal dimension, and propose training
strategies and architectural modifications to enable shape control. We evaluate
our method on the image-driven video editing task showing similar performance
to the state-of-the-art, and showcasing novel shape-editing capabilities.
Further details, code and examples are available on our project page:
https://helia95.github.io/vase-website/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page https://helia95.github.io/vase-website/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Not all Minorities are Equal: Empty-Class-Aware Distillation for
  Heterogeneous Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuangpu Guo, Yuhe Ding, Jian Liang, Ran He, Zilei Wang, Tieniu Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data heterogeneity, characterized by disparities in local data distribution
across clients, poses a significant challenge in federated learning.
Substantial efforts have been devoted to addressing the heterogeneity in local
label distribution. As minority classes suffer from worse accuracy due to
overfitting on local imbalanced data, prior methods often incorporate
class-balanced learning techniques during local training. Despite the improved
mean accuracy across all classes, we observe that empty classes-referring to
categories absent from a client's data distribution-are still not well
recognized. This paper introduces FedED, a novel approach in heterogeneous
federated learning that integrates both empty-class distillation and logit
suppression simultaneously. Specifically, empty-class distillation leverages
knowledge distillation during local training on each client to retain essential
information related to empty classes from the global model. Moreover, logit
suppression directly penalizes network logits for non-label classes,
effectively addressing misclassifications in minority classes that may be
biased toward majority classes. Extensive experiments validate the efficacy of
FedED, surpassing previous state-of-the-art methods across diverse datasets
with varying degrees of label distribution shift.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Anatomy-aware and acquisition-agnostic joint registration with
  SynthMorph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11329v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11329v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malte Hoffmann, Andrew Hoopes, Douglas N. Greve, Bruce Fischl, Adrian V. Dalca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Affine image registration is a cornerstone of medical-image analysis. While
classical algorithms can achieve excellent accuracy, they solve a
time-consuming optimization for every image pair. Deep-learning (DL) methods
learn a function that maps an image pair to an output transform. Evaluating the
function is fast, but capturing large transforms can be challenging, and
networks tend to struggle if a test-image characteristic shifts from the
training domain, such as resolution. Most affine methods are agnostic to
anatomy, meaning the registration will be inaccurate if algorithms consider all
structures in the image.
  We address these shortcomings with SynthMorph, an easy-to-use DL tool for
joint affine-deformable registration of any brain image without preprocessing,
right off the MRI scanner. First, we leverage a strategy to train networks with
wildly varying images synthesized from label maps, yielding robust performance
across acquisition specifics unseen at training. Second, we optimize the
spatial overlap of select anatomical labels. This enables networks to
distinguish anatomy of interest from irrelevant structures, removing the need
for preprocessing that excludes content which would impinge on anatomy-specific
registration. Third, we combine the affine model with a deformable hypernetwork
that lets users choose the optimal deformation-field regularity for their
specific data, at registration time, in a fraction of the time required by
classical methods.
  We rigorously analyze how competing architectures learn affine transforms and
compare state-of-the-art registration tools across an extremely diverse set of
neuroimaging data, aiming to truly capture the behavior of methods in the real
world. SynthMorph demonstrates consistent and improved accuracy. It is
available at https://w3id.org/synthmorph, as a single complete end-to-end
solution for registration of brain MRI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 22 figures, 4 tables, affine registration, deformable
  registration, deep learning, hypernetwork, domain shift, neuroimaging</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Generalize towards Unseen Domains via a Content-Aware Style
  Invariant Model for Disease Detection from Chest X-rays 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.13991v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.13991v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Zunaed, Md. Aynal Haque, Taufiq Hasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Performance degradation due to distribution discrepancy is a longstanding
challenge in intelligent imaging, particularly for chest X-rays (CXRs). Recent
studies have demonstrated that CNNs are biased toward styles (e.g.,
uninformative textures) rather than content (e.g., shape), in stark contrast to
the human vision system. Radiologists tend to learn visual cues from CXRs and
thus perform well across multiple domains. Motivated by this, we employ the
novel on-the-fly style randomization modules at both image (SRM-IL) and feature
(SRM-FL) levels to create rich style perturbed features while keeping the
content intact for robust cross-domain performance. Previous methods simulate
unseen domains by constructing new styles via interpolation or swapping styles
from existing data, limiting them to available source domains during training.
However, SRM-IL samples the style statistics from the possible value range of a
CXR image instead of the training data to achieve more diversified
augmentations. Moreover, we utilize pixel-wise learnable parameters in the
SRM-FL compared to pre-defined channel-wise mean and standard deviations as
style embeddings for capturing more representative style features.
Additionally, we leverage consistency regularizations on global semantic
features and predictive distributions from with and without style-perturbed
versions of the same CXR to tweak the model's sensitivity toward content
markers for accurate predictions. Our proposed method, trained on CheXpert and
MIMIC-CXR datasets, achieves 77.32$\pm$0.35, 88.38$\pm$0.19, 82.63$\pm$0.13
AUCs(%) on the unseen domain test datasets, i.e., BRAX, VinDr-CXR, and NIH
chest X-ray14, respectively, compared to 75.56$\pm$0.80, 87.57$\pm$0.46,
82.07$\pm$0.19 from state-of-the-art models on five-fold cross-validation with
statistically significant results in thoracic disease classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UpFusion: Novel View Diffusion from Unposed Sparse View Observations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06661v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06661v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bharath Raj Nagoor Kani, Hsin-Ying Lee, Sergey Tulyakov, Shubham Tulsiani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose UpFusion, a system that can perform novel view synthesis and infer
3D representations for an object given a sparse set of reference images without
corresponding pose information. Current sparse-view 3D inference methods
typically rely on camera poses to geometrically aggregate information from
input views, but are not robust in-the-wild when such information is
unavailable/inaccurate. In contrast, UpFusion sidesteps this requirement by
learning to implicitly leverage the available images as context in a
conditional generative model for synthesizing novel views. We incorporate two
complementary forms of conditioning into diffusion models for leveraging the
input views: a) via inferring query-view aligned features using a scene-level
transformer, b) via intermediate attentional layers that can directly observe
the input image tokens. We show that this mechanism allows generating
high-fidelity novel views while improving the synthesis quality given
additional (unposed) images. We evaluate our approach on the Co3Dv2 and Google
Scanned Objects datasets and demonstrate the benefits of our method over
pose-reliant sparse-view methods as well as single-view methods that cannot
leverage additional views. Finally, we also show that our learned model can
generalize beyond the training categories and even allow reconstruction from
self-captured images of generic objects in-the-wild.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://upfusion3d.github.io/ v2: Fixed a citation
  mistake</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Audiovisual Masked Autoencoders <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05922v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05922v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mariana-Iuliana Georgescu, Eduardo Fonseca, Radu Tudor Ionescu, Mario Lucic, Cordelia Schmid, Anurag Arnab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Can we leverage the audiovisual information already present in video to
improve self-supervised representation learning? To answer this question, we
study various pretraining architectures and objectives within the masked
autoencoding framework, motivated by the success of similar methods in natural
language and image understanding. We show that we can achieve significant
improvements on audiovisual downstream classification tasks, surpassing the
state-of-the-art on VGGSound and AudioSet. Furthermore, we can leverage our
audiovisual pretraining scheme for multiple unimodal downstream tasks using a
single audiovisual pretrained model. We additionally demonstrate the
transferability of our representations, achieving state-of-the-art audiovisual
results on Epic Kitchens without pretraining specifically for this dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From 2D Images to 3D Model:Weakly Supervised Multi-View Face
  Reconstruction with Deep Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.03842v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.03842v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiguang Zhao, Chaolong Yang, Jianan Ye, Rui Zhang, Yuyao Yan, Xi Yang, Bin Dong, Amir Hussain, Kaizhu Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While weakly supervised multi-view face reconstruction (MVR) is garnering
increased attention, one critical issue still remains open: how to effectively
fuse multiple image information to reconstruct high-precision 3D models. In
this regard, we propose a novel model called Deep Fusion MVR (DF-MVR) and
design a multi-view encoding to single decoding framework with skip
connections, able to extract, integrate, and compensate deep features with
attention from multi-view images. Furthermore, we adopt the involution kernel
to enrich deep fusion features with channel features. In addition, we develop
the face parse network to learn, identify, and emphasize the critical common
face area within multi-view images. Experiments on Pixel-Face and Bosphorus
datasets indicate the superiority of our model. Without 3D annotation, DF-MVR
achieves 5.2% and 3.0% RMSE improvement over the existing weakly supervised
MVRs respectively on Pixel-Face and Bosphorus dataset. Code will be available
publicly at https://github.com/weiguangzhao/DF_MVR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics
  (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17033v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17033v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anahita Fathi Kazerooni, Nastaran Khalili, Xinyang Liu, Debanjan Haldar, Zhifan Jiang, Syed Muhammed Anwar, Jake Albrecht, Maruf Adewole, Udunna Anazodo, Hannah Anderson, Sina Bagheri, Ujjwal Baid, Timothy Bergquist, Austin J. Borja, Evan Calabrese, Verena Chung, Gian-Marco Conte, Farouk Dako, James Eddy, Ivan Ezhov, Ariana Familiar, Keyvan Farahani, Shuvanjan Haldar, Juan Eugenio Iglesias, Anastasia Janas, Elaine Johansen, Blaise V Jones, Florian Kofler, Dominic LaBella, Hollie Anne Lai, Koen Van Leemput, Hongwei Bran Li, Nazanin Maleki, Aaron S McAllister, Zeke Meier, Bjoern Menze, Ahmed W Moawad, Khanak K Nandolia, Julija Pavaine, Marie Piraud, Tina Poussaint, Sanjay P Prabhu, Zachary Reitman, Andres Rodriguez, Jeffrey D Rudie, Ibraheem Salman Shaikh, Lubdha M. Shah, Nakul Sheth, Russel Taki Shinohara, Wenxin Tu, Karthik Viswanathan, Chunhao Wang, Jeffrey B Ware, Benedikt Wiestler, Walter Wiggins, Anna Zapaishchykova, Mariam Aboian, Miriam Bornhorst, Peter de Blank, Michelle Deutsch, Maryam Fouladi, Lindsey Hoffman, Benjamin Kann, Margot Lazow, Leonie Mikael, Ali Nabavizadeh, Roger Packer, Adam Resnick, Brian Rood, Arastoo Vossough, Spyridon Bakas, Marius George Linguraru
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pediatric tumors of the central nervous system are the most common cause of
cancer-related death in children. The five-year survival rate for high-grade
gliomas in children is less than 20\%. Due to their rarity, the diagnosis of
these entities is often delayed, their treatment is mainly based on historic
treatment concepts, and clinical trials require multi-institutional
collaborations. The MICCAI Brain Tumor Segmentation (BraTS) Challenge is a
landmark community benchmark event with a successful history of 12 years of
resource creation for the segmentation and analysis of adult glioma. Here we
present the CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023 challenge, which
represents the first BraTS challenge focused on pediatric brain tumors with
data acquired across multiple international consortia dedicated to pediatric
neuro-oncology and clinical trials. The BraTS-PEDs 2023 challenge focuses on
benchmarking the development of volumentric segmentation algorithms for
pediatric brain glioma through standardized quantitative performance evaluation
metrics utilized across the BraTS 2023 cluster of challenges. Models gaining
knowledge from the BraTS-PEDs multi-parametric structural MRI (mpMRI) training
data will be evaluated on separate validation and unseen test mpMRI dataof
high-grade pediatric glioma. The CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023
challenge brings together clinicians and AI/imaging scientists to lead to
faster development of automated segmentation techniques that could benefit
clinical trials, and ultimately the care of children with brain tumors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Generate Training <span class="highlight-title">Dataset</span>s for Robust Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.02535v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.02535v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marwane Hariat, Olivier Laurent, Rémi Kazmierczak, Shihao Zhang, Andrei Bursuc, Angela Yao, Gianni Franchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation methods have advanced significantly. Still, their
robustness to real-world perturbations and object types not seen during
training remains a challenge, particularly in safety-critical applications. We
propose a novel approach to improve the robustness of semantic segmentation
techniques by leveraging the synergy between label-to-image generators and
image-to-label segmentation models. Specifically, we design Robusta, a novel
robust conditional generative adversarial network to generate realistic and
plausible perturbed images that can be used to train reliable segmentation
models. We conduct in-depth studies of the proposed generative model, assess
the performance and robustness of the downstream segmentation network, and
demonstrate that our approach can significantly enhance the robustness in the
face of real-world perturbations, distribution shifts, and out-of-distribution
samples. Our results suggest that this approach could be valuable in
safety-critical applications, where the reliability of perception modules such
as semantic segmentation is of utmost importance and comes with a limited
computational budget in inference. We release our code at
https://github.com/ENSTA-U2IS/robusta.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at WACV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Survey and <span class="highlight-title">Benchmark</span> of Automatic Surface Reconstruction from Point
  Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.13656v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.13656v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raphael Sulzer, Renaud Marlet, Bruno Vallet, Loic Landrieu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a comprehensive survey and benchmark of both traditional and
learning-based methods for surface reconstruction from point clouds. This task
is particularly challenging for real-world acquisitions due to factors like
noise, outliers, non-uniform sampling, and missing data. Traditional approaches
often simplify the problem by imposing handcrafted priors on either the input
point clouds or the resulting surface, a process that can necessitate tedious
hyperparameter tuning. Conversely, deep learning models have the capability to
directly learn the properties of input point clouds and desired surfaces from
data. We study the influence of these handcrafted and learned priors on the
precision and robustness of surface reconstruction techniques. We evaluate
various time-tested and contemporary methods in a standardized manner. When
both trained and evaluated on point clouds with identical characteristics, the
learning-based models consistently produce superior surfaces compared to their
traditional counterparts$\unicode{x2013}$even in scenarios involving novel
shape categories. However, traditional methods demonstrate greater resilience
to the diverse array of point cloud anomalies commonly found in real-world 3D
acquisitions. For the benefit of the research community, we make our code and
datasets available, inviting further enhancements to learning-based surface
reconstruction. This can be accessed at
https://github.com/raphaelsulzer/dsr-benchmark .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HawkRover: An Autonomous mmWave Vehicular Communication Testbed with
  Multi-sensor Fusion and Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01822v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01822v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Zhu, Haijian Sun, Mingyue Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Connected and automated vehicles (CAVs) have become a transformative
technology that can change our daily life. Currently, millimeter-wave (mmWave)
bands are identified as the promising CAV connectivity solution. While it can
provide high data rate, their realization faces many challenges such as high
attenuation during mmWave signal propagation and mobility management. Existing
solution has to initiate pilot signal to measure channel information, then
apply signal processing to calculate the best narrow beam towards the receiver
end to guarantee sufficient signal power. This process takes significant
overhead and time, hence not suitable for vehicles. In this study, we propose
an autonomous and low-cost testbed to collect extensive co-located mmWave
signal and other sensors data such as LiDAR (Light Detection and Ranging),
cameras, ultrasonic, etc, traditionally for ``automated'', to facilitate mmWave
vehicular communications. Intuitively, these sensors can build a 3D map around
the vehicle and signal propagation path can be estimated, eliminating iterative
the process via pilot signals. This multimodal data fusion, together with AI,
is expected to bring significant advances in ``connected'' research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE conferences for future publications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamically Masked Discriminator for Generative Adversarial Networks <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07716v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07716v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentian Zhang, Haozhe Liu, Bing Li, Jinheng Xie, Yawen Huang, Yuexiang Li, Yefeng Zheng, Bernard Ghanem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training Generative Adversarial Networks (GANs) remains a challenging
problem. The discriminator trains the generator by learning the distribution of
real/generated data. However, the distribution of generated data changes
throughout the training process, which is difficult for the discriminator to
learn. In this paper, we propose a novel method for GANs from the viewpoint of
online continual learning. We observe that the discriminator model, trained on
historically generated data, often slows down its adaptation to the changes in
the new arrival generated data, which accordingly decreases the quality of
generated results. By treating the generated data in training as a stream, we
propose to detect whether the discriminator slows down the learning of new
knowledge in generated data. Therefore, we can explicitly enforce the
discriminator to learn new knowledge fast. Particularly, we propose a new
discriminator, which automatically detects its retardation and then dynamically
masks its features, such that the discriminator can adaptively learn the
temporally-vary distribution of generated data. Experimental results show our
method outperforms the state-of-the-art approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated v2 -- NeurIPS 2023 camera ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Few-shot Adaptation of Multi-modal Foundation Models: A Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01736v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01736v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Liu, Tianshu Zhang, Wenwen Dai, Wenwen Cai, Xiaocong Zhou, Delong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal (vision-language) models, such as CLIP, are replacing traditional
supervised pre-training models (e.g., ImageNet-based pre-training) as the new
generation of visual foundation models. These models with robust and aligned
semantic representations learned from billions of internet image-text pairs and
can be applied to various downstream tasks in a zero-shot manner. However, in
some fine-grained domains like medical imaging and remote sensing, the
performance of multi-modal foundation models often leaves much to be desired.
Consequently, many researchers have begun to explore few-shot adaptation
methods for these models, gradually deriving three main technical approaches:
1) prompt-based methods, 2) adapter-based methods, and 3) external
knowledge-based methods. Nevertheless, this rapidly developing field has
produced numerous results without a comprehensive survey to systematically
organize the research progress. Therefore, in this survey, we introduce and
analyze the research advancements in few-shot adaptation methods for
multi-modal models, summarizing commonly used datasets and experimental setups,
and comparing the results of different methods. In addition, due to the lack of
reliable theoretical support for existing methods, we derive the few-shot
adaptation generalization error bound for multi-modal models. The theorem
reveals that the generalization error of multi-modal foundation models is
constrained by three factors: domain gap, model capacity, and sample size.
Based on this, we propose three possible solutions from the following aspects:
1) adaptive domain generalization, 2) adaptive model selection, and 3) adaptive
knowledge utilization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Expressive Speech-driven Facial Animation with controllable emotions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.02008v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.02008v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutong Chen, Junhong Zhao, Wei-Qiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is in high demand to generate facial animation with high realism, but it
remains a challenging task. Existing approaches of speech-driven facial
animation can produce satisfactory mouth movement and lip synchronization, but
show weakness in dramatic emotional expressions and flexibility in emotion
control. This paper presents a novel deep learning-based approach for
expressive facial animation generation from speech that can exhibit
wide-spectrum facial expressions with controllable emotion type and intensity.
We propose an emotion controller module to learn the relationship between the
emotion variations (e.g., types and intensity) and the corresponding facial
expression parameters. It enables emotion-controllable facial animation, where
the target expression can be continuously adjusted as desired. The qualitative
and quantitative evaluations show that the animation generated by our method is
rich in facial emotional expressiveness while retaining accurate lip movement,
outperforming other state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Fully Decoupled End-to-End Person Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.04967v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.04967v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengcheng Zhang, Xiao Bai, Jin Zheng, Xin Ning
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end person search aims to jointly detect and re-identify a target
person in raw scene images with a unified model. The detection task unifies all
persons while the re-id task discriminates different identities, resulting in
conflict optimal objectives. Existing works proposed to decouple end-to-end
person search to alleviate such conflict. Yet these methods are still
sub-optimal on one or two of the sub-tasks due to their partially decoupled
models, which limits the overall person search performance. In this paper, we
propose to fully decouple person search towards optimal person search. A
task-incremental person search network is proposed to incrementally construct
an end-to-end model for the detection and re-id sub-task, which decouples the
model architecture for the two sub-tasks. The proposed task-incremental network
allows task-incremental training for the two conflicting tasks. This enables
independent learning for different objectives thus fully decoupled the model
for persons earch. Comprehensive experimental evaluations demonstrate the
effectiveness of the proposed fully decoupled models for end-to-end person
search.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>DICTA 2023 Best Student Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Certification of Vision-Language Models Using Incremental
  Randomized Smoothing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09024v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09024v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        A K Nirala, A Joshi, C Hegde, S Sarkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key benefit of deep vision-language models such as CLIP is that they enable
zero-shot open vocabulary classification; the user has the ability to define
novel class labels via natural language prompts at inference time. However,
while CLIP-based zero-shot classifiers have demonstrated competitive
performance across a range of domain shifts, they remain highly vulnerable to
adversarial attacks. Therefore, ensuring the robustness of such models is
crucial for their reliable deployment in the wild.
  In this work, we introduce Open Vocabulary Certification (OVC), a fast
certification method designed for open-vocabulary models like CLIP via
randomized smoothing techniques. Given a base "training" set of prompts and
their corresponding certified CLIP classifiers, OVC relies on the observation
that a classifier with a novel prompt can be viewed as a perturbed version of
nearby classifiers in the base training set. Therefore, OVC can rapidly certify
the novel classifier using a variation of incremental randomized smoothing. By
using a caching trick, we achieve approximately two orders of magnitude
acceleration in the certification process for novel prompts. To achieve further
(heuristic) speedups, OVC approximates the embedding space at a given input
using a multivariate normal distribution bypassing the need for sampling via
forward passes through the vision backbone. We demonstrate the effectiveness of
OVC on through experimental evaluation using multiple vision-language backbones
on the CIFAR-10 and ImageNet test datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TSGAN: An Optical-to-SAR Dual Conditional GAN for Optical based SAR
  Temporal Shifting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00440v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00440v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moien Rangzan, Sara Attarchi, Richard Gloaguen, Seyed Kazem Alavipanah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In contrast to the well-investigated field of SAR-to-Optical translation,
this study explores the lesser-investigated domain of Optical-to-SAR
translation, a challenging field due to the ill-posed nature of this
translation. The complexity arises as a single optical data can have multiple
SAR representations based on the SAR viewing geometry. We propose a novel
approach, termed SAR Temporal Shifting, which inputs an optical data from the
desired timestamp along with a SAR data from a different temporal point but
with a consistent viewing geometry as the expected SAR data, both complemented
with a change map of optical data during the intervening period. This model
modifies the SAR data based on the changes observed in optical data to generate
the SAR data for the desired timestamp. Our model, a dual conditional
Generative Adversarial Network (GAN), named Temporal Shifting GAN (TSGAN),
incorporates a siamese encoder in both the Generator and the Discriminator. To
prevent the model from overfitting on the input SAR data, we employed a change
weighted loss function. Our approach surpasses traditional translation methods
by eliminating the GAN's fiction phenomenon, particularly in unchanged regions,
resulting in higher SSIM and PSNR in these areas. Additionally, modifications
to the Pix2Pix architecture and the inclusion of attention mechanisms have
enhanced the model's performance on all regions of the data. This research
paves the way for leveraging legacy optical datasets, the most abundant and
longstanding source of Earth imagery data, extending their use to SAR domains
and temporal analyses. To foster further research, we provide the code,
datasets used in our study, and a framework for generating paired SAR-Optical
datasets for new regions of interest. These resources are available on
github.com/moienr/TemporalGAN
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Comments: Added acknowledgments and corrected a typo. No changes to
  the main content</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SLP-Net:An efficient lightweight network for segmentation of skin
  lesions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12789v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12789v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Yang, Hong Peng, Chenggang Guo, Xiaohui Luo, Jun Wang, Xianzhong Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt treatment for melanoma is crucial. To assist physicians in identifying
lesion areas precisely in a quick manner, we propose a novel skin lesion
segmentation technique namely SLP-Net, an ultra-lightweight segmentation
network based on the spiking neural P(SNP) systems type mechanism. Most
existing convolutional neural networks achieve high segmentation accuracy while
neglecting the high hardware cost. SLP-Net, on the contrary, has a very small
number of parameters and a high computation speed. We design a lightweight
multi-scale feature extractor without the usual encoder-decoder structure.
Rather than a decoder, a feature adaptation module is designed to replace it
and implement multi-scale information decoding. Experiments at the ISIC2018
challenge demonstrate that the proposed model has the highest Acc and DSC among
the state-of-the-art methods, while experiments on the PH2 dataset also
demonstrate a favorable generalization ability. Finally, we compare the
computational complexity as well as the computational speed of the models in
experiments, where SLP-Net has the highest overall superiority
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-stages attention Breast cancer classification based on nonlinear
  spiking neural P neurons with autapses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12804v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12804v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Yang, Hong Peng, Xiaohui Luo, Jun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Breast cancer(BC) is a prevalent type of malignant tumor in women. Early
diagnosis and treatment are vital for enhancing the patients' survival rate.
Downsampling in deep networks may lead to loss of information, so for
compensating the detail and edge information and allowing convolutional neural
networks to pay more attention to seek the lesion region, we propose a
multi-stages attention architecture based on NSNP neurons with autapses. First,
unlike the single-scale attention acquisition methods of existing methods, we
set up spatial attention acquisition at each feature map scale of the
convolutional network to obtain an fusion global information on attention
guidance. Then we introduce a new type of NSNP variants called NSNP neurons
with autapses. Specifically, NSNP systems are modularized as feature encoders,
recoding the features extracted from convolutional neural network as well as
the fusion of attention information and preserve the key characteristic
elements in feature maps. This ensures the retention of valuable data while
gradually transforming high-dimensional complicated info into low-dimensional
ones. The proposed method is evaluated on the public dataset BreakHis at
various magnifications and classification tasks. It achieves a classification
accuracy of 96.32% at all magnification cases, outperforming state-of-the-art
methods. Ablation studies are also performed, verifying the proposed model's
efficacy. The source code is available at
XhuBobYoung/Breast-cancer-Classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training-free Content Injection using h-space in Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.15403v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.15403v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaeseok Jeong, Mingi Kwon, Youngjung Uh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models (DMs) synthesize high-quality images in various domains.
However, controlling their generative process is still hazy because the
intermediate variables in the process are not rigorously studied. Recently, the
bottleneck feature of the U-Net, namely $h$-space, is found to convey the
semantics of the resulting image. It enables StyleCLIP-like latent editing
within DMs. In this paper, we explore further usage of $h$-space beyond
attribute editing, and introduce a method to inject the content of one image
into another image by combining their features in the generative processes.
Briefly, given the original generative process of the other image, 1) we
gradually blend the bottleneck feature of the content with proper
normalization, and 2) we calibrate the skip connections to match the injected
content. Unlike custom-diffusion approaches, our method does not require
time-consuming optimization or fine-tuning. Instead, our method manipulates
intermediate features within a feed-forward generative process. Furthermore,
our method does not require supervision from external networks. The code is
available at https://curryjung.github.io/InjectFusion/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Handling Noisy Labels via One-Step Abductive Multi-Target Learning and
  Its Application to Helicobacter Pylori Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2011.14956v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2011.14956v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongquan Yang, Yiming Yang, Jie Chen, Jiayi Zheng, Zhongxi Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning from noisy labels is an important concern in plenty of real-world
scenarios. Various approaches for this concern first make corrections
corresponding to potentially noisy-labeled instances, and then update
predictive model with information of the made corrections. However, in specific
areas, such as medical histopathology whole slide image analysis (MHWSIA), it
is often difficult or impossible for experts to manually achieve the noisy-free
ground-truth labels which leads to labels with complex noise. This situation
raises two more difficult problems: 1) the methodology of approaches making
corrections corresponding to potentially noisy-labeled instances has
limitations due to the complex noise existing in labels; and 2) the appropriate
evaluation strategy for validation/testing is unclear because of the great
difficulty in collecting the noisy-free ground-truth labels. For the problem
1), we present one-step abductive multi-target learning (OSAMTL) that imposes a
one-step logical reasoning upon machine learning via a multi-target learning
procedure to constrain the predictions of the learning model to be subject to
our prior knowledge about the true target. For the problem 2), we propose a
logical assessment formula (LAF) that evaluates the logical rationality of the
outputs of an approach by estimating the consistencies between the predictions
of the learning model and the logical facts narrated from the results of the
one-step logical reasoning of OSAMTL. Based on the Helicobacter pylori (H.
pylori) segmentation task in MHWSIA, we show that OSAMTL enables the machine
learning model achieving logically more rational predictions, which is beyond
various state-of-the-art approaches in handling complex noisy labels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spiking NeRF: Representing the Real-World Geometry by a Discontinuous
  Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09077v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09077v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanfeng Liao, Qian Zheng, Yan Liu, Gang Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A crucial reason for the success of existing NeRF-based methods is to build a
neural density field for the geometry representation via multiple perceptron
layers (MLPs). MLPs are continuous functions, however, real geometry or density
field is frequently discontinuous at the interface between the air and the
surface. Such a contrary brings the problem of unfaithful geometry
representation. To this end, this paper proposes spiking NeRF, which leverages
spiking neurons and a hybrid Artificial Neural Network (ANN)-Spiking Neural
Network (SNN) framework to build a discontinuous density field for faithful
geometry representation. Specifically, we first demonstrate the reason why
continuous density fields will bring inaccuracy. Then, we propose to use the
spiking neurons to build a discontinuous density field. We conduct a
comprehensive analysis for the problem of existing spiking neuron models and
then provide the numerical relationship between the parameter of the spiking
neuron and the theoretical accuracy of geometry. Based on this, we propose a
bounded spiking neuron to build the discontinuous density field. Our method
achieves SOTA performance. The source code and the supplementary material are
available at https://github.com/liaozhanfeng/Spiking-NeRF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shadow Generation with Decomposed Mask Prediction and Attentive Shadow
  Filling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.17358v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.17358v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinhao Tao, Junyan Cao, Yan Hong, Li Niu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image composition refers to inserting a foreground object into a background
image to obtain a composite image. In this work, we focus on generating
plausible shadows for the inserted foreground object to make the composite
image more realistic. To supplement the existing small-scale dataset, we create
a large-scale dataset called RdSOBA with rendering techniques. Moreover, we
design a two-stage network named DMASNet with decomposed mask prediction and
attentive shadow filling. Specifically, in the first stage, we decompose shadow
mask prediction into box prediction and shape prediction. In the second stage,
we attend to reference background shadow pixels to fill the foreground shadow.
Abundant experiments prove that our DMASNet achieves better visual effects and
generalizes well to real composite images.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLASS-M: Adaptive stain separation-based contrastive learning with
  pseudo-labeling for histopathological image classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06978v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06978v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bodong Zhang, Hamid Manoochehri, Man Minh Ho, Fahimeh Fooladgar, Yosep Chong, Beatrice S. Knudsen, Deepika Sirohi, Tolga Tasdizen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Histopathological image classification is an important task in medical image
analysis. Recent approaches generally rely on weakly supervised learning due to
the ease of acquiring case-level labels from pathology reports. However,
patch-level classification is preferable in applications where only a limited
number of cases are available or when local prediction accuracy is critical. On
the other hand, acquiring extensive datasets with localized labels for training
is not feasible. In this paper, we propose a semi-supervised patch-level
histopathological image classification model, named CLASS-M, that does not
require extensively labeled datasets. CLASS-M is formed by two main parts: a
contrastive learning module that uses separated Hematoxylin and Eosin images
generated through an adaptive stain separation process, and a module with
pseudo-labels using MixUp. We compare our model with other state-of-the-art
models on two clear cell renal cell carcinoma datasets. We demonstrate that our
CLASS-M model has the best performance on both datasets. Our code is available
at github.com/BzhangURU/Paper_CLASS-M/tree/main
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Depth-Regularized Optimization for 3D Gaussian Splatting in Few-Shot
  Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.13398v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.13398v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaeyoung Chung, Jeongtaek Oh, Kyoung Mu Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a method to optimize Gaussian splatting with a
limited number of images while avoiding overfitting. Representing a 3D scene by
combining numerous Gaussian splats has yielded outstanding visual quality.
However, it tends to overfit the training views when only a small number of
images are available. To address this issue, we introduce a dense depth map as
a geometry guide to mitigate overfitting. We obtained the depth map using a
pre-trained monocular depth estimation model and aligning the scale and offset
using sparse COLMAP feature points. The adjusted depth aids in the color-based
optimization of 3D Gaussian splatting, mitigating floating artifacts, and
ensuring adherence to geometric constraints. We verify the proposed method on
the NeRF-LLFF dataset with varying numbers of few images. Our approach
demonstrates robust geometry compared to the original method that relies solely
on images. Project page: robot0321.github.io/DepthRegGS
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures; Project page: robot0321.github.io/DepthRegGS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Text-to-Image Editing via Cache-Enabled Sparse Diffusion
  Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17423v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17423v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Yu, Haoyang Li, Fangcheng Fu, Xupeng Miao, Bin Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the recent success of diffusion models, text-to-image generation is
becoming increasingly popular and achieves a wide range of applications. Among
them, text-to-image editing, or continuous text-to-image generation, attracts
lots of attention and can potentially improve the quality of generated images.
It's common to see that users may want to slightly edit the generated image by
making minor modifications to their input textual descriptions for several
rounds of diffusion inference. However, such an image editing process suffers
from the low inference efficiency of many existing diffusion models even using
GPU accelerators. To solve this problem, we introduce Fast Image Semantically
Edit (FISEdit), a cached-enabled sparse diffusion model inference engine for
efficient text-to-image editing. The key intuition behind our approach is to
utilize the semantic mapping between the minor modifications on the input text
and the affected regions on the output image. For each text editing step,
FISEdit can automatically identify the affected image regions and utilize the
cached unchanged regions' feature map to accelerate the inference process.
Extensive empirical results show that FISEdit can be $3.4\times$ and
$4.4\times$ faster than existing methods on NVIDIA TITAN RTX and A100 GPUs
respectively, and even generates more satisfactory images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A General Implicit Framework for Fast NeRF Composition and Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.04669v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.04669v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Gao, Ziyi Yang, Yunlu Zhao, Yuxiang Sun, Xiaogang Jin, Changqing Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A variety of Neural Radiance Fields (NeRF) methods have recently achieved
remarkable success in high render speed. However, current accelerating methods
are specialized and incompatible with various implicit methods, preventing
real-time composition over various types of NeRF works. Because NeRF relies on
sampling along rays, it is possible to provide general guidance for
acceleration. To that end, we propose a general implicit pipeline for composing
NeRF objects quickly. Our method enables the casting of dynamic shadows within
or between objects using analytical light sources while allowing multiple NeRF
objects to be seamlessly placed and rendered together with any arbitrary rigid
transformations. Mainly, our work introduces a new surface representation known
as Neural Depth Fields (NeDF) that quickly determines the spatial relationship
between objects by allowing direct intersection computation between rays and
implicit surfaces. It leverages an intersection neural network to query NeRF
for acceleration instead of depending on an explicit spatial structure.Our
proposed method is the first to enable both the progressive and interactive
composition of NeRF objects. Additionally, it also serves as a previewing
plugin for a range of existing NeRF works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fusion of Single and Integral Multispectral Aerial Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.17515v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.17515v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Youssef, Oliver Bimber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A novel hybrid (model- and learning-based) architecture is presented for
fusing the most significant features from conventional aerial images with the
ones from integral aerial images that are the result of synthetic aperture
sensing for removing occlusion. It combines the environment's spatial
references with features of unoccluded targets that would normally be hidden by
dense vegetation. Our method out-beats state-of-the-art two-channel and
multi-channel fusion approaches visually and quantitatively in common metrics,
such as mutual information, visual information fidelity, and peak
signal-to-noise ratio. The proposed model does not require manually tuned
parameters, can be extended to an arbitrary number and combinations of spectral
channels, and is reconfigurable for addressing different use cases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High-Fidelity Diffusion-based Image Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15707v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15707v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Hou, Guoqiang Wei, Zhibo Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have attained remarkable success in the domains of image
generation and editing. It is widely recognized that employing larger inversion
and denoising steps in diffusion model leads to improved image reconstruction
quality. However, the editing performance of diffusion models tends to be no
more satisfactory even with increasing denoising steps. The deficiency in
editing could be attributed to the conditional Markovian property of the
editing process, where errors accumulate throughout denoising steps. To tackle
this challenge, we first propose an innovative framework where a rectifier
module is incorporated to modulate diffusion model weights with residual
features, thereby providing compensatory information to bridge the fidelity
gap. Furthermore, we introduce a novel learning paradigm aimed at minimizing
error propagation during the editing process, which trains the editing
procedure in a manner similar to denoising score-matching. Extensive
experiments demonstrate that our proposed framework and training strategy
achieve high-fidelity reconstruction and editing results across various levels
of denoising steps, meanwhile exhibits exceptional performance in terms of both
quantitative metric and qualitative assessments. Moreover, we explore our
model's generalization through several applications like image-to-image
translation and out-of-domain image editing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InstructTA: Instruction-Tuned Targeted Attack for Large Vision-Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01886v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01886v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xunguang Wang, Zhenlan Ji, Pingchuan Ma, Zongjie Li, Shuai Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large vision-language models (LVLMs) have demonstrated their incredible
capability in image understanding and response generation. However, this rich
visual interaction also makes LVLMs vulnerable to adversarial examples. In this
paper, we formulate a novel and practical gray-box attack scenario that the
adversary can only access the visual encoder of the victim LVLM, without the
knowledge of its prompts (which are often proprietary for service providers and
not publicly available) and its underlying large language model (LLM). This
practical setting poses challenges to the cross-prompt and cross-model
transferability of targeted adversarial attack, which aims to confuse the LVLM
to output a response that is semantically similar to the attacker's chosen
target text. To this end, we propose an instruction-tuned targeted attack
(dubbed InstructTA) to deliver the targeted adversarial attack on LVLMs with
high transferability. Initially, we utilize a public text-to-image generative
model to "reverse" the target response into a target image, and employ GPT-4 to
infer a reasonable instruction $\boldsymbol{p}^\prime$ from the target
response. We then form a local surrogate model (sharing the same visual encoder
with the victim LVLM) to extract instruction-aware features of an adversarial
image example and the target image, and minimize the distance between these two
features to optimize the adversarial example. To further improve the
transferability, we augment the instruction $\boldsymbol{p}^\prime$ with
instructions paraphrased from an LLM. Extensive experiments demonstrate the
superiority of our proposed method in targeted attack performance and
transferability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rewrite Caption Semantics: Bridging Semantic Gaps for
  Language-Supervised Semantic Segmentation <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13505v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13505v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun Xing, Jian Kang, Aoran Xiao, Jiahao Nie, Ling Shao, Shijian Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Pre-training has demonstrated its remarkable zero-shot
recognition ability and potential to learn generalizable visual representations
from language supervision. Taking a step ahead, language-supervised semantic
segmentation enables spatial localization of textual inputs by learning pixel
grouping solely from image-text pairs. Nevertheless, the state-of-the-art
suffers from clear semantic gaps between visual and textual modality: plenty of
visual concepts appeared in images are missing in their paired captions. Such
semantic misalignment circulates in pre-training, leading to inferior zero-shot
performance in dense predictions due to insufficient visual concepts captured
in textual representations. To close such semantic gap, we propose Concept
Curation (CoCu), a pipeline that leverages CLIP to compensate for the missing
semantics. For each image-text pair, we establish a concept archive that
maintains potential visually-matched concepts with our proposed vision-driven
expansion and text-to-vision-guided ranking. Relevant concepts can thus be
identified via cluster-guided sampling and fed into pre-training, thereby
bridging the gap between visual and textual semantics. Extensive experiments
over a broad suite of 8 segmentation benchmarks show that CoCu achieves superb
zero-shot transfer performance and greatly boosts language-supervised
segmentation baseline by a large margin, suggesting the value of bridging
semantic gap in pre-training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023. Code is available at
  https://github.com/xing0047/rewrite</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Learning: Forget-free Winning Subnetworks for Video
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11973v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11973v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haeyong Kang, Jaehong Yoon, Sung Ju Hwang, Chang D. Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the
existence of efficient subnetworks within larger, dense networks, a
high-performing Winning Subnetwork (WSN) in terms of task performance under
appropriate sparsity conditions is considered for various continual learning
tasks. It leverages pre-existing weights from dense networks to achieve
efficient learning in Task Incremental Learning (TIL) scenarios. In Few-Shot
Class Incremental Learning (FSCIL), a variation of WSN referred to as the Soft
subnetwork (SoftNet) is designed to prevent overfitting when the data samples
are scarce. Furthermore, the sparse reuse of WSN weights is considered for
Video Incremental Learning (VIL). The use of Fourier Subneural Operator (FSO)
within WSN is considered. It enables compact encoding of videos and identifies
reusable subnetworks across varying bandwidths. We have integrated FSO into
different architectural frameworks for continual learning, including VIL, TIL,
and FSCIL. Our comprehensive experiments demonstrate FSO's effectiveness,
significantly improving task performance at various convolutional
representational levels. Specifically, FSO enhances higher-layer performance in
TIL and FSCIL and lower-layer performance in VIL
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2303.14962,
  arXiv:2306.11305</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VSFormer: Visual-Spatial Fusion Transformer for Correspondence Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.08774v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.08774v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tangfei Liao, Xiaoqin Zhang, Li Zhao, Tao Wang, Guobao Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Correspondence pruning aims to find correct matches (inliers) from an initial
set of putative correspondences, which is a fundamental task for many
applications. The process of finding is challenging, given the varying inlier
ratios between scenes/image pairs due to significant visual differences.
However, the performance of the existing methods is usually limited by the
problem of lacking visual cues (\eg texture, illumination, structure) of
scenes. In this paper, we propose a Visual-Spatial Fusion Transformer
(VSFormer) to identify inliers and recover camera poses accurately. Firstly, we
obtain highly abstract visual cues of a scene with the cross attention between
local features of two-view images. Then, we model these visual cues and
correspondences by a joint visual-spatial fusion module, simultaneously
embedding visual cues into correspondences for pruning. Additionally, to mine
the consistency of correspondences, we also design a novel module that combines
the KNN-based graph and the transformer, effectively capturing both local and
global contexts. Extensive experiments have demonstrated that the proposed
VSFormer outperforms state-of-the-art methods on outdoor and indoor benchmarks.
Our code is provided at the following repository:
https://github.com/sugar-fly/VSFormer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HEAP: Unsupervised Object Discovery and Localization with Contrastive
  Grouping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17492v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17492v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Zhang, Jinheng Xie, Yuan Yuan, Michael Bi Mi, Robby T. Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised object discovery and localization aims to detect or segment
objects in an image without any supervision. Recent efforts have demonstrated a
notable potential to identify salient foreground objects by utilizing
self-supervised transformer features. However, their scopes only build upon
patch-level features within an image, neglecting region/image-level and
cross-image relationships at a broader scale. Moreover, these methods cannot
differentiate various semantics from multiple instances. To address these
problems, we introduce Hierarchical mErging framework via contrAstive grouPing
(HEAP). Specifically, a novel lightweight head with cross-attention mechanism
is designed to adaptively group intra-image patches into semantically coherent
regions based on correlation among self-supervised features. Further, to ensure
the distinguishability among various regions, we introduce a region-level
contrastive clustering loss to pull closer similar regions across images. Also,
an image-level contrastive loss is present to push foreground and background
representations apart, with which foreground objects and background are
accordingly discovered. HEAP facilitates efficient hierarchical image
decomposition, which contributes to more accurate object discovery while also
enabling differentiation among objects of various classes. Extensive
experimental results on semantic segmentation retrieval, unsupervised object
discovery, and saliency detection tasks demonstrate that HEAP achieves
state-of-the-art performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accurate Leukocyte Detection Based on Deformable-DETR and Multi-Level
  Feature Fusion for Aiding Diagnosis of Blood Diseases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00926v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00926v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Chen, Chenyan Zhang, Ben Chen, Yiyu Huang, Yifei Sun, Changmiao Wang, Xianjun Fu, Yuxing Dai, Feiwei Qin, Yong Peng, Yu Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In standard hospital blood tests, the traditional process requires doctors to
manually isolate leukocytes from microscopic images of patients' blood using
microscopes. These isolated leukocytes are then categorized via automatic
leukocyte classifiers to determine the proportion and volume of different types
of leukocytes present in the blood samples, aiding disease diagnosis. This
methodology is not only time-consuming and labor-intensive, but it also has a
high propensity for errors due to factors such as image quality and
environmental conditions, which could potentially lead to incorrect subsequent
classifications and misdiagnosis. To address these issues, this paper proposes
an innovative method of leukocyte detection: the Multi-level Feature Fusion and
Deformable Self-attention DETR (MFDS-DETR). To tackle the issue of leukocyte
scale disparity, we designed the High-level Screening-feature Fusion Pyramid
(HS-FPN), enabling multi-level fusion. This model uses high-level features as
weights to filter low-level feature information via a channel attention module
and then merges the screened information with the high-level features, thus
enhancing the model's feature expression capability. Further, we address the
issue of leukocyte feature scarcity by incorporating a multi-scale deformable
self-attention module in the encoder and using the self-attention and
cross-deformable attention mechanisms in the decoder, which aids in the
extraction of the global features of the leukocyte feature maps. The
effectiveness, superiority, and generalizability of the proposed MFDS-DETR
method are confirmed through comparisons with other cutting-edge leukocyte
detection models using the private WBCDD, public LISC and BCCD datasets. Our
source code and private WBCCD dataset are available at
https://github.com/JustlfC03/MFDS-DETR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures, accept Computers in Biology and Medicine 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InternVid: A Large-scale Video-Text <span class="highlight-title">Dataset</span> for Multimodal Understanding
  and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.06942v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.06942v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, Conghui He, Ping Luo, Ziwei Liu, Yali Wang, Limin Wang, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces InternVid, a large-scale video-centric multimodal
dataset that enables learning powerful and transferable video-text
representations for multimodal understanding and generation. The InternVid
dataset contains over 7 million videos lasting nearly 760K hours, yielding 234M
video clips accompanied by detailed descriptions of total 4.1B words. Our core
contribution is to develop a scalable approach to autonomously build a
high-quality video-text dataset with large language models (LLM), thereby
showcasing its efficacy in learning video-language representation at scale.
Specifically, we utilize a multi-scale approach to generate video-related
descriptions. Furthermore, we introduce ViCLIP, a video-text representation
learning model based on ViT-L. Learned on InternVid via contrastive learning,
this model demonstrates leading zero-shot action recognition and competitive
video retrieval performance. Beyond basic video understanding tasks like
recognition and retrieval, our dataset and model have broad applications. They
are particularly beneficial for generating interleaved video-text data for
learning a video-centric dialogue system, advancing video-to-text and
text-to-video generation research. These proposed resources provide a tool for
researchers and practitioners interested in multimodal video understanding and
generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Data and Code:
  https://github.com/OpenGVLab/InternVideo/tree/main/Data/InternVid</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BLADE: Box-Level Supervised Amodal Segmentation through Directed
  Expansion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01642v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01642v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaochen Liu, Zhixuan Li, Tingting Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perceiving the complete shape of occluded objects is essential for human and
machine intelligence. While the amodal segmentation task is to predict the
complete mask of partially occluded objects, it is time-consuming and
labor-intensive to annotate the pixel-level ground truth amodal masks.
Box-level supervised amodal segmentation addresses this challenge by relying
solely on ground truth bounding boxes and instance classes as supervision,
thereby alleviating the need for exhaustive pixel-level annotations.
Nevertheless, current box-level methodologies encounter limitations in
generating low-resolution masks and imprecise boundaries, failing to meet the
demands of practical real-world applications. We present a novel solution to
tackle this problem by introducing a directed expansion approach from visible
masks to corresponding amodal masks. Our approach involves a hybrid end-to-end
network based on the overlapping region - the area where different instances
intersect. Diverse segmentation strategies are applied for overlapping regions
and non-overlapping regions according to distinct characteristics. To guide the
expansion of visible masks, we introduce an elaborately-designed connectivity
loss for overlapping regions, which leverages correlations with visible masks
and facilitates accurate amodal segmentation. Experiments are conducted on
several challenging datasets and the results show that our proposed method can
outperform existing state-of-the-art methods with large margins.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Frequency Domain Modality-invariant Feature Learning for
  Visible-infrared Person Re-Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01839v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01839v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulin Li, Tianzhu Zhang, Yongdong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visible-infrared person re-identification (VI-ReID) is challenging due to the
significant cross-modality discrepancies between visible and infrared images.
While existing methods have focused on designing complex network architectures
or using metric learning constraints to learn modality-invariant features, they
often overlook which specific component of the image causes the modality
discrepancy problem. In this paper, we first reveal that the difference in the
amplitude component of visible and infrared images is the primary factor that
causes the modality discrepancy and further propose a novel Frequency Domain
modality-invariant feature learning framework (FDMNet) to reduce modality
discrepancy from the frequency domain perspective. Our framework introduces two
novel modules, namely the Instance-Adaptive Amplitude Filter (IAF) module and
the Phrase-Preserving Normalization (PPNorm) module, to enhance the
modality-invariant amplitude component and suppress the modality-specific
component at both the image- and feature-levels. Extensive experimental results
on two standard benchmarks, SYSU-MM01 and RegDB, demonstrate the superior
performance of our FDMNet against state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video Understanding with Large Language Models: A Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17432v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17432v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang, Daoan Zhang, Jie An, Jingyang Lin, Rongyi Zhu, Ali Vosoughi, Chao Huang, Zeliang Zhang, Feng Zheng, Jianguo Zhang, Ping Luo, Jiebo Luo, Chenliang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the burgeoning growth of online video platforms and the escalating
volume of video content, the demand for proficient video understanding tools
has intensified markedly. Given the remarkable capabilities of Large Language
Models (LLMs) in language and multimodal tasks, this survey provides a detailed
overview of the recent advancements in video understanding harnessing the power
of LLMs (Vid-LLMs). The emergent capabilities of Vid-LLMs are surprisingly
advanced, particularly their ability for open-ended spatial-temporal reasoning
combined with commonsense knowledge, suggesting a promising path for future
video understanding. We examine the unique characteristics and capabilities of
Vid-LLMs, categorizing the approaches into four main types: LLM-based Video
Agents, Vid-LLMs Pretraining, Vid-LLMs Instruction Tuning, and Hybrid Methods.
Furthermore, this survey presents a comprehensive study of the tasks, datasets,
and evaluation methodologies for Vid-LLMs. Additionally, it explores the
expansive applications of Vid-LLMs across various domains, highlighting their
remarkable scalability and versatility in real-world video understanding
challenges. Finally, it summarizes the limitations of existing Vid-LLMs and
outlines directions for future research. For more information, readers are
recommended to visit the repository at
https://github.com/yunlong10/Awesome-LLMs-for-Video-Understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MLN-net: A multi-source medical image segmentation method for clustered
  microcalcifications using multiple layer normalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.02742v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.02742v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Wang, Zanting Ye, Xiang Xie, Haidong Cui, Tao Chen, Banteng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate segmentation of clustered microcalcifications in mammography is
crucial for the diagnosis and treatment of breast cancer. Despite exhibiting
expert-level accuracy, recent deep learning advancements in medical image
segmentation provide insufficient contribution to practical applications, due
to the domain shift resulting from differences in patient postures, individual
gland density, and imaging modalities of mammography etc. In this paper, a
novel framework named MLN-net, which can accurately segment multi-source images
using only single source images, is proposed for clustered microcalcification
segmentation. We first propose a source domain image augmentation method to
generate multi-source images, leading to improved generalization. And a
structure of multiple layer normalization (LN) layers is used to construct the
segmentation network, which can be found efficient for clustered
microcalcification segmentation in different domains. Additionally, a branch
selection strategy is designed for measuring the similarity of the source
domain data and the target domain data. To validate the proposed MLN-net,
extensive analyses including ablation experiments are performed, comparison of
12 baseline methods. Extensive experiments validate the effectiveness of
MLN-net in segmenting clustered microcalcifications from different domains and
the its segmentation accuracy surpasses state-of-the-art methods. Code will be
available at https://github.com/yezanting/MLN-NET-VERSON1.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 9 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploiting Spatial-Temporal Context for Interacting Hand Reconstruction
  on Monocular RGB Video 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.04074v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.04074v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weichao Zhao, Hezhen Hu, Wengang Zhou, Li li, Houqiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reconstructing interacting hands from monocular RGB data is a challenging
task, as it involves many interfering factors, e.g. self- and mutual occlusion
and similar textures. Previous works only leverage information from a single
RGB image without modeling their physically plausible relation, which leads to
inferior reconstruction results. In this work, we are dedicated to explicitly
exploiting spatial-temporal information to achieve better interacting hand
reconstruction. On one hand, we leverage temporal context to complement
insufficient information provided by the single frame, and design a novel
temporal framework with a temporal constraint for interacting hand motion
smoothness. On the other hand, we further propose an interpenetration detection
module to produce kinetically plausible interacting hands without physical
collisions. Extensive experiments are performed to validate the effectiveness
of our proposed framework, which achieves new state-of-the-art performance on
public benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VideoChat: Chat-Centric Video Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.06355v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.06355v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        KunChang Li, Yinan He, Yi Wang, Yizhuo Li, Wenhai Wang, Ping Luo, Yali Wang, Limin Wang, Yu Qiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we initiate an attempt of developing an end-to-end
chat-centric video understanding system, coined as VideoChat. It integrates
video foundation models and large language models via a learnable neural
interface, excelling in spatiotemporal reasoning, event localization, and
causal relationship inference. To instructively tune this system, we build a
video-centric instruction dataset, composed of thousands of videos associated
with detailed descriptions and conversations. This dataset emphasizes
spatiotemporal reasoning and captures causal relationships, providing a
valuable asset for training our chat-centric video understanding system.
Preliminary qualitative experiments demonstrate the potential of our system
across a broad spectrum of video applications, which could serve as a simple
prototype system for future research on chat-centric video understanding.
Access our code and data at https://github.com/OpenGVLab/Ask-Anything
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantifying Deep Learning Model Uncertainty in Conformal Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00876v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00876v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamed Karimi, Reza Samavi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precise estimation of predictive uncertainty in deep neural networks is a
critical requirement for reliable decision-making in machine learning and
statistical modeling, particularly in the context of medical AI. Conformal
Prediction (CP) has emerged as a promising framework for representing the model
uncertainty by providing well-calibrated confidence levels for individual
predictions. However, the quantification of model uncertainty in conformal
prediction remains an active research area, yet to be fully addressed. In this
paper, we explore state-of-the-art CP methodologies and their theoretical
foundations. We propose a probabilistic approach in quantifying the model
uncertainty derived from the produced prediction sets in conformal prediction
and provide certified boundaries for the computed uncertainty. By doing so, we
allow model uncertainty measured by CP to be compared by other uncertainty
quantification methods such as Bayesian (e.g., MC-Dropout and DeepEnsemble) and
Evidential approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in AAAI Second Symposium on Human Partnership with Medical
  AI: Design, Operationalization, and Ethics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-Shot Video Editing Using Off-The-Shelf Image Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17599v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17599v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen Wang, Yan Jiang, Kangyang Xie, Zide Liu, Hao Chen, Yue Cao, Xinlong Wang, Chunhua Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale text-to-image diffusion models achieve unprecedented success in
image generation and editing. However, how to extend such success to video
editing is unclear. Recent initial attempts at video editing require
significant text-to-video data and computation resources for training, which is
often not accessible. In this work, we propose vid2vid-zero, a simple yet
effective method for zero-shot video editing. Our vid2vid-zero leverages
off-the-shelf image diffusion models, and doesn't require training on any
video. At the core of our method is a null-text inversion module for
text-to-video alignment, a cross-frame modeling module for temporal
consistency, and a spatial regularization module for fidelity to the original
video. Without any training, we leverage the dynamic nature of the attention
mechanism to enable bi-directional temporal modeling at test time. Experiments
and analyses show promising results in editing attributes, subjects, places,
etc., in real-world videos. Code is made available at
\url{https://github.com/baaivision/vid2vid-zero}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Add customized video editing. Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FactoFormer: Factorized Hyperspectral Transformers with Self-Supervised
  Pretraining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.09431v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.09431v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaheer Mohamed, Maryam Haghighat, Tharindu Fernando, Sridha Sridharan, Clinton Fookes, Peyman Moghadam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperspectral images (HSIs) contain rich spectral and spatial information.
Motivated by the success of transformers in the field of natural language
processing and computer vision where they have shown the ability to learn long
range dependencies within input data, recent research has focused on using
transformers for HSIs. However, current state-of-the-art hyperspectral
transformers only tokenize the input HSI sample along the spectral dimension,
resulting in the under-utilization of spatial information. Moreover,
transformers are known to be data-hungry and their performance relies heavily
on large-scale pretraining, which is challenging due to limited annotated
hyperspectral data. Therefore, the full potential of HSI transformers has not
been fully realized. To overcome these limitations, we propose a novel
factorized spectral-spatial transformer that incorporates factorized
self-supervised pretraining procedures, leading to significant improvements in
performance. The factorization of the inputs allows the spectral and spatial
transformers to better capture the interactions within the hyperspectral data
cubes. Inspired by masked image modeling pretraining, we also devise efficient
masking strategies for pretraining each of the spectral and spatial
transformers. We conduct experiments on six publicly available datasets for HSI
classification task and demonstrate that our model achieves state-of-the-art
performance in all the datasets. The code for our model will be made available
at https://github.com/csiro-robotics/factoformer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Geoscience and Remote Sensing in
  December 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Histopathology Slide Indexing and Search: Are We There Yet? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.17019v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.17019v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Helen H. Shang, Mohammad Sadegh Nasr, Jai Prakash Veerla, Parisa Boodaghi Malidarreh, MD Jillur Rahman Saurav, Amir Hajighasemi, Manfred Huber, Chace Moleta, Jitin Makker, Jacob M. Luber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The search and retrieval of digital histopathology slides is an important
task that has yet to be solved. In this case study, we investigate the clinical
readiness of three state-of-the-art histopathology slide search engines,
Yottixel, SISH, and RetCCL, on three patients with solid tumors. We provide a
qualitative assessment of each model's performance in providing retrieval
results that are reliable and useful to pathologists. We found that all three
image search engines fail to produce consistently reliable results and have
difficulties in capturing granular and subtle features of malignancy, limiting
their diagnostic accuracy. Based on our findings, we also propose a minimal set
of requirements to further advance the development of accurate and reliable
histopathology image search engines for successful clinical adoption.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language-Assisted Deep Learning for Autistic Behaviors Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09310v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09310v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andong Deng, Taojiannan Yang, Chen Chen, Qian Chen, Leslie Neely, Sakiko Oyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Correctly recognizing the behaviors of children with Autism Spectrum Disorder
(ASD) is of vital importance for the diagnosis of Autism and timely early
intervention. However, the observation and recording during the treatment from
the parents of autistic children may not be accurate and objective. In such
cases, automatic recognition systems based on computer vision and machine
learning (in particular deep learning) technology can alleviate this issue to a
large extent. Existing human action recognition models can now achieve
persuasive performance on challenging activity datasets, e.g. daily activity,
and sports activity. However, problem behaviors in children with ASD are very
different from these general activities, and recognizing these problem
behaviors via computer vision is less studied. In this paper, we first evaluate
a strong baseline for action recognition, i.e. Video Swin Transformer, on two
autism behaviors datasets (SSBD and ESBD) and show that it can achieve high
accuracy and outperform the previous methods by a large margin, demonstrating
the feasibility of vision-based problem behaviors recognition. Moreover, we
propose language-assisted training to further enhance the action recognition
performance. Specifically, we develop a two-branch multimodal deep learning
framework by incorporating the "freely available" language description for each
type of problem behavior. Experimental results demonstrate that incorporating
additional language supervision can bring an obvious performance boost for the
autism problem behaviors recognition task as compared to using the video
information only (i.e. 3.49% improvement on ESBD and 1.46% on SSBD).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Smart Health Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Models as Masked Audio-Video Learners <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03937v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03937v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elvis Nunez, Yanzi Jin, Mohammad Rastegari, Sachin Mehta, Maxwell Horton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past several years, the synchronization between audio and visual
signals has been leveraged to learn richer audio-visual representations. Aided
by the large availability of unlabeled videos, many unsupervised training
frameworks have demonstrated impressive results in various downstream audio and
video tasks. Recently, Masked Audio-Video Learners (MAViL) has emerged as a
state-of-the-art audio-video pre-training framework. MAViL couples contrastive
learning with masked autoencoding to jointly reconstruct audio spectrograms and
video frames by fusing information from both modalities. In this paper, we
study the potential synergy between diffusion models and MAViL, seeking to
derive mutual benefits from these two frameworks. The incorporation of
diffusion into MAViL, combined with various training efficiency methodologies
that include the utilization of a masking ratio curriculum and adaptive batch
sizing, results in a notable 32% reduction in pre-training Floating-Point
Operations (FLOPS) and an 18% decrease in pre-training wall clock time.
Crucially, this enhanced efficiency does not compromise the model's performance
in downstream audio-classification tasks when compared to MAViL's performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version for the Machine Learning for Audio Workshop at
  NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Supervision by Denoising for Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.02952v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.02952v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sean I. Young, Adrian V. Dalca, Enzo Ferrante, Polina Golland, Christopher A. Metzler, Bruce Fischl, Juan Eugenio Iglesias
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning-based image reconstruction models, such as those based on the U-Net,
require a large set of labeled images if good generalization is to be
guaranteed. In some imaging domains, however, labeled data with pixel- or
voxel-level label accuracy are scarce due to the cost of acquiring them. This
problem is exacerbated further in domains like medical imaging, where there is
no single ground truth label, resulting in large amounts of repeat variability
in the labels. Therefore, training reconstruction networks to generalize better
by learning from both labeled and unlabeled examples (called semi-supervised
learning) is problem of practical and theoretical interest. However,
traditional semi-supervised learning methods for image reconstruction often
necessitate handcrafting a differentiable regularizer specific to some given
imaging problem, which can be extremely time-consuming. In this work, we
propose "supervision by denoising" (SUD), a framework that enables us to
supervise reconstruction models using their own denoised output as soft labels.
SUD unifies stochastic averaging and spatial denoising techniques under a
spatio-temporal denoising framework and alternates denoising and model weight
update steps in an optimization framework for semi-supervision. As example
applications, we apply SUD to two problems arising from biomedical imaging --
anatomical brain reconstruction (3D) and cortical parcellation (2D) -- to
demonstrate a significant improvement in the image reconstructions over
supervised-only and stochastic averaging baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the IEEE Transactions on Pattern Analysis and Machine
  Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision-Language Models can Identify Distracted Driver Behavior from
  Naturalistic Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.10159v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.10159v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Zahid Hasan, Jiajing Chen, Jiyang Wang, Mohammed Shaiqur Rahman, Ameya Joshi, Senem Velipasalar, Chinmay Hegde, Anuj Sharma, Soumik Sarkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recognizing the activities causing distraction in real-world driving
scenarios is critical for ensuring the safety and reliability of both drivers
and pedestrians on the roadways. Conventional computer vision techniques are
typically data-intensive and require a large volume of annotated training data
to detect and classify various distracted driving behaviors, thereby limiting
their efficiency and scalability. We aim to develop a generalized framework
that showcases robust performance with access to limited or no annotated
training data. Recently, vision-language models have offered large-scale
visual-textual pretraining that can be adapted to task-specific learning like
distracted driving activity recognition. Vision-language pretraining models,
such as CLIP, have shown significant promise in learning natural
language-guided visual representations. This paper proposes a CLIP-based driver
activity recognition approach that identifies driver distraction from
naturalistic driving images and videos. CLIP's vision embedding offers
zero-shot transfer and task-based finetuning, which can classify distracted
activities from driving video data. Our results show that this framework offers
state-of-the-art performance on zero-shot transfer and video-based CLIP for
predicting the driver's state on two public datasets. We propose both
frame-based and video-based frameworks developed on top of the CLIP's visual
representation for distracted driving detection and classification tasks and
report the results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ R-MAE: Regions Meet Masked Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05411v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05411v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duy-Kien Nguyen, Vaibhav Aggarwal, Yanghao Li, Martin R. Oswald, Alexander Kirillov, Cees G. M. Snoek, Xinlei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we explore regions as a potential visual analogue of words for
self-supervised image representation learning. Inspired by Masked Autoencoding
(MAE), a generative pre-training baseline, we propose masked region
autoencoding to learn from groups of pixels or regions. Specifically, we design
an architecture which efficiently addresses the one-to-many mapping between
images and regions, while being highly effective especially with high-quality
regions. When integrated with MAE, our approach (R-MAE) demonstrates consistent
improvements across various pre-training datasets and downstream detection and
segmentation benchmarks, with negligible computational overheads. Beyond the
quantitative evaluation, our analysis indicates the models pre-trained with
masked region autoencoding unlock the potential for interactive segmentation.
The code is provided at https://github.com/facebookresearch/r-mae.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training Diffusion Models with Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.13301v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.13301v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, Sergey Levine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models are a class of flexible generative models trained with an
approximation to the log-likelihood objective. However, most use cases of
diffusion models are not concerned with likelihoods, but instead with
downstream objectives such as human-perceived image quality or drug
effectiveness. In this paper, we investigate reinforcement learning methods for
directly optimizing diffusion models for such objectives. We describe how
posing denoising as a multi-step decision-making problem enables a class of
policy gradient algorithms, which we refer to as denoising diffusion policy
optimization (DDPO), that are more effective than alternative reward-weighted
likelihood approaches. Empirically, DDPO is able to adapt text-to-image
diffusion models to objectives that are difficult to express via prompting,
such as image compressibility, and those derived from human feedback, such as
aesthetic quality. Finally, we show that DDPO can improve prompt-image
alignment using feedback from a vision-language model without the need for
additional data collection or human annotation. The project's website can be
found at http://rl-diffusion.github.io .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">23</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ODIN: A Single Model for 2D and 3D Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02416v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02416v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayush Jain, Pushkal Katara, Nikolaos Gkanatsios, Adam W. Harley, Gabriel Sarch, Kriti Aggarwal, Vishrav Chaudhary, Katerina Fragkiadaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art models on contemporary 3D perception benchmarks like ScanNet
consume and label dataset-provided 3D point clouds, obtained through post
processing of sensed multiview RGB-D images. They are typically trained
in-domain, forego large-scale 2D pre-training and outperform alternatives that
featurize the posed RGB-D multiview images instead. The gap in performance
between methods that consume posed images versus post-processed 3D point clouds
has fueled the belief that 2D and 3D perception require distinct model
architectures. In this paper, we challenge this view and propose ODIN
(Omni-Dimensional INstance segmentation), a model that can segment and label
both 2D RGB images and 3D point clouds, using a transformer architecture that
alternates between 2D within-view and 3D cross-view information fusion. Our
model differentiates 2D and 3D feature operations through the positional
encodings of the tokens involved, which capture pixel coordinates for 2D patch
tokens and 3D coordinates for 3D feature tokens. ODIN achieves state-of-the-art
performance on ScanNet200, Matterport3D and AI2THOR 3D instance segmentation
benchmarks, and competitive performance on ScanNet, S3DIS and COCO. It
outperforms all previous works by a wide margin when the sensed 3D point cloud
is used in place of the point cloud sampled from 3D mesh. When used as the 3D
perception engine in an instructable embodied agent architecture, it sets a new
state-of-the-art on the TEACh action-from-dialogue benchmark. Our code and
checkpoints can be found at the project website: https://odin-seg.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Direction of Arrival Estimation Using Microphone Array Processing for
  Moving Humanoid Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02386v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02386v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vladimir Tourbabin, Boaz Rafaely
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The auditory system of humanoid robots has gained increased attention in
recent years. This system typically acquires the surrounding sound field by
means of a microphone array. Signals acquired by the array are then processed
using various methods. One of the widely applied methods is direction of
arrival estimation. The conventional direction of arrival estimation methods
assume that the array is fixed at a given position during the estimation.
However, this is not necessarily true for an array installed on a moving
humanoid robot. The array motion, if not accounted for appropriately, can
introduce a significant error in the estimated direction of arrival. The
current paper presents a signal model that takes the motion into account. Based
on this model, two processing methods are proposed. The first one compensates
for the motion of the robot. The second method is applicable to periodic
signals and utilizes the motion in order to enhance the performance to a level
beyond that of a stationary array. Numerical simulations and an experimental
study are provided, demonstrating that the motion compensation method almost
eliminates the motion-related error. It is also demonstrated that by using the
motion-based enhancement method it is possible to improve the direction of
arrival estimation performance, as compared to that obtained when using a
stationary array.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Learning in Robotic Ultrasound Imaging: Challenges and
  Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Bi, Zhongliang Jiang, Felix Duelmer, Dianye Huang, Nassir Navab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article reviews the recent advances in intelligent robotic ultrasound
(US) imaging systems. We commence by presenting the commonly employed robotic
mechanisms and control techniques in robotic US imaging, along with their
clinical applications. Subsequently, we focus on the deployment of machine
learning techniques in the development of robotic sonographers, emphasizing
crucial developments aimed at enhancing the intelligence of these systems. The
methods for achieving autonomous action reasoning are categorized into two sets
of approaches: those relying on implicit environmental data interpretation and
those using explicit interpretation. Throughout this exploration, we also
discuss practical challenges, including those related to the scarcity of
medical data, the need for a deeper understanding of the physical aspects
involved, and effective data representation approaches. Moreover, we conclude
by highlighting the open problems in the field and analyzing different possible
perspectives on how the community could move forward in this research area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Annual Review of Control, Robotics, and Autonomous
  Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AERIAL-CORE: AI-Powered Aerial Robots for Inspection and Maintenance of
  Electrical Power Infrastructures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anibal Ollero, Alejandro Suarez, Christos Papaioannidis, Ioannis Pitas, Juan M. Marredo, Viet Duong, Emad Ebeid, Vit Kratky, Martin Saska, Chloe Hanoune, Amr Afifi, Antonio Franchi, Charalampos Vourtsis, Dario Floreano, Goran Vasiljevic, Stjepan Bogdan, Alvaro Caballero, Fabio Ruggiero, Vincenzo Lippiello, Carlos Matilla, Giovanni Cioffi, Davide Scaramuzza, Jose R. Martinez-de-Dios, Begona C. Arrue, Carlos Martin, Krzysztof Zurad, Carlos Gaitan, Jacob Rodriguez, Antonio Munoz, Antidio Viguria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale infrastructures are prone to deterioration due to age,
environmental influences, and heavy usage. Ensuring their safety through
regular inspections and maintenance is crucial to prevent incidents that can
significantly affect public safety and the environment. This is especially
pertinent in the context of electrical power networks, which, while essential
for energy provision, can also be sources of forest fires. Intelligent drones
have the potential to revolutionize inspection and maintenance, eliminating the
risks for human operators, increasing productivity, reducing inspection time,
and improving data collection quality. However, most of the current methods and
technologies in aerial robotics have been trialed primarily in indoor testbeds
or outdoor settings under strictly controlled conditions, always within the
line of sight of human operators. Additionally, these methods and technologies
have typically been evaluated in isolation, lacking comprehensive integration.
This paper introduces the first autonomous system that combines various
innovative aerial robots. This system is designed for extended-range
inspections beyond the visual line of sight, features aerial manipulators for
maintenance tasks, and includes support mechanisms for human operators working
at elevated heights. The paper further discusses the successful validation of
this system on numerous electrical power lines, with aerial robots executing
flights over 10 kilometers away from their ground control stations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Do Pedestrians' Perception Change toward Autonomous Vehicles during
  Unmarked Midblock Multilane Crossings: Role of AV Operation and Signal
  Indication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengjiao Zou, Jennifer Harper Ogle, Patrick Gerard, Weimin Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the primary impediments hindering the widespread acceptance of
autonomous vehicles (AVs) among pedestrians is their limited comprehension of
AVs. This study employs virtual reality (VR) to provide pedestrians with an
immersive environment for engaging with and comprehending AVs during unmarked
midblock multilane crossings. Diverse AV driving behaviors were modeled to
exhibit negotiation behavior with a yellow signal indication or non-yielding
behavior with a blue signal indication. This paper aims to investigate the
impact of various factors, such as AV behavior and signaling, pedestrian past
behavior, etc., on pedestrians' perception change of AVs. Before and after the
VR experiment, participants completed surveys assessing their perception of
AVs, focusing on two main aspects: "Attitude" and "System Effectiveness." The
Wilcoxon signed-rank test results demonstrated that both pedestrians' overall
attitude score toward AVs and trust in the effectiveness of AV systems
significantly increased following the VR experiment. Notably, individuals who
exhibited a greater trust in the yellow signals were more inclined to display a
higher attitude score toward AVs and to augment their trust in the
effectiveness of AV systems. This indicates that the design of the yellow
signal instills pedestrians with greater confidence in their interactions with
AVs. Further, pedestrians who exhibit more aggressive crossing behavior are
less likely to change their perception towards AVs as compared to those
pedestrians with more positive crossing behaviors. It is concluded that
integrating this paper's devised AV behavior and signaling within an immersive
VR setting facilitated pedestrian engagement with AVs, thereby changing their
perception of AVs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enabling Digitalization in Modular Robotic Systems Integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02227v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02227v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniella Tola
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating robot systems into manufacturing lines is a time-consuming
process. In the era of digitalization, the research and development of new
technologies is crucial for improving integration processes. Numerous
challenges, including the lack of standardization, as well as intricate
stakeholder relationships, complicate the process of robotic systems
integration. This process typically consists of acquisition, integration, and
deployment of the robot systems. This thesis focuses on three areas that help
automate and simplify robotic systems integration. In the first area, related
to acquisition, a constraint-based configurator is demonstrated that resolves
compatibility challenges between robot devices, and automates the configuration
process. This reduces the risk of integrating incompatible devices and
decreases the need for experts during the configuration phase. In the second
area, related to integration, the interoperable modeling format, Unified Robot
Description Format (URDF), is investigated, where a detailed analysis is
performed, revealing significant inconsistencies and critical improvements.
This format is widely used for kinematic modeling and 3D visualization of
robots, and its models can be reused across simulation tools. Improving this
format benefits a wide range of users, including robotics engineers,
researchers, and students. In the third area, related to deployment, Digital
Twins (DTs) for robot systems are explored, as these improve efficiency and
reduce downtime. A comprehensive literature review of DTs is conducted, and a
case study of modular robot systems is developed. This research can accelerate
the adoption of DTs in the robotics industry. These insights and approaches
improve the process of robotic systems integration, offering valuable
contributions that future research can build upon, ultimately driving
efficiency, and reducing costs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>PhD thesis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inherently robust suboptimal MPC for autonomous racing with anytime
  feasible SQP 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02194v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02194v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Logan Numerow, Andrea Zanelli, Andrea Carron, Melanie N. Zeilinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the increasing need for high-performance controllers in
applications like autonomous driving has motivated the development of
optimization routines tailored to specific control problems. In this paper, we
propose an efficient inexact model predictive control (MPC) strategy for
autonomous miniature racing with inherent robustness properties. We rely on a
feasible sequential quadratic programming (SQP) algorithm capable of generating
feasible intermediate iterates such that the solver can be stopped after any
number of iterations, without jeopardizing recursive feasibility. In this way,
we provide a strategy that computes suboptimal and yet feasible solutions with
a computational footprint that is much lower than state-of-the-art methods
based on the computation of locally optimal solutions. Under suitable
assumptions on the terminal set and on the controllability properties of the
system, we can state that, for any sufficiently small disturbance affecting the
system's dynamics, recursive feasibility can be guaranteed. We validate the
effectiveness of the proposed strategy in simulation and by deploying it onto a
physical experiment with autonomous miniature race cars. Both the simulation
and experimental results demonstrate that, using the feasible SQP method, a
feasible solution can be obtained with moderate additional computational effort
compared to strategies that resort to early termination without providing a
feasible solution. At the same time, the proposed method is significantly
faster than the state-of-the-art solver Ipopt.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimating continuous data of wrist joint angles using ultrasound images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02152v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02152v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yo Kobayashi, Yoshihiro Katagi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ultrasound imaging has recently been introduced as a sensing interface for
joint motion estimation. The use of ultrasound images as an estimation method
is expected to improve the control performance of assistive devices and
human--machine interfaces. This study aimed to estimate continuous wrist joint
angles using ultrasound images. Specifically, in an experiment, joint angle
information was obtained during extension--flexion movements, and ultrasound
images of the associated muscles were acquired. Using the features obtained
from ultrasound images, a multivariate linear regression model was used to
estimate the joint angles. The coordinates of the feature points obtained using
optical flow from the ultrasound images were used as explanatory variables of
the multivariate linear regression model. The model was trained and tested for
each trial by each participant to verify the estimation accuracy. The results
show that the mean and standard deviation of the estimation accuracy for all
trials were root mean square error (RMSE)=1.82 $\pm$ 0.54 deg and coefficient
of determination (R2)=0.985 $\pm$ 0.009. Our method achieves a highly accurate
estimation of joint angles compared with previous studies using other signals,
such as surface electromyography, while the multivariate linear regression
model is simple and both computational and model training costs are low.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mobile ALOHA: Learning Bimanual Mobile Manipulation with Low-Cost
  Whole-Body Teleoperation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02117v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02117v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zipeng Fu, Tony Z. Zhao, Chelsea Finn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning from human demonstrations has shown impressive performance
in robotics. However, most results focus on table-top manipulation, lacking the
mobility and dexterity necessary for generally useful tasks. In this work, we
develop a system for imitating mobile manipulation tasks that are bimanual and
require whole-body control. We first present Mobile ALOHA, a low-cost and
whole-body teleoperation system for data collection. It augments the ALOHA
system with a mobile base, and a whole-body teleoperation interface. Using data
collected with Mobile ALOHA, we then perform supervised behavior cloning and
find that co-training with existing static ALOHA datasets boosts performance on
mobile manipulation tasks. With 50 demonstrations for each task, co-training
can increase success rates by up to 90%, allowing Mobile ALOHA to autonomously
complete complex mobile manipulation tasks such as sauteing and serving a piece
of shrimp, opening a two-door wall cabinet to store heavy cooking pots, calling
and entering an elevator, and lightly rinsing a used pan using a kitchen
faucet. Project website: https://mobile-aloha.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://mobile-aloha.github.io (Zipeng Fu and Tony
  Z. Zhao are project co-leads, Chelsea Finn is the advisor)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Perceptions of Humanoid Robots in Caregiving: A Study of Skilled Nursing
  Home and Long Term Care Administrators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rana Imtiaz, Arshia Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the aging population increases and the shortage of healthcare workers
increases, the need to examine other means for caring for the aging population
increases. One such means is the use of humanoid robots to care for social,
emotional, and physical wellbeing of the people above 65. Understanding skilled
and long term care nursing home administrators' perspectives on humanoid robots
in caregiving is crucial as their insights shape the implementation of robots
and their potential impact on resident well-being and quality of life. This
authors surveyed two hundred and sixty nine nursing homes executives to
understand their perspectives on the use of humanoid robots in their nursing
home facilities. The data was coded and results revealed that the executives
were keen on exploring other avenues for care such as robotics that would
enhance their nursing homes abilities to care for their residents. Qualitative
analysis reveals diverse perspectives on integrating humanoid robots in nursing
homes. While acknowledging benefits like improved engagement and staff support,
concerns persist about costs, impacts on human interaction, and doubts about
robot effectiveness. This highlights complex barriers financial, technical, and
human and emphasizes the need for strategic implementation. It underscores the
importance of thorough training, role clarity, and showcasing technology
benefits to ensure efficiency and satisfaction among staff and residents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Autonomous Multi-Rotor UAVs: A Holistic Approach to Design,
  Optimization, and Fabrication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02541v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02541v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aniruth A, Chirag Satpathy, Jothika K, Nitteesh M, Gokulraj M, Venkatram K, Harshith G, Shristi S, Anushka Vani, Jonathan Spurgeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unmanned Aerial Vehicles (UAVs) have become pivotal in domains spanning
military, agriculture, surveillance, and logistics, revolutionizing data
collection and environmental interaction. With the advancement in drone
technology, there is a compelling need to develop a holistic methodology for
designing UAVs. This research focuses on establishing a procedure encompassing
conceptual design, use of composite materials, weight optimization, stability
analysis, avionics integration, advanced manufacturing, and incorporation of
autonomous payload delivery through object detection models tailored to satisfy
specific applications while maintaining cost efficiency. The study conducts a
comparative assessment of potential composite materials and various quadcopter
frame configurations. The novel features include a payload-dropping mechanism,
a unibody arm fixture, and the utilization of carbon-fibre-balsa composites. A
quadcopter is designed and analyzed using the proposed methodology, followed by
its fabrication using additive manufacturing and vacuum bagging techniques. A
computer vision-based deep learning model enables precise delivery of payloads
by autonomously detecting targets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robot-Assisted Deep Venous Thrombosis Ultrasound Examination using
  Virtual Fixture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dianye Huang, Chenguang Yang, Mingchuan Zhou, Angelos Karlas, Nassir Navab, Zhongliang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Venous Thrombosis (DVT) is a common vascular disease with blood clots
inside deep veins, which may block blood flow or even cause a life-threatening
pulmonary embolism. A typical exam for DVT using ultrasound (US) imaging is by
pressing the target vein until its lumen is fully compressed. However, the
compression exam is highly operator-dependent. To alleviate intra- and
inter-variations, we present a robotic US system with a novel hybrid force
motion control scheme ensuring position and force tracking accuracy, and soft
landing of the probe onto the target surface. In addition, a path-based virtual
fixture is proposed to realize easy human-robot interaction for repeat
compression operation at the lesion location. To ensure the biometric
measurements obtained in different examinations are comparable, the 6D scanning
path is determined in a coarse-to-fine manner using both an external RGBD
camera and US images. The RGBD camera is first used to extract a rough scanning
path on the object. Then, the segmented vascular lumen from US images are used
to optimize the scanning path to ensure the visibility of the target object. To
generate a continuous scan path for developing virtual fixtures, an arc-length
based path fitting model considering both position and orientation is proposed.
Finally, the whole system is evaluated on a human-like arm phantom with an
uneven surface.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted Paper IEEE T-ASE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StROL: Stabilized and Robust Online Learning from Humans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.09863v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.09863v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaunak A. Mehta, Forrest Meng, Andrea Bajcsy, Dylan P. Losey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robots often need to learn the human's reward function online, during the
current interaction. This real-time learning requires fast but approximate
learning rules: when the human's behavior is noisy or suboptimal, current
approximations can result in unstable robot learning. Accordingly, in this
paper we seek to enhance the robustness and convergence properties of gradient
descent learning rules when inferring the human's reward parameters. We model
the robot's learning algorithm as a dynamical system over the human preference
parameters, where the human's true (but unknown) preferences are the
equilibrium point. This enables us to perform Lyapunov stability analysis to
derive the conditions under which the robot's learning dynamics converge. Our
proposed algorithm (StROL) uses these conditions to learn robust-by-design
learning rules: given the original learning dynamics, StROL outputs a modified
learning rule that now converges to the human's true parameters under a larger
set of human inputs. In practice, these autonomously generated learning rules
can correctly infer what the human is trying to convey, even when the human is
noisy, biased, and suboptimal. Across simulations and a user study we find that
StROL results in a more accurate estimate and less regret than state-of-the-art
approaches for online reward learning. See videos and code here:
https://github.com/VT-Collab/StROL_RAL
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Multi-Step Manipulation Tasks from A Single Human Demonstration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15346v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15346v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingkun Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning from human demonstrations has exhibited remarkable achievements in
robot manipulation. However, the challenge remains to develop a robot system
that matches human capabilities and data efficiency in learning and
generalizability, particularly in complex, unstructured real-world scenarios.
We propose a system that processes RGBD videos to translate human actions to
robot primitives and identifies task-relevant key poses of objects using
Grounded Segment Anything. We then address challenges for robots in replicating
human actions, considering the human-robot differences in kinematics and
collision geometry. To test the effectiveness of our system, we conducted
experiments focusing on manual dishwashing. With a single human demonstration
recorded in a mockup kitchen, the system achieved 50-100% success for each step
and up to a 40% success rate for the whole task with different objects in a
home kitchen. Videos are available at https://robot-dishwashing.github.io
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aerial Manipulator Force Control Using Control Barrier Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.07709v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.07709v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitris Chaikalis, Vinicius Goncalves, Nikolaos Evangeliou, Anthony Tzes, Farshad Khorrami
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article studies the problem of applying normal forces on a surface,
using an underactuated aerial vehicle equipped with a dexterous robotic arm. A
force-motion high-level controller is designed based on a Lyapunov function
encompassing alignment and exerted force errors. This controller is coupled
with a Control Barrier Function constraint under an optimization scheme using
Quadratic Programming. This aims to enforce a prescribed relationship between
the approaching motion for the end-effector and its alignment with the surface,
thus ensuring safe operation. An adaptive low-level controller is devised for
the aerial vehicle, capable of tracking velocity commands generated by the
high-level controller. Simulations and experiments are presented to demonstrate
the force exertion stability and safety of the controller in cases of large
disturbances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hybrid and Oriented Harmonic Potentials for Safe Task Execution in
  Unknown Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.07537v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.07537v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuaikang Wang, Meng Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Harmonic potentials provide globally convergent potential fields that are
provably free of local minima. Due to its analytical format, it is particularly
suitable for generating safe and reliable robot navigation policies. However,
for complex environments that consist of a large number of overlapping
non-sphere obstacles, the computation of associated transformation functions
can be tedious. This becomes more apparent when: (i) the workspace is initially
unknown and the underlying potential fields are updated constantly as the robot
explores it; (ii) the high-level mission consists of sequential navigation
tasks among numerous regions, requiring the robot to switch between different
potentials. Thus, this work proposes an efficient and automated scheme to
construct harmonic potentials incrementally online as guided by the task
automaton. A novel two-layer harmonic tree (HT) structure is introduced that
facilitates the hybrid combination of oriented search algorithms for task
planning and harmonic-based navigation controllers for non-holonomic robots.
Both layers are adapted efficiently and jointly during online execution to
reflect the actual feasibility and cost of navigation within the updated
workspace. Global safety and convergence are ensured both for the high-level
task plan and the low-level robot trajectory. Known issues such as oscillation
or long-detours for purely potential-based methods and sharp-turns or high
computation complexity for purely search-based methods are prevented. Extensive
numerical simulation and hardware experiments are conducted against several
strong baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Safe and Collaborative Robotic Ultrasound Tissue Scanning in
  Neurosurgery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.08409v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.08409v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Dyck, Alistair Weld, Julian Klodmann, Alexander Kirst, Luke Dixon, Giulio Anichini, Sophie Camp, Alin Albu-Schäffer, Stamatia Giannarou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intraoperative ultrasound imaging is used to facilitate safe brain tumour
resection. However, due to challenges with image interpretation and the
physical scanning, this tool has yet to achieve widespread adoption in
neurosurgery. In this paper, we introduce the components and workflow of a
novel, versatile robotic platform for intraoperative ultrasound tissue scanning
in neurosurgery. An RGB-D camera attached to the robotic arm allows for
automatic object localisation with ArUco markers, and 3D surface reconstruction
as a triangular mesh using the ImFusion Suite software solution. Impedance
controlled guidance of the US probe along arbitrary surfaces, represented as a
mesh, enables collaborative US scanning, i.e., autonomous, teleoperated and
hands-on guided data acquisition. A preliminary experiment evaluates the
suitability of the conceptual workflow and system components for probe landing
on a custom-made soft-tissue phantom. Further assessment in future experiments
will be necessary to prove the effectiveness of the presented platform.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 7 figures, accepted (05 December 2023) for publication in
  IEEE Transaction on Medical Robotics and Bionics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sample-efficient Reinforcement Learning in Robotic Table Tennis <span class="chip">ICRA 2021</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2011.03275v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2011.03275v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Tebbe, Lukas Krauch, Yapeng Gao, Andreas Zell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) has achieved some impressive recent successes in
various computer games and simulations. Most of these successes are based on
having large numbers of episodes from which the agent can learn. In typical
robotic applications, however, the number of feasible attempts is very limited.
In this paper we present a sample-efficient RL algorithm applied to the example
of a table tennis robot. In table tennis every stroke is different, with
varying placement, speed and spin. An accurate return therefore has to be found
depending on a high-dimensional continuous state space. To make learning in few
trials possible the method is embedded into our robot system. In this way we
can use a one-step environment. The state space depends on the ball at hitting
time (position, velocity, spin) and the action is the racket state
(orientation, velocity) at hitting. An actor-critic based deterministic policy
gradient algorithm was developed for accelerated learning. Our approach
performs competitively both in a simulation and on the real robot in a number
of challenging scenarios. Accurate results are obtained without pre-training in
under $200$ episodes of training. The video presenting our experiments is
available at https://youtu.be/uRAtdoL6Wpw.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at ICRA 2021 (Xian, China)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast and Continual Learning for Hybrid Control Policies using
  Generalized Benders Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00917v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00917v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hybrid model predictive control with both continuous and discrete variables
is widely applicable to robotic control tasks, especially those involving
contact with the environment. Due to the combinatorial complexity, the solving
speed of hybrid MPC can be insufficient for real-time applications. In this
paper, we proposed a hybrid MPC solver based on Generalized Benders
Decomposition (GBD). The algorithm enumerates and stores cutting planes online
inside a finite buffer. After a short cold-start phase, the stored cuts provide
warm-starts for the new problem instances to enhance the solving speed. Despite
the disturbance and randomly changing environment, the solving speed maintains.
Leveraging on the sparsity of feasibility cuts, we also propose a fast
algorithm for Benders master problems. Our solver is validated through
controlling a cart-pole system with randomly moving soft contact walls, and a
free-flying robot navigating around obstacles. The results show that with
significantly less data than previous works, the solver reaches competitive
speeds to the off-the-shelf solver Gurobi despite the Python overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A more complete version of the previous paper "Generalized Benders
  Decomposition with Continual Learning for Hybrid Model Predictive Control in
  Dynamic Environment". The updated version fixes some minor issues and typos.
  arXiv admin note: substantial text overlap with arXiv:2310.03344</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comparative Study of Nonlinear MPC and Differential-Flatness-Based
  Control for Quadrotor Agile Flight 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.01365v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.01365v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sihao Sun, Angel Romero, Philipp Foehn, Elia Kaufmann, Davide Scaramuzza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate trajectory tracking control for quadrotors is essential for safe
navigation in cluttered environments. However, this is challenging in agile
flights due to nonlinear dynamics, complex aerodynamic effects, and actuation
constraints. In this article, we empirically compare two state-of-the-art
control frameworks: the nonlinear-model-predictive controller (NMPC) and the
differential-flatness-based controller (DFBC), by tracking a wide variety of
agile trajectories at speeds up to 20 m/s (i.e.,72 km/h). The comparisons are
performed in both simulation and real-world environments to systematically
evaluate both methods from the aspect of tracking accuracy, robustness, and
computational efficiency. We show the superiority of NMPC in tracking
dynamically infeasible trajectories, at the cost of higher computation time and
risk of numerical convergence issues. For both methods, we also quantitatively
study the effect of adding an inner-loop controller using the incremental
nonlinear dynamic inversion (INDI) method, and the effect of adding an
aerodynamic drag model. Our real-world experiments, performed in one of the
world's largest motion capture systems, demonstrate more than 78% tracking
error reduction of both NMPC and DFBC, indicating the necessity of using an
inner-loop controller and aerodynamic drag model for agile trajectory tracking.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Not Only Rewards But Also Constraints: Applications on Legged Robot
  Locomotion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.12517v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.12517v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunho Kim, Hyunsik Oh, Jeonghyun Lee, Jinhyeok Choi, Gwanghyeon Ji, Moonkyu Jung, Donghoon Youm, Jemin Hwangbo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Several earlier studies have shown impressive control performance in complex
robotic systems by designing the controller using a neural network and training
it with model-free reinforcement learning. However, these outstanding
controllers with natural motion style and high task performance are developed
through extensive reward engineering, which is a highly laborious and
time-consuming process of designing numerous reward terms and determining
suitable reward coefficients. In this work, we propose a novel reinforcement
learning framework for training neural network controllers for complex robotic
systems consisting of both rewards and constraints. To let the engineers
appropriately reflect their intent to constraints and handle them with minimal
computation overhead, two constraint types and an efficient policy optimization
algorithm are suggested. The learning framework is applied to train locomotion
controllers for several legged robots with different morphology and physical
attributes to traverse challenging terrains. Extensive simulation and
real-world experiments demonstrate that performant controllers can be trained
with significantly less reward engineering, by tuning only a single reward
coefficient. Furthermore, a more straightforward and intuitive engineering
process can be utilized, thanks to the interpretability and generalizability of
constraints. The summary video is available at https://youtu.be/KAlm3yskhvM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Transactions on Robotics (T-RO)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Multi-IMU Calibration Using Visual-Inertial Odometry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12411v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12411v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Hartzer, Srikanth Saripalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work presents a centralized multi-IMU filter framework with online
intrinsic and extrinsic calibration for unsynchronized inertial measurement
units that is robust against changes in calibration parameters. The novel
EKF-based method estimates the positional and rotational offsets of the system
of sensors as well as their intrinsic biases without the use of rigid body
geometric constraints. Additionally, the filter is flexible in the total number
of sensors used while leveraging the commonly used MSCKF framework for camera
measurements. The filter framework has been validated using Monte Carlo
simulation as well as experimentally. In both simulations and experiments,
using multiple IMU measurement streams within the proposed filter framework
outperforms the use of a single IMU in a filter prediction step while also
producing consistent and accurate estimates of initial calibration errors.
Compared to current state-of-the-art optimizers, the filter produces similar
intrinsic and extrinsic calibration parameters for each sensor. Finally, an
open source repository has been provided at
https://github.com/unmannedlab/ekf-cal containing both the online estimator and
the simulation used for testing and evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Range-Visual-Inertial Sensor Fusion for Micro Aerial Vehicle
  Localization and <span class="highlight-title">Navigation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.09056v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.09056v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhishek Goudar, Wenda Zhao, Angela P. Schoellig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a fixed-lag smoother-based sensor fusion architecture to leverage
the complementary benefits of range-based sensors and visual-inertial odometry
(VIO) for localization. We use two fixed-lag smoothers (FLS) to decouple
accurate state estimation and high-rate pose generation for closed-loop
control. The first FLS combines ultrawideband (UWB)-based range measurements
and VIO to estimate the robot trajectory and any systematic biases that affect
the range measurements in cluttered environments. The second FLS estimates
smooth corrections to VIO to generate pose estimates at a high rate for online
control. The proposed method is lightweight and can run on a computationally
constrained micro-aerial vehicle (MAV). We validate our approach through
closed-loop flight tests involving dynamic trajectories in multiple real-world
cluttered indoor environments. Our method achieves
decimeter-to-sub-decimeter-level positioning accuracy using off-the-shelf
sensors and decimeter-level tracking accuracy with minimally-tuned open-source
controllers.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TR-DETR: Task-Reciprocal Transformer for Joint Moment Retrieval and
  Highlight Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02309v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02309v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Sun, Mingyao Zhou, Wenjing Chen, Wei Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video moment retrieval (MR) and highlight detection (HD) based on natural
language queries are two highly related tasks, which aim to obtain relevant
moments within videos and highlight scores of each video clip. Recently,
several methods have been devoted to building DETR-based networks to solve both
MR and HD jointly. These methods simply add two separate task heads after
multi-modal feature extraction and feature interaction, achieving good
performance. Nevertheless, these approaches underutilize the reciprocal
relationship between two tasks. In this paper, we propose a task-reciprocal
transformer based on DETR (TR-DETR) that focuses on exploring the inherent
reciprocity between MR and HD. Specifically, a local-global multi-modal
alignment module is first built to align features from diverse modalities into
a shared latent space. Subsequently, a visual feature refinement is designed to
eliminate query-irrelevant information from visual features for modal
interaction. Finally, a task cooperation module is constructed to refine the
retrieval pipeline and the highlight score prediction process by utilizing the
reciprocity between MR and HD. Comprehensive experiments on QVHighlights,
Charades-STA and TVSum datasets demonstrate that TR-DETR outperforms existing
state-of-the-art methods. Codes are available at
\url{https://github.com/mingyao1120/TR-DETR}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI-24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Siamese Residual Neural Network for Musical Shape Evaluation in Piano
  Performance Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoquan Li, Stephan Weiss, Yijun Yan, Yinhe Li, Jinchang Ren, John Soraghan, Ming Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding and identifying musical shape plays an important role in music
education and performance assessment. To simplify the otherwise time- and
cost-intensive musical shape evaluation, in this paper we explore how
artificial intelligence (AI) driven models can be applied. Considering musical
shape evaluation as a classification problem, a light-weight Siamese residual
neural network (S-ResNN) is proposed to automatically identify musical shapes.
To assess the proposed approach in the context of piano musical shape
evaluation, we have generated a new dataset, containing 4116 music pieces
derived by 147 piano preparatory exercises and performed in 28 categories of
musical shapes. The experimental results show that the S-ResNN significantly
outperforms a number of benchmark methods in terms of the precision, recall and
F1 score.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>X.Li, S.Weiss, Y.Yan, Y.Li, J.Ren, J.Soraghan, M.Gong,"Siamese
  residual neural network for musical shape evaluation in piano performance
  assessment" in Proc. of the 31st European Signal Processing Conference,
  Helsinki, Finland</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hierarchical Aligned Multimodal Learning for NER on Tweet Posts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.08372v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.08372v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peipei Liu, Hong Li, Yimo Ren, Jie Liu, Shuaizong Si, Hongsong Zhu, Limin Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mining structured knowledge from tweets using named entity recognition (NER)
can be beneficial for many down stream applications such as recommendation and
intention understanding. With tweet posts tending to be multimodal, multimodal
named entity recognition (MNER) has attracted more attention. In this paper, we
propose a novel approach, which can dynamically align the image and text
sequence and achieve the multi-level cross-modal learning to augment textual
word representation for MNER improvement. To be specific, our framework can be
split into three main stages: the first stage focuses on intra-modality
representation learning to derive the implicit global and local knowledge of
each modality, the second evaluates the relevance between the text and its
accompanying image and integrates different grained visual information based on
the relevance, the third enforces semantic refinement via iterative cross-modal
interactions and co-attention. We conduct experiments on two open datasets, and
the results and detailed analysis demonstrate the advantage of our model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Models as Masked Audio-Video Learners <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.03937v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.03937v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elvis Nunez, Yanzi Jin, Mohammad Rastegari, Sachin Mehta, Maxwell Horton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past several years, the synchronization between audio and visual
signals has been leveraged to learn richer audio-visual representations. Aided
by the large availability of unlabeled videos, many unsupervised training
frameworks have demonstrated impressive results in various downstream audio and
video tasks. Recently, Masked Audio-Video Learners (MAViL) has emerged as a
state-of-the-art audio-video pre-training framework. MAViL couples contrastive
learning with masked autoencoding to jointly reconstruct audio spectrograms and
video frames by fusing information from both modalities. In this paper, we
study the potential synergy between diffusion models and MAViL, seeking to
derive mutual benefits from these two frameworks. The incorporation of
diffusion into MAViL, combined with various training efficiency methodologies
that include the utilization of a masking ratio curriculum and adaptive batch
sizing, results in a notable 32% reduction in pre-training Floating-Point
Operations (FLOPS) and an 18% decrease in pre-training wall clock time.
Crucially, this enhanced efficiency does not compromise the model's performance
in downstream audio-classification tasks when compared to MAViL's performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version for the Machine Learning for Audio Workshop at
  NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-01-03T00:00:00Z">2024-01-03</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">48</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Theoretical guarantees on the best-of-n alignment policy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01879v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01879v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Beirami, Alekh Agarwal, Jonathan Berant, Alexander D'Amour, Jacob Eisenstein, Chirag Nagpal, Ananda Theertha Suresh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A simple and effective method for the alignment of generative models is the
best-of-$n$ policy, where $n$ samples are drawn from a base policy, and ranked
based on a reward function, and the highest ranking one is selected. A commonly
used analytical expression in the literature claims that the KL divergence
between the best-of-$n$ policy and the base policy is equal to $\log (n) -
(n-1)/n.$ We disprove the validity of this claim, and show that it is an upper
bound on the actual KL divergence. We also explore the tightness of this upper
bound in different regimes. Finally, we propose a new estimator for the KL
divergence and empirically show that it provides a tight approximation through
a few examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Vision Check-up for Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pratyusha Sharma, Tamar Rott Shaham, Manel Baradad, Stephanie Fu, Adrian Rodriguez-Munoz, Shivam Duggal, Phillip Isola, Antonio Torralba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What does learning to model relationships between strings teach large
language models (LLMs) about the visual world? We systematically evaluate LLMs'
abilities to generate and recognize an assortment of visual concepts of
increasing complexity and then demonstrate how a preliminary visual
representation learning system can be trained using models of text. As language
models lack the ability to consume or output visual information as pixels, we
use code to represent images in our study. Although LLM-generated images do not
look like natural images, results on image generation and the ability of models
to correct these generated images indicate that precise modeling of strings can
teach language models about numerous aspects of the visual world. Furthermore,
experiments on self-supervised visual representation learning, utilizing images
generated with text models, highlight the potential to train vision models
capable of making semantic assessments of natural images using just LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multilingual Instruction Tuning With Just a Pinch of Multilinguality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uri Shaham, Jonathan Herzig, Roee Aharoni, Idan Szpektor, Reut Tsarfaty, Matan Eyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As instruction-tuned large language models (LLMs) gain global adoption, their
ability to follow instructions in multiple languages becomes increasingly
crucial. One promising approach is cross-lingual transfer, where a model
acquires specific functionality on some language by finetuning on another
language. In this work, we investigate how multilinguality during instruction
tuning of a multilingual LLM affects instruction-following across languages. We
first show that many languages transfer some instruction-following capabilities
to other languages from even monolingual tuning. Furthermore, we find that only
40 multilingual examples in an English tuning set substantially improve
multilingual instruction-following, both in seen and unseen languages during
tuning. In general, we observe that models tuned on multilingual mixtures
exhibit comparable or superior performance in several languages compared to
monolingually tuned models, despite training on 10x fewer examples in those
languages. Finally, we find that increasing the number of languages in the
instruction tuning set from 1 to only 2, 3, or 4 increases cross-lingual
generalization. Our results suggest that building massively multilingual
instruction-tuned models can be done with only a very small set of multilingual
instruction-responses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating Semi-Supervised Learning Algorithms in Text <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01843v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01843v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Himmet Toprak Kesgin, Mehmet Fatih Amasyali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using large training datasets enhances the generalization capabilities of
neural networks. Semi-supervised learning (SSL) is useful when there are few
labeled data and a lot of unlabeled data. SSL methods that use data
augmentation are most successful for image datasets. In contrast, texts do not
have consistent augmentation methods as images. Consequently, methods that use
augmentation are not as effective in text data as they are in image data. In
this study, we compared SSL algorithms that do not require augmentation; these
are self-training, co-training, tri-training, and tri-training with
disagreement. In the experiments, we used 4 different text datasets for
different tasks. We examined the algorithms from a variety of perspectives by
asking experiment questions and suggested several improvements. Among the
algorithms, tri-training with disagreement showed the closest performance to
the Oracle; however, performance gap shows that new semi-supervised algorithms
or improvements in existing methods are needed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Innovations in Intelligent Systems and Applications Conference (ASYU)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Iterative Mask Filling: An Effective Text Augmentation Method Using
  Masked Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Himmet Toprak Kesgin, Mehmet Fatih Amasyali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data augmentation is an effective technique for improving the performance of
machine learning models. However, it has not been explored as extensively in
natural language processing (NLP) as it has in computer vision. In this paper,
we propose a novel text augmentation method that leverages the Fill-Mask
feature of the transformer-based BERT model. Our method involves iteratively
masking words in a sentence and replacing them with language model predictions.
We have tested our proposed method on various NLP tasks and found it to be
effective in many cases. Our results are presented along with a comparison to
existing augmentation methods. Experimental results show that our proposed
method significantly improves performance, especially on topic classification
datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in International Conference on Advanced Engineering,
  Technology and Applications (ICAETA 2023). The final version is available
  online at https://link.springer.com/chapter/10.1007/978-3-031-50920-9_35</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physio: An LLM-Based Physiotherapy Advisor 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rúben Almeida, Hugo Sousa, Luís F. Cunha, Nuno Guimarães, Ricardo Campos, Alípio Jorge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The capabilities of the most recent language models have increased the
interest in integrating them into real-world applications. However, the fact
that these models generate plausible, yet incorrect text poses a constraint
when considering their use in several domains. Healthcare is a prime example of
a domain where text-generative trustworthiness is a hard requirement to
safeguard patient well-being. In this paper, we present Physio, a chat-based
application for physical rehabilitation. Physio is capable of making an initial
diagnosis while citing reliable health sources to support the information
provided. Furthermore, drawing upon external knowledge databases, Physio can
recommend rehabilitation exercises and over-the-counter medication for symptom
relief. By combining these features, Physio can leverage the power of
generative models for language processing while also conditioning its response
on dependable and verifiable sources. A live demo of Physio is available at
https://physio.inesctec.pt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Demo, ECIR 2024, 3rd Sword AI challenge 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Navigating Uncertainty: Optimizing API Dependency for Hallucination
  Reduction in Closed-Book Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre Erbacher, Louis Falissar, Vincent Guigue, Laure Soulier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Large Language Models (LLM) are able to accumulate and restore
knowledge, they are still prone to hallucination. Especially when faced with
factual questions, LLM cannot only rely on knowledge stored in parameters to
guarantee truthful and correct answers. Augmenting these models with the
ability to search on external information sources, such as the web, is a
promising approach to ground knowledge to retrieve information. However,
searching in a large collection of documents introduces additional
computational/time costs. An optimal behavior would be to query external
resources only when the LLM is not confident about answers. In this paper, we
propose a new LLM able to self-estimate if it is able to answer directly or
needs to request an external tool. We investigate a supervised approach by
introducing a hallucination masking mechanism in which labels are generated
using a close book question-answering task. In addition, we propose to leverage
parameter-efficient fine-tuning techniques to train our model on a small amount
of data. Our model directly provides answers for $78.2\%$ of the known queries
and opts to search for $77.2\%$ of the unknown ones. This results in the API
being utilized only $62\%$ of the time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-target Stance Detection by Exploiting Target Analytical
  Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daijun Ding, Rong Chen, Bowen Zhang, Xu Huang, Li Dong, Xiaowen Zhao, Ge Song, Liwen Jing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-target stance detection (CTSD) is an important task, which infers the
attitude of the destination target by utilizing annotated data derived from the
source target. One important approach in CTSD is to extract domain-invariant
features to bridge the knowledge gap between multiple targets. However, the
analysis of informal and short text structure, and implicit expressions,
complicate the extraction of domain-invariant knowledge. In this paper, we
propose a Multi-Perspective Prompt-Tuning (MPPT) model for CTSD that uses the
analysis perspective as a bridge to transfer knowledge. First, we develop a
two-stage instruct-based chain-of-thought method (TsCoT) to elicit target
analysis perspectives and provide natural language explanations (NLEs) from
multiple viewpoints by formulating instructions based on large language model
(LLM). Second, we propose a multi-perspective prompt-tuning framework
(MultiPLN) to fuse the NLEs into the stance predictor. Extensive experiments
results demonstrate the superiority of MPPT against the state-of-the-art
baseline methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VGA: Vision and Graph Fused Attention Network for Rumor Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Bai, Caiyan Jia, Ziying Song, Chaoqun Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of social media, rumors have been spread broadly on
social media platforms, causing great harm to society. Beside textual
information, many rumors also use manipulated images or conceal textual
information within images to deceive people and avoid being detected, making
multimodal rumor detection be a critical problem. The majority of multimodal
rumor detection methods mainly concentrate on extracting features of source
claims and their corresponding images, while ignoring the comments of rumors
and their propagation structures. These comments and structures imply the
wisdom of crowds and are proved to be crucial to debunk rumors. Moreover, these
methods usually only extract visual features in a basic manner, seldom consider
tampering or textual information in images. Therefore, in this study, we
propose a novel Vision and Graph Fused Attention Network (VGA) for rumor
detection to utilize propagation structures among posts so as to obtain the
crowd opinions and further explore visual tampering features, as well as the
textual information hidden in images. We conduct extensive experiments on three
datasets, demonstrating that VGA can effectively detect multimodal rumors and
outperform state-of-the-art methods significantly.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating Large Language Models in Semantic Parsing for Conversational
  Question Answering over Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phillip Schneider, Manuel Klettner, Kristiina Jokinen, Elena Simperl, Florian Matthes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational question answering systems often rely on semantic parsing to
enable interactive information retrieval, which involves the generation of
structured database queries from a natural language input. For
information-seeking conversations about facts stored within a knowledge graph,
dialogue utterances are transformed into graph queries in a process that is
called knowledge-based conversational question answering. This paper evaluates
the performance of large language models that have not been explicitly
pre-trained on this task. Through a series of experiments on an extensive
benchmark dataset, we compare models of varying sizes with different prompting
techniques and identify common issue types in the generated output. Our results
demonstrate that large language models are capable of generating graph queries
from dialogues, with significant improvements achievable through few-shot
prompting and fine-tuning techniques, especially for smaller models that
exhibit lower zero-shot performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICAART 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WordArt Designer API: User-Driven Artistic Typography Synthesis with
  Large Language Models on ModelScope <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun-Yan He, Zhi-Qi Cheng, Chenyang Li, Jingdong Sun, Wangmeng Xiang, Yusen Hu, Xianhui Lin, Xiaoyang Kang, Zengke Jin, Bin Luo, Yifeng Geng, Xuansong Xie, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the WordArt Designer API, a novel framework for
user-driven artistic typography synthesis utilizing Large Language Models
(LLMs) on ModelScope. We address the challenge of simplifying artistic
typography for non-professionals by offering a dynamic, adaptive, and
computationally efficient alternative to traditional rigid templates. Our
approach leverages the power of LLMs to understand and interpret user input,
facilitating a more intuitive design process. We demonstrate through various
case studies how users can articulate their aesthetic preferences and
functional requirements, which the system then translates into unique and
creative typographic designs. Our evaluations indicate significant improvements
in user satisfaction, design flexibility, and creative expression over existing
systems. The WordArt Designer API not only democratizes the art of typography
but also opens up new possibilities for personalized digital communication and
design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Spotlight Paper from the Workshop on Machine Learning for Creativity
  and Design at the 37th Conference on Neural Information Processing Systems
  (NeurIPS 2023). 5 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Patterns of Persistence and Diffusibility across World's Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01698v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01698v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyi Chen, Johannes Bjerva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language similarities can be caused by genetic relatedness, areal contact,
universality, or chance. Colexification, i.e.~a type of similarity where a
single lexical form is used to convey multiple meanings, is underexplored. In
our work, we shed light on the linguistic causes of cross-lingual similarity in
colexification and phonology, by exploring genealogical stability (persistence)
and contact-induced change (diffusibility). We construct large-scale graphs
incorporating semantic, genealogical, phonological and geographical data for
1,966 languages. We then show the potential of this resource, by investigating
several established hypotheses from previous work in linguistics, while
proposing new ones. Our results strongly support a previously established
hypothesis in the linguistic literature, while offering contradicting evidence
to another. Our large scale resource opens for further research across
disciplines, e.g.~in multilingual NLP and comparative linguistics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting challenge moments from students' discourse: A comparison of
  GPT-4 to two traditional natural language processing approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01692v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01692v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wannapon Suraworachet, Jennifer Seon, Mutlu Cukurova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective collaboration requires groups to strategically regulate themselves
to overcome challenges. Research has shown that groups may fail to regulate due
to differences in members' perceptions of challenges which may benefit from
external support. In this study, we investigated the potential of leveraging
three distinct natural language processing models: an expert knowledge
rule-based model, a supervised machine learning (ML) model and a Large Language
model (LLM), in challenge detection and challenge dimension identification
(cognitive, metacognitive, emotional and technical/other challenges) from
student discourse, was investigated. The results show that the supervised ML
and the LLM approaches performed considerably well in both tasks, in contrast
to the rule-based approach, whose efficacy heavily relies on the engineered
features by experts. The paper provides an extensive discussion of the three
approaches' performance for automated detection and support of students'
challenge moments in collaborative learning activities. It argues that,
although LLMs provide many advantages, they are unlikely to be the panacea to
issues of the detection and feedback provision of socially shared regulation of
learning due to their lack of reliability, as well as issues of validity
evaluation, privacy and confabulation. We conclude the paper with a discussion
on additional considerations, including model transparency to explore feasible
and meaningful analytical feedback for students and educators using LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MLPs Compass: What is learned when MLPs are combined with PLMs? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01667v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01667v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Zhou, Wenyu Chen, Yong Cao, Dingyi Zeng, Wanlong Liu, Hong Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Transformer-based pre-trained language models and their variants
exhibit strong semantic representation capabilities, the question of
comprehending the information gain derived from the additional components of
PLMs remains an open question in this field. Motivated by recent efforts that
prove Multilayer-Perceptrons (MLPs) modules achieving robust structural capture
capabilities, even outperforming Graph Neural Networks (GNNs), this paper aims
to quantify whether simple MLPs can further enhance the already potent ability
of PLMs to capture linguistic information. Specifically, we design a simple yet
effective probing framework containing MLPs components based on BERT structure
and conduct extensive experiments encompassing 10 probing tasks spanning three
distinct linguistic levels. The experimental results demonstrate that MLPs can
indeed enhance the comprehension of linguistic structure by PLMs. Our research
provides interpretable and valuable insights into crafting variations of PLMs
utilizing MLPs for tasks that emphasize diverse linguistic structures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Social Media Ready Caption Generation for Brands 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Himanshu Maheshwari, Koustava Goswami, Apoorv Saxena, Balaji Vasan Srinivasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social media advertisements are key for brand marketing, aiming to attract
consumers with captivating captions and pictures or logos. While previous
research has focused on generating captions for general images, incorporating
brand personalities into social media captioning remains unexplored. Brand
personalities are shown to be affecting consumers' behaviours and social
interactions and thus are proven to be a key aspect of marketing strategies.
Current open-source multimodal LLMs are not directly suited for this task.
Hence, we propose a pipeline solution to assist brands in creating engaging
social media captions that align with the image and the brand personalities.
Our architecture is based on two parts: a the first part contains an image
captioning model that takes in an image that the brand wants to post online and
gives a plain English caption; b the second part takes in the generated caption
along with the target brand personality and outputs a catchy
personality-aligned social media caption. Along with brand personality, our
system also gives users the flexibility to provide hashtags, Instagram handles,
URLs, and named entities they want the caption to contain, making the captions
more semantically related to the social media handles. Comparative evaluations
against various baselines demonstrate the effectiveness of our approach, both
qualitatively and quantitatively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can AI Be as Creative as Humans? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Wang, James Zou, Michael Mozer, Linjun Zhang, Anirudh Goyal, Alex Lamb, Zhun Deng, Michael Qizhe Xie, Hannah Brown, Kenji Kawaguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creativity serves as a cornerstone for societal progress and innovation, but
its assessment remains a complex and often subjective endeavor. With the rise
of advanced generative AI models capable of tasks once reserved for human
creativity, the study of AI's creative potential becomes imperative for its
responsible development and application. This paper addresses the complexities
in defining and evaluating creativity by introducing a new concept called
Relative Creativity. Instead of trying to define creativity universally, we
shift the focus to whether AI can match the creative abilities of a
hypothetical human. This perspective draws inspiration from the Turing Test,
expanding upon it to address the challenges and subjectivities inherent in
evaluating creativity. This methodological shift facilitates a statistically
quantifiable evaluation of AI's creativity, which we term Statistical
Creativity. This approach allows for direct comparisons of AI's creative
abilities with those of specific human groups. Building on this foundation, we
discuss the application of statistical creativity in contemporary
prompt-conditioned autoregressive models. In addition to defining and analyzing
a measure of creativity, we introduce an actionable training guideline,
effectively bridging the gap between theoretical quantification of creativity
and practical model training. Through these multifaceted contributions, the
paper establishes a cohesive, continuously evolving, and transformative
framework for assessing and fostering statistical creativity in AI models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper presents "Relative Creativity," comparing AI creativity to
  human creativity, inspired by the Turing Test. It introduces "Statistical
  Creativity" for measurable assessment and provides AI training guidelines to
  foster AI's creative capabilities</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Model Capabilities in Perioperative Risk Prediction and
  Prognostication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01620v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01620v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Chung, Christine T Fong, Andrew M Walters, Nima Aghaeepour, Meliha Yetisgen, Vikas N O'Reilly-Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate whether general-domain large language models such as GPT-4
Turbo can perform risk stratification and predict post-operative outcome
measures using a description of the procedure and a patient's clinical notes
derived from the electronic health record. We examine predictive performance on
8 different tasks: prediction of ASA Physical Status Classification, hospital
admission, ICU admission, unplanned admission, hospital mortality, PACU Phase 1
duration, hospital duration, and ICU duration. Few-shot and chain-of-thought
prompting improves predictive performance for several of the tasks. We achieve
F1 scores of 0.50 for ASA Physical Status Classification, 0.81 for ICU
admission, and 0.86 for hospital mortality. Performance on duration prediction
tasks were universally poor across all prompt strategies. Current generation
large language models can assist clinicians in perioperative risk
stratification on classification tasks and produce high-quality natural
language summaries and explanations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GPT-4V(ision) is a Generalist Web Agent, if Grounded 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, Yu Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent development on large multimodal models (LMMs), especially
GPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries
of multimodal models beyond traditional tasks like image captioning and visual
question answering. In this work, we explore the potential of LMMs like GPT-4V
as a generalist web agent that can follow natural language instructions to
complete tasks on any given website. We propose SEEACT, a generalist web agent
that harnesses the power of LMMs for integrated visual understanding and acting
on the web. We evaluate on the recent MIND2WEB benchmark. In addition to
standard offline evaluation on cached websites, we enable a new online
evaluation setting by developing a tool that allows running web agents on live
websites. We show that GPT-4V presents a great potential for web agents - it
can successfully complete 50% of the tasks on live websites if we manually
ground its textual plans into actions on the websites. This substantially
outperforms text-only LLMs like GPT-4 or smaller models (FLAN-T5 and BLIP-2)
specifically fine-tuned for web agents. However, grounding still remains a
major challenge. Existing LMM grounding strategies like set-of-mark prompting
turns out not effective for web agents, and the best grounding strategy we
develop in this paper leverages both the HTML text and visuals. Yet, there is
still a substantial gap with oracle grounding, leaving ample room for further
improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PLLaMa: An Open-source Large Language Model for Plant Science 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianjun Yang, Junfeng Gao, Wenxin Xue, Erik Alexandersson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have exhibited remarkable capabilities in
understanding and interacting with natural language across various sectors.
However, their effectiveness is limited in specialized areas requiring high
accuracy, such as plant science, due to a lack of specific expertise in these
fields. This paper introduces PLLaMa, an open-source language model that
evolved from LLaMa-2. It's enhanced with a comprehensive database, comprising
more than 1.5 million scholarly articles in plant science. This development
significantly enriches PLLaMa with extensive knowledge and proficiency in plant
and agricultural sciences. Our initial tests, involving specific datasets
related to plants and agriculture, show that PLLaMa substantially improves its
understanding of plant science-related topics. Moreover, we have formed an
international panel of professionals, including plant scientists, agricultural
engineers, and plant breeders. This team plays a crucial role in verifying the
accuracy of PLLaMa's responses to various academic inquiries, ensuring its
effective and reliable application in the field. To support further research
and development, we have made the model's checkpoints and source codes
accessible to the scientific community. These resources are available for
download at \url{https://github.com/Xianjun-Yang/PLLaMa}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MedSumm: A Multimodal Approach to Summarizing Code-Mixed Hindi-English
  Clinical Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akash Ghosh, Arkadeep Acharya, Prince Jha, Aniket Gaudgaul, Rajdeep Majumdar, Sriparna Saha, Aman Chadha, Raghav Jain, Setu Sinha, Shivani Agarwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the healthcare domain, summarizing medical questions posed by patients is
critical for improving doctor-patient interactions and medical decision-making.
Although medical data has grown in complexity and quantity, the current body of
research in this domain has primarily concentrated on text-based methods,
overlooking the integration of visual cues. Also prior works in the area of
medical question summarisation have been limited to the English language. This
work introduces the task of multimodal medical question summarization for
codemixed input in a low-resource setting. To address this gap, we introduce
the Multimodal Medical Codemixed Question Summarization MMCQS dataset, which
combines Hindi-English codemixed medical queries with visual aids. This
integration enriches the representation of a patient's medical condition,
providing a more comprehensive perspective. We also propose a framework named
MedSumm that leverages the power of LLMs and VLMs for this task. By utilizing
our MMCQS dataset, we demonstrate the value of integrating visual information
from images to improve the creation of medically detailed summaries. This
multimodal strategy not only improves healthcare decision-making but also
promotes a deeper comprehension of patient queries, paving the way for future
exploration in personalized and responsive medical care. Our dataset, code, and
pre-trained models will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECIR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hallucinations in Neural Automatic Speech Recognition: Identifying
  Errors and Hallucinatory Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rita Frieske, Bertram E. Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucinations are a type of output error produced by deep neural networks.
While this has been studied in natural language processing, they have not been
researched previously in automatic speech recognition. Here, we define
hallucinations in ASR as transcriptions generated by a model that are
semantically unrelated to the source utterance, yet still fluent and coherent.
The similarity of hallucinations to probable natural language outputs of the
model creates a danger of deception and impacts the credibility of the system.
We show that commonly used metrics, such as word error rates, cannot
differentiate between hallucinatory and non-hallucinatory models. To address
this, we propose a perturbation-based method for assessing the susceptibility
of an automatic speech recognition (ASR) model to hallucination at test time,
which does not require access to the training dataset. We demonstrate that this
method helps to distinguish between hallucinatory and non-hallucinatory models
that have similar baseline word error rates. We further explore the
relationship between the types of ASR errors and the types of dataset noise to
determine what types of noise are most likely to create hallucinatory outputs.
We devise a framework for identifying hallucinations by analysing their
semantic connection with the ground truth and their fluency. Finally, we
discover how to induce hallucinations with a random noise injection to the
utterance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GOAT-Bench: Safety Insights to Large Multimodal Models through
  Meme-Based Social Abuse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01523v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01523v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongzhan Lin, Ziyang Luo, Bo Wang, Ruichao Yang, Jing Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exponential growth of social media has profoundly transformed how
information is created, disseminated, and absorbed, exceeding any precedent in
the digital age. Regrettably, this explosion has also spawned a significant
increase in the online abuse of memes. Evaluating the negative impact of memes
is notably challenging, owing to their often subtle and implicit meanings,
which are not directly conveyed through the overt text and imagery. In light of
this, large multimodal models (LMMs) have emerged as a focal point of interest
due to their remarkable capabilities in handling diverse multimodal tasks. In
response to this development, our paper aims to thoroughly examine the capacity
of various LMMs (e.g. GPT-4V) to discern and respond to the nuanced aspects of
social abuse manifested in memes. We introduce the comprehensive meme
benchmark, GOAT-Bench, comprising over 6K varied memes encapsulating themes
such as implicit hate speech, sexism, and cyberbullying, etc. Utilizing
GOAT-Bench, we delve into the ability of LMMs to accurately assess hatefulness,
misogyny, offensiveness, sarcasm, and harmful content. Our extensive
experiments across a range of LMMs reveal that current models still exhibit a
deficiency in safety awareness, showing insensitivity to various forms of
implicit abuse. We posit that this shortfall represents a critical impediment
to the realization of safe artificial intelligence. The GOAT-Bench and
accompanying resources are publicly accessible at https://goatlmm.github.io/,
contributing to ongoing research in this vital field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Utilizing Neural Transducers for Two-Stage Text-to-Speech via Semantic
  Token Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01498v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01498v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minchan Kim, Myeonghun Jeong, Byoung Jin Choi, Semin Kim, Joun Yeop Lee, Nam Soo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel text-to-speech (TTS) framework centered around a neural
transducer. Our approach divides the whole TTS pipeline into semantic-level
sequence-to-sequence (seq2seq) modeling and fine-grained acoustic modeling
stages, utilizing discrete semantic tokens obtained from wav2vec2.0 embeddings.
For a robust and efficient alignment modeling, we employ a neural transducer
named token transducer for the semantic token prediction, benefiting from its
hard monotonic alignment constraints. Subsequently, a non-autoregressive (NAR)
speech generator efficiently synthesizes waveforms from these semantic tokens.
Additionally, a reference speech controls temporal dynamics and acoustic
conditions at each stage. This decoupled framework reduces the training
complexity of TTS while allowing each stage to focus on semantic and acoustic
modeling. Our experimental results on zero-shot adaptive TTS demonstrate that
our model surpasses the baseline in terms of speech quality and speaker
similarity, both objectively and subjectively. We also delve into the inference
speed and prosody control capabilities of our approach, highlighting the
potential of neural transducers in TTS frameworks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Two-Stage Multimodal Emotion Recognition Model Based on Graph
  Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Ai, FuChen Zhang, Tao Meng, YunTao Shou, HongEn Shao, Keqin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In terms of human-computer interaction, it is becoming more and more
important to correctly understand the user's emotional state in a conversation,
so the task of multimodal emotion recognition (MER) started to receive more
attention. However, existing emotion classification methods usually perform
classification only once. Sentences are likely to be misclassified in a single
round of classification. Previous work usually ignores the similarities and
differences between different morphological features in the fusion process. To
address the above issues, we propose a two-stage emotion recognition model
based on graph contrastive learning (TS-GCL). First, we encode the original
dataset with different preprocessing modalities. Second, a graph contrastive
learning (GCL) strategy is introduced for these three modal data with other
structures to learn similarities and differences within and between modalities.
Finally, we use MLP twice to achieve the final emotion classification. This
staged classification method can help the model to better focus on different
levels of emotional information, thereby improving the performance of the
model. Extensive experiments show that TS-GCL has superior performance on
IEMOCAP and MELD datasets compared with previous methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Natural Language Processing and Multimodal Stock Price Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01487v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01487v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Taylor, Jerry Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of financial decision-making, predicting stock prices is
pivotal. Artificial intelligence techniques such as long short-term memory
networks (LSTMs), support-vector machines (SVMs), and natural language
processing (NLP) models are commonly employed to predict said prices. This
paper utilizes stock percentage change as training data, in contrast to the
traditional use of raw currency values, with a focus on analyzing publicly
released news articles. The choice of percentage change aims to provide models
with context regarding the significance of price fluctuations and overall price
change impact on a given stock. The study employs specialized BERT natural
language processing models to predict stock price trends, with a particular
emphasis on various data modalities. The results showcase the capabilities of
such strategies with a small natural language processing model to accurately
predict overall stock trends, and highlight the effectiveness of certain data
features and sector-specific data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A First Look at Information Highlighting in Stack Overflow Answers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahla Shaan Ahmed, Shaowei Wang, Yuan Tian,  Tse-Hsun,  Chen, Haoxiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Context: Navigating the knowledge of Stack Overflow (SO) remains challenging.
To make the posts vivid to users, SO allows users to write and edit posts with
Markdown or HTML so that users can leverage various formatting styles (e.g.,
bold, italic, and code) to highlight the important information. Nonetheless,
there have been limited studies on the highlighted information. Objective: We
carried out the first large-scale exploratory study on the information
highlighted in SO answers in our recent study. To extend our previous study, we
develop approaches to automatically recommend highlighted content with
formatting styles using neural network architectures initially designed for the
Named Entity Recognition task. Method: In this paper, we studied 31,169,429
answers of Stack Overflow. For training recommendation models, we choose CNN
and BERT models for each type of formatting (i.e., Bold, Italic, Code, and
Heading) using the information highlighting dataset we collected from SO
answers. Results: Our models based on CNN architecture achieve precision
ranging from 0.71 to 0.82. The trained model for automatic code content
highlighting achieves a recall of 0.73 and an F1 score of 0.71, outperforming
the trained models for other formatting styles. The BERT models have even lower
recalls and F1 scores than the CNN models. Our analysis of failure cases
indicates that the majority of the failure cases are missing identification
(i.e., the model misses the content that is supposed to be highlighted) due to
the models tend to learn the frequently highlighted words while struggling to
learn less frequent words. Conclusion: Our findings suggest that it is possible
to develop recommendation models for highlighting information for answers with
different formatting styles on Stack Overflow.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work is submitted to Information and Software Technology Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Question-Answering Based Summarization of Electronic Health Records
  using Retrieval Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Walid Saba, Suzanne Wendelken, James. Shanahan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Summarization of electronic health records (EHRs) can substantially minimize
'screen time' for both patients as well as medical personnel. In recent years
summarization of EHRs have employed machine learning pipelines using state of
the art neural models. However, these models have produced less than adequate
results that are attributed to the difficulty of obtaining sufficient annotated
data for training. Moreover, the requirement to consider the entire content of
an EHR in summarization has resulted in poor performance due to the fact that
attention mechanisms in modern large language models (LLMs) adds a quadratic
complexity in terms of the size of the input. We propose here a method that
mitigates these shortcomings by combining semantic search, retrieval augmented
generation (RAG) and question-answering using the latest LLMs. In our approach
summarization is the extraction of answers to specific questions that are
deemed important by subject-matter experts (SMEs). Our approach is quite
efficient; requires minimal to no training; does not suffer from the
'hallucination' problem of LLMs; and it ensures diversity, since the summary
will not have repeated content but diverse answers to specific questions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Zero-Shot Abstractive Summarization in the Era of Large
  Language Models from the Perspective of Position Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01989v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01989v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anshuman Chhabra, Hadi Askari, Prasant Mohapatra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We characterize and study zero-shot abstractive summarization in Large
Language Models (LLMs) by measuring position bias, which we propose as a
general formulation of the more restrictive lead bias phenomenon studied
previously in the literature. Position bias captures the tendency of a model
unfairly prioritizing information from certain parts of the input text over
others, leading to undesirable behavior. Through numerous experiments on four
diverse real-world datasets, we study position bias in multiple LLM models such
as GPT 3.5-Turbo, Llama-2, and Dolly-v2, as well as state-of-the-art pretrained
encoder-decoder abstractive summarization models such as Pegasus and BART. Our
findings lead to novel insights and discussion on performance and position bias
of models for zero-shot summarization tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Mechanistic Understanding of Alignment Algorithms: A Case Study on DPO
  and Toxicity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Lee, Xiaoyan Bai, Itamar Pres, Martin Wattenberg, Jonathan K. Kummerfeld, Rada Mihalcea
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While alignment algorithms are now commonly used to tune pre-trained language
models towards a user's preferences, we lack explanations for the underlying
mechanisms in which models become ``aligned'', thus making it difficult to
explain phenomena like jailbreaks. In this work we study a popular algorithm,
direct preference optimization (DPO), and the mechanisms by which it reduces
toxicity. Namely, we first study how toxicity is represented and elicited in a
pre-trained language model, GPT2-medium. We then apply DPO with a carefully
crafted pairwise dataset to reduce toxicity. We examine how the resulting model
averts toxic outputs, and find that capabilities learned from pre-training are
not removed, but rather bypassed. We use this insight to demonstrate a simple
method to un-align the model, reverting it back to its toxic behavior.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Instruct-Imagen: Image Generation with Multi-modal Instruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01952v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01952v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hexiang Hu, Kelvin C. K. Chan, Yu-Chuan Su, Wenhu Chen, Yandong Li, Kihyuk Sohn, Yang Zhao, Xue Ben, Boqing Gong, William Cohen, Ming-Wei Chang, Xuhui Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents instruct-imagen, a model that tackles heterogeneous image
generation tasks and generalizes across unseen tasks. We introduce *multi-modal
instruction* for image generation, a task representation articulating a range
of generation intents with precision. It uses natural language to amalgamate
disparate modalities (e.g., text, edge, style, subject, etc.), such that
abundant generation intents can be standardized in a uniform format.
  We then build instruct-imagen by fine-tuning a pre-trained text-to-image
diffusion model with a two-stage framework. First, we adapt the model using the
retrieval-augmented training, to enhance model's capabilities to ground its
generation on external multimodal context. Subsequently, we fine-tune the
adapted model on diverse image generation tasks that requires vision-language
understanding (e.g., subject-driven generation, etc.), each paired with a
multi-modal instruction encapsulating the task's essence. Human evaluation on
various image generation datasets reveals that instruct-imagen matches or
surpasses prior task-specific models in-domain and demonstrates promising
generalization to unseen and more complex tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalist embedding models are better at short-context clinical
  semantic search than specialized embedding models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01943v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01943v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean-Baptiste Excoffier, Tom Roehr, Alexei Figueroa, Michalis Papaaioannou, Keno Bressem, Matthieu Ortala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing use of tools and solutions based on Large Language Models
(LLMs) for various tasks in the medical domain has become a prominent trend.
Their use in this highly critical and sensitive domain has thus raised
important questions about their robustness, especially in response to
variations in input, and the reliability of the generated outputs. This study
addresses these questions by constructing a textual dataset based on the
ICD-10-CM code descriptions, widely used in US hospitals and containing many
clinical terms, and their easily reproducible rephrasing. We then benchmarked
existing embedding models, either generalist or specialized in the clinical
domain, in a semantic search task where the goal was to correctly match the
rephrased text to the original description. Our results showed that generalist
models performed better than clinical models, suggesting that existing clinical
specialized models are more sensitive to small changes in input that confuse
them. The highlighted problem of specialized models may be due to the fact that
they have not been trained on sufficient data, and in particular on datasets
that are not diverse enough to have a reliable global language understanding,
which is still necessary for accurate handling of medical documents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 1 figure, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AstroLLaMA-Chat: Scaling AstroLLaMA with Conversational and Diverse
  <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ernest Perkowski, Rui Pan, Tuan Dung Nguyen, Yuan-Sen Ting, Sandor Kruk, Tong Zhang, Charlie O'Neill, Maja Jablonska, Michael J. Smith, Kevin Schawinski, Kartheik Iyer, Ioana Ciucă for UniverseTBD
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the potential of enhancing LLM performance in astronomy-focused
question-answering through targeted, continual pre-training. By employing a
compact 7B-parameter LLaMA-2 model and focusing exclusively on a curated set of
astronomy corpus -- comprising abstracts, introductions, and conclusions -- we
achieve notable improvements in specialized topic comprehension. While general
LLMs like GPT-4 outperform in broader question-answering scenarios due to
superior reasoning capabilities, our findings suggest that continual
pre-training with limited resources can still enhance model performance on
specialized topics. Additionally, we present an extension of AstroLLaMA: the
fine-tuning of the 7B LLaMA model on a domain-specific conversational dataset,
culminating in the release of the chat-enabled AstroLLaMA for community use.
Comprehensive quantitative benchmarking is currently in progress and will be
detailed in an upcoming full paper. The model, AstroLLaMA-Chat, is now
available at https://huggingface.co/universeTBD, providing the first
open-source conversational AI tool tailored for the astronomy community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 1 figure, model is available at
  https://huggingface.co/universeTBD, submitted to RNAAS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive Survey of Hallucination Mitigation Techniques in Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01313v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01313v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. M Towhidul Islam Tonmoy, S M Mehedi Zaman, Vinija Jain, Anku Rani, Vipula Rawte, Aman Chadha, Amitava Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) continue to advance in their ability to write
human-like text, a key challenge remains around their tendency to hallucinate
generating content that appears factual but is ungrounded. This issue of
hallucination is arguably the biggest hindrance to safely deploying these
powerful LLMs into real-world production systems that impact people's lives.
The journey toward widespread adoption of LLMs in practical settings heavily
relies on addressing and mitigating hallucinations. Unlike traditional AI
systems focused on limited tasks, LLMs have been exposed to vast amounts of
online text data during training. While this allows them to display impressive
language fluency, it also means they are capable of extrapolating information
from the biases in training data, misinterpreting ambiguous prompts, or
modifying the information to align superficially with the input. This becomes
hugely alarming when we rely on language generation capabilities for sensitive
applications, such as summarizing medical records, financial analysis reports,
etc. This paper presents a comprehensive survey of over 32 techniques developed
to mitigate hallucination in LLMs. Notable among these are Retrieval Augmented
Generation (Lewis et al, 2021), Knowledge Retrieval (Varshney et al,2023),
CoNLI (Lei et al, 2023), and CoVe (Dhuliawala et al, 2023). Furthermore, we
introduce a detailed taxonomy categorizing these methods based on various
parameters, such as dataset utilization, common tasks, feedback mechanisms, and
retriever types. This classification helps distinguish the diverse approaches
specifically designed to tackle hallucination issues in LLMs. Additionally, we
analyze the challenges and limitations inherent in these techniques, providing
a solid foundation for future research in addressing hallucinations and related
phenomena within the realm of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieval-Augmented Generation for Large Language Models: A Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10997v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10997v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfan Gao, Yun Xiong, Xinyu Gao, Kangxiang Jia, Jinliu Pan, Yuxi Bi, Yi Dai, Jiawei Sun, Qianyu Guo, Meng Wang, Haofen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) demonstrate significant capabilities but face
challenges such as hallucination, outdated knowledge, and non-transparent,
untraceable reasoning processes. Retrieval-Augmented Generation (RAG) has
emerged as a promising solution by incorporating knowledge from external
databases. This enhances the accuracy and credibility of the models,
particularly for knowledge-intensive tasks, and allows for continuous knowledge
updates and integration of domain-specific information. RAG synergistically
merges LLMs' intrinsic knowledge with the vast, dynamic repositories of
external databases. This comprehensive review paper offers a detailed
examination of the progression of RAG paradigms, encompassing the Naive RAG,
the Advanced RAG, and the Modular RAG. It meticulously scrutinizes the
tripartite foundation of RAG frameworks, which includes the retrieval , the
generation and the augmentation techniques. The paper highlights the
state-of-the-art technologies embedded in each of these critical components,
providing a profound understanding of the advancements in RAG systems.
Furthermore, this paper introduces the metrics and benchmarks for assessing RAG
models, along with the most up-to-date evaluation framework. In conclusion, the
paper delineates prospective avenues for research, including the identification
of challenges, the expansion of multi-modalities, and the progression of the
RAG infrastructure and its ecosystem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing Work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quality and Quantity of Machine Translation References for Automated
  Metrics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01283v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01283v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vilém Zouhar, Ondřej Bojar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic machine translation metrics often use human translations to
determine the quality system translations. Common wisdom in the field dictates
that the human references should be of very high quality. However, there are no
cost-benefit analyses that could be used to guide practitioners who plan to
collect references for machine translation evaluation. We find that
higher-quality references lead to better metric correlations with humans at the
segment-level. Having up to 7 references per segment and taking their average
helps all metrics. Interestingly, the references from vendors of different
qualities can be mixed together and improve metric success. Higher quality
references, however, cost more to create and we frame this as an optimization
problem: given a specific budget, what references should be collected to
maximize metric success. These findings can be used by evaluators of shared
tasks when references need to be created under a certain budget.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Survey of Text Watermarking in the Era of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07913v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07913v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aiwei Liu, Leyi Pan, Yijian Lu, Jingjing Li, Xuming Hu, Lijie Wen, Irwin King, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text watermarking algorithms play a crucial role in the copyright protection
of textual content, yet their capabilities and application scenarios have been
limited historically. The recent developments in large language models (LLMs)
have opened new opportunities for the advancement of text watermarking
techniques. LLMs not only enhance the capabilities of text watermarking
algorithms through their text understanding and generation abilities but also
necessitate the use of text watermarking algorithms for their own copyright
protection. This paper conducts a comprehensive survey of the current state of
text watermarking technology, covering four main aspects: (1) an overview and
comparison of different text watermarking techniques; (2) evaluation methods
for text watermarking algorithms, including their success rates, impact on text
quality, robustness, and unforgeability; (3) potential application scenarios
for text watermarking technology; (4) current challenges and future directions
for development. This survey aims to provide researchers with a thorough
understanding of text watermarking technology, thereby promoting its further
advancement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Jina Embeddings 2: 8192-Token General-Purpose Text Embeddings for Long
  Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19923v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19923v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Günther, Jackmin Ong, Isabelle Mohr, Alaeddine Abdessalem, Tanguy Abel, Mohammad Kalim Akram, Susana Guzman, Georgios Mastrapas, Saba Sturua, Bo Wang, Maximilian Werk, Nan Wang, Han Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text embedding models have emerged as powerful tools for transforming
sentences into fixed-sized feature vectors that encapsulate semantic
information. While these models are essential for tasks like information
retrieval, semantic clustering, and text re-ranking, most existing open-source
models, especially those built on architectures like BERT, struggle to
represent lengthy documents and often resort to truncation. One common approach
to mitigate this challenge involves splitting documents into smaller paragraphs
for embedding. However, this strategy results in a much larger set of vectors,
consequently leading to increased memory consumption and computationally
intensive vector searches with elevated latency.
  To address these challenges, we introduce Jina Embeddings 2, an open-source
text embedding model capable of accommodating up to 8192 tokens. This model is
designed to transcend the conventional 512-token limit and adeptly process long
documents. Jina Embeddings 2 not only achieves state-of-the-art performance on
a range of embedding-related tasks in the MTEB benchmark but also matches the
performance of OpenAI's proprietary ada-002 model. Additionally, our
experiments indicate that an extended context can enhance performance in tasks
such as NarrativeQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EQ-Bench: An Emotional Intelligence <span class="highlight-title">Benchmark</span> for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06281v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06281v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel J. Paech
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce EQ-Bench, a novel benchmark designed to evaluate aspects of
emotional intelligence in Large Language Models (LLMs). We assess the ability
of LLMs to understand complex emotions and social interactions by asking them
to predict the intensity of emotional states of characters in a dialogue. The
benchmark is able to discriminate effectively between a wide range of models.
We find that EQ-Bench correlates strongly with comprehensive multi-domain
benchmarks like MMLU (Hendrycks et al., 2020) (r=0.97), indicating that we may
be capturing similar aspects of broad intelligence. Our benchmark produces
highly repeatable results using a set of 60 English-language questions. We also
provide open-source code for an automated benchmarking pipeline at
https://github.com/EQ-bench/EQ-Bench and a leaderboard at https://eqbench.com
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Understanding the Effects of RLHF on LLM Generalisation and Diversity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.06452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.06452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina, Eric Hambro, Edward Grefenstette, Roberta Raileanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) fine-tuned with reinforcement learning from
human feedback (RLHF) have been used in some of the most widely deployed AI
models to date, such as OpenAI's ChatGPT or Anthropic's Claude. % , or Meta's
LLaMA-2. While there has been significant work developing these methods, our
understanding of the benefits and downsides of each stage in RLHF is still
limited. To fill this gap, we present an extensive analysis of how each stage
of the process (i.e.~supervised fine-tuning (SFT), reward modelling, and RLHF)
affects two key properties: out-of-distribution (OOD) generalisation and output
diversity. OOD generalisation is crucial given the wide range of real-world
scenarios in which these models are being used, while output diversity refers
to the model's ability to generate varied outputs and is important for a
variety of use cases. We perform our analysis across two base models on both
summarisation and instruction following tasks, the latter being highly relevant
for current LLM use cases. We find that RLHF generalises better than SFT to new
inputs, particularly as the distribution shift between train and test becomes
larger. However, RLHF significantly reduces output diversity compared to SFT
across a variety of measures, implying a tradeoff in current LLM fine-tuning
methods between generalisation and diversity. Our results provide guidance on
which fine-tuning method should be used depending on the application, and show
that more research is needed to improve the tradeoff between generalisation and
diversity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available here: https://github.com/facebookresearch/rlfh-gen-div</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vietnamese Poem Generation & The Prospect Of Cross-Language Poem-To-Poem
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01078v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01078v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Triet Minh Huynh, Quan Le Bao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Poetry generation has been a challenging task in the field of Natural
Language Processing, as it requires the model to understand the nuances of
language, sentiment, and style. In this paper, we propose using Large Language
Models to generate Vietnamese poems from natural language prompts, thereby
facilitating an intuitive process with enhanced content control. Our most
efficacious model, the GPT-3 Babbage variant, achieves a custom evaluation
score of 0.8, specifically tailored to the "luc bat" genre of Vietnamese
poetry. Furthermore, we also explore the idea of paraphrasing poems into normal
text prompts and yield a relatively high score of 0.718 in the "luc bat" genre.
This experiment presents the potential for cross-Language poem-to-poem
translation with translated poems as the inputs while concurrently maintaining
complete control over the generated content.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fairness Certification for Natural Language Processing and Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Freiberger, Erik Buchmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Processing (NLP) plays an important role in our daily lives,
particularly due to the enormous progress of Large Language Models (LLM).
However, NLP has many fairness-critical use cases, e.g., as an expert system in
recruitment or as an LLM-based tutor in education. Since NLP is based on human
language, potentially harmful biases can diffuse into NLP systems and produce
unfair results, discriminate against minorities or generate legal issues.
Hence, it is important to develop a fairness certification for NLP approaches.
We follow a qualitative research approach towards a fairness certification for
NLP. In particular, we have reviewed a large body of literature on algorithmic
fairness, and we have conducted semi-structured expert interviews with a wide
range of experts from that area. We have systematically devised six fairness
criteria for NLP, which can be further refined into 18 sub-categories. Our
criteria offer a foundation for operationalizing and testing processes to
certify fairness, both from the perspective of the auditor and the audited
organization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In depth discussion of our results can be found in the Appendix B</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What's the Magic Word? A Control Theory of LLM Prompting <span class="chip">ICLR 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04444v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04444v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aman Bhargava, Cameron Witkowski, Manav Shah, Matt Thomson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt engineering is crucial for deploying LLMs but is poorly understood
mathematically. We formalize LLM systems as a class of discrete stochastic
dynamical systems to explore prompt engineering through the lens of control
theory. We investigate the reachable set of output token sequences $R_y(\mathbf
x_0)$ for which there exists a control input sequence $\mathbf u$ for each
$\mathbf y \in R_y(\mathbf x_0)$ that steers the LLM to output $\mathbf y$ from
initial state sequence $\mathbf x_0$. We offer analytic analysis on the
limitations on the controllability of self-attention in terms of reachable set,
where we prove an upper bound on the reachable set of outputs $R_y(\mathbf
x_0)$ as a function of the singular values of the parameter matrices. We
present complementary empirical analysis on the controllability of a panel of
LLMs, including Falcon-7b, Llama-7b, and Falcon-40b. Our results demonstrate a
lower bound on the reachable set of outputs $R_y(\mathbf x_0)$ w.r.t. initial
state sequences $\mathbf x_0$ sampled from the Wikitext dataset. We find that
the correct next Wikitext token following sequence $\mathbf x_0$ is reachable
over 97% of the time with prompts of $k\leq 10$ tokens. We also establish that
the top 75 most likely next tokens, as estimated by the LLM itself, are
reachable at least 85% of the time with prompts of $k\leq 10$ tokens.
Intriguingly, short prompt sequences can dramatically alter the likelihood of
specific outputs, even making the least likely tokens become the most likely
ones. This control-centric analysis of LLMs demonstrates the significant and
poorly understood role of input sequences in steering output probabilities,
offering a foundational perspective for enhancing language model system
capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 8 figures. Under review for ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LMBot: Distilling Graph Knowledge into Language Model for Graph-less
  Deployment in Twitter Bot Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.17408v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.17408v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijian Cai, Zhaoxuan Tan, Zhenyu Lei, Zifeng Zhu, Hongrui Wang, Qinghua Zheng, Minnan Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As malicious actors employ increasingly advanced and widespread bots to
disseminate misinformation and manipulate public opinion, the detection of
Twitter bots has become a crucial task. Though graph-based Twitter bot
detection methods achieve state-of-the-art performance, we find that their
inference depends on the neighbor users multi-hop away from the targets, and
fetching neighbors is time-consuming and may introduce bias. At the same time,
we find that after finetuning on Twitter bot detection, pretrained language
models achieve competitive performance and do not require a graph structure
during deployment. Inspired by this finding, we propose a novel bot detection
framework LMBot that distills the knowledge of graph neural networks (GNNs)
into language models (LMs) for graph-less deployment in Twitter bot detection
to combat the challenge of data dependency. Moreover, LMBot is compatible with
graph-based and graph-less datasets. Specifically, we first represent each user
as a textual sequence and feed them into the LM for domain adaptation. For
graph-based datasets, the output of LMs provides input features for the GNN,
enabling it to optimize for bot detection and distill knowledge back to the LM
in an iterative, mutually enhancing process. Armed with the LM, we can perform
graph-less inference, which resolves the graph data dependency and sampling
bias issues. For datasets without graph structure, we simply replace the GNN
with an MLP, which has also shown strong performance. Our experiments
demonstrate that LMBot achieves state-of-the-art performance on four Twitter
bot detection benchmarks. Extensive studies also show that LMBot is more
robust, versatile, and efficient compared to graph-based Twitter bot detection
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In the Name of Fairness: Assessing the Bias in Clinical Record
  De-identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.11348v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.11348v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Xiao, Shulammite Lim, Tom Joseph Pollard, Marzyeh Ghassemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data sharing is crucial for open science and reproducible research, but the
legal sharing of clinical data requires the removal of protected health
information from electronic health records. This process, known as
de-identification, is often achieved through the use of machine learning
algorithms by many commercial and open-source systems. While these systems have
shown compelling results on average, the variation in their performance across
different demographic groups has not been thoroughly examined. In this work, we
investigate the bias of de-identification systems on names in clinical notes
via a large-scale empirical analysis. To achieve this, we create 16 name sets
that vary along four demographic dimensions: gender, race, name popularity, and
the decade of popularity. We insert these names into 100 manually curated
clinical templates and evaluate the performance of nine public and private
de-identification methods. Our findings reveal that there are statistically
significant performance gaps along a majority of the demographic dimensions in
most methods. We further illustrate that de-identification quality is affected
by polysemy in names, gender context, and clinical note characteristics. To
mitigate the identified gaps, we propose a simple and method-agnostic solution
by fine-tuning de-identification methods with clinical context and diverse
names. Overall, it is imperative to address the bias in existing methods
immediately so that downstream stakeholders can build high-quality systems to
serve all demographic parties fairly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by FAccT 2023; updated appendix with the de-identification
  performance of GPT-4</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DialCLIP: Empowering CLIP as Multi-Modal Dialog Retriever 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01076v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01076v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichao Yin, Binyuan Hui, Min Yang, Fei Huang, Yongbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, substantial advancements in pre-trained vision-language models have
greatly enhanced the capabilities of multi-modal dialog systems. These models
have demonstrated significant improvements by fine-tuning on downstream tasks.
However, the existing pre-trained models primarily focus on effectively
capturing the alignment between vision and language modalities, often ignoring
the intricate nature of dialog context. In this paper, we propose a
parameter-efficient prompt-tuning method named DialCLIP for multi-modal dialog
retrieval. Specifically, our approach introduces a multi-modal context prompt
generator to learn context features which are subsequently distilled into
prompts within the pre-trained vision-language model CLIP. Besides, we
introduce domain prompt to mitigate the disc repancy from the downstream dialog
data. To facilitate various types of retrieval, we also design multiple experts
to learn mappings from CLIP outputs to multi-modal representation space, with
each expert being responsible to one specific retrieval type. Extensive
experiments show that DialCLIP achieves state-of-the-art performance on two
widely recognized benchmark datasets (i.e., PhotoChat and MMDialog) by tuning a
mere 0.04% of the total parameters. These results highlight the efficacy and
efficiency of our proposed approach, underscoring its potential to advance the
field of multi-modal dialog retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conflicts, Villains, Resolutions: Towards models of Narrative Media
  Framing <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02052v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02052v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lea Frermann, Jiatong Li, Shima Khanehzar, Gosia Mikolajczak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite increasing interest in the automatic detection of media frames in
NLP, the problem is typically simplified as single-label classification and
adopts a topic-like view on frames, evading modelling the broader
document-level narrative. In this work, we revisit a widely used
conceptualization of framing from the communication sciences which explicitly
captures elements of narratives, including conflict and its resolution, and
integrate it with the narrative framing of key entities in the story as heroes,
victims or villains. We adapt an effective annotation paradigm that breaks a
complex annotation task into a series of simpler binary questions, and present
an annotated data set of English news articles, and a case study on the framing
of climate change in articles from news outlets across the political spectrum.
Finally, we explore automatic multi-label prediction of our frames with
supervised and semi-supervised approaches, and present a novel retrieval-based
method which is both effective and transparent in its predictions. We conclude
with a discussion of opportunities and challenges for future work on
document-level models of narrative framing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPEED: Speculative Pipelined Execution for Efficient Decoding <span class="chip">NeurIPS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12072v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12072v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Hasan Genc, Kurt Keutzer, Amir Gholami, Sophia Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Large Language Models (LLMs) based on the Transformer architecture
have recently emerged as a dominant foundation model for a wide range of
Natural Language Processing tasks. Nevertheless, their application in real-time
scenarios has been highly restricted due to the significant inference latency
associated with these models. This is particularly pronounced due to the
autoregressive nature of generative LLM inference, where tokens are generated
sequentially since each token depends on all previous output tokens. It is
therefore challenging to achieve any token-level parallelism, making inference
extremely memory-bound. In this work, we propose SPEED, which improves
inference efficiency by speculatively executing multiple future tokens in
parallel with the current token using predicted values based on early-layer
hidden states. For Transformer decoders that employ parameter sharing, the
memory operations for the tokens executing in parallel can be amortized, which
allows us to accelerate generative LLM inference. We demonstrate the efficiency
of our method in terms of latency reduction relative to model accuracy and
demonstrate how speculation allows for training deeper decoders with parameter
sharing with minimal runtime overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS Workshop on Efficient Natural Language and Speech Processing
  (2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lon-ea at SemEval-2023 Task 11: A Comparison of Activation Functions for
  Soft and Hard Label Prediction <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02468v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02468v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peyman Hosseini, Mehran Hosseini, Sana Sabah Al-Azzawi, Marcus Liwicki, Ignacio Castro, Matthew Purver
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the influence of different activation functions in the output layer
of deep neural network models for soft and hard label prediction in the
learning with disagreement task. In this task, the goal is to quantify the
amount of disagreement via predicting soft labels. To predict the soft labels,
we use BERT-based preprocessors and encoders and vary the activation function
used in the output layer, while keeping other parameters constant. The soft
labels are then used for the hard label prediction. The activation functions
considered are sigmoid as well as a step-function that is added to the model
post-training and a sinusoidal activation function, which is introduced for the
first time in this paper.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ACL 2023 SemEval Workshop as selected task paper</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">106</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LEAP-VO: Long-term Effective Any Point Tracking for Visual Odometry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weirong Chen, Le Chen, Rui Wang, Marc Pollefeys
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual odometry estimates the motion of a moving camera based on visual
input. Existing methods, mostly focusing on two-view point tracking, often
ignore the rich temporal context in the image sequence, thereby overlooking the
global motion patterns and providing no assessment of the full trajectory
reliability. These shortcomings hinder performance in scenarios with occlusion,
dynamic objects, and low-texture areas. To address these challenges, we present
the Long-term Effective Any Point Tracking (LEAP) module. LEAP innovatively
combines visual, inter-track, and temporal cues with mindfully selected anchors
for dynamic track estimation. Moreover, LEAP's temporal probabilistic
formulation integrates distribution updates into a learnable iterative
refinement module to reason about point-wise uncertainty. Based on these
traits, we develop LEAP-VO, a robust visual odometry system adept at handling
occlusions and dynamic scenes. Our mindful integration showcases a novel
practice by employing long-term point tracking as the front-end. Extensive
experiments demonstrate that the proposed pipeline significantly outperforms
existing baselines across various visual odometry benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Audio to Photoreal Embodiment: Synthesizing Humans in Conversations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01885v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01885v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evonne Ng, Javier Romero, Timur Bagautdinov, Shaojie Bai, Trevor Darrell, Angjoo Kanazawa, Alexander Richard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a framework for generating full-bodied photorealistic avatars that
gesture according to the conversational dynamics of a dyadic interaction. Given
speech audio, we output multiple possibilities of gestural motion for an
individual, including face, body, and hands. The key behind our method is in
combining the benefits of sample diversity from vector quantization with the
high-frequency details obtained through diffusion to generate more dynamic,
expressive motion. We visualize the generated motion using highly
photorealistic avatars that can express crucial nuances in gestures (e.g.
sneers and smirks). To facilitate this line of research, we introduce a
first-of-its-kind multi-view conversational dataset that allows for
photorealistic reconstruction. Experiments show our model generates appropriate
and diverse gestures, outperforming both diffusion- and VQ-only methods.
Furthermore, our perceptual evaluation highlights the importance of
photorealism (vs. meshes) in accurately assessing subtle motion details in
conversational gestures. Code and dataset available online.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Step length measurement in the wild using FMCW radar 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01868v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01868v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parthipan Siva, Alexander Wong, Patricia Hewston, George Ioannidis, Dr. Jonathan Adachi, Dr. Alexander Rabinovich, Andrea Lee, Alexandra Papaioannou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With an aging population, numerous assistive and monitoring technologies are
under development to enable older adults to age in place. To facilitate aging
in place predicting risk factors such as falls, and hospitalization and
providing early interventions are important. Much of the work on ambient
monitoring for risk prediction has centered on gait speed analysis, utilizing
privacy-preserving sensors like radar. Despite compelling evidence that
monitoring step length, in addition to gait speed, is crucial for predicting
risk, radar-based methods have not explored step length measurement in the
home. Furthermore, laboratory experiments on step length measurement using
radars are limited to proof of concept studies with few healthy subjects. To
address this gap, a radar-based step length measurement system for the home is
proposed based on detection and tracking using radar point cloud, followed by
Doppler speed profiling of the torso to obtain step lengths in the home. The
proposed method was evaluated in a clinical environment, involving 35 frail
older adults, to establish its validity. Additionally, the method was assessed
in people's homes, with 21 frail older adults who had participated in the
clinical assessment. The proposed radar-based step length measurement method
was compared to the gold standard Zeno Walkway Gait Analysis System, revealing
a 4.5cm/8.3% error in a clinical setting. Furthermore, it exhibited excellent
reliability (ICC(2,k)=0.91, 95% CI 0.82 to 0.96) in uncontrolled home settings.
The method also proved accurate in uncontrolled home settings, as indicated by
a strong agreement (ICC(3,k)=0.81 (95% CI 0.53 to 0.92)) between home
measurements and in-clinic assessments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Vision Check-up for Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pratyusha Sharma, Tamar Rott Shaham, Manel Baradad, Stephanie Fu, Adrian Rodriguez-Munoz, Shivam Duggal, Phillip Isola, Antonio Torralba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What does learning to model relationships between strings teach large
language models (LLMs) about the visual world? We systematically evaluate LLMs'
abilities to generate and recognize an assortment of visual concepts of
increasing complexity and then demonstrate how a preliminary visual
representation learning system can be trained using models of text. As language
models lack the ability to consume or output visual information as pixels, we
use code to represent images in our study. Although LLM-generated images do not
look like natural images, results on image generation and the ability of models
to correct these generated images indicate that precise modeling of strings can
teach language models about numerous aspects of the visual world. Furthermore,
experiments on self-supervised visual representation learning, utilizing images
generated with text models, highlight the potential to train vision models
capable of making semantic assessments of natural images using just LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Synthetic <span class="highlight-title">dataset</span> of ID and Travel Document 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Boned, Maxime Talarmain, Nabil Ghanmi, Guillaume Chiron, Sanket Biswas, Ahmad Montaser Awal, Oriol Ramos Terrades
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a new synthetic dataset of ID and travel documents,
called SIDTD. The SIDTD dataset is created to help training and evaluating
forged ID documents detection systems. Such a dataset has become a necessity as
ID documents contain personal information and a public dataset of real
documents can not be released. Moreover, forged documents are scarce, compared
to legit ones, and the way they are generated varies from one fraudster to
another resulting in a class of high intra-variability. In this paper we
trained state-of-the-art models on this dataset and we compare them to the
performance achieved in larger, but private, datasets. The creation of this
dataset will help to document image analysis community to progress in the task
of ID document verification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Frequency Domain Modality-invariant Feature Learning for
  Visible-infrared Person Re-Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01839v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01839v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulin Li, Tianzhu Zhang, Yongdong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visible-infrared person re-identification (VI-ReID) is challenging due to the
significant cross-modality discrepancies between visible and infrared images.
While existing methods have focused on designing complex network architectures
or using metric learning constraints to learn modality-invariant features, they
often overlook which specific component of the image causes the modality
discrepancy problem. In this paper, we first reveal that the difference in the
amplitude component of visible and infrared images is the primary factor that
causes the modality discrepancy and further propose a novel Frequency Domain
modality-invariant feature learning framework (FDMNet) to reduce modality
discrepancy from the frequency domain perspective. Our framework introduces two
novel modules, namely the Instance-Adaptive Amplitude Filter (IAF) module and
the Phrase-Preserving Normalization (PPNorm) module, to enhance the
modality-invariant amplitude component and suppress the modality-specific
component at both the image- and feature-levels. Extensive experimental results
on two standard benchmarks, SYSU-MM01 and RegDB, demonstrate the superior
performance of our FDMNet against state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Moonshot: Towards Controllable Video Generation and Editing with
  Multimodal Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Junhao Zhang, Dongxu Li, Hung Le, Mike Zheng Shou, Caiming Xiong, Doyen Sahoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most existing video diffusion models (VDMs) are limited to mere text
conditions. Thereby, they are usually lacking in control over visual appearance
and geometry structure of the generated videos. This work presents Moonshot, a
new video generation model that conditions simultaneously on multimodal inputs
of image and text. The model builts upon a core module, called multimodal video
block (MVB), which consists of conventional spatialtemporal layers for
representing video features, and a decoupled cross-attention layer to address
image and text inputs for appearance conditioning. In addition, we carefully
design the model architecture such that it can optionally integrate with
pre-trained image ControlNet modules for geometry visual conditions, without
needing of extra training overhead as opposed to prior methods. Experiments
show that with versatile multimodal conditioning mechanisms, Moonshot
demonstrates significant improvement on visual quality and temporal consistency
compared to existing models. In addition, the model can be easily repurposed
for a variety of generative applications, such as personalized video
generation, image animation and video editing, unveiling its potential to serve
as a fundamental architecture for controllable video generation. Models will be
made public on https://github.com/salesforce/LAVIS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project page: https://showlab.github.io/Moonshot/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HawkRover: An Autonomous mmWave Vehicular Communication Testbed with
  Multi-sensor Fusion and Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Zhu, Haijian Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Connected and automated vehicles (CAVs) have become a transformative
technology that can change our daily life. Currently, millimeter-wave (mmWave)
bands are identified as the promising CAV connectivity solution. While it can
provide high data rate, their realization faces many challenges such as high
attenuation during mmWave signal propagation and mobility management. Existing
solution has to initiate pilot signal to measure channel information, then
apply signal processing to calculate the best narrow beam towards the receiver
end to guarantee sufficient signal power. This process takes significant
overhead and time, hence not suitable for vehicles. In this study, we propose
an autonomous and low-cost testbed to collect extensive co-located mmWave
signal and other sensors data such as LiDAR (Light Detection and Ranging),
cameras, ultrasonic, etc, traditionally for ``automated'', to facilitate mmWave
vehicular communications. Intuitively, these sensors can build a 3D map around
the vehicle and signal propagation path can be estimated, eliminating iterative
the process via pilot signals. This multimodal data fusion, together with AI,
is expected to bring significant advances in ``connected'' research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE conferences for future publications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detours for Navigating Instructional Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kumar Ashutosh, Zihui Xue, Tushar Nagarajan, Kristen Grauman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the video detours problem for navigating instructional videos.
Given a source video and a natural language query asking to alter the how-to
video's current path of execution in a certain way, the goal is to find a
related ''detour video'' that satisfies the requested alteration. To address
this challenge, we propose VidDetours, a novel video-language approach that
learns to retrieve the targeted temporal segments from a large repository of
how-to's using video-and-text conditioned queries. Furthermore, we devise a
language-based pipeline that exploits how-to video narration text to create
weakly supervised training data. We demonstrate our idea applied to the domain
of how-to cooking videos, where a user can detour from their current recipe to
find steps with alternate ingredients, tools, and techniques. Validating on a
ground truth annotated dataset of 16K samples, we show our model's significant
improvements over best available methods for video retrieval and question
answering, with recall rates exceeding the state of the art by 35%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ aMUSEd: An Open MUSE Reproduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suraj Patil, William Berman, Robin Rombach, Patrick von Platen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present aMUSEd, an open-source, lightweight masked image model (MIM) for
text-to-image generation based on MUSE. With 10 percent of MUSE's parameters,
aMUSEd is focused on fast image generation. We believe MIM is under-explored
compared to latent diffusion, the prevailing approach for text-to-image
generation. Compared to latent diffusion, MIM requires fewer inference steps
and is more interpretable. Additionally, MIM can be fine-tuned to learn
additional styles with only a single image. We hope to encourage further
exploration of MIM by demonstrating its effectiveness on large-scale
text-to-image generation and releasing reproducible training code. We also
release checkpoints for two models which directly produce images at 256x256 and
512x512 resolutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VGA: Vision and Graph Fused Attention Network for Rumor Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Bai, Caiyan Jia, Ziying Song, Chaoqun Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of social media, rumors have been spread broadly on
social media platforms, causing great harm to society. Beside textual
information, many rumors also use manipulated images or conceal textual
information within images to deceive people and avoid being detected, making
multimodal rumor detection be a critical problem. The majority of multimodal
rumor detection methods mainly concentrate on extracting features of source
claims and their corresponding images, while ignoring the comments of rumors
and their propagation structures. These comments and structures imply the
wisdom of crowds and are proved to be crucial to debunk rumors. Moreover, these
methods usually only extract visual features in a basic manner, seldom consider
tampering or textual information in images. Therefore, in this study, we
propose a novel Vision and Graph Fused Attention Network (VGA) for rumor
detection to utilize propagation structures among posts so as to obtain the
crowd opinions and further explore visual tampering features, as well as the
textual information hidden in images. We conduct extensive experiments on three
datasets, demonstrating that VGA can effectively detect multimodal rumors and
outperform state-of-the-art methods significantly.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FullLoRA-AT: Efficiently Boosting the Robustness of Pretrained Vision
  Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Yuan, Jie Zhang, Shiguang Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the Vision Transformer (ViT) model has gradually become
mainstream in various computer vision tasks, and the robustness of the model
has received increasing attention. However, existing large models tend to
prioritize performance during training, potentially neglecting the robustness,
which may lead to serious security concerns. In this paper, we establish a new
challenge: exploring how to use a small number of additional parameters for
adversarial finetuning to quickly and effectively enhance the adversarial
robustness of a standardly trained model. To address this challenge, we develop
the novel LNLoRA module, incorporating a learnable layer normalization before
the conventional LoRA module, which helps mitigate magnitude differences in
parameters between the adversarial and standard training paradigms.
  Furthermore, we propose the FullLoRA-AT framework by integrating the
learnable LNLoRA modules into all key components of ViT-based models while
keeping the pretrained model frozen, which can significantly improve the model
robustness via adversarial finetuning in a parameter-efficient manner.
  Extensive experiments on CIFAR-10, CIFAR-100, and Imagenette demonstrate the
superiority of our proposed FullLoRA-AT framework. It achieves comparable
robustness with full finetuning while only requiring about 5% of the learnable
parameters. This also effectively addresses concerns regarding extra model
storage space and enormous training time caused by adversarial finetuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Robust Semantic Segmentation against Patch-based Attack via
  Attention Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01750v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01750v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Yuan, Jie Zhang, Yude Wang, Shiguang Shan, Xilin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The attention mechanism has been proven effective on various visual tasks in
recent years. In the semantic segmentation task, the attention mechanism is
applied in various methods, including the case of both Convolution Neural
Networks (CNN) and Vision Transformer (ViT) as backbones. However, we observe
that the attention mechanism is vulnerable to patch-based adversarial attacks.
Through the analysis of the effective receptive field, we attribute it to the
fact that the wide receptive field brought by global attention may lead to the
spread of the adversarial patch. To address this issue, in this paper, we
propose a Robust Attention Mechanism (RAM) to improve the robustness of the
semantic segmentation model, which can notably relieve the vulnerability
against patch-based attacks. Compared to the vallina attention mechanism, RAM
introduces two novel modules called Max Attention Suppression and Random
Attention Dropout, both of which aim to refine the attention matrix and limit
the influence of a single adversarial patch on the semantic segmentation
results of other positions. Extensive experiments demonstrate the effectiveness
of our RAM to improve the robustness of semantic segmentation models against
various patch-based attack methods under different attack settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 3 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-shot Image Generation via Information Transfer from the Built
  Geodesic Surface 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuexing Han, Liheng Ruan, Bing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Images generated by most of generative models trained with limited data often
exhibit deficiencies in either fidelity, diversity, or both. One effective
solution to address the limitation is few-shot generative model adaption.
However, the type of approaches typically rely on a large-scale pre-trained
model, serving as a source domain, to facilitate information transfer to the
target domain. In this paper, we propose a method called Information Transfer
from the Built Geodesic Surface (ITBGS), which contains two module: Feature
Augmentation on Geodesic Surface (FAGS); Interpolation and Regularization
(I\&R). With the FAGS module, a pseudo-source domain is created by projecting
image features from the training dataset into the Pre-Shape Space, subsequently
generating new features on the Geodesic surface. Thus, no pre-trained models is
needed for the adaption process during the training of generative models with
FAGS. I\&R module are introduced for supervising the interpolated images and
regularizing their relative distances, respectively, to further enhance the
quality of generated images. Through qualitative and quantitative experiments,
we demonstrate that the proposed method consistently achieves optimal or
comparable results across a diverse range of semantically distinct datasets,
even in extremely few-shot scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few-shot Adaptation of Multi-modal Foundation Models: A Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Liu, Tianshu Zhang, Wenwen Dai, Wenwen Cai Xiaocong Zhou, Delong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal (vision-language) models, such as CLIP, are replacing traditional
supervised pre-training models (e.g., ImageNet-based pre-training) as the new
generation of visual foundation models. These models with robust and aligned
semantic representations learned from billions of internet image-text pairs and
can be applied to various downstream tasks in a zero-shot manner. However, in
some fine-grained domains like medical imaging and remote sensing, the
performance of multi-modal foundation models often leaves much to be desired.
Consequently, many researchers have begun to explore few-shot adaptation
methods for these models, gradually deriving three main technical approaches:
1) prompt-based methods, 2) adapter-based methods, and 3) external
knowledge-based methods. Nevertheless, this rapidly developing field has
produced numerous results without a comprehensive survey to systematically
organize the research progress. Therefore, in this survey, we introduce and
analyze the research advancements in few-shot adaptation methods for
multi-modal models, summarizing commonly used datasets and experimental setups,
and comparing the results of different methods. In addition, due to the lack of
reliable theoretical support for existing methods, we derive the few-shot
adaptation generalization error bound for multi-modal models. The theorem
reveals that the generalization error of multi-modal foundation models is
constrained by three factors: domain gap, model capacity, and sample size.
Based on this, we propose three possible solutions from the following aspects:
1) adaptive domain generalization, 2) adaptive model selection, and 3) adaptive
knowledge utilization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Keypoints for Robotic Cloth Manipulation using Synthetic Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01734v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01734v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Lips, Victor-Louis De Gusseme, Francis wyffels
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assistive robots should be able to wash, fold or iron clothes. However, due
to the variety, deformability and self-occlusions of clothes, creating
general-purpose robot systems for cloth manipulation is challenging. Synthetic
data is a promising direction to improve generalization, though its usability
is often limited by the sim-to-real gap. To advance the use of synthetic data
for cloth manipulation and to enable tasks such as robotic folding, we present
a synthetic data pipeline to train keypoint detectors for almost flattened
cloth items. To test its performance, we have also collected a real-world
dataset. We train detectors for both T-shirts, towels and shorts and obtain an
average precision of 64.3%. Fine-tuning on real-world data improves performance
to 74.2%. Additional insight is provided by discussing various failure modes of
the keypoint detectors and by comparing different approaches to obtain cloth
meshes and materials. We also quantify the remaining sim-to-real gap and argue
that further improvements to the fidelity of cloth assets will be required to
further reduce this gap. The code, dataset and trained models are available
online.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to journal on 20/12</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STAF: 3D Human Mesh Recovery from Video with Spatio-Temporal Alignment
  Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Yao, Hongwen Zhang, Yunlian Sun, Jinhui Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recovery of 3D human mesh from monocular images has significantly been
developed in recent years. However, existing models usually ignore spatial and
temporal information, which might lead to mesh and image misalignment and
temporal discontinuity. For this reason, we propose a novel Spatio-Temporal
Alignment Fusion (STAF) model. As a video-based model, it leverages coherence
clues from human motion by an attention-based Temporal Coherence Fusion Module
(TCFM). As for spatial mesh-alignment evidence, we extract fine-grained local
information through predicted mesh projection on the feature maps. Based on the
spatial features, we further introduce a multi-stage adjacent Spatial Alignment
Fusion Module (SAFM) to enhance the feature representation of the target frame.
In addition to the above, we propose an Average Pooling Module (APM) to allow
the model to focus on the entire input sequence rather than just the target
frame. This method can remarkably improve the smoothness of recovery results
from video. Extensive experiments on 3DPW, MPII3D, and H36M demonstrate the
superiority of STAF. We achieve a state-of-the-art trade-off between precision
and smoothness. Our code and more video results are on the project page
https://yw0208.github.io/staf/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://yw0208.github.io/staf/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lightweight Adaptive Feature De-drifting for Compressed Image
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Peng, Yang Cao, Yuejin Sun, Yang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  JPEG is a widely used compression scheme to efficiently reduce the volume of
transmitted images. The artifacts appear among blocks due to the information
loss, which not only affects the quality of images but also harms the
subsequent high-level tasks in terms of feature drifting. High-level vision
models trained on high-quality images will suffer performance degradation when
dealing with compressed images, especially on mobile devices. Numerous
learning-based JPEG artifact removal methods have been proposed to handle
visual artifacts. However, it is not an ideal choice to use these JPEG artifact
removal methods as a pre-processing for compressed image classification for the
following reasons: 1. These methods are designed for human vision rather than
high-level vision models; 2. These methods are not efficient enough to serve as
pre-processing on resource-constrained devices. To address these issues, this
paper proposes a novel lightweight AFD module to boost the performance of
pre-trained image classification models when facing compressed images. First, a
FDE-Net is devised to generate the spatial-wise FDM in the DCT domain. Next,
the estimated FDM is transmitted to the FE-Net to generate the mapping
relationship between degraded features and corresponding high-quality features.
A simple but effective RepConv block equipped with structural
re-parameterization is utilized in FE-Net, which enriches feature
representation in the training phase while maintaining efficiency in the
deployment phase. After training on limited compressed images, the AFD-Module
can serve as a "plug-and-play" model for pre-trained classification models to
improve their performance on compressed images. Experiments demonstrate that
our proposed AFD module can comprehensively improve the accuracy of the
pre-trained classification models and significantly outperform the existing
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Multimedia 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Local Adaptive Clustering Based Image Matching for Automatic Visual
  Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01720v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01720v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhizhen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monitoring cameras are extensively utilized in industrial production to
monitor equipment running. With advancements in computer vision, device
recognition using image features is viable. This paper presents a
vision-assisted identification system that implements real-time automatic
equipment labeling through image matching in surveillance videos. The system
deploys the ORB algorithm to extract image features and the GMS algorithm to
remove incorrect matching points. According to the principles of clustering and
template locality, a method known as Local Adaptive Clustering (LAC) has been
established to enhance label positioning. This method segments matching
templates using the cluster center, which improves the efficiency and stability
of labels. The experimental results demonstrate that LAC effectively curtails
the label drift.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fact-checking based fake news detection: a review 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzhou Yang, Yangming Zhou, Qichao Ying, Zhenxing Qian, Dan Zeng, Liang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper reviews and summarizes the research results on fact-based fake
news from the perspectives of tasks and problems, algorithm strategies, and
datasets. First, the paper systematically explains the task definition and core
problems of fact-based fake news detection. Second, the paper summarizes the
existing detection methods based on the algorithm principles. Third, the paper
analyzes the classic and newly proposed datasets in the field, and summarizes
the experimental results on each dataset. Finally, the paper summarizes the
advantages and disadvantages of existing methods, proposes several challenges
that methods in this field may face, and looks forward to the next stage of
research. It is hoped that this paper will provide reference for subsequent
work in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Invited short review paper (in Chinese)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WordArt Designer API: User-Driven Artistic Typography Synthesis with
  Large Language Models on ModelScope <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun-Yan He, Zhi-Qi Cheng, Chenyang Li, Jingdong Sun, Wangmeng Xiang, Yusen Hu, Xianhui Lin, Xiaoyang Kang, Zengke Jin, Bin Luo, Yifeng Geng, Xuansong Xie, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the WordArt Designer API, a novel framework for
user-driven artistic typography synthesis utilizing Large Language Models
(LLMs) on ModelScope. We address the challenge of simplifying artistic
typography for non-professionals by offering a dynamic, adaptive, and
computationally efficient alternative to traditional rigid templates. Our
approach leverages the power of LLMs to understand and interpret user input,
facilitating a more intuitive design process. We demonstrate through various
case studies how users can articulate their aesthetic preferences and
functional requirements, which the system then translates into unique and
creative typographic designs. Our evaluations indicate significant improvements
in user satisfaction, design flexibility, and creative expression over existing
systems. The WordArt Designer API not only democratizes the art of typography
but also opens up new possibilities for personalized digital communication and
design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Spotlight Paper from the Workshop on Machine Learning for Creativity
  and Design at the 37th Conference on Neural Information Processing Systems
  (NeurIPS 2023). 5 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AID-DTI: Accelerating High-fidelity Diffusion Tensor Imaging with
  Detail-Preserving Model-based Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01693v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01693v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxin Fan, Jian Cheng, Cheng Li, Xinrui Ma, Jing Yang, Juan Zou, Ruoyou Wu, Qiegen Liu, Shanshan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has shown great potential in accelerating diffusion tensor
imaging (DTI). Nevertheless, existing methods tend to suffer from Rician noise
and detail loss in reconstructing the DTI-derived parametric maps especially
when sparsely sampled q-space data are used. This paper proposes a novel
method, AID-DTI (Accelerating hIgh fiDelity Diffusion Tensor Imaging), to
facilitate fast and accurate DTI with only six measurements. AID-DTI is
equipped with a newly designed Singular Value Decomposition (SVD)-based
regularizer, which can effectively capture fine details while suppressing noise
during network training. Experimental results on Human Connectome Project (HCP)
data consistently demonstrate that the proposed method estimates DTI parameter
maps with fine-grained details and outperforms three state-of-the-art methods
both quantitatively and qualitatively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ODTrack: Online Dense Temporal Token Learning for Visual Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaozong Zheng, Bineng Zhong, Qihua Liang, Zhiyi Mo, Shengping Zhang, Xianxian Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online contextual reasoning and association across consecutive video frames
are critical to perceive instances in visual tracking. However, most current
top-performing trackers persistently lean on sparse temporal relationships
between reference and search frames via an offline mode. Consequently, they can
only interact independently within each image-pair and establish limited
temporal correlations. To alleviate the above problem, we propose a simple,
flexible and effective video-level tracking pipeline, named \textbf{ODTrack},
which densely associates the contextual relationships of video frames in an
online token propagation manner. ODTrack receives video frames of arbitrary
length to capture the spatio-temporal trajectory relationships of an instance,
and compresses the discrimination features (localization information) of a
target into a token sequence to achieve frame-to-frame association. This new
solution brings the following benefits: 1) the purified token sequences can
serve as prompts for the inference in the next video frame, whereby past
information is leveraged to guide future inference; 2) the complex online
update strategies are effectively avoided by the iterative propagation of token
sequences, and thus we can achieve more efficient model representation and
computation. ODTrack achieves a new \textit{SOTA} performance on seven
benchmarks, while running at real-time speed. Code and models are available at
\url{https://github.com/GXNU-ZhongLab/ODTrack}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modality Exchange Network for Retinogeniculate Visual Pathway
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01685v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01685v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hua Han, Cheng Li, Lei Xie, Yuanjing Feng, Alou Diakite, Shanshan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate segmentation of the retinogeniculate visual pathway (RGVP) aids in
the diagnosis and treatment of visual disorders by identifying disruptions or
abnormalities within the pathway. However, the complex anatomical structure and
connectivity of RGVP make it challenging to achieve accurate segmentation. In
this study, we propose a novel Modality Exchange Network (ME-Net) that
effectively utilizes multi-modal magnetic resonance (MR) imaging information to
enhance RGVP segmentation. Our ME-Net has two main contributions. Firstly, we
introduce an effective multi-modal soft-exchange technique. Specifically, we
design a channel and spatially mixed attention module to exchange modality
information between T1-weighted and fractional anisotropy MR images. Secondly,
we propose a cross-fusion module that further enhances the fusion of
information between the two modalities. Experimental results demonstrate that
our method outperforms existing state-of-the-art approaches in terms of RGVP
segmentation performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Performance Evaluation of GPS Trajectory Rasterization Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Necip Enes Gengec, Ergin Tari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The availability of the Global Positioning System (GPS) trajectory data is
increasing along with the availability of different GPS receivers and with the
increasing use of various mobility services. GPS trajectory is an important
data source which is used in traffic density detection, transport mode
detection, mapping data inferences with the use of different methods such as
image processing and machine learning methods. While the data size increases,
efficient representation of this type of data is becoming difficult to be used
in these methods. A common approach is the representation of GPS trajectory
information such as average speed, bearing, etc. in raster image form and
applying analysis methods. In this study, we evaluate GPS trajectory data
rasterization using the spatial join functions of QGIS, PostGIS+QGIS, and our
iterative spatial structured grid aggregation implementation coded in the
Python programming language. Our implementation is also parallelizable, and
this parallelization is also included as the fourth method. According to the
results of experiment carried out with an example GPS trajectory dataset, QGIS
method and PostGIS+QGIS method showed relatively low performance with respect
to our method using the metric of total processing time. PostGIS+QGIS method
achieved the best results for spatial join though its total performance
decreased quickly while test area size increases. On the other hand, both of
our methods' performances decrease directly proportional to GPS point. And our
methods' performance can be increased proportional to the increase with the
number of processor cores and/or with multiple computing clusters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transformer RGBT Tracking with Spatio-Temporal Multimodal Tokens 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dengdi Sun, Yajie Pan, Andong Lu, Chenglong Li, Bin Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many RGBT tracking researches primarily focus on modal fusion design, while
overlooking the effective handling of target appearance changes. While some
approaches have introduced historical frames or fuse and replace initial
templates to incorporate temporal information, they have the risk of disrupting
the original target appearance and accumulating errors over time. To alleviate
these limitations, we propose a novel Transformer RGBT tracking approach, which
mixes spatio-temporal multimodal tokens from the static multimodal templates
and multimodal search regions in Transformer to handle target appearance
changes, for robust RGBT tracking. We introduce independent dynamic template
tokens to interact with the search region, embedding temporal information to
address appearance changes, while also retaining the involvement of the initial
static template tokens in the joint feature extraction process to ensure the
preservation of the original reliable target appearance information that
prevent deviations from the target appearance caused by traditional temporal
updates. We also use attention mechanisms to enhance the target features of
multimodal template tokens by incorporating supplementary modal cues, and make
the multimodal search region tokens interact with multimodal dynamic template
tokens via attention mechanisms, which facilitates the conveyance of
multimodal-enhanced target change information. Our module is inserted into the
transformer backbone network and inherits joint feature extraction,
search-template matching, and cross-modal interaction. Extensive experiments on
three RGBT benchmark datasets show that the proposed approach maintains
competitive performance compared to other state-of-the-art tracking algorithms
while running at 39.1 FPS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simultaneous q-Space Sampling Optimization and Reconstruction for Fast
  and High-fidelity Diffusion Magnetic Resonance Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01662v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01662v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Yang, Jian Cheng, Cheng Li, Wenxin Fan, Juan Zou, Ruoyou Wu, Shanshan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Magnetic Resonance Imaging (dMRI) plays a crucial role in the
noninvasive investigation of tissue microstructural properties and structural
connectivity in the \textit{in vivo} human brain. However, to effectively
capture the intricate characteristics of water diffusion at various directions
and scales, it is important to employ comprehensive q-space sampling.
Unfortunately, this requirement leads to long scan times, limiting the clinical
applicability of dMRI. To address this challenge, we propose SSOR, a
Simultaneous q-Space sampling Optimization and Reconstruction framework. We
jointly optimize a subset of q-space samples using a continuous representation
of spherical harmonic functions and a reconstruction network. Additionally, we
integrate the unique properties of diffusion magnetic resonance imaging (dMRI)
in both the q-space and image domains by applying $l1$-norm and total-variation
regularization. The experiments conducted on HCP data demonstrate that SSOR has
promising strengths both quantitatively and qualitatively and exhibits
robustness to noise.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffYOLO: Object Detection for Anti-Noise via YOLO and Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichen Liu, Huajian Zhang, Daqing Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object detection models represented by YOLO series have been widely used and
have achieved great results on the high quality datasets, but not all the
working conditions are ideal. To settle down the problem of locating targets on
low quality datasets, the existing methods either train a new object detection
network, or need a large collection of low-quality datasets to train. However,
we propose a framework in this paper and apply it on the YOLO models called
DiffYOLO. Specifically, we extract feature maps from the denoising diffusion
probabilistic models to enhance the well-trained models, which allows us
fine-tune YOLO on high-quality datasets and test on low-quality datasets. The
results proved this framework can not only prove the performance on noisy
datasets, but also prove the detection results on high-quality test datasets.
We will supplement more experiments later (with various datasets and network
architectures).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AIGCBench: Comprehensive Evaluation of Image-to-Video Content Generated
  by AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanda Fan, Chunjie Luo, Jianfeng Zhan, Wanling Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The burgeoning field of Artificial Intelligence Generated Content (AIGC) is
witnessing rapid advancements, particularly in video generation. This paper
introduces AIGCBench, a pioneering comprehensive and scalable benchmark
designed to evaluate a variety of video generation tasks, with a primary focus
on Image-to-Video (I2V) generation. AIGCBench tackles the limitations of
existing benchmarks, which suffer from a lack of diverse datasets, by including
a varied and open-domain image-text dataset that evaluates different
state-of-the-art algorithms under equivalent conditions. We employ a novel text
combiner and GPT-4 to create rich text prompts, which are then used to generate
images via advanced Text-to-Image models. To establish a unified evaluation
framework for video generation tasks, our benchmark includes 11 metrics
spanning four dimensions to assess algorithm performance. These dimensions are
control-video alignment, motion effects, temporal consistency, and video
quality. These metrics are both reference video-dependent and video-free,
ensuring a comprehensive evaluation strategy. The evaluation standard proposed
correlates well with human judgment, providing insights into the strengths and
weaknesses of current I2V algorithms. The findings from our extensive
experiments aim to stimulate further research and development in the I2V field.
AIGCBench represents a significant step toward creating standardized benchmarks
for the broader AIGC landscape, proposing an adaptable and equitable framework
for future assessments of video generation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ De-Confusing Pseudo-Labels in Source-Free Domain Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Idit Diamant, Idan Achituve, Arnon Netzer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Source-free domain adaptation (SFDA) aims to transfer knowledge learned from
a source domain to an unlabeled target domain, where the source data is
unavailable during adaptation. Existing approaches for SFDA focus on
self-training usually including well-established entropy minimization and
pseudo-labeling techniques. Recent work suggested a co-learning strategy to
improve the quality of the generated target pseudo-labels using robust
pretrained networks such as Swin-B. However, since the generated pseudo-labels
depend on the source model, they may be noisy due to domain shift. In this
paper, we view SFDA from the perspective of label noise learning and learn to
de-confuse the pseudo-labels. More specifically, we learn a noise transition
matrix of the pseudo-labels to capture the label corruption of each class and
learn the underlying true label distribution. Estimating the noise transition
matrix enables a better true class-posterior estimation results with better
prediction accuracy. We demonstrate the effectiveness of our approach applied
with several SFDA methods: SHOT, SHOT++, and AaD. We obtain state-of-the-art
results on three domain adaptation datasets: VisDA, DomainNet, and OfficeHome.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2212.03795</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SIGNeRF: Scene Integrated Generation for Neural Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01647v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01647v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan-Niklas Dihlmann, Andreas Engelhardt, Hendrik Lensch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in image diffusion models have recently led to notable improvements
in the generation of high-quality images. In combination with Neural Radiance
Fields (NeRFs), they enabled new opportunities in 3D generation. However, most
generative 3D approaches are object-centric and applying them to editing
existing photorealistic scenes is not trivial. We propose SIGNeRF, a novel
approach for fast and controllable NeRF scene editing and scene-integrated
object generation. A new generative update strategy ensures 3D consistency
across the edited images, without requiring iterative optimization. We find
that depth-conditioned diffusion models inherently possess the capability to
generate 3D consistent views by requesting a grid of images instead of single
views. Based on these insights, we introduce a multi-view reference sheet of
modified images. Our method updates an image collection consistently based on
the reference sheet and refines the original NeRF with the newly generated
image set in one go. By exploiting the depth conditioning mechanism of the
image diffusion model, we gain fine control over the spatial location of the
edit and enforce shape guidance by a selected region or an external mesh.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://signerf.jdihlmann.com</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prototypical Information Bottlenecking and Disentangling for Multimodal
  Cancer Survival Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilan Zhang, Yingxue Xu, Jianqi Chen, Fengying Xie, Hao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal learning significantly benefits cancer survival prediction,
especially the integration of pathological images and genomic data. Despite
advantages of multimodal learning for cancer survival prediction, massive
redundancy in multimodal data prevents it from extracting discriminative and
compact information: (1) An extensive amount of intra-modal task-unrelated
information blurs discriminability, especially for gigapixel whole slide images
(WSIs) with many patches in pathology and thousands of pathways in genomic
data, leading to an ``intra-modal redundancy" issue. (2) Duplicated information
among modalities dominates the representation of multimodal data, which makes
modality-specific information prone to being ignored, resulting in an
``inter-modal redundancy" issue. To address these, we propose a new framework,
Prototypical Information Bottlenecking and Disentangling (PIBD), consisting of
Prototypical Information Bottleneck (PIB) module for intra-modal redundancy and
Prototypical Information Disentanglement (PID) module for inter-modal
redundancy. Specifically, a variant of information bottleneck, PIB, is proposed
to model prototypes approximating a bunch of instances for different risk
levels, which can be used for selection of discriminative instances within
modality. PID module decouples entangled multimodal data into compact distinct
components: modality-common and modality-specific knowledge, under the guidance
of the joint prototypical distribution. Extensive experiments on five cancer
benchmark datasets demonstrated our superiority over other methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ S3Net: Innovating Stereo Matching and Semantic Segmentation with a
  Single-Branch Semantic Stereo Network in Satellite Epipolar Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01643v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01643v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyuan Yang, Guanzhou Chen, Xiaoliang Tan, Tong Wang, Jiaqi Wang, Xiaodong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stereo matching and semantic segmentation are significant tasks in binocular
satellite 3D reconstruction. However, previous studies primarily view these as
independent parallel tasks, lacking an integrated multitask learning framework.
This work introduces a solution, the Single-branch Semantic Stereo Network
(S3Net), which innovatively combines semantic segmentation and stereo matching
using Self-Fuse and Mutual-Fuse modules. Unlike preceding methods that utilize
semantic or disparity information independently, our method dentifies and
leverages the intrinsic link between these two tasks, leading to a more
accurate understanding of semantic information and disparity estimation.
Comparative testing on the US3D dataset proves the effectiveness of our S3Net.
Our model improves the mIoU in semantic segmentation from 61.38 to 67.39, and
reduces the D1-Error and average endpoint error (EPE) in disparity estimation
from 10.051 to 9.579 and 1.439 to 1.403 respectively, surpassing existing
competitive methods. Our codes are available at:https://github.com/CVEO/S3Net.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BLADE: Box-Level Supervised Amodal Segmentation through Directed
  Expansion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaochen Liu, Zhixuan Li, Tingting Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perceiving the complete shape of occluded objects is essential for human and
machine intelligence. While the amodal segmentation task is to predict the
complete mask of partially occluded objects, it is time-consuming and
labor-intensive to annotate the pixel-level ground truth amodal masks.
Box-level supervised amodal segmentation addresses this challenge by relying
solely on ground truth bounding boxes and instance classes as supervision,
thereby alleviating the need for exhaustive pixel-level annotations.
Nevertheless, current box-level methodologies encounter limitations in
generating low-resolution masks and imprecise boundaries, failing to meet the
demands of practical real-world applications. We present a novel solution to
tackle this problem by introducing a directed expansion approach from visible
masks to corresponding amodal masks. Our approach involves a hybrid end-to-end
network based on the overlapping region - the area where different instances
intersect. Diverse segmentation strategies are applied for overlapping regions
and non-overlapping regions according to distinct characteristics. To guide the
expansion of visible masks, we introduce an elaborately-designed connectivity
loss for overlapping regions, which leverages correlations with visible masks
and facilitates accurate amodal segmentation. Experiments are conducted on
several challenging datasets and the results show that our proposed method can
outperform existing state-of-the-art methods with large margins.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2024;</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context-Aware Interaction Network for RGB-T Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Lv, Zhi Liu, Gongyang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  RGB-T semantic segmentation is a key technique for autonomous driving scenes
understanding. For the existing RGB-T semantic segmentation methods, however,
the effective exploration of the complementary relationship between different
modalities is not implemented in the information interaction between multiple
levels. To address such an issue, the Context-Aware Interaction Network
(CAINet) is proposed for RGB-T semantic segmentation, which constructs
interaction space to exploit auxiliary tasks and global context for explicitly
guided learning. Specifically, we propose a Context-Aware Complementary
Reasoning (CACR) module aimed at establishing the complementary relationship
between multimodal features with the long-term context in both spatial and
channel dimensions. Further, considering the importance of global contextual
and detailed information, we propose the Global Context Modeling (GCM) module
and Detail Aggregation (DA) module, and we introduce specific auxiliary
supervision to explicitly guide the context interaction and refine the
segmentation map. Extensive experiments on two benchmark datasets of MFNet and
PST900 demonstrate that the proposed CAINet achieves state-of-the-art
performance. The code is available at https://github.com/YingLv1106/CAINet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures, Accepted by IEEE Transactions on Multimedia 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GPT-4V(ision) is a Generalist Web Agent, if Grounded 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyuan Zheng, Boyu Gou, Jihyung Kil, Huan Sun, Yu Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent development on large multimodal models (LMMs), especially
GPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries
of multimodal models beyond traditional tasks like image captioning and visual
question answering. In this work, we explore the potential of LMMs like GPT-4V
as a generalist web agent that can follow natural language instructions to
complete tasks on any given website. We propose SEEACT, a generalist web agent
that harnesses the power of LMMs for integrated visual understanding and acting
on the web. We evaluate on the recent MIND2WEB benchmark. In addition to
standard offline evaluation on cached websites, we enable a new online
evaluation setting by developing a tool that allows running web agents on live
websites. We show that GPT-4V presents a great potential for web agents - it
can successfully complete 50% of the tasks on live websites if we manually
ground its textual plans into actions on the websites. This substantially
outperforms text-only LLMs like GPT-4 or smaller models (FLAN-T5 and BLIP-2)
specifically fine-tuned for web agents. However, grounding still remains a
major challenge. Existing LMM grounding strategies like set-of-mark prompting
turns out not effective for web agents, and the best grounding strategy we
develop in this paper leverages both the HTML text and visuals. Yet, there is
still a substantial gap with oracle grounding, leaving ample room for further
improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Prompt with Distribution-Based Feature Replay for Few-Shot
  Class-Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zitong Huang, Ze Chen, Zhixing Chen, Erjin Zhou, Xinxing Xu, Rick Siow Mong Goh, Yong Liu, Chunmei Feng, Wangmeng Zuo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot Class-Incremental Learning (FSCIL) aims to continuously learn new
classes based on very limited training data without forgetting the old ones
encountered. Existing studies solely relied on pure visual networks, while in
this paper we solved FSCIL by leveraging the Vision-Language model (e.g., CLIP)
and propose a simple yet effective framework, named Learning Prompt with
Distribution-based Feature Replay (LP-DiF). We observe that simply using CLIP
for zero-shot evaluation can substantially outperform the most influential
methods. Then, prompt tuning technique is involved to further improve its
adaptation ability, allowing the model to continually capture specific
knowledge from each session. To prevent the learnable prompt from forgetting
old knowledge in the new session, we propose a pseudo-feature replay approach.
Specifically, we preserve the old knowledge of each class by maintaining a
feature-level Gaussian distribution with a diagonal covariance matrix, which is
estimated by the image features of training images and synthesized features
generated from a VAE. When progressing to a new session, pseudo-features are
sampled from old-class distributions combined with training images of the
current session to optimize the prompt, thus enabling the model to learn new
knowledge while retaining old knowledge. Experiments on three prevalent
benchmarks, i.e., CIFAR100, mini-ImageNet, CUB-200, and two more challenging
benchmarks, i.e., SUN-397 and CUB-200$^*$ proposed in this paper showcase the
superiority of LP-DiF, achieving new state-of-the-art (SOTA) in FSCIL. Code is
publicly available at https://github.com/1170300714/LP-DiF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MLIP: Medical Language-Image Pre-training with Masked Local
  Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01591v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01591v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarun Liu, Hong-Yu Zhou, Cheng Li, Weijian Huang, Hao Yang, Yong Liang, Shanshan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing contrastive language-image pre-training aims to learn a joint
representation by matching abundant image-text pairs. However, the number of
image-text pairs in medical datasets is usually orders of magnitude smaller
than that in natural datasets. Besides, medical image-text pairs often involve
numerous complex fine-grained correspondences. This paper aims to enhance the
data efficiency by introducing multiple-to-multiple local relationship modeling
to capture denser supervisions. More specifically, we propose a Medical
Language-Image Pre-training (MLIP) framework, which exploits the limited
image-text medical data more efficiently through patch-sentence matching.
Furthermore, we introduce a masked contrastive learning strategy with semantic
integrity estimation to reduce redundancy in images while preserving the
underlying semantics. Our evaluation results show that MLIP outperforms
previous work in zero/few-shot classification and few-shot segmentation tasks
by a large margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-Time Human Fall Detection using a Lightweight Pose Estimation
  Technique 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ekram Alam, Abu Sufian, Paramartha Dutta, Marco Leo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The elderly population is increasing rapidly around the world. There are no
enough caretakers for them. Use of AI-based in-home medical care systems is
gaining momentum due to this. Human fall detection is one of the most important
tasks of medical care system for the aged people. Human fall is a common
problem among elderly people. Detection of a fall and providing medical help as
early as possible is very important to reduce any further complexity. The
chances of death and other medical complications can be reduced by detecting
and providing medical help as early as possible after the fall. There are many
state-of-the-art fall detection techniques available these days, but the
majority of them need very high computing power. In this paper, we proposed a
lightweight and fast human fall detection system using pose estimation. We used
`Movenet' for human joins key-points extraction. Our proposed method can work
in real-time on any low-computing device with any basic camera. All computation
can be processed locally, so there is no problem of privacy of the subject. We
used two datasets `GMDCSA' and `URFD' for the experiment. We got the
sensitivity value of 0.9375 and 0.9167 for the dataset `GMDCSA' and `URFD'
respectively. The source code and the dataset GMDCSA of our work are available
online to access.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing the medical foundation model with multi-scale and
  cross-modality feature learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01583v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01583v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijian Huang, Cheng Li, Hong-Yu Zhou, Jiarun Liu, Hao Yang, Yong Liang, Shanshan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of multi-modal medical foundation models has attracted
significant attention in the field of medicine and healthcare due to their
promising prospects in various clinical applications. One area of focus in this
research direction is the extractions of features at different scales. While
previous studies have explored feature learning at individual scales,
investigation on integrating the diverse scales and modalities of information
is lacking, which may hinder the potential for mutual reinforcement among these
features. This paper aims to bridge this gap by proposing a method that
effectively exploits multi-scale and cross-modality information to enhance the
performance of medical foundation models. The proposed method simultaneously
exploit features at the local, instance, modality and global aspects,
facilitating comprehensive representation learning within the models. We
evaluate the effectiveness of the proposed method on six open-source datasets
across different clinical tasks, demonstrating its ability to enhance the
performance of medical foundation models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Context-Guided Spatio-Temporal Video Grounding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Gu, Heng Fan, Yan Huang, Tiejian Luo, Libo Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatio-temporal video grounding (or STVG) task aims at locating a
spatio-temporal tube for a specific instance given a text query. Despite
advancements, current methods easily suffer the distractors or heavy object
appearance variations in videos due to insufficient object information from the
text, leading to degradation. Addressing this, we propose a novel framework,
context-guided STVG (CG-STVG), which mines discriminative instance context for
object in videos and applies it as a supplementary guidance for target
localization. The key of CG-STVG lies in two specially designed modules,
including instance context generation (ICG), which focuses on discovering
visual context information (in both appearance and motion) of the instance, and
instance context refinement (ICR), which aims to improve the instance context
from ICG by eliminating irrelevant or even harmful information from the
context. During grounding, ICG, together with ICR, are deployed at each
decoding stage of a Transformer architecture for instance context learning.
Particularly, instance context learned from one decoding stage is fed to the
next stage, and leveraged as a guidance containing rich and discriminative
object feature to enhance the target-awareness in decoding feature, which
conversely benefits generating better new instance context for improving
localization finally. Compared to existing methods, CG-STVG enjoys object
information in text query and guidance from mined instance visual context for
more accurate target localization. In our experiments on three benchmarks,
including HCSTVG-v1/-v2 and VidSTG, CG-STVG sets new state-of-the-arts in
m_tIoU and m_vIoU on all of them, showing its efficacy. The code will be
released at https://github.com/HengLan/CGSTVG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Test-Time Personalization with Meta Prompt for Gaze Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01577v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01577v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huan Liu, Julia Qi, Zhenhao Li, Mohammad Hassanpour, Yang Wang, Konstantinos Plataniotis, Yuanhao Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the recent remarkable achievement in gaze estimation, efficient and
accurate personalization of gaze estimation without labels is a practical
problem but rarely touched on in the literature. To achieve efficient
personalization, we take inspiration from the recent advances in Natural
Language Processing (NLP) by updating a negligible number of parameters,
"prompts", at the test time. Specifically, the prompt is additionally attached
without perturbing original network and can contain less than 1% of a
ResNet-18's parameters. Our experiments show high efficiency of the prompt
tuning approach. The proposed one can be 10 times faster in terms of adaptation
speed than the methods compared. However, it is non-trivial to update the
prompt for personalized gaze estimation without labels. At the test time, it is
essential to ensure that the minimizing of particular unsupervised loss leads
to the goals of minimizing gaze estimation error. To address this difficulty,
we propose to meta-learn the prompt to ensure that its updates align with the
goal. Our experiments show that the meta-learned prompt can be effectively
adapted even with a simple symmetry loss. In addition, we experiment on four
cross-dataset validations to show the remarkable advantages of the proposed
method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Generalization of Invisible Facial Privacy Cloak via Gradient
  Accumulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01575v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01575v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuannan Liu, Yaoyao Zhong, Weihong Deng, Hongzhi Shi, Xingchen Cui, Yunfeng Yin, Dongchao Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The blooming of social media and face recognition (FR) systems has increased
people's concern about privacy and security. A new type of adversarial privacy
cloak (class-universal) can be applied to all the images of regular users, to
prevent malicious FR systems from acquiring their identity information. In this
work, we discover the optimization dilemma in the existing methods -- the local
optima problem in large-batch optimization and the gradient information
elimination problem in small-batch optimization. To solve these problems, we
propose Gradient Accumulation (GA) to aggregate multiple small-batch gradients
into a one-step iterative gradient to enhance the gradient stability and reduce
the usage of quantization operations. Experiments show that our proposed method
achieves high performance on the Privacy-Commons dataset against black-box face
recognition models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Transformer-Based Adaptive Semantic Aggregation Method for UAV Visual
  Geo-Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shishen Li, Cuiwei Liu, Huaijun Qiu, Zhaokui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the task of Unmanned Aerial Vehicles (UAV) visual
geo-localization, which aims to match images of the same geographic target
taken by different platforms, i.e., UAVs and satellites. In general, the key to
achieving accurate UAV-satellite image matching lies in extracting visual
features that are robust against viewpoint changes, scale variations, and
rotations. Current works have shown that part matching is crucial for UAV
visual geo-localization since part-level representations can capture image
details and help to understand the semantic information of scenes. However, the
importance of preserving semantic characteristics in part-level representations
is not well discussed. In this paper, we introduce a transformer-based adaptive
semantic aggregation method that regards parts as the most representative
semantics in an image. Correlations of image patches to different parts are
learned in terms of the transformer's feature map. Then our method decomposes
part-level features into an adaptive sum of all patch features. By doing this,
the learned parts are encouraged to focus on patches with typical semantics.
Extensive experiments on the University-1652 dataset have shown the superiority
of our method over the current works.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ View Distribution Alignment with Progressive Adversarial Learning for
  UAV Visual Geo-Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cuiwei Liu, Jiahao Liu, Huaijun Qiu, Zhaokui Li, Xiangbin Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unmanned Aerial Vehicle (UAV) visual geo-localization aims to match images of
the same geographic target captured from different views, i.e., the UAV view
and the satellite view. It is very challenging due to the large appearance
differences in UAV-satellite image pairs. Previous works map images captured by
UAVs and satellites to a shared feature space and employ a classification
framework to learn location-dependent features while neglecting the overall
distribution shift between the UAV view and the satellite view. In this paper,
we address these limitations by introducing distribution alignment of the two
views to shorten their distance in a common space. Specifically, we propose an
end-to-end network, called PVDA (Progressive View Distribution Alignment).
During training, feature encoder, location classifier, and view discriminator
are jointly optimized by a novel progressive adversarial learning strategy.
Competition between feature encoder and view discriminator prompts both of them
to be stronger. It turns out that the adversarial learning is progressively
emphasized until UAV-view images are indistinguishable from satellite-view
images. As a result, the proposed PVDA becomes powerful in learning
location-dependent yet view-invariant features with good scalability towards
unseen images of new locations. Compared to the state-of-the-art methods, the
proposed PVDA requires less inference time but has achieved superior
performance on the University-1652 dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AttentionLut: Attention Fusion-based Canonical Polyadic LUT for
  Real-time Image Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kang Fu, Yicong Peng, Zicheng Zhang, Qihang Xu, Xiaohong Liu, Jia Wang, Guangtao Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, many algorithms have employed image-adaptive lookup tables (LUTs)
to achieve real-time image enhancement. Nonetheless, a prevailing trend among
existing methods has been the employment of linear combinations of basic LUTs
to formulate image-adaptive LUTs, which limits the generalization ability of
these methods. To address this limitation, we propose a novel framework named
AttentionLut for real-time image enhancement, which utilizes the attention
mechanism to generate image-adaptive LUTs. Our proposed framework consists of
three lightweight modules. We begin by employing the global image context
feature module to extract image-adaptive features. Subsequently, the attention
fusion module integrates the image feature with the priori attention feature
obtained during training to generate image-adaptive canonical polyadic tensors.
Finally, the canonical polyadic reconstruction module is deployed to
reconstruct image-adaptive residual 3DLUT, which is subsequently utilized for
enhancing input images. Experiments on the benchmark MIT-Adobe FiveK dataset
demonstrate that the proposed method achieves better enhancement performance
quantitatively and qualitatively than the state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One-Step Late Fusion Multi-view Clustering with Compressed Subspace 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyuan Ou, Pei Zhang, Sihang Zhou, En Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Late fusion multi-view clustering (LFMVC) has become a rapidly growing class
of methods in the multi-view clustering (MVC) field, owing to its excellent
computational speed and clustering performance. One bottleneck faced by
existing late fusion methods is that they are usually aligned to the average
kernel function, which makes the clustering performance highly dependent on the
quality of datasets. Another problem is that they require subsequent k-means
clustering after obtaining the consensus partition matrix to get the final
discrete labels, and the resulting separation of the label learning and cluster
structure optimization processes limits the integrity of these models. To
address the above issues, we propose an integrated framework named One-Step
Late Fusion Multi-view Clustering with Compressed Subspace (OS-LFMVC-CS).
Specifically, we use the consensus subspace to align the partition matrix while
optimizing the partition fusion, and utilize the fused partition matrix to
guide the learning of discrete labels. A six-step iterative optimization
approach with verified convergence is proposed. Sufficient experiments on
multiple datasets validate the effectiveness and efficiency of our proposed
method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-modal Learning with Missing Modality in Predicting Axillary Lymph
  Node Metastasis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01553v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01553v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shichuan Zhang, Sunyi Zheng, Zhongyi Shui, Honglin Li, Lin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal Learning has attracted widespread attention in medical image
analysis. Using multi-modal data, whole slide images (WSIs) and clinical
information, can improve the performance of deep learning models in the
diagnosis of axillary lymph node metastasis. However, clinical information is
not easy to collect in clinical practice due to privacy concerns, limited
resources, lack of interoperability, etc. Although patient selection can ensure
the training set to have multi-modal data for model development, missing
modality of clinical information can appear during test. This normally leads to
performance degradation, which limits the use of multi-modal models in the
clinic. To alleviate this problem, we propose a bidirectional distillation
framework consisting of a multi-modal branch and a single-modal branch. The
single-modal branch acquires the complete multi-modal knowledge from the
multi-modal branch, while the multi-modal learns the robust features of WSI
from the single-modal. We conduct experiments on a public dataset of Lymph Node
Metastasis in Early Breast Cancer to validate the method. Our approach not only
achieves state-of-the-art performance with an AUC of 0.861 on the test set
without missing data, but also yields an AUC of 0.842 when the rate of missing
modality is 80\%. This shows the effectiveness of the approach in dealing with
multi-modal data and missing modality. Such a model has the potential to
improve treatment decision-making for early breast cancer patients who have
axillary lymph node metastatic status.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CRA-PCN: Point Cloud Completion with Intra- and Inter-level
  Cross-Resolution Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Rong, Haoran Zhou, Lixin Yuan, Cheng Mei, Jiahao Wang, Tong Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Point cloud completion is an indispensable task for recovering complete point
clouds due to incompleteness caused by occlusion, limited sensor resolution,
etc. The family of coarse-to-fine generation architectures has recently
exhibited great success in point cloud completion and gradually became
mainstream. In this work, we unveil one of the key ingredients behind these
methods: meticulously devised feature extraction operations with explicit
cross-resolution aggregation. We present Cross-Resolution Transformer that
efficiently performs cross-resolution aggregation with local attention
mechanisms. With the help of our recursive designs, the proposed operation can
capture more scales of features than common aggregation operations, which is
beneficial for capturing fine geometric characteristics. While prior
methodologies have ventured into various manifestations of inter-level
cross-resolution aggregation, the effectiveness of intra-level one and their
combination has not been analyzed. With unified designs, Cross-Resolution
Transformer can perform intra- or inter-level cross-resolution aggregation by
switching inputs. We integrate two forms of Cross-Resolution Transformers into
one up-sampling block for point generation, and following the coarse-to-fine
manner, we construct CRA-PCN to incrementally predict complete shapes with
stacked up-sampling blocks. Extensive experiments demonstrate that our method
outperforms state-of-the-art methods by a large margin on several widely used
benchmarks. Codes are available at https://github.com/EasyRy/CRA-PCN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting of Implicit Neural Representation-based Image Denoiser 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01548v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01548v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zipei Yan, Zhengji Liu, Jizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit Neural Representation (INR) has emerged as an effective method for
unsupervised image denoising. However, INR models are typically
overparameterized; consequently, these models are prone to overfitting during
learning, resulting in suboptimal results, even noisy ones. To tackle this
problem, we propose a general recipe for regularizing INR models in image
denoising. In detail, we propose to iteratively substitute the supervision
signal with the mean value derived from both the prediction and supervision
signal during the learning process. We theoretically prove that such a simple
iterative substitute can gradually enhance the signal-to-noise ratio of the
supervision signal, thereby benefiting INR models during the learning process.
Our experimental results demonstrate that INR models can be effectively
regularized by the proposed approach, relieving overfitting and boosting image
denoising performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2024, code: https://github.com/TIDS-Lab/ITS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DDN-SLAM: Real-time Dense Dynamic Neural Implicit SLAM with Joint
  Semantic Encoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingrui Li, Jiaming He, Guangan Jiang, Hongyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose DDN-SLAM, a real-time dense neural implicit semantic SLAM system
designed for dynamic scenes. While existing neural implicit SLAM systems
perform well in static scenes, they often encounter challenges in real-world
environments with dynamic interferences, leading to ineffective tracking and
mapping. DDN-SLAM utilizes the priors provided by the deep semantic system,
combined with conditional probability fields, for segmentation.By constructing
depth-guided static masks and employing joint multi-resolution hashing
encoding, we ensure fast hole filling and high-quality mapping while mitigating
the effects of dynamic information interference. To enhance tracking
robustness, we utilize sparse feature points validated with optical flow and
keyframes, enabling loop closure detection and global bundle optimization.
Furthermore, DDN-SLAM supports monocular, stereo, and RGB-D inputs, operating
robustly at a frequency of 20-30Hz. Extensive experiments on 6 virtual/real
datasets demonstrate that our method outperforms state-of-the-art approaches in
both dynamic and static scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11pages, 4figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaborative Perception for Connected and Autonomous Driving:
  Challenges, Possible Solutions and Opportunities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01544v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01544v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Senkang Hu, Zhengru Fang, Yiqin Deng, Xianhao Chen, Yuguang Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving has attracted significant attention from both academia and
industries, which is expected to offer a safer and more efficient driving
system. However, current autonomous driving systems are mostly based on a
single vehicle, which has significant limitations which still poses threats to
driving safety. Collaborative perception with connected and autonomous vehicles
(CAVs) shows a promising solution to overcoming these limitations. In this
article, we first identify the challenges of collaborative perception, such as
data sharing asynchrony, data volume, and pose errors. Then, we discuss the
possible solutions to address these challenges with various technologies, where
the research opportunities are also elaborated. Furthermore, we propose a
scheme to deal with communication efficiency and latency problems, which is a
channel-aware collaborative perception framework to dynamically adjust the
communication graph and minimize latency, thereby improving perception
performance while increasing communication efficiency. Finally, we conduct
experiments to demonstrate the effectiveness of our proposed scheme.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Retraining-free Model Quantization via One-Shot Weight-Coupling Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01543v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01543v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Tang, Yuan Meng, Jiacheng Jiang, Shuzhao Xie, Rongwei Lu, Xinzhu Ma, Zhi Wang, Wenwu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantization is of significance for compressing the over-parameterized deep
neural models and deploying them on resource-limited devices. Fixed-precision
quantization suffers from performance drop due to the limited numerical
representation ability. Conversely, mixed-precision quantization (MPQ) is
advocated to compress the model effectively by allocating heterogeneous
bit-width for layers. MPQ is typically organized into a searching-retraining
two-stage process. Previous works only focus on determining the optimal
bit-width configuration in the first stage efficiently, while ignoring the
considerable time costs in the second stage. However, retraining always
consumes hundreds of GPU-hours on the cutting-edge GPUs, thus hindering
deployment efficiency significantly. In this paper, we devise a one-shot
training-searching paradigm for mixed-precision model compression.
Specifically, in the first stage, all potential bit-width configurations are
coupled and thus optimized simultaneously within a set of shared weights.
However, our observations reveal a previously unseen and severe bit-width
interference phenomenon among highly coupled weights during optimization,
leading to considerable performance degradation under a high compression ratio.
To tackle this problem, we first design a bit-width scheduler to dynamically
freeze the most turbulent bit-width of layers during training, to ensure the
rest bit-widths converged properly. Then, taking inspiration from information
theory, we present an information distortion mitigation technique to align the
behaviour of the bad-performing bit-widths to the well-performing ones.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DDPM based X-ray Image Synthesizer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Praveen Mahaulpatha, Thulana Abeywardane, Tomson George
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Access to high-quality datasets in the medical industry limits machine
learning model performance. To address this issue, we propose a Denoising
Diffusion Probabilistic Model (DDPM) combined with a UNet architecture for
X-ray image synthesis. Focused on pneumonia medical condition, our methodology
employs over 3000 pneumonia X-ray images obtained from Kaggle for training.
Results demonstrate the effectiveness of our approach, as the model
successfully generated realistic images with low Mean Squared Error (MSE). The
synthesized images showed distinct differences from non-pneumonia images,
highlighting the model's ability to capture key features of positive cases.
Beyond pneumonia, the applications of this synthesizer extend to various
medical conditions, provided an ample dataset is available. The capability to
produce high-quality images can potentially enhance machine learning models'
performance, aiding in more accurate and efficient medical diagnoses. This
innovative DDPM-based X-ray photo synthesizer presents a promising avenue for
addressing the scarcity of positive medical image datasets, paving the way for
improved medical image analysis and diagnosis in the healthcare industry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Glance and Focus: Memory Prompting for Multi-Event Video Question
  Answering <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01529v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01529v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyi Bai, Ruiping Wang, Xilin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Question Answering (VideoQA) has emerged as a vital tool to evaluate
agents' ability to understand human daily behaviors. Despite the recent success
of large vision language models in many multi-modal tasks, complex situation
reasoning over videos involving multiple human-object interaction events still
remains challenging. In contrast, humans can easily tackle it by using a series
of episode memories as anchors to quickly locate question-related key moments
for reasoning. To mimic this effective reasoning strategy, we propose the
Glance-Focus model. One simple way is to apply an action detection model to
predict a set of actions as key memories. However, these actions within a
closed set vocabulary are hard to generalize to various video domains. Instead
of that, we train an Encoder-Decoder to generate a set of dynamic event
memories at the glancing stage. Apart from using supervised bipartite matching
to obtain the event memories, we further design an unsupervised memory
generation method to get rid of dependence on event annotations. Next, at the
focusing stage, these event memories act as a bridge to establish the
correlation between the questions with high-level event concepts and low-level
lengthy video content. Given the question, the model first focuses on the
generated key event memory, then focuses on the most relevant moment for
reasoning through our designed multi-level cross-attention mechanism. We
conduct extensive experiments on four Multi-Event VideoQA benchmarks including
STAR, EgoTaskQA, AGQA, and NExT-QA. Our proposed model achieves
state-of-the-art results, surpassing current large models in various
challenging reasoning tasks. The code and models are available at
https://github.com/ByZ0e/Glance-Focus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal self-supervised learning for lesion localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01524v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01524v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Yang, Hong-Yu Zhou, Cheng Li, Weijian Huang, Jiarun Liu, Yong Liang, Shanshan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal deep learning utilizing imaging and diagnostic reports has made
impressive progress in the field of medical imaging diagnostics, demonstrating
a particularly strong capability for auxiliary diagnosis in cases where
sufficient annotation information is lacking. Nonetheless, localizing diseases
accurately without detailed positional annotations remains a challenge.
Although existing methods have attempted to utilize local information to
achieve fine-grained semantic alignment, their capability in extracting the
fine-grained semantics of the comprehensive contextual within reports is
limited. To solve this problem, we introduce a new method that takes full
sentences from textual reports as the basic units for local semantic alignment.
Our approach combines chest X-ray images with their corresponding textual
reports, performing contrastive learning at both global and local levels. The
leading results obtained by our method on multiple datasets confirm its
efficacy in the task of lesion localization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LORE++: Logical Location Regression Network for Table Structure
  Recognition with Pre-training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rujiao Long, Hangdi Xing, Zhibo Yang, Qi Zheng, Zhi Yu, Cong Yao, Fei Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Table structure recognition (TSR) aims at extracting tables in images into
machine-understandable formats. Recent methods solve this problem by predicting
the adjacency relations of detected cell boxes or learning to directly generate
the corresponding markup sequences from the table images. However, existing
approaches either count on additional heuristic rules to recover the table
structures, or face challenges in capturing long-range dependencies within
tables, resulting in increased complexity. In this paper, we propose an
alternative paradigm. We model TSR as a logical location regression problem and
propose a new TSR framework called LORE, standing for LOgical location
REgression network, which for the first time regresses logical location as well
as spatial location of table cells in a unified network. Our proposed LORE is
conceptually simpler, easier to train, and more accurate than other paradigms
of TSR. Moreover, inspired by the persuasive success of pre-trained models on a
number of computer vision and natural language processing tasks, we propose two
pre-training tasks to enrich the spatial and logical representations at the
feature level of LORE, resulting in an upgraded version called LORE++. The
incorporation of pre-training in LORE++ has proven to enjoy significant
advantages, leading to a substantial enhancement in terms of accuracy,
generalization, and few-shot capability compared to its predecessor.
Experiments on standard benchmarks against methods of previous paradigms
demonstrate the superiority of LORE++, which highlights the potential and
promising prospect of the logical location regression paradigm for TSR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2303.03730</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ S$^{2}$-DMs:Skip-Step Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan Wang, Shuangyin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have emerged as powerful generative tools, rivaling GANs in
sample quality and mirroring the likelihood scores of autoregressive models. A
subset of these models, exemplified by DDIMs, exhibit an inherent asymmetry:
they are trained over $T$ steps but only sample from a subset of $T$ during
generation. This selective sampling approach, though optimized for speed,
inadvertently misses out on vital information from the unsampled steps, leading
to potential compromises in sample quality. To address this issue, we present
the S$^{2}$-DMs, which is a new training method by using an innovative
$L_{skip}$, meticulously designed to reintegrate the information omitted during
the selective sampling phase. The benefits of this approach are manifold: it
notably enhances sample quality, is exceptionally simple to implement, requires
minimal code modifications, and is flexible enough to be compatible with
various sampling algorithms. On the CIFAR10 dataset, models trained using our
algorithm showed an improvement of 3.27% to 14.06% over models trained with
traditional methods across various sampling algorithms (DDIMs, PNDMs, DEIS) and
different numbers of sampling steps (10, 20, ..., 1000). On the CELEBA dataset,
the improvement ranged from 8.97% to 27.08%. Access to the code and additional
resources is provided in the github.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Answering from Sure to Uncertain: Uncertainty-Aware Curriculum Learning
  for Video Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haopeng Li, Qiuhong Ke, Mingming Gong, Tom Drummond
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While significant advancements have been made in video question answering
(VideoQA), the potential benefits of enhancing model generalization through
tailored difficulty scheduling have been largely overlooked in existing
research. This paper seeks to bridge that gap by incorporating VideoQA into a
curriculum learning (CL) framework that progressively trains models from
simpler to more complex data. Recognizing that conventional self-paced CL
methods rely on training loss for difficulty measurement, which might not
accurately reflect the intricacies of video-question pairs, we introduce the
concept of uncertainty-aware CL. Here, uncertainty serves as the guiding
principle for dynamically adjusting the difficulty. Furthermore, we address the
challenge posed by uncertainty by presenting a probabilistic modeling approach
for VideoQA. Specifically, we conceptualize VideoQA as a stochastic computation
graph, where the hidden representations are treated as stochastic variables.
This yields two distinct types of uncertainty: one related to the inherent
uncertainty in the data and another pertaining to the model's confidence. In
practice, we seamlessly integrate the VideoQA model into our framework and
conduct comprehensive experiments. The findings affirm that our approach not
only achieves enhanced performance but also effectively quantifies uncertainty
in the context of VideoQA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sports-QA: A Large-Scale Video Question Answering <span class="highlight-title">Benchmark</span> for Complex
  and Professional Sports 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01505v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01505v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haopeng Li, Andong Deng, Qiuhong Ke, Jun Liu, Hossein Rahmani, Yulan Guo, Bernt Schiele, Chen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning over sports videos for question answering is an important task with
numerous applications, such as player training and information retrieval.
However, this task has not been explored due to the lack of relevant datasets
and the challenging nature it presents. Most datasets for video question
answering (VideoQA) focus mainly on general and coarse-grained understanding of
daily-life videos, which is not applicable to sports scenarios requiring
professional action understanding and fine-grained motion analysis. In this
paper, we introduce the first dataset, named Sports-QA, specifically designed
for the sports VideoQA task. The Sports-QA dataset includes various types of
questions, such as descriptions, chronologies, causalities, and counterfactual
conditions, covering multiple sports. Furthermore, to address the
characteristics of the sports VideoQA task, we propose a new Auto-Focus
Transformer (AFT) capable of automatically focusing on particular scales of
temporal information for question answering. We conduct extensive experiments
on Sports-QA, including baseline studies and the evaluation of different
methods. The results demonstrate that our AFT achieves state-of-the-art
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Pixel to Slide image: Polarization Modality-based Pathological
  Diagnosis Using Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01496v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01496v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia Dong, Yao Yao, Yang Dong, Hui Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Thyroid cancer is the most common endocrine malignancy, and accurately
distinguishing between benign and malignant thyroid tumors is crucial for
developing effective treatment plans in clinical practice. Pathologically,
thyroid tumors pose diagnostic challenges due to improper specimen sampling. In
this study, we have designed a three-stage model using representation learning
to integrate pixel-level and slice-level annotations for distinguishing thyroid
tumors. This structure includes a pathology structure recognition method to
predict structures related to thyroid tumors, an encoder-decoder network to
extract pixel-level annotation information by learning the feature
representations of image blocks, and an attention-based learning mechanism for
the final classification task. This mechanism learns the importance of
different image blocks in a pathological region, globally considering the
information from each block. In the third stage, all information from the image
blocks in a region is aggregated using attention mechanisms, followed by
classification to determine the category of the region. Experimental results
demonstrate that our proposed method can predict microscopic structures more
accurately. After color-coding, the method achieves results on unstained
pathology slides that approximate the quality of Hematoxylin and eosin
staining, reducing the need for stained pathology slides. Furthermore, by
leveraging the concept of indirect measurement and extracting polarized
features from structures correlated with lesions, the proposed method can also
classify samples where membrane structures cannot be obtained through sampling,
providing a potential objective and highly accurate indirect diagnostic
technique for thyroid tumors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incorporating Geo-Diverse Knowledge into Prompting for Increased
  Geographical Robustness in Object Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyle Buettner, Sina Malakouti, Xiang Lorraine Li, Adriana Kovashka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing object recognition models have been shown to lack robustness in
diverse geographical scenarios due to significant domain shifts in design and
context. Class representations need to be adapted to more accurately reflect an
object concept under these shifts. In the absence of training data from target
geographies, we hypothesize that geography-specific descriptive knowledge of
object categories can be leveraged to enhance robustness. For this purpose, we
explore the feasibility of probing a large-language model for
geography-specific object knowledge, and we investigate integrating knowledge
in zero-shot and learnable soft prompting with the CLIP vision-language model.
In particular, we propose a geography knowledge regularization method to ensure
that soft prompts trained on a source set of geographies generalize to an
unseen target set of geographies. Our gains on DollarStreet when generalizing
from a model trained only on data from Europe are as large as +2.8 on countries
from Africa, and +4.6 on the hardest classes. We further show competitive
performance vs. few-shot target training, and provide insights into how
descriptive knowledge captures geographical differences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token Propagation Controller for Efficient Vision Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision transformers (ViTs) have achieved promising results on a variety of
Computer Vision tasks, however their quadratic complexity in the number of
input tokens has limited their application specially in resource-constrained
settings. Previous approaches that employ gradual token reduction to address
this challenge assume that token redundancy in one layer implies redundancy in
all the following layers. We empirically demonstrate that this assumption is
often not correct, i.e., tokens that are redundant in one layer can be useful
in later layers. We employ this key insight to propose a novel token
propagation controller (TPC) that incorporates two different
token-distributions, i.e., pause probability and restart probability to control
the reduction and reuse of tokens respectively, which results in more efficient
token utilization. To improve the estimates of token distributions, we propose
a smoothing mechanism that acts as a regularizer and helps remove noisy
outliers. Furthermore, to improve the training-stability of our proposed TPC,
we introduce a model stabilizer that is able to implicitly encode local image
structures and minimize accuracy fluctuations during model training. We present
extensive experimental results on the ImageNet-1K dataset using DeiT, LV-ViT
and Swin models to demonstrate the effectiveness of our proposed method. For
example, compared to baseline models, our proposed method improves the
inference speed of the DeiT-S by 250% while increasing the classification
accuracy by 1.0%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WACV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GPS-SSL: Guided Positive Sampling to Inject Prior Into Self-Supervised
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aarash Feizi, Randall Balestriero, Adriana Romero-Soriano, Reihaneh Rabbany
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Guided Positive Sampling Self-Supervised Learning (GPS-SSL), a
general method to inject a priori knowledge into Self-Supervised Learning (SSL)
positive samples selection. Current SSL methods leverage Data-Augmentations
(DA) for generating positive samples and incorporate prior knowledge - an
incorrect, or too weak DA will drastically reduce the quality of the learned
representation. GPS-SSL proposes instead to design a metric space where
Euclidean distances become a meaningful proxy for semantic relationship. In
that space, it is now possible to generate positive samples from nearest
neighbor sampling. Any prior knowledge can now be embedded into that metric
space independently from the employed DA. From its simplicity, GPS-SSL is
applicable to any SSL method, e.g. SimCLR or BYOL. A key benefit of GPS-SSL is
in reducing the pressure in tailoring strong DAs. For example GPS-SSL reaches
85.58% on Cifar10 with weak DA while the baseline only reaches 37.51%. We
therefore move a step forward towards the goal of making SSL less reliant on
DA. We also show that even when using strong DAs, GPS-SSL outperforms the
baselines on under-studied domains. We evaluate GPS-SSL along with multiple
baseline SSL methods on numerous downstream datasets from different domains
when the models use strong or minimal data augmentations. We hope that GPS-SSL
will open new avenues in studying how to inject a priori knowledge into SSL in
a principled manner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AUPIMO: Redefining Visual Anomaly Detection <span class="highlight-title">Benchmark</span>s with High Speed
  and Low Tolerance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01984v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01984v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joao P. C. Bertoldo, Dick Ameln, Ashwin Vaidya, Samet Akçay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in visual anomaly detection research have seen AUROC and
AUPRO scores on public benchmark datasets such as MVTec and VisA converge
towards perfect recall, giving the impression that these benchmarks are
near-solved. However, high AUROC and AUPRO scores do not always reflect
qualitative performance, which limits the validity of these metrics in
real-world applications. We argue that the artificial ceiling imposed by the
lack of an adequate evaluation metric restrains progression of the field, and
it is crucial that we revisit the evaluation metrics used to rate our
algorithms. In response, we introduce Per-IMage Overlap (PIMO), a novel metric
that addresses the shortcomings of AUROC and AUPRO. PIMO retains the
recall-based nature of the existing metrics but introduces two distinctions:
the assignment of curves (and respective area under the curve) is per-image,
and its X-axis relies solely on normal images. Measuring recall per image
simplifies instance score indexing and is more robust to noisy annotations. As
we show, it also accelerates computation and enables the usage of statistical
tests to compare models. By imposing low tolerance for false positives on
normal images, PIMO provides an enhanced model validation procedure and
highlights performance variations across datasets. Our experiments demonstrate
that PIMO offers practical advantages and nuanced performance insights that
redefine anomaly detection benchmarks -- notably challenging the perception
that MVTec AD and VisA datasets have been solved by contemporary models.
Available on GitHub: https://github.com/jpcbertoldo/aupimo.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Truly Zero-shot Compositional Visual Reasoning with LLMs as
  Programmers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandar Stanić, Sergi Caelles, Michael Tschannen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual reasoning is dominated by end-to-end neural networks scaled to
billions of model parameters and training examples. However, even the largest
models struggle with compositional reasoning, generalization, fine-grained
spatial and temporal reasoning, and counting. Visual reasoning with large
language models (LLMs) as controllers can, in principle, address these
limitations by decomposing the task and solving subtasks by orchestrating a set
of (visual) tools. Recently, these models achieved great performance on tasks
such as compositional visual question answering, visual grounding, and video
temporal reasoning. Nevertheless, in their current form, these models heavily
rely on human engineering of in-context examples in the prompt, which are often
dataset- and task-specific and require significant labor by highly skilled
programmers. In this work, we present a framework that mitigates these issues
by introducing spatially and temporally abstract routines and by leveraging a
small number of labeled examples to automatically generate in-context examples,
thereby avoiding human-created in-context examples. On a number of visual
reasoning tasks, we show that our framework leads to consistent gains in
performance, makes LLMs as controllers setup more robust, and removes the need
for human engineering of in-context examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FMGS: Foundation Model Embedded 3D Gaussian Splatting for Holistic 3D
  Scene Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01970v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01970v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingxing Zuo, Pouya Samangouei, Yunwen Zhou, Yan Di, Mingyang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Precisely perceiving the geometric and semantic properties of real-world 3D
objects is crucial for the continued evolution of augmented reality and robotic
applications. To this end, we present \algfull{} (\algname{}), which
incorporates vision-language embeddings of foundation models into 3D Gaussian
Splatting (GS). The key contribution of this work is an efficient method to
reconstruct and represent 3D vision-language models. This is achieved by
distilling feature maps generated from image-based foundation models into those
rendered from our 3D model. To ensure high-quality rendering and fast training,
we introduce a novel scene representation by integrating strengths from both GS
and multi-resolution hash encodings (MHE). Our effective training procedure
also introduces a pixel alignment loss that makes the rendered feature distance
of same semantic entities close, following the pixel-level semantic boundaries.
Our results demonstrate remarkable multi-view semantic consistency,
facilitating diverse downstream tasks, beating state-of-the-art methods by
$\mathbf{10.2}$ percent on open-vocabulary language-based object detection,
despite that we are $\mathbf{851\times}$ faster for inference. This research
explores the intersection of vision, language, and 3D scene representation,
paving the way for enhanced scene understanding in uncontrolled real-world
environments. We plan to release the code upon paper acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, Project page coming soon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Instruct-Imagen: Image Generation with Multi-modal Instruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01952v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01952v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hexiang Hu, Kelvin C. K. Chan, Yu-Chuan Su, Wenhu Chen, Yandong Li, Kihyuk Sohn, Yang Zhao, Xue Ben, Boqing Gong, William Cohen, Ming-Wei Chang, Xuhui Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents instruct-imagen, a model that tackles heterogeneous image
generation tasks and generalizes across unseen tasks. We introduce *multi-modal
instruction* for image generation, a task representation articulating a range
of generation intents with precision. It uses natural language to amalgamate
disparate modalities (e.g., text, edge, style, subject, etc.), such that
abundant generation intents can be standardized in a uniform format.
  We then build instruct-imagen by fine-tuning a pre-trained text-to-image
diffusion model with a two-stage framework. First, we adapt the model using the
retrieval-augmented training, to enhance model's capabilities to ground its
generation on external multimodal context. Subsequently, we fine-tune the
adapted model on diverse image generation tasks that requires vision-language
understanding (e.g., subject-driven generation, etc.), each paired with a
multi-modal instruction encapsulating the task's essence. Human evaluation on
various image generation datasets reveals that instruct-imagen matches or
surpasses prior task-specific models in-domain and demonstrates promising
generalization to unseen and more complex tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can We Generate Realistic Hands Only Using Convolution? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01951v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01951v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehran Hosseini, Peyman Hosseini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The enduring inability of image generative models to recreate intricate
geometric features, such as those present in human hands and fingers has been
an ongoing problem in image generation for nearly a decade. While strides have
been made by increasing model sizes and diversifying training datasets, this
issue remains prevalent across all models, from denoising diffusion models to
Generative Adversarial Networks (GAN), pointing to a fundamental shortcoming in
the underlying architectures. In this paper, we demonstrate how this problem
can be mitigated by augmenting convolution layers geometric capabilities
through providing them with a single input channel incorporating the relative
$n$-dimensional Cartesian coordinate system. We show that this drastically
improves quality of hand and face images generated by GANs and Variational
AutoEncoders (VAE).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Contains 17 pages, 14 figures, and 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Object-Centric Learning from Multiple Unspecified
  Viewpoints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01922v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01922v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyang Yuan, Tonglin Chen, Zhimeng Shen, Bin Li, Xiangyang Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual scenes are extremely diverse, not only because there are infinite
possible combinations of objects and backgrounds but also because the
observations of the same scene may vary greatly with the change of viewpoints.
When observing a multi-object visual scene from multiple viewpoints, humans can
perceive the scene compositionally from each viewpoint while achieving the
so-called ``object constancy'' across different viewpoints, even though the
exact viewpoints are untold. This ability is essential for humans to identify
the same object while moving and to learn from vision efficiently. It is
intriguing to design models that have a similar ability. In this paper, we
consider a novel problem of learning compositional scene representations from
multiple unspecified (i.e., unknown and unrelated) viewpoints without using any
supervision and propose a deep generative model which separates latent
representations into a viewpoint-independent part and a viewpoint-dependent
part to solve this problem. During the inference, latent representations are
randomly initialized and iteratively updated by integrating the information in
different viewpoints with neural networks. Experiments on several specifically
designed synthetic datasets have shown that the proposed method can effectively
learn from multiple unspecified viewpoints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2112.03568</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PressureVision++: Estimating Fingertip Pressure from Diverse RGB Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.02310v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.02310v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Grady, Jeremy A. Collins, Chengcheng Tang, Christopher D. Twigg, Kunal Aneja, James Hays, Charles C. Kemp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Touch plays a fundamental role in manipulation for humans; however, machine
perception of contact and pressure typically requires invasive sensors. Recent
research has shown that deep models can estimate hand pressure based on a
single RGB image. However, evaluations have been limited to controlled settings
since collecting diverse data with ground-truth pressure measurements is
difficult. We present a novel approach that enables diverse data to be captured
with only an RGB camera and a cooperative participant. Our key insight is that
people can be prompted to apply pressure in a certain way, and this prompt can
serve as a weak label to supervise models to perform well under varied
conditions. We collect a novel dataset with 51 participants making fingertip
contact with diverse objects. Our network, PressureVision++, outperforms human
annotators and prior work. We also demonstrate an application of
PressureVision++ to mixed reality where pressure estimation allows everyday
surfaces to be used as arbitrary touch-sensitive interfaces. Code, data, and
models are available online.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics
  (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17033v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17033v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anahita Fathi Kazerooni, Nastaran Khalili, Xinyang Liu, Debanjan Haldar, Zhifan Jiang, Syed Muhammed Anwar, Jake Albrecht, Maruf Adewole, Udunna Anazodo, Hannah Anderson, Sina Bagheri, Ujjwal Baid, Timothy Bergquist, Austin J. Borja, Evan Calabrese, Verena Chung, Gian-Marco Conte, Farouk Dako, James Eddy, Ivan Ezhov, Ariana Familiar, Keyvan Farahani, Shuvanjan Haldar, Juan Eugenio Iglesias, Anastasia Janas, Elaine Johansen, Blaise V Jones, Florian Kofler, Dominic LaBella, Hollie Anne Lai, Koen Van Leemput, Hongwei Bran Li, Nazanin Maleki, Aaron S McAllister, Zeke Meier, Bjoern Menze, Ahmed W Moawad, Khanak K Nandolia, Julija Pavaine, Marie Piraud, Tina Poussaint, Sanjay P Prabhu, Zachary Reitman, Andres Rodriguez, Jeffrey D Rudie, Ibraheem Salman Shaikh, Lubdha M. Shah, Nakul Sheth, Russel Taki Shinohara, Wenxin Tu, Karthik Viswanathan, Chunhao Wang, Jeffrey B Ware, Benedikt Wiestler, Walter Wiggins, Anna Zapaishchykova, Mariam Aboian, Miriam Bornhorst, Peter de Blank, Michelle Deutsch, Maryam Fouladi, Lindsey Hoffman, Benjamin Kann, Margot Lazow, Leonie Mikael, Ali Nabavizadeh, Roger Packer, Adam Resnick, Brian Rood, Arastoo Vossough, Spyridon Bakas, Marius George Linguraru
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pediatric tumors of the central nervous system are the most common cause of
cancer-related death in children. The five-year survival rate for high-grade
gliomas in children is less than 20\%. Due to their rarity, the diagnosis of
these entities is often delayed, their treatment is mainly based on historic
treatment concepts, and clinical trials require multi-institutional
collaborations. The MICCAI Brain Tumor Segmentation (BraTS) Challenge is a
landmark community benchmark event with a successful history of 12 years of
resource creation for the segmentation and analysis of adult glioma. Here we
present the CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023 challenge, which
represents the first BraTS challenge focused on pediatric brain tumors with
data acquired across multiple international consortia dedicated to pediatric
neuro-oncology and clinical trials. The BraTS-PEDs 2023 challenge focuses on
benchmarking the development of volumentric segmentation algorithms for
pediatric brain glioma through standardized quantitative performance evaluation
metrics utilized across the BraTS 2023 cluster of challenges. Models gaining
knowledge from the BraTS-PEDs multi-parametric structural MRI (mpMRI) training
data will be evaluated on separate validation and unseen test mpMRI dataof
high-grade pediatric glioma. The CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023
challenge brings together clinicians and AI/imaging scientists to lead to
faster development of automated segmentation techniques that could benefit
clinical trials, and ultimately the care of children with brain tumors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ M3D: <span class="highlight-title">Dataset</span> Condensation by Minimizing Maximum Mean Discrepancy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15927v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15927v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hansong Zhang, Shikun Li, Pengju Wang, Dan Zeng, Shiming Ge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training state-of-the-art (SOTA) deep models often requires extensive data,
resulting in substantial training and storage costs. To address these
challenges, dataset condensation has been developed to learn a small synthetic
set that preserves essential information from the original large-scale dataset.
Nowadays, optimization-oriented methods have been the primary method in the
field of dataset condensation for achieving SOTA results. However, the bi-level
optimization process hinders the practical application of such methods to
realistic and larger datasets. To enhance condensation efficiency, previous
works proposed Distribution-Matching (DM) as an alternative, which
significantly reduces the condensation cost. Nonetheless, current DM-based
methods have yielded less comparable results to optimization-oriented methods
due to their focus on aligning only the first moment of the distributions. In
this paper, we present a novel DM-based method named M3D for dataset
condensation by Minimizing the Maximum Mean Discrepancy between feature
representations of the synthetic and real images. By embedding their
distributions in a reproducing kernel Hilbert space, we align all orders of
moments of the distributions of real and synthetic images, resulting in a more
generalized condensed set. Notably, our method even surpasses the SOTA
optimization-oriented method IDC on the high-resolution ImageNet dataset.
Extensive analysis is conducted to verify the effectiveness of the proposed
method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted in AAAI-24</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SYNTA: A novel approach for deep learning-based image analysis in muscle
  histopathology using photo-realistic synthetic data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.14650v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.14650v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonid Mill, Oliver Aust, Jochen A. Ackermann, Philipp Burger, Monica Pascual, Katrin Palumbo-Zerr, Gerhard Krönke, Stefan Uderhardt, Georg Schett, Christoph S. Clemen, Rolf Schröder, Christian Holtzhausen, Samir Jabari, Andreas Maier, Anika Grüneboom
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence (AI), machine learning, and deep learning (DL)
methods are becoming increasingly important in the field of biomedical image
analysis. However, to exploit the full potential of such methods, a
representative number of experimentally acquired images containing a
significant number of manually annotated objects is needed as training data.
Here we introduce SYNTA (synthetic data) as a novel approach for the generation
of synthetic, photo-realistic, and highly complex biomedical images as training
data for DL systems. We show the versatility of our approach in the context of
muscle fiber and connective tissue analysis in histological sections. We
demonstrate that it is possible to perform robust and expert-level segmentation
tasks on previously unseen real-world data, without the need for manual
annotations using synthetic training data alone. Being a fully parametric
technique, our approach poses an interpretable and controllable alternative to
Generative Adversarial Networks (GANs) and has the potential to significantly
accelerate quantitative image analysis in a variety of biomedical applications
in microscopy and beyond.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distribution Matching for Multi-Task Learning of Classification Tasks: a
  Large-Scale Study on Faces & Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01219v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01219v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitrios Kollias, Viktoriia Sharmanska, Stefanos Zafeiriou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Task Learning (MTL) is a framework, where multiple related tasks are
learned jointly and benefit from a shared representation space, or parameter
transfer. To provide sufficient learning support, modern MTL uses annotated
data with full, or sufficiently large overlap across tasks, i.e., each input
sample is annotated for all, or most of the tasks. However, collecting such
annotations is prohibitive in many real applications, and cannot benefit from
datasets available for individual tasks. In this work, we challenge this setup
and show that MTL can be successful with classification tasks with little, or
non-overlapping annotations, or when there is big discrepancy in the size of
labeled data per task. We explore task-relatedness for co-annotation and
co-training, and propose a novel approach, where knowledge exchange is enabled
between the tasks via distribution matching. To demonstrate the general
applicability of our method, we conducted diverse case studies in the domains
of affective computing, face recognition, species recognition, and shopping
item classification using nine datasets. Our large-scale study of affective
tasks for basic expression recognition and facial action unit detection
illustrates that our approach is network agnostic and brings large performance
improvements compared to the state-of-the-art in both tasks and across all
studied databases. In all case studies, we show that co-training via
task-relatedness is advantageous and prevents negative transfer (which occurs
when MT model's performance is worse than that of at least one single-task
model).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at AAAI 2024. arXiv admin note: text overlap with
  arXiv:2105.03790</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SVGDreamer: Text Guided SVG Generation with Diffusion Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16476v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16476v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ximing Xing, Haitao Zhou, Chuang Wang, Jing Zhang, Dong Xu, Qian Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, text-guided scalable vector graphics (SVGs) synthesis has shown
promise in domains such as iconography and sketch. However, existing
text-to-SVG generation methods lack editability and struggle with visual
quality and result diversity. To address these limitations, we propose a novel
text-guided vector graphics synthesis method called SVGDreamer. SVGDreamer
incorporates a semantic-driven image vectorization (SIVE) process that enables
the decomposition of synthesis into foreground objects and background, thereby
enhancing editability. Specifically, the SIVE process introduce attention-based
primitive control and an attention-mask loss function for effective control and
manipulation of individual elements. Additionally, we propose a Vectorized
Particle-based Score Distillation (VPSD) approach to tackle the challenges of
color over-saturation, vector primitives over-smoothing, and limited result
diversity in existing text-to-SVG generation methods. Furthermore, on the basis
of VPSD, we introduce Reward Feedback Learning (ReFL) to accelerate VPSD
convergence and improve aesthetic appeal. Extensive experiments have been
conducted to validate the effectiveness of SVGDreamer, demonstrating its
superiority over baseline methods in terms of editability, visual quality, and
diversity. The code and demo of SVGDreamer can be found at
\href{https://ximinng.github.io/SVGDreamer-project/}{https://ximinng.github.io/SVGDreamer-project/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 15 figures, project link:
  https://ximinng.github.io/SVGDreamer-project/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fading memory as inductive bias in residual recurrent networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.14823v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.14823v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Igor Dubinin, Felix Effenberger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Residual connections have been proposed as an architecture-based inductive
bias to mitigate the problem of exploding and vanishing gradients and increased
task performance in both feed-forward and recurrent networks (RNNs) when
trained with the backpropagation algorithm. Yet, little is known about how
residual connections in RNNs influence their dynamics and fading memory
properties. Here, we introduce weakly coupled residual recurrent networks
(WCRNNs) in which residual connections result in well-defined Lyapunov
exponents and allow for studying properties of fading memory. We investigate
how the residual connections of WCRNNs influence their performance, network
dynamics, and memory properties on a set of benchmark tasks. We show that
several distinct forms of residual connections yield effective inductive biases
that result in increased network expressivity. In particular, those are
residual connections that (i) result in network dynamics at the proximity of
the edge of chaos, (ii) allow networks to capitalize on characteristic spectral
properties of the data, and (iii) result in heterogeneous memory properties. In
addition, we demonstrate how our results can be extended to non-linear
residuals and introduce a weakly coupled residual initialization scheme that
can be used for Elman RNNs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inversion-by-Inversion: Exemplar-based Sketch-to-Photo Synthesis via
  Stochastic Differential Equations without Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.07665v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.07665v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ximing Xing, Chuang Wang, Haitao Zhou, Zhihao Hu, Chongxuan Li, Dong Xu, Qian Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exemplar-based sketch-to-photo synthesis allows users to generate
photo-realistic images based on sketches. Recently, diffusion-based methods
have achieved impressive performance on image generation tasks, enabling
highly-flexible control through text-driven generation or energy functions.
However, generating photo-realistic images with color and texture from sketch
images remains challenging for diffusion models. Sketches typically consist of
only a few strokes, with most regions left blank, making it difficult for
diffusion-based methods to produce photo-realistic images. In this work, we
propose a two-stage method named ``Inversion-by-Inversion" for exemplar-based
sketch-to-photo synthesis. This approach includes shape-enhancing inversion and
full-control inversion. During the shape-enhancing inversion process, an
uncolored photo is generated with the guidance of a shape-energy function. This
step is essential to ensure control over the shape of the generated photo. In
the full-control inversion process, we propose an appearance-energy function to
control the color and texture of the final generated photo.Importantly, our
Inversion-by-Inversion pipeline is training-free and can accept different types
of exemplars for color and texture control. We conducted extensive experiments
to evaluate our proposed method, and the results demonstrate its effectiveness.
The code and project can be found at
https://ximinng.github.io/inversion-by-inversion-project/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HOI4D: A 4D Egocentric <span class="highlight-title">Dataset</span> for Category-Level Human-Object
  Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.01577v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.01577v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunze Liu, Yun Liu, Che Jiang, Kangbo Lyu, Weikang Wan, Hao Shen, Boqiang Liang, Zhoujie Fu, He Wang, Li Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present HOI4D, a large-scale 4D egocentric dataset with rich annotations,
to catalyze the research of category-level human-object interaction. HOI4D
consists of 2.4M RGB-D egocentric video frames over 4000 sequences collected by
4 participants interacting with 800 different object instances from 16
categories over 610 different indoor rooms. Frame-wise annotations for panoptic
segmentation, motion segmentation, 3D hand pose, category-level object pose and
hand action have also been provided, together with reconstructed object meshes
and scene point clouds. With HOI4D, we establish three benchmarking tasks to
promote category-level HOI from 4D visual signals including semantic
segmentation of 4D dynamic point cloud sequences, category-level object pose
tracking, and egocentric action segmentation with diverse interaction targets.
In-depth analysis shows HOI4D poses great challenges to existing methods and
produces great research opportunities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive Study of Object Tracking in Low-Light Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16250v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16250v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anqi Yi, Nantheera Anantrasirichai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate object tracking in low-light environments is crucial, particularly
in surveillance and ethology applications. However, achieving this is
significantly challenging due to the poor quality of captured sequences.
Factors such as noise, color imbalance, and low contrast contribute to these
challenges. This paper presents a comprehensive study examining the impact of
these distortions on automatic object trackers. Additionally, we propose a
solution to enhance tracking performance by integrating denoising and low-light
enhancement methods into the transformer-based object tracking system.
Experimental results show that the proposed tracker, trained with low-light
synthetic datasets, outperforms both the vanilla MixFormer and Siam R-CNN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian posterior approximation with stochastic ensembles <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08123v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08123v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oleksandr Balabanov, Bernhard Mehlig, Hampus Linander
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce ensembles of stochastic neural networks to approximate the
Bayesian posterior, combining stochastic methods such as dropout with deep
ensembles. The stochastic ensembles are formulated as families of distributions
and trained to approximate the Bayesian posterior with variational inference.
We implement stochastic ensembles based on Monte Carlo dropout, DropConnect and
a novel non-parametric version of dropout and evaluate them on a toy problem
and CIFAR image classification. For both tasks, we test the quality of the
posteriors directly against Hamiltonian Monte Carlo simulations. Our results
show that stochastic ensembles provide more accurate posterior estimates than
other popular baselines for Bayesian inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SkateboardAI: The Coolest Video Action Recognition for Skateboarding <span class="chip">CVPR 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.11467v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.11467v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanxiao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Impressed by the coolest skateboarding sports program from 2021 Tokyo Olympic
Games, we are the first to curate the original real-world video datasets
"SkateboardAI" in the wild, even self-design and implement diverse uni-modal
and multi-modal video action recognition approaches to recognize different
tricks accurately. For uni-modal methods, we separately apply (1) CNN and LSTM;
(2) CNN and BiLSTM; (3) CNN and BiLSTM with effective attention mechanisms; (4)
Transformer-based action recognition pipeline. Transferred to the multi-modal
conditions, we investigated the two-stream Inflated-3D architecture on
"SkateboardAI" datasets to compare its performance with uni-modal cases. In
sum, our objective is developing an excellent AI sport referee for the coolest
skateboarding competitions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The original first-author work has been accepted and presented by
  CVPR 2022 WiCV Workshop (This is the long-version paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lookup Table meets Local Laplacian Filter: Pyramid Reconstruction
  Network for Tone Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17190v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17190v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Zhang, Ming Tian, Zhiqiang Li, Bin Xu, Qingbo Lu, Changxin Gao, Nong Sang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tone mapping aims to convert high dynamic range (HDR) images to low dynamic
range (LDR) representations, a critical task in the camera imaging pipeline. In
recent years, 3-Dimensional LookUp Table (3D LUT) based methods have gained
attention due to their ability to strike a favorable balance between
enhancement performance and computational efficiency. However, these methods
often fail to deliver satisfactory results in local areas since the look-up
table is a global operator for tone mapping, which works based on pixel values
and fails to incorporate crucial local information. To this end, this paper
aims to address this issue by exploring a novel strategy that integrates global
and local operators by utilizing closed-form Laplacian pyramid decomposition
and reconstruction. Specifically, we employ image-adaptive 3D LUTs to
manipulate the tone in the low-frequency image by leveraging the specific
characteristics of the frequency information. Furthermore, we utilize local
Laplacian filters to refine the edge details in the high-frequency components
in an adaptive manner. Local Laplacian filters are widely used to preserve edge
details in photographs, but their conventional usage involves manual tuning and
fixed implementation within camera imaging pipelines or photo editing tools. We
propose to learn parameter value maps progressively for local Laplacian filters
from annotated data using a lightweight network. Our model achieves
simultaneous global tone manipulation and local edge detail preservation in an
end-to-end manner. Extensive experimental results on two benchmark datasets
demonstrate that the proposed method performs favorably against
state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures, accepted by NeurlPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpyroPose: SE(3) Pyramids for Object Pose Distribution Estimation <span class="chip">ICCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05308v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05308v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rasmus Laurvig Haugaard, Frederik Hagelskjær, Thorbjørn Mosekjær Iversen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object pose estimation is a core computer vision problem and often an
essential component in robotics. Pose estimation is usually approached by
seeking the single best estimate of an object's pose, but this approach is
ill-suited for tasks involving visual ambiguity. In such cases it is desirable
to estimate the uncertainty as a pose distribution to allow downstream tasks to
make informed decisions. Pose distributions can have arbitrary complexity which
motivates estimating unparameterized distributions, however, until now they
have only been used for orientation estimation on SO(3) due to the difficulty
in training on and normalizing over SE(3). We propose a novel method for pose
distribution estimation on SE(3). We use a hierarchical grid, a pyramid, which
enables efficient importance sampling during training and sparse evaluation of
the pyramid at inference, allowing real time 6D pose distribution estimation.
Our method outperforms state-of-the-art methods on SO(3), and to the best of
our knowledge, we provide the first quantitative results on pose distribution
estimation on SE(3). Code will be available at spyropose.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCVW 2023 (R6D)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Model with Perceptual Loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00110v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00110v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shanchuan Lin, Xiao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models trained with mean squared error loss tend to generate
unrealistic samples. Current state-of-the-art models rely on classifier-free
guidance to improve sample quality, yet its surprising effectiveness is not
fully understood. In this paper, We show that the effectiveness of
classifier-free guidance partly originates from it being a form of implicit
perceptual guidance. As a result, we can directly incorporate perceptual loss
in diffusion training to improve sample quality. Since the score matching
objective used in diffusion training strongly resembles the denoising
autoencoder objective used in unsupervised training of perceptual networks, the
diffusion model itself is a perceptual network and can be used to generate
meaningful perceptual loss. We propose a novel self-perceptual objective that
results in diffusion models capable of generating more realistic samples. For
conditional generation, our method only improves sample quality without
entanglement with the conditional input and therefore does not sacrifice sample
diversity. Our method can also improve sample quality for unconditional
generation, which was not possible with classifier-free guidance before.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Align Your Gaussians: Text-to-4D with Dynamic 3D Gaussians and Composed
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.13763v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.13763v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huan Ling, Seung Wook Kim, Antonio Torralba, Sanja Fidler, Karsten Kreis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-guided diffusion models have revolutionized image and video generation
and have also been successfully used for optimization-based 3D object
synthesis. Here, we instead focus on the underexplored text-to-4D setting and
synthesize dynamic, animated 3D objects using score distillation methods with
an additional temporal dimension. Compared to previous work, we pursue a novel
compositional generation-based approach, and combine text-to-image,
text-to-video, and 3D-aware multiview diffusion models to provide feedback
during 4D object optimization, thereby simultaneously enforcing temporal
consistency, high-quality visual appearance and realistic geometry. Our method,
called Align Your Gaussians (AYG), leverages dynamic 3D Gaussian Splatting with
deformation fields as 4D representation. Crucial to AYG is a novel method to
regularize the distribution of the moving 3D Gaussians and thereby stabilize
the optimization and induce motion. We also propose a motion amplification
mechanism as well as a new autoregressive synthesis scheme to generate and
combine multiple 4D sequences for longer generation. These techniques allow us
to synthesize vivid dynamic scenes, outperform previous work qualitatively and
quantitatively and achieve state-of-the-art text-to-4D performance. Due to the
Gaussian 4D representation, different 4D animations can be seamlessly combined,
as we demonstrate. AYG opens up promising avenues for animation, simulation and
digital content creation as well as synthetic data generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page:
  https://research.nvidia.com/labs/toronto-ai/AlignYourGaussians/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regressor-Segmenter Mutual Prompt Learning for Crowd Counting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01711v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01711v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyue Guo, Li Yuan, Zhaoyi Yan, Binghui Chen, Yaowei Wang, Qixiang Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crowd counting has achieved significant progress by training regressors to
predict instance positions. In heavily crowded scenarios, however, regressors
are challenged by uncontrollable annotation variance, which causes density map
bias and context information inaccuracy. In this study, we propose mutual
prompt learning (mPrompt), which leverages a regressor and a segmenter as
guidance for each other, solving bias and inaccuracy caused by annotation
variance while distinguishing foreground from background. In specific, mPrompt
leverages point annotations to tune the segmenter and predict pseudo head masks
in a way of point prompt learning. It then uses the predicted segmentation
masks, which serve as spatial constraint, to rectify biased point annotations
as context prompt learning. mPrompt defines a way of mutual information
maximization from prompt learning, mitigating the impact of annotation variance
while improving model accuracy. Experiments show that mPrompt significantly
reduces the Mean Average Error (MAE), demonstrating the potential to be general
framework for down-stream vision tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>mPrompt defines a way of mutual information maximization from prompt
  learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Follow Your Pose: Pose-Guided Text-to-Video Generation using Pose-Free
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.01186v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.01186v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Ma, Yingqing He, Xiaodong Cun, Xintao Wang, Siran Chen, Ying Shan, Xiu Li, Qifeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating text-editable and pose-controllable character videos have an
imperious demand in creating various digital human. Nevertheless, this task has
been restricted by the absence of a comprehensive dataset featuring paired
video-pose captions and the generative prior models for videos. In this work,
we design a novel two-stage training scheme that can utilize easily obtained
datasets (i.e.,image pose pair and pose-free video) and the pre-trained
text-to-image (T2I) model to obtain the pose-controllable character videos.
Specifically, in the first stage, only the keypoint-image pairs are used only
for a controllable text-to-image generation. We learn a zero-initialized
convolutional encoder to encode the pose information. In the second stage, we
finetune the motion of the above network via a pose-free video dataset by
adding the learnable temporal self-attention and reformed cross-frame
self-attention blocks. Powered by our new designs, our method successfully
generates continuously pose-controllable character videos while keeps the
editing and concept composition ability of the pre-trained T2I model. The code
and models will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://follow-your-pose.github.io/; Github repository:
  https://github.com/mayuelala/FollowYourPose</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accurate and Fast Compressed Video Captioning <span class="chip">ICCV 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.12867v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.12867v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaojie Shen, Xin Gu, Kai Xu, Heng Fan, Longyin Wen, Libo Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing video captioning approaches typically require to first sample video
frames from a decoded video and then conduct a subsequent process (e.g.,
feature extraction and/or captioning model learning). In this pipeline, manual
frame sampling may ignore key information in videos and thus degrade
performance. Additionally, redundant information in the sampled frames may
result in low efficiency in the inference of video captioning. Addressing this,
we study video captioning from a different perspective in compressed domain,
which brings multi-fold advantages over the existing pipeline: 1) Compared to
raw images from the decoded video, the compressed video, consisting of
I-frames, motion vectors and residuals, is highly distinguishable, which allows
us to leverage the entire video for learning without manual sampling through a
specialized model design; 2) The captioning model is more efficient in
inference as smaller and less redundant information is processed. We propose a
simple yet effective end-to-end transformer in the compressed domain for video
captioning that enables learning from the compressed video for captioning. We
show that even with a simple design, our method can achieve state-of-the-art
performance on different benchmarks while running almost 2x faster than
existing approaches. Code is available at https://github.com/acherstyx/CoCap.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCV 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VitalLens: Take A Vital Selfie 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06892v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06892v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philipp V. Rouast
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This report introduces VitalLens, an app that estimates vital signs such as
heart rate and respiration rate from selfie video in real time. VitalLens uses
a computer vision model trained on a diverse dataset of video and physiological
sensor data. We benchmark performance on several diverse datasets, including
VV-Medium, which consists of 289 unique participants. VitalLens outperforms
several existing methods including POS and MTTS-CAN on all datasets while
maintaining a fast inference speed. On VV-Medium, VitalLens achieves mean
absolute errors of 0.71 bpm for heart rate estimation, and 0.76 bpm for
respiratory rate estimation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GMMFormer: Gaussian-Mixture-Model Based Transformer for Efficient
  Partially Relevant Video Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05195v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05195v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuting Wang, Jinpeng Wang, Bin Chen, Ziyun Zeng, Shu-Tao Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a text query, partially relevant video retrieval (PRVR) seeks to find
untrimmed videos containing pertinent moments in a database. For PRVR, clip
modeling is essential to capture the partial relationship between texts and
videos. Current PRVR methods adopt scanning-based clip construction to achieve
explicit clip modeling, which is information-redundant and requires a large
storage overhead. To solve the efficiency problem of PRVR methods, this paper
proposes GMMFormer, a Gaussian-Mixture-Model based Transformer which models
clip representations implicitly. During frame interactions, we incorporate
Gaussian-Mixture-Model constraints to focus each frame on its adjacent frames
instead of the whole video. Then generated representations will contain
multi-scale clip information, achieving implicit clip modeling. In addition,
PRVR methods ignore semantic differences between text queries relevant to the
same video, leading to a sparse embedding space. We propose a query diverse
loss to distinguish these text queries, making the embedding space more
intensive and contain more semantic information. Extensive experiments on three
large-scale video datasets (i.e., TVR, ActivityNet Captions, and Charades-STA)
demonstrate the superiority and efficiency of GMMFormer. Code is available at
\url{https://github.com/huangmozhi9527/GMMFormer}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2024. Code is released at
  https://github.com/huangmozhi9527/GMMFormer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Improved Baseline for Reasoning Segmentation with Large Language
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17240v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17240v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Senqiao Yang, Tianyuan Qu, Xin Lai, Zhuotao Tian, Bohao Peng, Shu Liu, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While LISA effectively bridges the gap between segmentation and large
language models to enable reasoning segmentation, it poses certain limitations:
unable to distinguish different instances of the target region, and constrained
by the pre-defined textual response formats. In this work, we introduce LISA++,
an update to the existing LISA model, focusing on improving core
functionalities while keeping the base architecture intact. The main
enhancements in LISA++ include: \textbf{1) Enhanced Segmentation}: The instance
segmentation ability has been added, providing a more detailed scene analysis
along with the existing multi-region semantic segmentation. \textbf{2) More
Natural Conversation}: Improved capability for multi-turn dialogue, with the
ability to incorporate segmentation results directly into text responses, i.e.,
Segmentation in Dialogue (SiD). These improvements are achieved by curating the
existing samples of generic segmentation datasets, aimed specifically at
enhancing the segmentation and conversational skills without structural change
and additional data sources. Comparative analysis with the original LISA model
shows significant advancements in these areas, positioning LISA++ as a notable
upgrade in visual understanding and interaction. LISA++'s adaptability and
improved features highlight the versatility of the mask-as-embedding paradigm
proposed by LISA, and the potential as a foundational model for diverse
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Tech report. The LaTex compilation crash was fixed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EmotionGesture: Audio-Driven Diverse Emotional Co-Speech 3D Gesture
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18891v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18891v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingqun Qi, Chen Liu, Lincheng Li, Jie Hou, Haoran Xin, Xin Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating vivid and diverse 3D co-speech gestures is crucial for various
applications in animating virtual avatars. While most existing methods can
generate gestures from audio directly, they usually overlook that emotion is
one of the key factors of authentic co-speech gesture generation. In this work,
we propose EmotionGesture, a novel framework for synthesizing vivid and diverse
emotional co-speech 3D gestures from audio. Considering emotion is often
entangled with the rhythmic beat in speech audio, we first develop an
Emotion-Beat Mining module (EBM) to extract the emotion and audio beat features
as well as model their correlation via a transcript-based visual-rhythm
alignment. Then, we propose an initial pose based Spatial-Temporal Prompter
(STP) to generate future gestures from the given initial poses. STP effectively
models the spatial-temporal correlations between the initial poses and the
future gestures, thus producing the spatial-temporal coherent pose prompt. Once
we obtain pose prompts, emotion, and audio beat features, we will generate 3D
co-speech gestures through a transformer architecture. However, considering the
poses of existing datasets often contain jittering effects, this would lead to
generating unstable gestures. To address this issue, we propose an effective
objective function, dubbed Motion-Smooth Loss. Specifically, we model motion
offset to compensate for jittering ground-truth by forcing gestures to be
smooth. Last, we present an emotion-conditioned VAE to sample emotion features,
enabling us to generate diverse emotional results. Extensive experiments
demonstrate that our framework outperforms the state-of-the-art, achieving
vivid and diverse emotional co-speech 3D gestures. Our code and dataset will be
released at the project page:
https://xingqunqi-lab.github.io/Emotion-Gesture-Web/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OriCon3D: Effective 3D Object Detection using Orientation and Confidence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.14484v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.14484v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dhyey Manish Rajani, Surya Pratap Singh, Rahul Kashyap Swayampakula
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose an advanced methodology for the detection of 3D
objects and precise estimation of their spatial positions from a single image.
Unlike conventional frameworks that rely solely on center-point and dimension
predictions, our research leverages a deep convolutional neural network-based
3D object weighted orientation regression paradigm. These estimates are then
seamlessly integrated with geometric constraints obtained from a 2D bounding
box, resulting in derivation of a comprehensive 3D bounding box. Our novel
network design encompasses two key outputs. The first output involves the
estimation of 3D object orientation through the utilization of a
discrete-continuous loss function. Simultaneously, the second output predicts
objectivity-based confidence scores with minimal variance. Additionally, we
also introduce enhancements to our methodology through the incorporation of
lightweight residual feature extractors. By combining the derived estimates
with the geometric constraints inherent in the 2D bounding box, our approach
significantly improves the accuracy of 3D object pose determination, surpassing
baseline methodologies. Our method is rigorously evaluated on the KITTI 3D
object detection benchmark, demonstrating superior performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ECCV Caption: Correcting False Negatives by Collecting
  Machine-and-Human-verified Image-Caption Associations for MS-COCO <span class="chip">ECCV 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.03359v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.03359v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanghyuk Chun, Wonjae Kim, Song Park, Minsuk Chang, Seong Joon Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image-Text matching (ITM) is a common task for evaluating the quality of
Vision and Language (VL) models. However, existing ITM benchmarks have a
significant limitation. They have many missing correspondences, originating
from the data construction process itself. For example, a caption is only
matched with one image although the caption can be matched with other similar
images and vice versa. To correct the massive false negatives, we construct the
Extended COCO Validation (ECCV) Caption dataset by supplying the missing
associations with machine and human annotators. We employ five state-of-the-art
ITM models with diverse properties for our annotation process. Our dataset
provides x3.6 positive image-to-caption associations and x8.5 caption-to-image
associations compared to the original MS-COCO. We also propose to use an
informative ranking-based metric mAP@R, rather than the popular Recall@K (R@K).
We re-evaluate the existing 25 VL models on existing and proposed benchmarks.
Our findings are that the existing benchmarks, such as COCO 1K R@K, COCO 5K
R@K, CxC R@1 are highly correlated with each other, while the rankings change
when we shift to the ECCV mAP@R. Lastly, we delve into the effect of the bias
introduced by the choice of machine annotator. Source code and dataset are
available at https://github.com/naver-ai/eccv-caption
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ECCV 2022; 32 pages (2.3MB); Code and dataset:
  https://github.com/naver-ai/eccv-caption; v5 fixes errors in Table 4: the
  COCO 1K R@1 numbers were incorrect. All other tables and figures are correct.
  v5 also adds RSUM scores in Tab 4 and 5: RSUM has a high correlation with
  COCO 1K recalls; v4 fixes errors in v3 -- see the v4 comment for details</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieval-Augmented Egocentric Video Captioning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00789v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00789v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jilan Xu, Yifei Huang, Junlin Hou, Guo Chen, Yuejie Zhang, Rui Feng, Weidi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding human actions from videos of first-person view poses
significant challenges. Most prior approaches explore representation learning
on egocentric videos only, while overlooking the potential benefit of
exploiting existing large-scale third-person videos. In this paper, (1) we
develop EgoInstructor, a retrieval-augmented multimodal captioning model that
automatically retrieves semantically relevant third-person instructional videos
to enhance the video captioning of egocentric videos. (2) For training the
cross-view retrieval module, we devise an automatic pipeline to discover
ego-exo video pairs from distinct large-scale egocentric and exocentric
datasets. (3) We train the cross-view retrieval module with a novel EgoExoNCE
loss that pulls egocentric and exocentric video features closer by aligning
them to shared text features that describe similar actions. (4) Through
extensive experiments, our cross-view retrieval module demonstrates superior
performance across seven benchmarks. Regarding egocentric video captioning,
EgoInstructor exhibits significant improvements by leveraging third-person
videos as references.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SCUNet++: Swin-UNet and CNN Bottleneck Hybrid Architecture with
  Multi-Fusion Dense Skip Connection for Pulmonary Embolism CT Image
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14705v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14705v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Chen, Binfeng Zou, Zhaoxin Guo, Yiyu Huang, Yifan Huang, Feiwei Qin, Qinhai Li, Changmiao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pulmonary embolism (PE) is a prevalent lung disease that can lead to right
ventricular hypertrophy and failure in severe cases, ranking second in severity
only to myocardial infarction and sudden death. Pulmonary artery CT angiography
(CTPA) is a widely used diagnostic method for PE. However, PE detection
presents challenges in clinical practice due to limitations in imaging
technology. CTPA can produce noises similar to PE, making confirmation of its
presence time-consuming and prone to overdiagnosis. Nevertheless, the
traditional segmentation method of PE can not fully consider the hierarchical
structure of features, local and global spatial features of PE CT images. In
this paper, we propose an automatic PE segmentation method called SCUNet++
(Swin Conv UNet++). This method incorporates multiple fusion dense skip
connections between the encoder and decoder, utilizing the Swin Transformer as
the encoder. And fuses features of different scales in the decoder subnetwork
to compensate for spatial information loss caused by the inevitable
downsampling in Swin-UNet or other state-of-the-art methods, effectively
solving the above problem. We provide a theoretical analysis of this method in
detail and validate it on publicly available PE CT image datasets FUMPE and
CAD-PE. The experimental results indicate that our proposed method achieved a
Dice similarity coefficient (DSC) of 83.47% and a Hausdorff distance 95th
percentile (HD95) of 3.83 on the FUMPE dataset, as well as a DSC of 83.42% and
an HD95 of 5.10 on the CAD-PE dataset. These findings demonstrate that our
method exhibits strong performance in PE segmentation tasks, potentially
enhancing the accuracy of automatic segmentation of PE and providing a powerful
diagnostic tool for clinical physicians. Our source code and new FUMPE dataset
are available at https://github.com/JustlfC03/SCUNet-plusplus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures, accept WACV2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Credible Teacher for Semi-Supervised Object Detection in Open Scene 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00695v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00695v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyu Zhuang, Kuo Wang, Liang Lin, Guanbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semi-Supervised Object Detection (SSOD) has achieved resounding success by
leveraging unlabeled data to improve detection performance. However, in Open
Scene Semi-Supervised Object Detection (O-SSOD), unlabeled data may contains
unknown objects not observed in the labeled data, which will increase
uncertainty in the model's predictions for known objects. It is detrimental to
the current methods that mainly rely on self-training, as more uncertainty
leads to the lower localization and classification precision of pseudo labels.
To this end, we propose Credible Teacher, an end-to-end framework. Credible
Teacher adopts an interactive teaching mechanism using flexible labels to
prevent uncertain pseudo labels from misleading the model and gradually reduces
its uncertainty through the guidance of other credible pseudo labels. Empirical
results have demonstrated our method effectively restrains the adverse effect
caused by O-SSOD and significantly outperforms existing counterparts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accpet by ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CityPulse: Fine-Grained Assessment of Urban Change with Street View Time
  Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01107v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01107v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyuan Huang, Zejia Wu, Jiajun Wu, Jackelyn Hwang, Ram Rajagopal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Urban transformations have profound societal impact on both individuals and
communities at large. Accurately assessing these shifts is essential for
understanding their underlying causes and ensuring sustainable urban planning.
Traditional measurements often encounter constraints in spatial and temporal
granularity, failing to capture real-time physical changes. While street view
imagery, capturing the heartbeat of urban spaces from a pedestrian point of
view, can add as a high-definition, up-to-date, and on-the-ground visual proxy
of urban change. We curate the largest street view time series dataset to date,
and propose an end-to-end change detection model to effectively capture
physical alterations in the built environment at scale. We demonstrate the
effectiveness of our proposed method by benchmark comparisons with previous
literature and implementing it at the city-wide level. Our approach has the
potential to supplement existing dataset and serve as a fine-grained and
accurate assessment of urban change.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deshadow-Anything: When Segment Anything Model Meets Zero-shot shadow
  removal 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11715v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11715v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Feng Zhang, Tian Yi Song, Jia Wei Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Segment Anything (SAM), an advanced universal image segmentation model
trained on an expansive visual dataset, has set a new benchmark in image
segmentation and computer vision. However, it faced challenges when it came to
distinguishing between shadows and their backgrounds. To address this, we
developed Deshadow-Anything, considering the generalization of large-scale
datasets, and we performed Fine-tuning on large-scale datasets to achieve image
shadow removal. The diffusion model can diffuse along the edges and textures of
an image, helping to remove shadows while preserving the details of the image.
Furthermore, we design Multi-Self-Attention Guidance (MSAG) and adaptive input
perturbation (DDPM-AIP) to accelerate the iterative training speed of
diffusion. Experiments on shadow removal tasks demonstrate that these methods
can effectively improve image restoration performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>it needs revised</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diff-PCR: Diffusion-Based Correspondence Searching in Doubly Stochastic
  Matrix Space for Point Cloud Registration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00436v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00436v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianliang Wu, Haobo Jiang, Yaqing Ding, Lei Luo, Jin Xie, Jian Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficiently finding optimal correspondences between point clouds is crucial
for solving both rigid and non-rigid point cloud registration problems.
Existing methods often rely on geometric or semantic feature embedding to
establish correspondences and estimate transformations or flow fields.
Recently, state-of-the-art methods have employed RAFT-like iterative updates to
refine the solution. However, these methods have certain limitations. Firstly,
their iterative refinement design lacks transparency, and their iterative
updates follow a fixed path during the refinement process, which can lead to
suboptimal results. Secondly, these methods overlook the importance of refining
or optimizing correspondences (or matching matrices) as a precursor to solving
transformations or flow fields. They typically compute candidate
correspondences based on distances in the point feature space. However, they
only project the candidate matching matrix into some matrix space once with
Sinkhorn or dual softmax operations to obtain final correspondences. This
one-shot projected matching matrix may be far from the globally optimal one,
and these approaches do not consider the distribution of the target matching
matrix. In this paper, we propose a novel approach that exploits the Denoising
Diffusion Model to predict a searching gradient for the optimal matching matrix
within the Doubly Stochastic Matrix Space. During the reverse denoising
process, our method iteratively searches for better solutions along this
denoising gradient, which points towards the maximum likelihood direction of
the target matching matrix. Our method offers flexibility by allowing the
search to start from any initial matching matrix provided by the online
backbone or white noise. Experimental evaluations on the 3DMatch/3DLoMatch and
4DMatch/4DLoMatch datasets demonstrate the effectiveness of our newly designed
framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FuRPE: Learning Full-body Reconstruction from Part Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.00731v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.00731v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoxin Fan, Yuqing Pan, Hao Xu, Zhenbo Song, Zhicheng Wang, Kejian Wu, Hongyan Liu, Jun He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of full-body reconstruction, the scarcity of annotated data
often impedes the efficacy of prevailing methods. To address this issue, we
introduce FuRPE, a novel framework that employs part-experts and an ingenious
pseudo ground-truth selection scheme to derive high-quality pseudo labels.
These labels, central to our approach, equip our network with the capability to
efficiently learn from the available data. Integral to FuRPE is a unique
exponential moving average training strategy and expert-derived feature
distillation strategy. These novel elements of FuRPE not only serve to further
refine the model but also to reduce potential biases that may arise from
inaccuracies in pseudo labels, thereby optimizing the network's training
process and enhancing the robustness of the model. We apply FuRPE to train both
two-stage and fully convolutional single-stage full-body reconstruction
networks. Our exhaustive experiments on numerous benchmark datasets illustrate
a substantial performance boost over existing methods, underscoring FuRPE's
potential to reshape the state-of-the-art in full-body reconstruction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SCLIP: Rethinking Self-Attention for Dense Vision-Language Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.01597v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.01597v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Wang, Jieru Mei, Alan Yuille
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in contrastive language-image pretraining (CLIP) have
demonstrated strong capabilities in zero-shot classification by aligning visual
representations with target text embeddings in an image level. However, in
dense prediction tasks, CLIP often struggles to localize visual features within
an image and fails to give accurate pixel-level predictions, which prevents it
from functioning as a generalized visual foundation model. In this work, we aim
to enhance CLIP's potential for semantic segmentation with minimal
modifications to its pretrained models. By rethinking self-attention, we
surprisingly find that CLIP can adapt to dense prediction tasks by simply
introducing a novel Correlative Self-Attention (CSA) mechanism. Specifically,
we replace the traditional self-attention block of CLIP vision encoder's last
layer by our CSA module and reuse its pretrained projection matrices of query,
key, and value, leading to a training-free adaptation approach for CLIP's
zero-shot semantic segmentation. Extensive experiments show the advantage of
CSA: we obtain a 38.2% average zero-shot mIoU across eight semantic
segmentation benchmarks highlighted in this paper, significantly outperforming
the existing SoTA's 33.9% and the vanilla CLIP's 14.1%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Navigating Data Heterogeneity in Federated Learning A Semi-Supervised
  Federated Object Detection <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17097v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17097v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taehyeon Kim, Eric Lin, Junu Lee, Christian Lau, Vaikkunth Mugunthan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) has emerged as a potent framework for training models
across distributed data sources while maintaining data privacy. Nevertheless,
it faces challenges with limited high-quality labels and non-IID client data,
particularly in applications like autonomous driving. To address these hurdles,
we navigate the uncharted waters of Semi-Supervised Federated Object Detection
(SSFOD). We present a pioneering SSFOD framework, designed for scenarios where
labeled data reside only at the server while clients possess unlabeled data.
Notably, our method represents the inaugural implementation of SSFOD for
clients with 0% labeled non-IID data, a stark contrast to previous studies that
maintain some subset of labels at each client. We propose FedSTO, a two-stage
strategy encompassing Selective Training followed by Orthogonally enhanced
full-parameter training, to effectively address data shift (e.g. weather
conditions) between server and clients. Our contributions include selectively
refining the backbone of the detector to avert overfitting, orthogonality
regularization to boost representation divergence, and local EMA-driven pseudo
label assignment to yield high-quality pseudo labels. Extensive validation on
prominent autonomous driving datasets (BDD100K, Cityscapes, and SODA10M)
attests to the efficacy of our approach, demonstrating state-of-the-art
results. Remarkably, FedSTO, using just 20-30% of labels, performs nearly as
well as fully-supervised centralized training methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LinFlo-Net: A two-stage deep learning method to generate simulation
  ready meshes of the heart 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.20065v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.20065v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arjun Narayanan, Fanwei Kong, Shawn Shadden
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a deep learning model to automatically generate computer models of
the human heart from patient imaging data with an emphasis on its capability to
generate thin-walled cardiac structures. Our method works by deforming a
template mesh to fit the cardiac structures to the given image. Compared with
prior deep learning methods that adopted this approach, our framework is
designed to minimize mesh self-penetration, which typically arises when
deforming surface meshes separated by small distances. We achieve this by using
a two-stage diffeomorphic deformation process along with a novel loss function
derived from the kinematics of motion that penalizes surface contact and
interpenetration. Our model demonstrates comparable accuracy with
state-of-the-art methods while additionally producing meshes free of
self-intersections. The resultant meshes are readily usable in physics based
simulation, minimizing the need for post-processing and cleanup.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted manuscript in the Journal of Biomechanical Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diabetic Retinopathy Using Gaussian Filter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15216v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15216v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roshan Vasu Muddaluru, Sharvaani Ravikumar Thoguluva, Shruti Prabha, Tanuja Konda Reddy, Dr. Suja P
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The retina is an essential component of the visual system, and maintaining
eyesight depends on the timely and correct detection of disorders. This
research specifically addresses the early-stage detection and severity
classification of diabetic retinopathy (DR), a serious public health hazard. We
compare the results of different deep learning models such as InceptionV3,
DenseNet121 and other CNN based models by using different image filters, such
as Gaussian, grayscale and Gabor. These models could detect subtle pathological
alterations and use that information to estimate the risk of retinal illnesses.
The objective is to improve the diagnostic processes for diabetic retinopathy,
the primary cause of diabetes-related blindness, by utilizing deep learning
models. A comparative analysis between Greyscale, Gaussian and Gabor filters
has been provided after applying these filters on the retinal images. The
Gaussian filter resulted to be the most promising filter giving the best
accuracies for all the models. The best performing model was InceptionV3 which
gave an accuracy of 96% on Gaussian images, therefore Gaussian filter emerged
as our most promising filter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures, conference, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">21</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Control Barrier Functions using Uncertainty Estimation with
  Application to Mobile Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01881v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01881v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ersin Das, Joel W. Burdick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model uncertainty poses a significant challenge to the implementation of
safety-critical control systems. With this as motivation, this paper proposes a
safe control design approach that guarantees the robustness of nonlinear
feedback systems in the presence of matched or unmatched unmodelled system
dynamics and external disturbances. Our approach couples control barrier
functions (CBFs) with a new uncertainty/disturbance estimator to ensure robust
safety against input and state-dependent model uncertainties. We prove upper
bounds on the estimator's error and estimated outputs. We use an uncertainty
estimator-based composite feedback control law to adaptively improve robust
control performance under hard safety constraints by compensating for the
matched uncertainty. Then, we robustify existing CBF constraints with this
uncertainty estimate and the estimation error bounds to ensure robust safety
via a quadratic program (CBF-QP). We also extend our method to higher-order
CBFs (HOCBFs) to achieve safety under unmatched uncertainty, which causes
relative degree differences with respect to control input and disturbance. We
assume the relative degree difference is at most one, resulting in a
second-order cone (SOC) condition. The proposed robust HOCBFs method is
demonstrated in a simulation of an uncertain elastic actuator control problem.
Finally, the efficacy of our method is experimentally demonstrated on a tracked
robot with slope-induced matched and unmatched perturbations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Many-Objective-Optimized Semi-Automated Robotic Disassembly Sequences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01817v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01817v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takuya Kiyokawa, Kensuke Harada, Weiwei Wan, Tomoki Ishikura, Naoya Miyaji, Genichiro Matsuda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study tasckles the problem of many-objective sequence optimization for
semi-automated robotic disassembly operations. To this end, we employ a
many-objective genetic algorithm (MaOGA) algorithm inspired by the
Non-dominated Sorting Genetic Algorithm (NSGA)-III, along with
robotic-disassembly-oriented constraints and objective functions derived from
geometrical and robot simulations using 3-dimensional (3D) geometrical
information stored in a 3D Computer-Aided Design (CAD) model of the target
product. The MaOGA begins by generating a set of initial chromosomes based on a
contact and connection graph (CCG), rather than random chromosomes, to avoid
falling into a local minimum and yield repeatable convergence. The optimization
imposes constraints on feasibility and stability as well as objective functions
regarding difficulty, efficiency, prioritization, and allocability to generate
a sequence that satisfies many preferred conditions under mandatory
requirements for semi-automated robotic disassembly. The NSGA-III-inspired
MaOGA also utilizes non-dominated sorting and niching with reference lines to
further encourage steady and stable exploration and uniformly lower the overall
evaluation values. Our sequence generation experiments for a complex product
(36 parts) demonstrated that the proposed method can consistently produce
feasible and stable sequences with a 100% success rate, bringing the multiple
preferred conditions closer to the optimal solution required for semi-automated
robotic disassembly operations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fuzzy Logic Controller Design for Mobile Robot Outdoor <span class="highlight-title">Navigation</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01756v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01756v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        A. Wondosen, Dereje Shiferaw
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many researchers around the world are researching to get control solutions
that enhance robots' ability to navigate in dynamic environments autonomously.
However, until these days robots have limited capability and many navigation
tasks on Earth and other planets have been difficult so far. This paperwork
presents the development of a control system for a differential drive-wheeled
mobile robot that autonomously controls its position, heading, and speed based
on destination information given and surrounding data gathered through mounted
proximity and GPS sensors. The intelligence of this control system is
implemented by using a fuzzy logic algorithm which is a very powerful tool to
handle un-modeled systems like the dynamically changing environment dealt with
in this research. The fuzzy controller is used to address the problems
associated with navigation in an obstacle-strewn environment. Such issues
include position estimation, path planning, and obstacle avoidance. In this
study modeling, design, and simulation of the system have been done. The
simulation result shows that the developed mobile robot travels successfully
from any location to the destination location without colliding with obstacles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Soft Continuum Robot with Self-Controllable Variable Curvature 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinran Wang, Qiujie Lu, Dongmyoung Lee, Zhongxue Gan, Nicolas Rojas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a new type of soft continuum robot, called SCoReS,
which is capable of self-controlling continuously its curvature at the segment
level; in contrast to previous designs which either require external forces or
machine elements, or whose variable curvature capabilities are discrete --
depending on the number of locking mechanisms and segments. The ability to have
a variable curvature, whose control is continuous and independent from external
factors, makes a soft continuum robot more adaptive in constrained
environments, similar to what is observed in nature in the elephant's trunk or
ostrich's neck for instance which exhibit multiple curvatures. To this end, our
soft continuum robot enables reconfigurable variable curvatures utilizing a
variable stiffness growing spine based on micro-particle granular jamming for
the first time. We detail the design of the proposed robot, presenting its
modeling through beam theory and FEA simulation -- which is validated through
experiments. The robot's versatile bending profiles are then explored in
experiments and an application to grasp fruits at different configurations is
demonstrated.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accpeted for IEEE Robotics and Automation letters in January 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed Pose-graph Optimization with Multi-level Partitioning for
  Collaborative SLAM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01657v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01657v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cunhao Li, Peng Yi, Guanghui Guo, Yiguang Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The back-end module of Distributed Collaborative Simultaneous Localization
and Mapping (DCSLAM) requires solving a nonlinear Pose Graph Optimization (PGO)
under a distributed setting, also known as SE(d)-synchronization. Most existing
distributed graph optimization algorithms employ a simple sequential
partitioning scheme, which may result in unbalanced subgraph dimensions due to
the different geographic locations of each robot, and hence imposes extra
communication load. Moreover, the performance of current Riemannian
optimization algorithms can be further accelerated. In this letter, we propose
a novel distributed pose graph optimization algorithm combining multi-level
partitioning with an accelerated Riemannian optimization method. Firstly, we
employ the multi-level graph partitioning algorithm to preprocess the naive
pose graph to formulate a balanced optimization problem. In addition, inspired
by the accelerated coordinate descent method, we devise an Improved Riemannian
Block Coordinate Descent (IRBCD) algorithm and the critical point obtained is
globally optimal. Finally, we evaluate the effects of four common graph
partitioning approaches on the correlation of the inter-subgraphs, and discover
that the Highest scheme has the best partitioning performance. Also, we
implement simulations to quantitatively demonstrate that our proposed algorithm
outperforms the state-of-the-art distributed pose graph optimization protocols.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Motion Control of Interactive Robotic Arms Based on Mixed Reality
  Development 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01644v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01644v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanxiao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixed Reality (MR) is constantly evolving to inspire new patterns of robot
manipulation for more advanced Human- Robot Interaction under the 4th
Industrial Revolution Paradigm. Consider that Mixed Reality aims to connect
physical and digital worlds to provide special immersive experiences, it is
necessary to establish the information exchange platform and robot control
systems within the developed MR scenarios. In this work, we mainly present
multiple effective motion control methods applied on different interactive
robotic arms (e.g., UR5, UR5e, myCobot) for the Unity-based development of MR
applications, including GUI control panel, text input control panel,
end-effector object dynamic tracking and ROS-Unity digital-twin connection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The full paper has been accepted by CompAuto 2023 with an online Oral
  Presentation. (http://www.icca.net/, The 3rd International Conference on
  Computers and Automation, December 7-9, 2023, Paris France)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DDN-SLAM: Real-time Dense Dynamic Neural Implicit SLAM with Joint
  Semantic Encoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingrui Li, Jiaming He, Guangan Jiang, Hongyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose DDN-SLAM, a real-time dense neural implicit semantic SLAM system
designed for dynamic scenes. While existing neural implicit SLAM systems
perform well in static scenes, they often encounter challenges in real-world
environments with dynamic interferences, leading to ineffective tracking and
mapping. DDN-SLAM utilizes the priors provided by the deep semantic system,
combined with conditional probability fields, for segmentation.By constructing
depth-guided static masks and employing joint multi-resolution hashing
encoding, we ensure fast hole filling and high-quality mapping while mitigating
the effects of dynamic information interference. To enhance tracking
robustness, we utilize sparse feature points validated with optical flow and
keyframes, enabling loop closure detection and global bundle optimization.
Furthermore, DDN-SLAM supports monocular, stereo, and RGB-D inputs, operating
robustly at a frequency of 20-30Hz. Extensive experiments on 6 virtual/real
datasets demonstrate that our method outperforms state-of-the-art approaches in
both dynamic and static scenes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11pages, 4figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pontryagin Neural Operator for Solving Parametric General-Sum
  Differential Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Zhang, Mukesh Ghimire, Zhe Xu, Wenlong Zhang, Yi Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The values of two-player general-sum differential games are viscosity
solutions to Hamilton-Jacobi-Isaacs (HJI) equations. Value and policy
approximations for such games suffer from the curse of dimensionality (CoD).
Alleviating CoD through physics-informed neural networks (PINN) encounters
convergence issues when value discontinuity is present due to state
constraints. On top of these challenges, it is often necessary to learn
generalizable values and policies across a parametric space of games, e.g., for
game parameter inference when information is incomplete. To address these
challenges, we propose in this paper a Pontryagin-mode neural operator that
outperforms existing state-of-the-art (SOTA) on safety performance across games
with parametric state constraints. Our key contribution is the introduction of
a costate loss defined on the discrepancy between forward and backward costate
rollouts, which are computationally cheap. We show that the discontinuity of
costate dynamics (in the presence of state constraints) effectively enables the
learning of discontinuous values, without requiring manually supervised data as
suggested by the current SOTA. More importantly, we show that the close
relationship between costates and policies makes the former critical in
learning feedback control policies with generalizable safety performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to L4DC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation of automated driving system safety metrics with logged
  vehicle trajectory data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xintao Yan, Shuo Feng, David J. LeBlanc, Carol Flannagan, Henry X. Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time safety metrics are important for the automated driving system (ADS)
to assess the risk of driving situations and to assist the decision-making.
Although a number of real-time safety metrics have been proposed in the
literature, systematic performance evaluation of these safety metrics has been
lacking. As different behavioral assumptions are adopted in different safety
metrics, it is difficult to compare the safety metrics and evaluate their
performance. To overcome this challenge, in this study, we propose an
evaluation framework utilizing logged vehicle trajectory data, in that vehicle
trajectories for both subject vehicle (SV) and background vehicles (BVs) are
obtained and the prediction errors caused by behavioral assumptions can be
eliminated. Specifically, we examine whether the SV is in a collision
unavoidable situation at each moment, given all near-future trajectories of
BVs. In this way, we level the ground for a fair comparison of different safety
metrics, as a good safety metric should always alarm in advance to the
collision unavoidable moment. When trajectory data from a large number of trips
are available, we can systematically evaluate and compare different metrics'
statistical performance. In the case study, three representative real-time
safety metrics, including the time-to-collision (TTC), the PEGASUS Criticality
Metric (PCM), and the Model Predictive Instantaneous Safety Metric (MPrISM),
are evaluated using a large-scale simulated trajectory dataset. The proposed
evaluation framework is important for researchers, practitioners, and
regulators to characterize different metrics, and to select appropriate metrics
for different applications. Moreover, by conducting failure analysis on moments
when a safety metric failed, we can identify its potential weaknesses which are
valuable for its potential refinements and improvements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ To Lead or to Follow? Adaptive Robot Task Planning in Human-Robot
  Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01483v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01483v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Noormohammadi-Asl, Stephen L. Smith, Kerstin Dautenhahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adaptive task planning is fundamental to ensuring effective and seamless
human-robot collaboration. This paper introduces a robot task planning
framework that takes into account both human leading/following preferences and
performance, specifically focusing on task allocation and scheduling in
collaborative settings. We present a proactive task allocation approach with
three primary objectives: enhancing team performance, incorporating human
preferences, and upholding a positive human perception of the robot and the
collaborative experience. Through a user study, involving an autonomous mobile
manipulator robot working alongside participants in a collaborative scenario,
we confirm that the task planning framework successfully attains all three
intended goals, thereby contributing to the advancement of adaptive task
planning in human-robot collaboration. This paper mainly focuses on the first
two objectives, and we discuss the third objective, participants' perception of
the robot, tasks, and collaboration in a companion paper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing UAV-UGV Coalition Operations: A Hybrid Clustering and
  Multi-Agent Reinforcement Learning Approach for Path Planning in Obstructed
  Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01481v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01481v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shamyo Brotee, Farhan Kabir, Md. Abdur Razzaque, Palash Roy, Md. Mamun-Or-Rashid, Md. Rafiul Hassan, Mohammad Mehedi Hassan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the most critical applications undertaken by coalitions of Unmanned
Aerial Vehicles (UAVs) and Unmanned Ground Vehicles (UGVs) is reaching
predefined targets by following the most time-efficient routes while avoiding
collisions. Unfortunately, UAVs are hampered by limited battery life, and UGVs
face challenges in reachability due to obstacles and elevation variations.
Existing literature primarily focuses on one-to-one coalitions, which
constrains the efficiency of reaching targets. In this work, we introduce a
novel approach for a UAV-UGV coalition with a variable number of vehicles,
employing a modified mean-shift clustering algorithm to segment targets into
multiple zones. Each vehicle utilizes Multi-agent Deep Deterministic Policy
Gradient (MADDPG) and Multi-agent Proximal Policy Optimization (MAPPO), two
advanced reinforcement learning algorithms, to form an effective coalition for
navigating obstructed environments without collisions. This approach of
assigning targets to various circular zones, based on density and range,
significantly reduces the time required to reach these targets. Moreover,
introducing variability in the number of UAVs and UGVs in a coalition enhances
task efficiency by enabling simultaneous multi-target engagement. The results
of our experimental evaluation demonstrate that our proposed method
substantially surpasses current state-of-the-art techniques, nearly doubling
efficiency in terms of target navigation time and task completion rate.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Demonstrating Mobile Manipulation in the Wild: A Metrics-Driven Approach <span class="chip">RSS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01474v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01474v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Bajracharya, James Borders, Richard Cheng, Dan Helmick, Lukas Kaul, Dan Kruse, John Leichty, Jeremy Ma, Carolyn Matl, Frank Michel, Chavdar Papazov, Josh Petersen, Krishna Shankar, Mark Tjersland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present our general-purpose mobile manipulation system consisting of a
custom robot platform and key algorithms spanning perception and planning. To
extensively test the system in the wild and benchmark its performance, we
choose a grocery shopping scenario in an actual, unmodified grocery store. We
derive key performance metrics from detailed robot log data collected during
six week-long field tests, spread across 18 months. These objective metrics,
gained from complex yet repeatable tests, drive the direction of our research
efforts and let us continuously improve our system's performance. We find that
thorough end-to-end system-level testing of a complex mobile manipulation
system can serve as a reality-check for state-of-the-art methods in robotics.
This effectively grounds robotics research efforts in real world needs and
challenges, which we deem highly useful for the advancement of the field. To
this end, we share our key insights and takeaways to inspire and accelerate
similar system-level research projects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at RSS 2023 [Best Demo Paper Award]</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Time-Indexing as Inductive Bias in Deep RL for Sequential
  Manipulation Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01993v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01993v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. Nomaan Qureshi, Ben Eisner, David Held
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While solving complex manipulation tasks, manipulation policies often need to
learn a set of diverse skills to accomplish these tasks. The set of skills is
often quite multimodal - each one may have a quite distinct distribution of
actions and states. Standard deep policy-learning algorithms often model
policies as deep neural networks with a single output head (deterministic or
stochastic). This structure requires the network to learn to switch between
modes internally, which can lead to lower sample efficiency and poor
performance. In this paper we explore a simple structure which is conducive to
skill learning required for so many of the manipulation tasks. Specifically, we
propose a policy architecture that sequentially executes different action heads
for fixed durations, enabling the learning of primitive skills such as reaching
and grasping. Our empirical evaluation on the Metaworld tasks reveals that this
simple structure outperforms standard policy learning methods, highlighting its
potential for improved skill acquisition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EV-Planner: Energy-Efficient Robot <span class="highlight-title">Navigation</span> via Event-Based
  Physics-Guided Neuromorphic Planner 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.11349v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.11349v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sourav Sanyal, Rohan Kumar Manna, Kaushik Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-based object tracking is an essential precursor to performing
autonomous aerial navigation in order to avoid obstacles. Biologically inspired
neuromorphic event cameras are emerging as a powerful alternative to
frame-based cameras, due to their ability to asynchronously detect varying
intensities (even in poor lighting conditions), high dynamic range, and
robustness to motion blur. Spiking neural networks (SNNs) have gained traction
for processing events asynchronously in an energy-efficient manner. On the
other hand, physics-based artificial intelligence (AI) has gained prominence
recently, as they enable embedding system knowledge via physical modeling
inside traditional analog neural networks (ANNs). In this letter, we present an
event-based physics-guided neuromorphic planner (EV-Planner) to perform
obstacle avoidance using neuromorphic event cameras and physics-based AI. We
consider the task of autonomous drone navigation where the mission is to detect
moving gates and fly through them while avoiding a collision. We use event
cameras to perform object detection using a shallow spiking neural network in
an unsupervised fashion. Utilizing the physical equations of the brushless DC
motors present in the drone rotors, we train a lightweight energy-aware
physics-guided neural network (PgNN) with depth inputs. This predicts the
optimal flight time responsible for generating near-minimum energy paths. We
spawn the drone in the Gazebo simulator and implement a sensor-fused
vision-to-planning neuro-symbolic framework using Robot Operating System (ROS).
Simulation results for safe collision-free flight trajectories are presented
with performance analysis, ablation study and potential future research
directions
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted for publication at IEEE Robotics and Automation Letters</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Manipulating Trajectory Prediction with Backdoors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.13863v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.13863v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaouther Messaoud, Kathrin Grosse, Mickael Chen, Matthieu Cord, Patrick Pérez, Alexandre Alahi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous vehicles ought to predict the surrounding agents' trajectories to
allow safe maneuvers in uncertain and complex traffic situations. As companies
increasingly apply trajectory prediction in the real world, security becomes a
relevant concern. In this paper, we focus on backdoors - a security threat
acknowledged in other fields but so far overlooked for trajectory prediction.
To this end, we describe and investigate four triggers that could affect
trajectory prediction. We then show that these triggers (for example, a braking
vehicle), when correlated with a desired output (for example, a curve) during
training, cause the desired output of a state-of-the-art trajectory prediction
model. In other words, the model has good benign performance but is vulnerable
to backdoors. This is the case even if the trigger maneuver is performed by a
non-casual agent behind the target vehicle. As a side-effect, our analysis
reveals interesting limitations within trajectory prediction models. Finally,
we evaluate a range of defenses against backdoors. While some, like simple
offroad checks, do not enable detection for all triggers, clustering is a
promising candidate to support manual inspection to find backdoors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpyroPose: SE(3) Pyramids for Object Pose Distribution Estimation <span class="chip">ICCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05308v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05308v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rasmus Laurvig Haugaard, Frederik Hagelskjær, Thorbjørn Mosekjær Iversen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object pose estimation is a core computer vision problem and often an
essential component in robotics. Pose estimation is usually approached by
seeking the single best estimate of an object's pose, but this approach is
ill-suited for tasks involving visual ambiguity. In such cases it is desirable
to estimate the uncertainty as a pose distribution to allow downstream tasks to
make informed decisions. Pose distributions can have arbitrary complexity which
motivates estimating unparameterized distributions, however, until now they
have only been used for orientation estimation on SO(3) due to the difficulty
in training on and normalizing over SE(3). We propose a novel method for pose
distribution estimation on SE(3). We use a hierarchical grid, a pyramid, which
enables efficient importance sampling during training and sparse evaluation of
the pyramid at inference, allowing real time 6D pose distribution estimation.
Our method outperforms state-of-the-art methods on SO(3), and to the best of
our knowledge, we provide the first quantitative results on pose distribution
estimation on SE(3). Code will be available at spyropose.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICCVW 2023 (R6D)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Optimization-based Cable Force Allocation for Geometric
  Control of a Multirotor Team Transporting a Payload 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.02359v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.02359v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khaled Wahba, Wolfgang Hönig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider transporting a heavy payload that is attached to multiple
multirotors. The current state-of-the-art controllers either do not avoid
inter-robot collision at all, leading to crashes when tasked with carrying
payloads that are small in size compared to the cable lengths, or use
computational demanding nonlinear optimization. We propose an efficient
optimization-based cable force allocation for a geometric payload transport
controller to effectively avoid such collisions, while retaining the stability
properties of the geometric controller. Our approach introduces a cascade of
carefully designed quadratic programs that can be solved efficiently on highly
constrained embedded flight controllers.
  We show that our approach exceeds the state-of-the-art controllers in terms
of scalability by at least an order of magnitude for up to 10 robots. We
demonstrate our method on challenging scenarios with up to three small
multirotors with various payloads and cable lengths, where our controller runs
in realtime directly on a microcontroller on the robots.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE RA-L, 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling Planning for Automated Driving using Simplistic Synthetic Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18942v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18942v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Stoll, Markus Mazzola, Maxim Dolgov, Jürgen Mathes, Nicolas Möser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We challenge the perceived consensus that the application of deep learning to
solve the automated driving planning task necessarily requires huge amounts of
real-world data or highly realistic simulation. Focusing on a roundabout
scenario, we show that this requirement can be relaxed in favour of targeted,
simplistic simulated data. A benefit is that such data can be easily generated
for critical scenarios that are typically underrepresented in realistic
datasets. By applying vanilla behavioural cloning almost exclusively to
lightweight simulated data, we achieve reliable and comfortable driving in a
real-world test vehicle. We leverage an incremental development approach that
includes regular in-vehicle testing to identify sim-to-real gaps, targeted data
augmentation, and training scenario variations. In addition to a detailed
description of the methodology, we share our lessons learned, touching upon
scenario generation, simulation features, and evaluation metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Similar but Different: A Survey of Ground Segmentation and
  Traversability Estimation for Terrestrial Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.16839v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.16839v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyungtae Lim, Minho Oh, Seungjae Lee, Seunguk Ahn, Hyun Myung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing demand for mobile robots and autonomous vehicles, several
approaches for long-term robot navigation have been proposed. Among these
techniques, ground segmentation and traversability estimation play important
roles in perception and path planning, respectively. Even though these two
techniques appear similar, their objectives are different. Ground segmentation
divides data into ground and non-ground elements; thus, it is used as a
preprocessing stage to extract objects of interest by rejecting ground points.
In contrast, traversability estimation identifies and comprehends areas in
which robots can move safely. Nevertheless, some researchers use these terms
without clear distinction, leading to misunderstanding the two concepts.
Therefore, in this study, we survey related literature and clearly distinguish
ground and traversable regions considering four aspects: a) maneuverability of
robot platforms, b) position of a robot in the surroundings, c) subset relation
of negative obstacles, and d) subset relation of deformable objects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Simulation-based Approach to Kinematics Analysis of a Quadruped Robot
  and Prototype Leg Testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06365v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06365v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abid Shahriar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Kinematics analysis is a crucial part of multiple joint-enabled robots. A
multi-joint enabled robot requires extensive mathematical calculations to be
done so the end effector's position can be determined with respect to the other
connective joints involved and their respective frames in a specific coordinate
system. For a locomotive quadruped robot, it is essential to determine two
types of kinematics for the robot's leg position on the coordinate. For the
part of forward kinematics, it measures the position, and joint angles can be
calculated using inverse kinematics. Mathematical derivation of the joint
angles is derived here, and Python-based simulation has been done to verify and
simulate the robot's locomotion. This approach has been tested beneficial over
other methods as Python-based code is used which makes it easier to do serial
communication and therefore it could be deployed in a micro-controller unit to
interact with a prototype leg.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 10 Figures. In this updated version, several typographical
  errors have been corrected for improved clarity. The introduction has been
  expanded to provide a more comprehensive overview of the related work.
  Additionally, more figures have been included to better illustrate the
  concepts and results. Also, a better title has been given</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modeling multi-legged robot locomotion with slipping and its
  experimental validation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.20669v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.20669v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyou Wu, Dan Zhao, Shai Revzen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-legged robots with six or more legs are not in common use, despite
designs with superior stability, maneuverability, and a low number of actuators
being available for over 20 years. This may be in part due to the difficulty in
modeling multi-legged motion with slipping and producing reliable predictions
of body velocity. Here we present a detailed measurement of the foot contact
forces in a hexapedal robot with multiple sliding contacts, and provide an
algorithm for predicting these contact forces and the body velocity. The
algorithm relies on the recently published observation that even while
slipping, multi-legged robots are principally kinematic, and employ a friction
law ansatz that allows us to compute the shape-change to body-velocity
connection and the foot contact forces. This results in the ability to simulate
motion plans for a large number of potentially slipping legs. In homogeneous
environments, this can run in (parallel) logarithmic time of the planning
horizon
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">6</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VGA: Vision and Graph Fused Attention Network for Rumor Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01759v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01759v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Bai, Caiyan Jia, Ziying Song, Chaoqun Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of social media, rumors have been spread broadly on
social media platforms, causing great harm to society. Beside textual
information, many rumors also use manipulated images or conceal textual
information within images to deceive people and avoid being detected, making
multimodal rumor detection be a critical problem. The majority of multimodal
rumor detection methods mainly concentrate on extracting features of source
claims and their corresponding images, while ignoring the comments of rumors
and their propagation structures. These comments and structures imply the
wisdom of crowds and are proved to be crucial to debunk rumors. Moreover, these
methods usually only extract visual features in a basic manner, seldom consider
tampering or textual information in images. Therefore, in this study, we
propose a novel Vision and Graph Fused Attention Network (VGA) for rumor
detection to utilize propagation structures among posts so as to obtain the
crowd opinions and further explore visual tampering features, as well as the
textual information hidden in images. We conduct extensive experiments on three
datasets, demonstrating that VGA can effectively detect multimodal rumors and
outperform state-of-the-art methods significantly.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WordArt Designer API: User-Driven Artistic Typography Synthesis with
  Large Language Models on ModelScope <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01699v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01699v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun-Yan He, Zhi-Qi Cheng, Chenyang Li, Jingdong Sun, Wangmeng Xiang, Yusen Hu, Xianhui Lin, Xiaoyang Kang, Zengke Jin, Bin Luo, Yifeng Geng, Xuansong Xie, Jingren Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the WordArt Designer API, a novel framework for
user-driven artistic typography synthesis utilizing Large Language Models
(LLMs) on ModelScope. We address the challenge of simplifying artistic
typography for non-professionals by offering a dynamic, adaptive, and
computationally efficient alternative to traditional rigid templates. Our
approach leverages the power of LLMs to understand and interpret user input,
facilitating a more intuitive design process. We demonstrate through various
case studies how users can articulate their aesthetic preferences and
functional requirements, which the system then translates into unique and
creative typographic designs. Our evaluations indicate significant improvements
in user satisfaction, design flexibility, and creative expression over existing
systems. The WordArt Designer API not only democratizes the art of typography
but also opens up new possibilities for personalized digital communication and
design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Spotlight Paper from the Workshop on Machine Learning for Creativity
  and Design at the 37th Conference on Neural Information Processing Systems
  (NeurIPS 2023). 5 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token Propagation Controller for Efficient Vision Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision transformers (ViTs) have achieved promising results on a variety of
Computer Vision tasks, however their quadratic complexity in the number of
input tokens has limited their application specially in resource-constrained
settings. Previous approaches that employ gradual token reduction to address
this challenge assume that token redundancy in one layer implies redundancy in
all the following layers. We empirically demonstrate that this assumption is
often not correct, i.e., tokens that are redundant in one layer can be useful
in later layers. We employ this key insight to propose a novel token
propagation controller (TPC) that incorporates two different
token-distributions, i.e., pause probability and restart probability to control
the reduction and reuse of tokens respectively, which results in more efficient
token utilization. To improve the estimates of token distributions, we propose
a smoothing mechanism that acts as a regularizer and helps remove noisy
outliers. Furthermore, to improve the training-stability of our proposed TPC,
we introduce a model stabilizer that is able to implicitly encode local image
structures and minimize accuracy fluctuations during model training. We present
extensive experimental results on the ImageNet-1K dataset using DeiT, LV-ViT
and Swin models to demonstrate the effectiveness of our proposed method. For
example, compared to baseline models, our proposed method improves the
inference speed of the DeiT-S by 250% while increasing the classification
accuracy by 1.0%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WACV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MULTI-CASE: A Transformer-based Ethics-aware Multimodal Investigative
  Intelligence Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01955v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01955v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian T. Fischer, Yannick Metz, Lucas Joos, Matthias Miller, Daniel A. Keim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI-driven models are increasingly deployed in operational analytics
solutions, for instance, in investigative journalism or the intelligence
community. Current approaches face two primary challenges: ethical and privacy
concerns, as well as difficulties in efficiently combining heterogeneous data
sources for multimodal analytics. To tackle the challenge of multimodal
analytics, we present MULTI-CASE, a holistic visual analytics framework
tailored towards ethics-aware and multimodal intelligence exploration, designed
in collaboration with domain experts. It leverages an equal joint agency
between human and AI to explore and assess heterogeneous information spaces,
checking and balancing automation through Visual Analytics. MULTI-CASE operates
on a fully-integrated data model and features type-specific analysis with
multiple linked components, including a combined search, annotated text view,
and graph-based analysis. Parts of the underlying entity detection are based on
a RoBERTa-based language model, which we tailored towards user requirements
through fine-tuning. An overarching knowledge exploration graph combines all
information streams, provides in-situ explanations, transparent source
attribution, and facilitates effective exploration. To assess our approach, we
conducted a comprehensive set of evaluations: We benchmarked the underlying
language model on relevant NER tasks, achieving state-of-the-art performance.
The demonstrator was assessed according to intelligence capability assessments,
while the methodology was evaluated according to ethics design guidelines. As a
case study, we present our framework in an investigative journalism setting,
supporting war crime investigations. Finally, we conduct a formative user
evaluation with domain experts in law enforcement. Our evaluations confirm that
our framework facilitates human agency and steering in security-sensitive
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 3 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GMMFormer: Gaussian-Mixture-Model Based Transformer for Efficient
  Partially Relevant Video Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.05195v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.05195v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuting Wang, Jinpeng Wang, Bin Chen, Ziyun Zeng, Shu-Tao Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a text query, partially relevant video retrieval (PRVR) seeks to find
untrimmed videos containing pertinent moments in a database. For PRVR, clip
modeling is essential to capture the partial relationship between texts and
videos. Current PRVR methods adopt scanning-based clip construction to achieve
explicit clip modeling, which is information-redundant and requires a large
storage overhead. To solve the efficiency problem of PRVR methods, this paper
proposes GMMFormer, a Gaussian-Mixture-Model based Transformer which models
clip representations implicitly. During frame interactions, we incorporate
Gaussian-Mixture-Model constraints to focus each frame on its adjacent frames
instead of the whole video. Then generated representations will contain
multi-scale clip information, achieving implicit clip modeling. In addition,
PRVR methods ignore semantic differences between text queries relevant to the
same video, leading to a sparse embedding space. We propose a query diverse
loss to distinguish these text queries, making the embedding space more
intensive and contain more semantic information. Extensive experiments on three
large-scale video datasets (i.e., TVR, ActivityNet Captions, and Charades-STA)
demonstrate the superiority and efficiency of GMMFormer. Code is available at
\url{https://github.com/huangmozhi9527/GMMFormer}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2024. Code is released at
  https://github.com/huangmozhi9527/GMMFormer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EmotionGesture: Audio-Driven Diverse Emotional Co-Speech 3D Gesture
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18891v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18891v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingqun Qi, Chen Liu, Lincheng Li, Jie Hou, Haoran Xin, Xin Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating vivid and diverse 3D co-speech gestures is crucial for various
applications in animating virtual avatars. While most existing methods can
generate gestures from audio directly, they usually overlook that emotion is
one of the key factors of authentic co-speech gesture generation. In this work,
we propose EmotionGesture, a novel framework for synthesizing vivid and diverse
emotional co-speech 3D gestures from audio. Considering emotion is often
entangled with the rhythmic beat in speech audio, we first develop an
Emotion-Beat Mining module (EBM) to extract the emotion and audio beat features
as well as model their correlation via a transcript-based visual-rhythm
alignment. Then, we propose an initial pose based Spatial-Temporal Prompter
(STP) to generate future gestures from the given initial poses. STP effectively
models the spatial-temporal correlations between the initial poses and the
future gestures, thus producing the spatial-temporal coherent pose prompt. Once
we obtain pose prompts, emotion, and audio beat features, we will generate 3D
co-speech gestures through a transformer architecture. However, considering the
poses of existing datasets often contain jittering effects, this would lead to
generating unstable gestures. To address this issue, we propose an effective
objective function, dubbed Motion-Smooth Loss. Specifically, we model motion
offset to compensate for jittering ground-truth by forcing gestures to be
smooth. Last, we present an emotion-conditioned VAE to sample emotion features,
enabling us to generate diverse emotional results. Extensive experiments
demonstrate that our framework outperforms the state-of-the-art, achieving
vivid and diverse emotional co-speech 3D gestures. Our code and dataset will be
released at the project page:
https://xingqunqi-lab.github.io/Emotion-Gesture-Web/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-01-02T00:00:00Z">2024-01-02</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">34</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ To Diverge or Not to Diverge: A Morphosyntactic Perspective on Machine
  Translation vs Human Translation <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01419v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01419v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaming Luo, Colin Cherry, George Foster
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We conduct a large-scale fine-grained comparative analysis of machine
translations (MT) against human translations (HT) through the lens of
morphosyntactic divergence. Across three language pairs and two types of
divergence defined as the structural difference between the source and the
target, MT is consistently more conservative than HT, with less morphosyntactic
diversity, more convergent patterns, and more one-to-one alignments. Through
analysis on different decoding algorithms, we attribute this discrepancy to the
use of beam search that biases MT towards more convergent patterns. This bias
is most amplified when the convergent pattern appears around 50% of the time in
training data. Lastly, we show that for a majority of morphosyntactic
divergences, their presence in HT is correlated with decreased MT performance,
presenting a greater challenge for MT systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TACL, pre-MIT Press publication version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantifying the Uniqueness of Donald Trump in Presidential Discourse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karen Zhou, Alexander A. Meitus, Milo Chase, Grace Wang, Anne Mykland, William Howell, Chenhao Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Does Donald Trump speak differently from other presidents? If so, in what
ways? Are these differences confined to any single medium of communication? To
investigate these questions, this paper introduces a novel metric of uniqueness
based on large language models, develops a new lexicon for divisive speech, and
presents a framework for comparing the lexical features of political opponents.
Applying these tools to a variety of corpora of presidential speeches, we find
considerable evidence that Trump's speech patterns diverge from those of all
major party nominees for the presidency in recent history. Some notable
findings include Trump's employment of particularly divisive and antagonistic
language targeting of his political opponents and his patterns of repetition
for emphasis. Furthermore, Trump is significantly more distinctive than his
fellow Republicans, whose uniqueness values are comparably closer to those of
the Democrats. These differences hold across a variety of measurement
strategies, arise on both the campaign trail and in official presidential
addresses, and do not appear to be an artifact of secular time trends.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Play Fine-Tuning Converts Weak Language Models to Strong Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01335v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01335v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixiang Chen, Yihe Deng, Huizhuo Yuan, Kaixuan Ji, Quanquan Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Harnessing the power of human-annotated data through Supervised Fine-Tuning
(SFT) is pivotal for advancing Large Language Models (LLMs). In this paper, we
delve into the prospect of growing a strong LLM out of a weak one without the
need for acquiring additional human-annotated data. We propose a new
fine-tuning method called Self-Play fIne-tuNing (SPIN), which starts from a
supervised fine-tuned model. At the heart of SPIN lies a self-play mechanism,
where the LLM refines its capability by playing against instances of itself.
More specifically, the LLM generates its own training data from its previous
iterations, refining its policy by discerning these self-generated responses
from those obtained from human-annotated data. Our method progressively
elevates the LLM from a nascent model to a formidable one, unlocking the full
potential of human-annotated demonstration data for SFT. Theoretically, we
prove that the global optimum to the training objective function of our method
is achieved only when the LLM policy aligns with the target data distribution.
Empirically, we evaluate our method on several benchmark datasets including the
HuggingFace Open LLM Leaderboard, MT-Bench, and datasets from Big-Bench. Our
results show that SPIN can significantly improve the LLM's performance across a
variety of benchmarks and even outperform models trained through direct
preference optimization (DPO) supplemented with extra GPT-4 preference data.
This sheds light on the promise of self-play, enabling the achievement of
human-level performance in LLMs without the need for expert opponents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 6 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TREC iKAT 2023: The Interactive Knowledge Assistance Track Overview 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Aliannejadi, Zahra Abbasiantaeb, Shubham Chatterjee, Jeffery Dalton, Leif Azzopardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational Information Seeking stands as a pivotal research area with
significant contributions from previous works. The TREC Interactive Knowledge
Assistance Track (iKAT) builds on the foundational work of the TREC
Conversational Assistance Track (CAsT). However, iKAT distinctively emphasizes
the creation and research of conversational search agents that adapt responses
based on user's prior interactions and present context. The challenge lies in
enabling Conversational Search Agents (CSA) to incorporate this personalized
context to efficiency and effectively guide users through the relevant
information to them. iKAT also emphasizes decisional search tasks, where users
sift through data and information to weigh up options in order to reach a
conclusion or perform an action. These tasks, prevalent in everyday
information-seeking decisions -- be it related to travel, health, or shopping
-- often revolve around a subset of high-level information operators where
queries or questions about the information space include: finding options,
comparing options, identifying the pros and cons of options, etc. Given the
different personas and their information need (expressed through the sequence
of questions), diverse conversation trajectories will arise -- because the
answers to these similar queries will be very different. In this paper, we
report on the first year of TREC iKAT, describing the task, topics, data
collection, and evaluation framework. We further review the submissions and
summarize the findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TREC 2023 Overview Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Autoregressive Text-to-Graph Framework for Joint Entity and Relation
  Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01326v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01326v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zaratiana Urchade, Nadi Tomeh, Pierre Holat, Thierry Charnois
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel method for joint entity and relation
extraction from unstructured text by framing it as a conditional sequence
generation problem. In contrast to conventional generative information
extraction models that are left-to-right token-level generators, our approach
is \textit{span-based}. It generates a linearized graph where nodes represent
text spans and edges represent relation triplets. Our method employs a
transformer encoder-decoder architecture with pointing mechanism on a dynamic
vocabulary of spans and relation types. Our model can capture the structural
characteristics and boundaries of entities and relations through span
representations while simultaneously grounding the generated output in the
original text thanks to the pointing mechanism. Evaluation on benchmark
datasets validates the effectiveness of our approach, demonstrating competitive
results. Code is available at https://github.com/urchade/ATG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01325v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01325v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongye Jin, Xiaotian Han, Jingfeng Yang, Zhimeng Jiang, Zirui Liu, Chia-Yuan Chang, Huiyuan Chen, Xia Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work elicits LLMs' inherent ability to handle long contexts without
fine-tuning. The limited length of the training sequence during training may
limit the application of Large Language Models (LLMs) on long input sequences
for inference. In this work, we argue that existing LLMs themselves have
inherent capabilities for handling long contexts. Based on this argument, we
suggest extending LLMs' context window by themselves to fully utilize the
inherent ability.We propose Self-Extend to stimulate LLMs' long context
handling potential. The basic idea is to construct bi-level attention
information: the group level and the neighbor level. The two levels are
computed by the original model's self-attention, which means the proposed does
not require any training. With only four lines of code modification, the
proposed method can effortlessly extend existing LLMs' context window without
any fine-tuning. We conduct comprehensive experiments and the results show that
the proposed method can effectively extend existing LLMs' context window's
length.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Legal Fictions: Profiling Legal Hallucinations in Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Dahl, Varun Magesh, Mirac Suzgun, Daniel E. Ho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have the potential to transform the practice of
law, but this potential is threatened by the presence of legal hallucinations
-- responses from these models that are not consistent with legal facts. We
investigate the extent of these hallucinations using an original suite of legal
queries, comparing LLMs' responses to structured legal metadata and examining
their consistency. Our work makes four key contributions: (1) We develop a
typology of legal hallucinations, providing a conceptual framework for future
research in this area. (2) We find that legal hallucinations are alarmingly
prevalent, occurring between 69% of the time with ChatGPT 3.5 and 88% with
Llama 2, when these models are asked specific, verifiable questions about
random federal court cases. (3) We illustrate that LLMs often fail to correct a
user's incorrect legal assumptions in a contra-factual question setup. (4) We
provide evidence that LLMs cannot always predict, or do not always know, when
they are producing legal hallucinations. Taken together, these findings caution
against the rapid and unsupervised integration of popular LLMs into legal
tasks. Even experienced lawyers must remain wary of legal hallucinations, and
the risks are highest for those who stand to benefit from LLMs the most -- pro
se litigants or those without access to traditional legal resources.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive Study of Knowledge Editing for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, Siyuan Cheng, Ziwen Xu, Xin Xu, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown extraordinary capabilities in
understanding and generating text that closely mirrors human communication.
However, a primary limitation lies in the significant computational demands
during training, arising from their extensive parameterization. This challenge
is further intensified by the dynamic nature of the world, necessitating
frequent updates to LLMs to correct outdated information or integrate new
knowledge, thereby ensuring their continued relevance. Note that many
applications demand continual model adjustments post-training to address
deficiencies or undesirable behaviors. There is an increasing interest in
efficient, lightweight methods for on-the-fly model modifications. To this end,
recent years have seen a burgeoning in the techniques of knowledge editing for
LLMs, which aim to efficiently modify LLMs' behaviors within specific domains
while preserving overall performance across various inputs. In this paper, we
first define the knowledge editing problem and then provide a comprehensive
review of cutting-edge approaches. Drawing inspiration from educational and
cognitive research theories, we propose a unified categorization criterion that
classifies knowledge editing methods into three groups: resorting to external
knowledge, merging knowledge into the model, and editing intrinsic knowledge.
Furthermore, we introduce a new benchmark, KnowEdit, for a comprehensive
empirical evaluation of representative knowledge editing approaches.
Additionally, we provide an in-depth analysis of knowledge location, which can
provide a deeper understanding of the knowledge structures inherent within
LLMs. Finally, we discuss several potential applications of knowledge editing,
outlining its broad and impactful implications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing work; 50 pages, 265 citations; benchmark is available at
  https://huggingface.co/datasets/zjunlp/KnowEdit; code is available at
  https://github.com/zjunlp/EasyEdit; paper list is available at
  https://github.com/zjunlp/KnowledgeEditingPapers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CharacterEval: A Chinese <span class="highlight-title">Benchmark</span> for Role-Playing Conversational Agent
  Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Tu, Shilong Fan, Zihang Tian, Rui Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the advent of large language models (LLMs) has revolutionized
generative agents. Among them, Role-Playing Conversational Agents (RPCAs)
attract considerable attention due to their ability to emotionally engage
users. However, the absence of a comprehensive benchmark impedes progress in
this field. To bridge this gap, we introduce CharacterEval, a Chinese benchmark
for comprehensive RPCA assessment, complemented by a tailored high-quality
dataset. The dataset comprises 1,785 multi-turn role-playing dialogues,
encompassing 23,020 examples and featuring 77 characters derived from Chinese
novels and scripts. It was carefully constructed, beginning with initial
dialogue extraction via GPT-4, followed by rigorous human-led quality control,
and enhanced with in-depth character profiles sourced from Baidu Baike.
CharacterEval employs a multifaceted evaluation approach, encompassing thirteen
targeted metrics on four dimensions. Comprehensive experiments on CharacterEval
demonstrate that Chinese LLMs exhibit more promising capabilities than GPT-4 in
Chinese role-playing conversation. Source code, data source and reward model
will be publicly accessible at https://github.com/morecry/CharacterEval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuchen Long, Zhaofan Qiu, Ting Yao, Tao Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent innovations and breakthroughs in diffusion models have
significantly expanded the possibilities of generating high-quality videos for
the given prompts. Most existing works tackle the single-scene scenario with
only one video event occurring in a single background. Extending to generate
multi-scene videos nevertheless is not trivial and necessitates to nicely
manage the logic in between while preserving the consistent visual appearance
of key content across video scenes. In this paper, we propose a novel
framework, namely VideoDrafter, for content-consistent multi-scene video
generation. Technically, VideoDrafter leverages Large Language Models (LLM) to
convert the input prompt into comprehensive multi-scene script that benefits
from the logical knowledge learnt by LLM. The script for each scene includes a
prompt describing the event, the foreground/background entities, as well as
camera movement. VideoDrafter identifies the common entities throughout the
script and asks LLM to detail each entity. The resultant entity description is
then fed into a text-to-image model to generate a reference image for each
entity. Finally, VideoDrafter outputs a multi-scene video by generating each
scene video via a diffusion process that takes the reference images, the
descriptive prompt of the event and camera movement into account. The diffusion
model incorporates the reference images as the condition and alignment to
strengthen the content consistency of multi-scene videos. Extensive experiments
demonstrate that VideoDrafter outperforms the SOTA video generation models in
terms of visual quality, content consistency, and user preference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://videodrafter.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-Shot Position Debiasing for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongkun Liu, Zheng Chen, Mengqi Zhang, Zhaochun Ren, Zhumin Chen, Pengjie Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning has been demonstrated to be an effective method to improve the
domain performance of large language models (LLMs). However, LLMs might fit the
dataset bias and shortcuts for prediction, leading to poor generation
performance. Experimental result shows that LLMs are prone to exhibit position
bias, i.e., leveraging information positioned at the beginning or end, or
specific positional cues within the input. Existing works on mitigating
position bias require external bias knowledge or annotated non-biased samples,
which is unpractical in reality. In this work, we propose a zero-shot position
debiasing (ZOE) framework to mitigate position bias for LLMs. ZOE leverages
unsupervised responses from pre-trained LLMs for debiasing, thus without any
external knowledge or datasets. To improve the quality of unsupervised
responses, we propose a master-slave alignment (MSA) module to prune these
responses. Experiments on eight datasets and five tasks show that ZOE
consistently outperforms existing methods in mitigating four types of position
biases. Besides, ZOE achieves this by sacrificing only a small performance on
biased samples, which is simple and effective.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 22 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty Resolution in Misinformation Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01197v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01197v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yury Orlovskiy, Camille Thibault, Anne Imouza, Jean-François Godbout, Reihaneh Rabbany, Kellin Pelrine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Misinformation poses a variety of risks, such as undermining public trust and
distorting factual discourse. Large Language Models (LLMs) like GPT-4 have been
shown effective in mitigating misinformation, particularly in handling
statements where enough context is provided. However, they struggle to assess
ambiguous or context-deficient statements accurately. This work introduces a
new method to resolve uncertainty in such statements. We propose a framework to
categorize missing information and publish category labels for the LIAR-New
dataset, which is adaptable to cross-domain content with missing information.
We then leverage this framework to generate effective user queries for missing
context. Compared to baselines, our method improves the rate at which generated
questions are answerable by the user by 38 percentage points and classification
performance by over 10 percentage points macro F1. Thus, this approach may
provide a valuable component for future misinformation mitigation pipelines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unifying Structured Data as Graph for Data-to-Text Pre-Training <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shujie Li, Liang Li, Ruiying Geng, Min Yang, Binhua Li, Guanghu Yuan, Wanwei He, Shao Yuan, Can Ma, Fei Huang, Yongbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-to-text (D2T) generation aims to transform structured data into natural
language text. Data-to-text pre-training has proved to be powerful in enhancing
D2T generation and yields impressive performances. However, previous
pre-training methods either oversimplified structured data into a sequence
without considering input structures or designed training objectives tailored
for a specific data structure (e.g., table or knowledge graph). In this paper,
we unify different types of structured data (i.e., table, key-value data,
knowledge graph) into the graph format and cast different data-to-text
generation tasks as graph-to-text generation. To effectively exploit the
structural information of the input graph, we propose a structure-enhanced
pre-training method for D2T generation by designing a structure-enhanced
Transformer. Concretely, we devise a position matrix for the Transformer,
encoding relative positional information of connected nodes in the input graph.
In addition, we propose a new attention matrix to incorporate graph structures
into the original Transformer by taking the available explicit connectivity
structure into account. Extensive experiments on six benchmark datasets show
the effectiveness of our model. Our source codes are available at
https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/unid2t.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for TACL. Pre-MIT Press publication version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling Comparative Sentiments in Vietnamese Product Reviews: A
  Sequential Classification Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01108v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01108v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ha Le, Bao Tran, Phuong Le, Tan Nguyen, Dac Nguyen, Ngoan Pham, Dang Huynh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Comparative opinion mining is a specialized field of sentiment analysis that
aims to identify and extract sentiments expressed comparatively. To address
this task, we propose an approach that consists of solving three sequential
sub-tasks: (i) identifying comparative sentence, i.e., if a sentence has a
comparative meaning, (ii) extracting comparative elements, i.e., what are
comparison subjects, objects, aspects, predicates, and (iii) classifying
comparison types which contribute to a deeper comprehension of user sentiments
in Vietnamese product reviews. Our method is ranked fifth at the Vietnamese
Language and Speech Processing (VLSP) 2023 challenge on Comparative Opinion
Mining (ComOM) from Vietnamese Product Reviews.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted manuscript at VLSP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quokka: An Open-source Large Language Model ChatBot for Material Science 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01089v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01089v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianjun Yang, Stephen D. Wilson, Linda Petzold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the development of a specialized chatbot for materials
science, leveraging the Llama-2 language model, and continuing pre-training on
the expansive research articles in the materials science domain from the S2ORC
dataset. The methodology involves an initial pretraining phase on over one
million domain-specific papers, followed by an instruction-tuning process to
refine the chatbot's capabilities. The chatbot is designed to assist
researchers, educators, and students by providing instant, context-aware
responses to queries in the field of materials science. We make the four
trained checkpoints (7B, 13B, with or without chat ability) freely available to
the research community at https://github.com/Xianjun-Yang/Quokka.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Discovering Significant Topics from Legal Decisions with Selective
  Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01068v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01068v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jerrold Soh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose and evaluate an automated pipeline for discovering significant
topics from legal decision texts by passing features synthesized with topic
models through penalised regressions and post-selection significance tests. The
method identifies case topics significantly correlated with outcomes,
topic-word distributions which can be manually-interpreted to gain insights
about significant topics, and case-topic weights which can be used to identify
representative cases for each topic. We demonstrate the method on a new dataset
of domain name disputes and a canonical dataset of European Court of Human
Rights violation cases. Topic models based on latent semantic analysis as well
as language model embeddings are evaluated. We show that topics derived by the
pipeline are consistent with legal doctrines in both areas and can be useful in
other related legal analysis tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is an accepted manuscript of work forthcoming in PhilTrans A.
  Please cite the publisher's version only</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLaMA Beyond English: An Empirical Study on Language Capability Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Zhao, Zhihao Zhang, Qi Zhang, Tao Gui, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent times, substantial advancements have been witnessed in large
language models (LLMs), exemplified by ChatGPT, showcasing remarkable
proficiency across a range of complex tasks. However, many mainstream LLMs
(e.g. LLaMA) are pretrained on English-dominant corpus, which limits their
performance in other non-English languages. In this paper, we focus on how to
effectively transfer the capabilities of language generation and following
instructions to a non-English language. To answer this question, we conduct an
extensive empirical investigation based on LLaMA, accumulating over 1440 GPU
hours. We analyze the impact of key factors such as vocabulary extension,
further pretraining, and instruction tuning on transfer. To accurately assess
the model's level of knowledge, we employ four widely used standardized testing
benchmarks: C-Eval, MMLU, AGI-Eval, and GAOKAO-Bench. Furthermore, a
comprehensive evaluation of the model's response quality is conducted,
considering aspects such as accuracy, fluency, informativeness, logical
coherence, and harmlessness, based on LLM-Eval, a benchmarks consisting
instruction tasks from 17 diverse categories. Our evaluation results
demonstrate that comparable performance to state-of-the-art transfer models can
be achieved with less than 1% of the pretraining data, both in terms of
knowledge alignment and response quality. Furthermore, the experimental
outcomes across the thirteen low-resource languages also exhibit similar
trends. We anticipate that the conclusions revealed by the experiments will aid
the community in developing non-English LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cheetah: Natural Language Generation for 517 African Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01053v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01053v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ife Adebara, AbdelRahim Elmadany, Muhammad Abdul-Mageed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-resource African languages pose unique challenges for natural language
processing (NLP) tasks, including natural language generation (NLG). In this
paper, we develop Cheetah, a massively multilingual NLG language model for
African languages. Cheetah supports 517 African languages and language
varieties, allowing us to address the scarcity of NLG resources and provide a
solution to foster linguistic diversity. We demonstrate the effectiveness of
Cheetah through comprehensive evaluations across seven generation downstream
tasks. In five of the seven tasks, Cheetah significantly outperforms other
models, showcasing its remarkable performance for generating coherent and
contextually appropriate text in a wide range of African languages. We
additionally conduct a detailed human evaluation to delve deeper into the
linguistic capabilities of Cheetah. The introduction of Cheetah has
far-reaching benefits for linguistic diversity. By leveraging pretrained models
and adapting them to specific languages, our approach facilitates the
development of practical NLG applications for African communities. The findings
of this study contribute to advancing NLP research in low-resource settings,
enabling greater accessibility and inclusion for African languages in a rapidly
expanding digital landscape. We will publicly release our models for research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Auffusion: Leveraging the Power of Diffusion and Large Language Models
  for Text-to-Audio Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01044v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01044v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinlong Xue, Yayue Deng, Yingming Gao, Ya Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in diffusion models and large language models (LLMs) have
significantly propelled the field of AIGC. Text-to-Audio (TTA), a burgeoning
AIGC application designed to generate audio from natural language prompts, is
attracting increasing attention. However, existing TTA studies often struggle
with generation quality and text-audio alignment, especially for complex
textual inputs. Drawing inspiration from state-of-the-art Text-to-Image (T2I)
diffusion models, we introduce Auffusion, a TTA system adapting T2I model
frameworks to TTA task, by effectively leveraging their inherent generative
strengths and precise cross-modal alignment. Our objective and subjective
evaluations demonstrate that Auffusion surpasses previous TTA approaches using
limited data and computational resource. Furthermore, previous studies in T2I
recognizes the significant impact of encoder choice on cross-modal alignment,
like fine-grained details and object bindings, while similar evaluation is
lacking in prior TTA works. Through comprehensive ablation studies and
innovative cross-attention map visualizations, we provide insightful
assessments of text-audio alignment in TTA. Our findings reveal Auffusion's
superior capability in generating audios that accurately match textual
descriptions, which further demonstrated in several related tasks, such as
audio style transfer, inpainting and other manipulations. Our implementation
and demos are available at https://auffusion.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Demo and implementation at https://auffusion.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Assessment Tests are Unreliable Measures of LLM Personality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.08163v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.08163v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshat Gupta, Xiaoyang Song, Gopala Anumanchipalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLM) evolve in their capabilities, various recent
studies have tried to quantify their behavior using psychological tools created
to study human behavior. One such example is the measurement of "personality"
of LLMs using self-assessment personality tests developed to measure human
personality. Yet almost none of these works verify the applicability of these
tests on LLMs. In this paper, we analyze the reliability of LLM personality
scores obtained from self-assessment personality tests using two simple
experiments. We first introduce the property of prompt sensitivity, where three
semantically equivalent prompts representing three intuitive ways of
administering self-assessment tests on LLMs are used to measure the personality
of the same LLM. We find that all three prompts lead to very different
personality scores, a difference that is statistically significant for all
traits in a large majority of scenarios. We then introduce the property of
option-order symmetry for personality measurement of LLMs. Since most of the
self-assessment tests exist in the form of multiple choice question (MCQ)
questions, we argue that the scores should also be robust to not just the
prompt template but also the order in which the options are presented. This
test unsurprisingly reveals that the self-assessment test scores are not robust
to the order of the options. These simple tests, done on ChatGPT and three
Llama2 models of different sizes, show that self-assessment personality tests
created for humans are unreliable measures of personality in LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models Are Not Strong Abstract Reasoners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.19555v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.19555v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaël Gendron, Qiming Bao, Michael Witbrock, Gillian Dobbie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models have shown tremendous performance on a large variety of
natural language processing tasks, ranging from text comprehension to common
sense reasoning. However, the mechanisms responsible for this success remain
opaque, and it is unclear whether LLMs can achieve human-like cognitive
capabilities or whether these models are still fundamentally circumscribed.
Abstract reasoning is a fundamental task for cognition, consisting of finding
and applying a general pattern from few data. Evaluating deep neural
architectures on this task could give insight into their potential limitations
regarding reasoning and their broad generalisation abilities, yet this is
currently an under-explored area. In this paper, we introduce a new benchmark
for evaluating language models beyond memorization on abstract reasoning tasks.
We perform extensive evaluations of state-of-the-art LLMs, showing that they
currently achieve very limited performance in contrast with other natural
language tasks, even when applying techniques that have been shown to improve
performance on other NLP tasks. We argue that guiding LLM generation to follow
causal paths could help improve the generalisation and reasoning abilities of
LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>50 pages, 14 pages for the main paper and 36 pages for the
  supplement, 35 figures, 17 tables. V3: performed additional experiments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05134v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05134v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Grimal, Hervé Le Borgne, Olivier Ferret, Julien Tourille
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The progress in the generation of synthetic images has made it crucial to
assess their quality. While several metrics have been proposed to assess the
rendering of images, it is crucial for Text-to-Image (T2I) models, which
generate images based on a prompt, to consider additional aspects such as to
which extent the generated image matches the important content of the prompt.
Moreover, although the generated images usually result from a random starting
point, the influence of this one is generally not considered. In this article,
we propose a new metric based on prompt templates to study the alignment
between the content specified in the prompt and the corresponding generated
images. It allows us to better characterize the alignment in terms of the type
of the specified objects, their number, and their color. We conducted a study
on several recent T2I models about various aspects. An additional interesting
result we obtained with our approach is that image quality can vary drastically
depending on the noise used as a seed for the images. We also quantify the
influence of the number of concepts in the prompt, their order as well as their
(color) attributes. Finally, our method allows us to identify some seeds that
produce better images than others, opening novel directions of research on this
understudied topic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multimodality and Attention Increase Alignment in Natural Language
  Prediction Between Humans and Computational Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.06035v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.06035v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Viktor Kewenig, Andrew Lampinen, Samuel A. Nastase, Christopher Edwards, Quitterie Lacome DEstalenx, Akilles Rechardt, Jeremy I Skipper, Gabriella Vigliocco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The potential of multimodal generative artificial intelligence (mAI) to
replicate human grounded language understanding, including the pragmatic,
context-rich aspects of communication, remains to be clarified. Humans are
known to use salient multimodal features, such as visual cues, to facilitate
the processing of upcoming words. Correspondingly, multimodal computational
models can integrate visual and linguistic data using a visual attention
mechanism to assign next-word probabilities. To test whether these processes
align, we tasked both human participants (N = 200) as well as several
state-of-the-art computational models with evaluating the predictability of
forthcoming words after viewing short audio-only or audio-visual clips with
speech. During the task, the model's attention weights were recorded and human
attention was indexed via eye tracking. Results show that predictability
estimates from humans aligned more closely with scores generated from
multimodal models vs. their unimodal counterparts. Furthermore, including an
attention mechanism doubled alignment with human judgments when visual and
linguistic context facilitated predictions. In these cases, the model's
attention patches and human eye tracking significantly overlapped. Our results
indicate that improved modeling of naturalistic language processing in mAI does
not merely depend on training diet but can be driven by multimodality in
combination with attention-based architectures. Humans and computational models
alike can leverage the predictive constraints of multimodal information by
attending to relevant features in the input.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 4 figures, submitted to Nature Human Behaviour</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ArtGPT-4: Towards Artistic-understanding Large Vision-Language Models
  with Enhanced Adapter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.07490v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.07490v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengqing Yuan, Xinyi Wang, Kun Wang, Lichao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, advancements in large language models have been remarkable,
with models such as ChatGPT demonstrating exceptional proficiency in diverse
linguistic tasks. The pre-training of large models with billions of parameters,
poses a formidable challenge, primarily due to the scarcity of datasets of a
commensurate scale for effective training. Nevertheless, innovative strategies
have emerged, including methods to fine-tune these pre-trained models using
fewer parameters set, as evidenced by models like MiniGPT-4 and LLaVA. Despite
their potential in various domains, these models remain limited in their
understanding of artistic imagery. They have yet to fully grasp the intricate
nuances of art images or to provide an objective articulation of the emotions
they evoke, in a manner akin to human perception. This work introduces
ArtGPT-4, a pioneering large vision-language model tailored to address the
deficiencies of contemporary models in artistic comprehension. ArtGPT-4
underwent training on image-text pairs utilizing a Tesla A100 device in a mere
2 hours, with a dataset comprising approximately 0.52M entries. Impressively,
the model can render images with an artistic-understanding and convey the
emotions they inspire, mirroring human interpretation. Additionally, this work
presents a unique dataset designed to evaluate the efficacy of vision-language
models. In subsequent evaluations, ArtGPT-4 not only achieved state-of-the-art
performance on the ArtEmis and ArtEmis-v2.0 datasets but also exceeded the
established benchmarks introduced in This study, lagging behind professional
artists' descriptions by a negligible 0.15 points on a 6-point scale. The code
and the pre-trained model are accessible in
https://huggingface.co/Tyrannosaurus/ArtGPT-4.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-Time Online Stock Forecasting Utilizing Integrated Quantitative and
  Qualitative Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.15218v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.15218v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Akash Bathini, Dagli Cihan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The application of Machine learning to finance has become a familiar
approach, even more so in stock market forecasting. The stock market is highly
volatile, and huge amounts of data are generated every minute globally. The
extraction of effective intelligence from this data is of critical importance.
However, a collaboration of numerical stock data with qualitative text data can
be a challenging task. In this work, we accomplish this by providing an
unprecedented, publicly available dataset with technical and fundamental data
and sentiment that we gathered from news archives, TV news captions, radio
transcripts, tweets, daily financial newspapers, etc. The text data entries
used for sentiment extraction total more than 1.4 Million. The dataset consists
of daily entries from January 2018 to December 2022 for eight companies
representing diverse industrial sectors and the Dow Jones Industrial Average
(DJIA) as a whole. Holistic Fundamental and Technical data is provided training
ready for Model learning and deployment. Most importantly, the data generated
could be used for incremental online learning with real-time data points
retrieved daily since no stagnant data was utilized. All the data was retired
from APIs or self-designed robust information retrieval technologies with
extremely low latency and zero monetary cost. These adaptable technologies
facilitate data extraction for any stock. Moreover, the utilization of
Spearman's rank correlation over real-time data, linking stock returns with
sentiment analysis has produced noteworthy results for the DJIA and the eight
other stocks, achieving accuracy levels surpassing 60%. The dataset is made
available at https://github.com/batking24/Huge-Stock-Dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structured Packing in LLM Training Improves Long Context Utilization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17296v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17296v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konrad Staniszewski, Szymon Tworkowski, Sebastian Jaszczur, Henryk Michalewski, Łukasz Kuciński, Piotr Miłoś
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in long-context Large Language Models (LCLMs) have generated
significant interest, especially in applications such as querying scientific
research papers. However, their potential is often limited by inadequate
context utilization. We identify the absence of long-range semantic
dependencies in typical training data as a primary hindrance. To address this,
we delve into the benefits of frequently incorporating related documents into
training inputs. Using the inherent directory structure of code data as a
source of training examples, we demonstrate improvements in perplexity, even
for tasks unrelated to coding. Building on these findings, but with a broader
focus, we introduce Structured Packing for Long Context (SPLiCe). SPLiCe is an
innovative method for creating training examples by using a retrieval method to
collate the most mutually relevant documents into a single training context.
Our results indicate that \method{} enhances model performance and can be used
to train large models to utilize long contexts better. We validate our results
by training a large $3$B model, showing both perplexity improvements and better
long-context performance on downstream tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RS5M and GeoRSCLIP: A Large Scale Vision-Language <span class="highlight-title">Dataset</span> and A Large
  Vision-Language Model for Remote Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11300v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11300v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zilun Zhang, Tiancheng Zhao, Yulong Guo, Jianwei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained Vision-Language Models (VLMs) utilizing extensive image-text
paired data have demonstrated unprecedented image-text association
capabilities, achieving remarkable results across various downstream tasks. A
critical challenge is how to make use of existing large-scale pre-trained VLMs,
which are trained on common objects, to perform the domain-specific transfer
for accomplishing domain-related downstream tasks. A critical challenge is how
to make use of existing large-scale pre-trained VLMs, which are trained on
common objects, to perform the domain-specific transfer for accomplishing
domain-related downstream tasks. In this paper, we propose a new framework that
includes the Domain pre-trained Vision-Language Model (DVLM), bridging the gap
between the General Vision-Language Model (GVLM) and domain-specific downstream
tasks. Moreover, we present an image-text paired dataset in the field of remote
sensing (RS), RS5M, which has 5 million RS images with English descriptions.
The dataset is obtained from filtering publicly available image-text paired
datasets and captioning label-only RS datasets with pre-trained VLM. These
constitute the first large-scale RS image-text paired dataset. Additionally, we
fine-tuned the CLIP model and tried several Parameter-Efficient Fine-Tuning
methods on RS5M to implement the DVLM. Experimental results show that our
proposed dataset is highly effective for various tasks, and our model GeoRSCLIP
improves upon the baseline or previous state-of-the-art model by $3\%\sim20\%$
in Zero-shot Classification (ZSC), $3\%\sim6\%$ in Remote Sensing Cross-Modal
Text-Image Retrieval (RSCTIR) and $4\%\sim5\%$ in Semantic Localization (SeLo)
tasks. Dataset and models have been released in:
\url{https://github.com/om-ai-lab/RS5M}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>RS5M dataset v5</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StyleSinger: Style Transfer for Out-of-Domain Singing Voice Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10741v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10741v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhang, Rongjie Huang, Ruiqi Li, JinZheng He, Yan Xia, Feiyang Chen, Xinyu Duan, Baoxing Huai, Zhou Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Style transfer for out-of-domain (OOD) singing voice synthesis (SVS) focuses
on generating high-quality singing voices with unseen styles (such as timbre,
emotion, pronunciation, and articulation skills) derived from reference singing
voice samples. However, the endeavor to model the intricate nuances of singing
voice styles is an arduous task, as singing voices possess a remarkable degree
of expressiveness. Moreover, existing SVS methods encounter a decline in the
quality of synthesized singing voices in OOD scenarios, as they rest upon the
assumption that the target vocal attributes are discernible during the training
phase. To overcome these challenges, we propose StyleSinger, the first singing
voice synthesis model for zero-shot style transfer of out-of-domain reference
singing voice samples. StyleSinger incorporates two critical approaches for
enhanced effectiveness: 1) the Residual Style Adaptor (RSA) which employs a
residual quantization module to capture diverse style characteristics in
singing voices, and 2) the Uncertainty Modeling Layer Normalization (UMLN) to
perturb the style attributes within the content representation during the
training phase and thus improve the model generalization. Our extensive
evaluations in zero-shot style transfer undeniably establish that StyleSinger
outperforms baseline models in both audio quality and similarity to the
reference singing voice samples. Access to singing voice samples can be found
at https://stylesinger.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Stochastic Analysis of the Linguistic Provenance of English Place
  Names 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.12850v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.12850v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Dalvean
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In English place name analysis, meanings are often derived from the
resemblance of roots in place names to topographical features, proper names
and/or habitation terms in one of the languages that have had an influence on
English place names. The problem here is that it is sometimes difficult to
determine the base language to use to interpret the roots. The purpose of this
paper is to stochastically determine the resemblance between 18799 English
place names and 84687 place names from Ireland, Scotland, Wales, Denmark,
Norway, Sweden, France, Germany, the Netherlands and Ancient Rome. Each English
place name is ranked according to the extent to which it resembles place names
from the other countries, and this provides a basis for determining the likely
language to use to interpret the place name. A number of observations can be
made using the ranking provided. In particular, it is found that `Harlington'
is the most archetypically English place name in the English sample, and `Anna'
is the least. Furthermore, it is found that the place names in the non-English
datasets are most similar to Norwegian place names and least similar to Welsh
place names.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-depth analysis of music structure as a text network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13631v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13631v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ping-Rui Tsai, Yen-Ting Chou, Nathan-Christopher Wang, Hui-Ling Chen, Hong-Yue Huang, Zih-Jia Luo, Tzay-Ming Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Music, enchanting and poetic, permeates every corner of human civilization.
Although music is not unfamiliar to people, our understanding of its essence
remains limited, and there is still no universally accepted scientific
description. This is primarily due to music being regarded as a product of both
reason and emotion, making it difficult to define. In this article, we focus on
the fundamental elements of music and construct an evolutionary network from
the perspective of music as a natural language, aligning with the statistical
characteristics of texts. Through this approach, we aim to comprehend the
structural differences in music across different periods, enabling a more
scientific exploration of music. Relying on the advantages of structuralism, we
can concentrate on the relationships and order between the physical elements of
music, rather than getting entangled in the blurred boundaries of science and
philosophy. The scientific framework we present not only conforms to past
conclusions in music, but also serves as a bridge that connects music to
natural language processing and knowledge graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.10477v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.10477v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Chen, Chunwei Wang, Kuo Yang, Jianhua Han, Lanqing Hong, Fei Mi, Hang Xu, Zhengying Liu, Wenyong Huang, Zhenguo Li, Dit-Yan Yeung, Lifeng Shang, Xin Jiang, Qun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of large language models (LLMs) has not only provided
numerous opportunities but also presented significant challenges. This becomes
particularly evident when LLMs inadvertently generate harmful or toxic content,
either unintentionally or because of intentional inducement. Existing alignment
methods usually direct LLMs toward the favorable outcomes by utilizing
human-annotated, flawless instruction-response pairs. Conversely, this study
proposes a novel alignment technique based on mistake analysis, which
deliberately exposes LLMs to erroneous content to learn the reasons for
mistakes and how to avoid them. In this case, mistakes are repurposed into
valuable data for alignment, effectively helping to avoid the production of
erroneous responses. Without external models or human annotations, our method
leverages a model's intrinsic ability to discern undesirable mistakes and
improves the safety of its generated responses. Experimental results reveal
that our method outperforms existing alignment approaches in enhancing model
safety while maintaining the overall utility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Search Model: Redefining Search Stack in the Era of LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.14587v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.14587v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern search engines are built on a stack of different components, including
query understanding, retrieval, multi-stage ranking, and question answering,
among others. These components are often optimized and deployed independently.
In this paper, we introduce a novel conceptual framework called large search
model, which redefines the conventional search stack by unifying search tasks
with one large language model (LLM). All tasks are formulated as autoregressive
text generation problems, allowing for the customization of tasks through the
use of natural language prompts. This proposed framework capitalizes on the
strong language understanding and reasoning capabilities of LLMs, offering the
potential to enhance search result quality while simultaneously simplifying the
existing cumbersome search stack. To substantiate the feasibility of this
framework, we present a series of proof-of-concept experiments and discuss the
potential challenges associated with implementing this approach within
real-world search systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGIR Forum, Vol. 57 No. 2 - December 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Study on the Calibration of In-context Learning <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04021v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04021v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanlin Zhang, Yi-Fan Zhang, Yaodong Yu, Dhruv Madeka, Dean Foster, Eric Xing, Himabindu Lakkaraju, Sham Kakade
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate uncertainty quantification is crucial for the safe deployment of
language models (LMs), and prior research has demonstrated improvements in the
calibration of modern LMs. Our study focuses on in-context learning (ICL), a
prevalent method for adapting static LMs through tailored prompts, and examines
the balance between performance and calibration across a broad spectrum of
natural language understanding and reasoning tasks. Through comprehensive
experiments, we observe that, with an increasing number of ICL examples, models
initially exhibit increased miscalibration before achieving better calibration
and miscalibration tends to arise in low-shot settings. Moreover, we find that
methods aimed at improving usability, such as fine-tuning and chain-of-thought
(CoT) prompting, can lead to miscalibration and unreliable natural language
explanations, suggesting that new methods may be required for scenarios where
models are expected to be reliable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Spotlight Talk at NeurIPS 2023 Workshop on Failure Modes in the Age
  of Foundation Models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reward-Augmented Decoding: Efficient Controlled Text Generation With a
  Unidirectional Reward Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.09520v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.09520v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haikang Deng, Colin Raffel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models have proven effective in a huge range of
downstream applications, they often generate text that is problematic or lacks
a desired attribute. In this paper, we introduce Reward-Augmented Decoding
(RAD), a text generation procedure that uses a small unidirectional reward
model to encourage a language model to generate text that has certain
properties. Specifically, RAD uses the reward model to score generations as
they are produced and rescales sampling probabilities to favor high-reward
tokens. By using a unidirectional reward model, RAD can cache activations from
prior generation steps to decrease computational overhead. Through experiments
on generating non-toxic and sentiment-controlled text, we demonstrate that RAD
performs best among methods that change only the generation procedure and
matches the performance of state-of-the-art methods that involve re-training
the language model. We further validate that RAD is effective on very large
language models while incurring a minimal computational overhead.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">53</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Hybrid Zoom using Camera Fusion on Mobile Phones 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01461v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01461v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaotong Wu, Wei-Sheng Lai, YiChang Shih, Charles Herrmann, Michael Krainin, Deqing Sun, Chia-Kai Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DSLR cameras can achieve multiple zoom levels via shifting lens distances or
swapping lens types. However, these techniques are not possible on smartphone
devices due to space constraints. Most smartphone manufacturers adopt a hybrid
zoom system: commonly a Wide (W) camera at a low zoom level and a Telephoto (T)
camera at a high zoom level. To simulate zoom levels between W and T, these
systems crop and digitally upsample images from W, leading to significant
detail loss. In this paper, we propose an efficient system for hybrid zoom
super-resolution on mobile devices, which captures a synchronous pair of W and
T shots and leverages machine learning models to align and transfer details
from T to W. We further develop an adaptive blending method that accounts for
depth-of-field mismatches, scene occlusion, flow uncertainty, and alignment
errors. To minimize the domain gap, we design a dual-phone camera rig to
capture real-world inputs and ground-truths for supervised training. Our method
generates a 12-megapixel image in 500ms on a mobile platform and compares
favorably against state-of-the-art methods under extensive evaluation on
real-world scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to SIGGRAPH Asia 2023 (ACM TOG). Project website:
  https://www.wslai.net/publications/fusion_zoom</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ColorizeDiffusion: Adjustable Sketch Colorization with Reference Image
  and Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01456v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01456v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dingkun Yan, Liang Yuan, Yuma Nishioka, Issei Fujishiro, Suguru Saito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, diffusion models have demonstrated their effectiveness in
generating extremely high-quality images and have found wide-ranging
applications, including automatic sketch colorization. However, most existing
models use text to guide the conditional generation, with fewer attempts
exploring the potential advantages of using image tokens as conditional inputs
for networks. As such, this paper exhaustively investigates image-guided
models, specifically targeting reference-based sketch colorization, which aims
to colorize sketch images using reference color images. We investigate three
critical aspects of reference-based diffusion models: the shortcomings compared
to text-based counterparts, the training strategies, and the capability in
zero-shot, sequential text-based manipulation. We introduce two variations of
an image-guided latent diffusion model using different image tokens from the
pre-trained CLIP image encoder, and we propose corresponding manipulation
methods to adjust their results sequentially using weighted text inputs. We
conduct comprehensive evaluations of our models through qualitative and
quantitative experiments, as well as a user study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Survey on Autonomous Driving <span class="highlight-title">Dataset</span>s: Data Statistic, Annotation, and
  Outlook 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01454v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01454v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyu Liu, Ekim Yurtsever, Xingcheng Zhou, Jonathan Fossaert, Yuning Cui, Bare Luka Zagar, Alois C. Knoll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving has rapidly developed and shown promising performance with
recent advances in hardware and deep learning methods. High-quality datasets
are fundamental for developing reliable autonomous driving algorithms. Previous
dataset surveys tried to review the datasets but either focused on a limited
number or lacked detailed investigation of the characters of datasets. To this
end, we present an exhaustive study of over 200 autonomous driving datasets
from multiple perspectives, including sensor modalities, data size, tasks, and
contextual conditions. We introduce a novel metric to evaluate the impact of
each dataset, which can also be a guide for establishing new datasets. We
further analyze the annotation process and quality of datasets. Additionally,
we conduct an in-depth analysis of the data distribution of several vital
datasets. Finally, we discuss the development trend of the future autonomous
driving datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ProbMCL: Simple Probabilistic Contrastive Learning for Multi-label
  Visual Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01448v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01448v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Sajedi, Samir Khaki, Yuri A. Lawryshyn, Konstantinos N. Plataniotis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-label image classification presents a challenging task in many domains,
including computer vision and medical imaging. Recent advancements have
introduced graph-based and transformer-based methods to improve performance and
capture label dependencies. However, these methods often include complex
modules that entail heavy computation and lack interpretability. In this paper,
we propose Probabilistic Multi-label Contrastive Learning (ProbMCL), a novel
framework to address these challenges in multi-label image classification
tasks. Our simple yet effective approach employs supervised contrastive
learning, in which samples that share enough labels with an anchor image based
on a decision threshold are introduced as a positive set. This structure
captures label dependencies by pulling positive pair embeddings together and
pushing away negative samples that fall below the threshold. We enhance
representation learning by incorporating a mixture density network into
contrastive learning and generating Gaussian mixture distributions to explore
the epistemic uncertainty of the feature encoder. We validate the effectiveness
of our framework through experimentation with datasets from the computer vision
and medical imaging domains. Our method outperforms the existing
state-of-the-art methods while achieving a low computational footprint on both
datasets. Visualization analyses also demonstrate that ProbMCL-learned
classifiers maintain a meaningful semantic topology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for the IEEE International Conference on
  Acoustics, Speech and Signal Processing (ICASSP) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Indoor Obstacle Discovery on Reflective Ground via Monocular Camera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Xue, Yicong Chang, Tianxi Wang, Yu Zhou, Anlong Ming
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual obstacle discovery is a key step towards autonomous navigation of
indoor mobile robots. Successful solutions have many applications in multiple
scenes. One of the exceptions is the reflective ground. In this case, the
reflections on the floor resemble the true world, which confuses the obstacle
discovery and leaves navigation unsuccessful. We argue that the key to this
problem lies in obtaining discriminative features for reflections and
obstacles. Note that obstacle and reflection can be separated by the ground
plane in 3D space. With this observation, we firstly introduce a
pre-calibration based ground detection scheme that uses robot motion to predict
the ground plane. Due to the immunity of robot motion to reflection, this
scheme avoids failed ground detection caused by reflection. Given the detected
ground, we design a ground-pixel parallax to describe the location of a pixel
relative to the ground. Based on this, a unified appearance-geometry feature
representation is proposed to describe objects inside rectangular boxes.
Eventually, based on segmenting by detection framework, an appearance-geometry
fusion regressor is designed to utilize the proposed feature to discover the
obstacles. It also prevents our model from concentrating too much on parts of
obstacles instead of whole obstacles. For evaluation, we introduce a new
dataset for Obstacle on Reflective Ground (ORG), which comprises 15 scenes with
various ground reflections, a total of more than 200 image sequences and 3400
RGB images. The pixel-wise annotations of ground and obstacle provide a
comparison to our method and other methods. By reducing the misdetection of the
reflection, the proposed approach outperforms others. The source code and the
dataset will be available at
https://github.com/XuefengBUPT/IndoorObstacleDiscovery-RG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Journal of Computer Vision (IJCV) 2023. Project Page:
  https://xuefeng-cvr.github.io/IODRG</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Off-Road LiDAR Intensity Based Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kasi Viswanath, Peng Jiang, Sujit PB, Srikanth Saripalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR is used in autonomous driving to provide 3D spatial information and
enable accurate perception in off-road environments, aiding in obstacle
detection, mapping, and path planning. Learning-based LiDAR semantic
segmentation utilizes machine learning techniques to automatically classify
objects and regions in LiDAR point clouds. Learning-based models struggle in
off-road environments due to the presence of diverse objects with varying
colors, textures, and undefined boundaries, which can lead to difficulties in
accurately classifying and segmenting objects using traditional geometric-based
features. In this paper, we address this problem by harnessing the LiDAR
intensity parameter to enhance object segmentation in off-road environments.
Our approach was evaluated in the RELLIS-3D data set and yielded promising
results as a preliminary analysis with improved mIoU for classes "puddle" and
"grass" compared to more complex deep learning-based benchmarks. The
methodology was evaluated for compatibility across both Velodyne and Ouster
LiDAR systems, assuring its cross-platform applicability. This analysis
advocates for the incorporation of calibrated intensity as a supplementary
input, aiming to enhance the prediction accuracy of learning based semantic
segmentation frameworks.
https://github.com/MOONLABIISERB/lidar-intensity-predictor/tree/main
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ISER 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SwapTransformer: highway overtaking tactical planner model via imitation
  learning on OSHA <span class="highlight-title">dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Shamsoshoara, Safin B Salih, Pedram Aghazadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the high-level decision-making problem in highway
scenarios regarding lane changing and over-taking other slower vehicles. In
particular, this paper aims to improve the Travel Assist feature for automatic
overtaking and lane changes on highways. About 9 million samples including lane
images and other dynamic objects are collected in simulation. This data;
Overtaking on Simulated HighwAys (OSHA) dataset is released to tackle this
challenge. To solve this problem, an architecture called SwapTransformer is
designed and implemented as an imitation learning approach on the OSHA dataset.
Moreover, auxiliary tasks such as future points and car distance network
predictions are proposed to aid the model in better understanding the
surrounding environment. The performance of the proposed solution is compared
with a multi-layer perceptron (MLP) and multi-head self-attention networks as
baselines in a simulation environment. We also demonstrate the performance of
the model with and without auxiliary tasks. All models are evaluated based on
different metrics such as time to finish each lap, number of overtakes, and
speed difference with speed limit. The evaluation shows that the
SwapTransformer model outperforms other models in different traffic densities
in the inference phase.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 12 Figures, 1 Algorithm, 2 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Street Gaussians for Modeling Dynamic Urban Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunzhi Yan, Haotong Lin, Chenxu Zhou, Weijie Wang, Haiyang Sun, Kun Zhan, Xianpeng Lang, Xiaowei Zhou, Sida Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper aims to tackle the problem of modeling dynamic urban street scenes
from monocular videos. Recent methods extend NeRF by incorporating tracked
vehicle poses to animate vehicles, enabling photo-realistic view synthesis of
dynamic urban street scenes. However, significant limitations are their slow
training and rendering speed, coupled with the critical need for high precision
in tracked vehicle poses. We introduce Street Gaussians, a new explicit scene
representation that tackles all these limitations. Specifically, the dynamic
urban street is represented as a set of point clouds equipped with semantic
logits and 3D Gaussians, each associated with either a foreground vehicle or
the background. To model the dynamics of foreground object vehicles, each
object point cloud is optimized with optimizable tracked poses, along with a
dynamic spherical harmonics model for the dynamic appearance. The explicit
representation allows easy composition of object vehicles and background, which
in turn allows for scene editing operations and rendering at 133 FPS
(1066$\times$1600 resolution) within half an hour of training. The proposed
method is evaluated on multiple challenging benchmarks, including KITTI and
Waymo Open datasets. Experiments show that the proposed method consistently
outperforms state-of-the-art methods across all datasets. Furthermore, the
proposed representation delivers performance on par with that achieved using
precise ground-truth poses, despite relying only on poses from an off-the-shelf
tracker. The code is available at https://zju3dv.github.io/street_gaussians/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://zju3dv.github.io/street_gaussians/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image Sculpting: Precise Object Editing with 3D Geometry Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiraphon Yenphraphai, Xichen Pan, Sainan Liu, Daniele Panozzo, Saining Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Image Sculpting, a new framework for editing 2D images by
incorporating tools from 3D geometry and graphics. This approach differs
markedly from existing methods, which are confined to 2D spaces and typically
rely on textual instructions, leading to ambiguity and limited control. Image
Sculpting converts 2D objects into 3D, enabling direct interaction with their
3D geometry. Post-editing, these objects are re-rendered into 2D, merging into
the original image to produce high-fidelity results through a coarse-to-fine
enhancement process. The framework supports precise, quantifiable, and
physically-plausible editing options such as pose editing, rotation,
translation, 3D composition, carving, and serial addition. It marks an initial
step towards combining the creative freedom of generative models with the
precision of graphics pipelines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and project page: https://image-sculpting.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep autoregressive modeling for land use land cover 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Krapu, Mark Borsuk, Ryan Calder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Land use / land cover (LULC) modeling is a challenging task due to long-range
dependencies between geographic features and distinct spatial patterns related
to topography, ecology, and human development. We identify a close connection
between modeling of spatial patterns of land use and the task of image
inpainting from computer vision and conduct a study of a modified PixelCNN
architecture with approximately 19 million parameters for modeling LULC. In
comparison with a benchmark spatial statistical model, we find that the former
is capable of capturing much richer spatial correlation patterns such as roads
and water bodies but does not produce a calibrated predictive distribution,
suggesting the need for additional tuning. We find evidence of predictive
underdispersion with regard to important ecologically-relevant land use
statistics such as patch count and adjacency which can be ameliorated to some
extent by manipulating sampling variability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrating Edges into U-Net Models with Explainable Activation Maps for
  Brain Tumor Segmentation using MR Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01303v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01303v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subin Sahayam, Umarani Jayaraman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manual delineation of tumor regions from magnetic resonance (MR) images is
time-consuming, requires an expert, and is prone to human error. In recent
years, deep learning models have been the go-to approach for the segmentation
of brain tumors. U-Net and its' variants for semantic segmentation of medical
images have achieved good results in the literature. However, U-Net and its'
variants tend to over-segment tumor regions and may not accurately segment the
tumor edges. The edges of the tumor are as important as the tumor regions for
accurate diagnosis, surgical precision, and treatment planning. In the proposed
work, the authors aim to extract edges from the ground truth using a
derivative-like filter followed by edge reconstruction to obtain an edge ground
truth in addition to the brain tumor ground truth. Utilizing both ground
truths, the author studies several U-Net and its' variant architectures with
and without tumor edges ground truth as a target along with the tumor ground
truth for brain tumor segmentation. The author used the BraTS2020 benchmark
dataset to perform the study and the results are tabulated for the dice and
Hausdorff95 metrics. The mean and median metrics are calculated for the whole
tumor (WT), tumor core (TC), and enhancing tumor (ET) regions. Compared to the
baseline U-Net and its variants, the models that learned edges along with the
tumor regions performed well in core tumor regions in both training and
validation datasets. The improved performance of edge-trained models trained on
baseline models like U-Net and V-Net achieved performance similar to baseline
state-of-the-art models like Swin U-Net and hybrid MR-U-Net. The edge-target
trained models are capable of generating edge maps that can be useful for
treatment planning. Additionally, for further explainability of the results,
the activation map generated by the hybrid MR-U-Net has been studied.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physics-informed Generalizable Wireless Channel Modeling with
  Segmentation and Deep Learning: Fundamentals, Methodologies, and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01288v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01288v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Zhu, Haijian Sun, Mingyue Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Channel modeling is fundamental in advancing wireless systems and has thus
attracted considerable research focus. Recent trends have seen a growing
reliance on data-driven techniques to facilitate the modeling process and yield
accurate channel predictions. In this work, we first provide a concise overview
of data-driven channel modeling methods, highlighting their limitations.
Subsequently, we introduce the concept and advantages of physics-informed
neural network (PINN)-based modeling and a summary of recent contributions in
this area. Our findings demonstrate that PINN-based approaches in channel
modeling exhibit promising attributes such as generalizability,
interpretability, and robustness. We offer a comprehensive architecture for
PINN methodology, designed to inform and inspire future model development. A
case-study of our recent work on precise indoor channel prediction with
semantic segmentation and deep learning is presented. The study concludes by
addressing the challenges faced and suggesting potential research directions in
this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Magazine for potential future publications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive Study of Knowledge Editing for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01286v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01286v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ningyu Zhang, Yunzhi Yao, Bozhong Tian, Peng Wang, Shumin Deng, Mengru Wang, Zekun Xi, Shengyu Mao, Jintian Zhang, Yuansheng Ni, Siyuan Cheng, Ziwen Xu, Xin Xu, Jia-Chen Gu, Yong Jiang, Pengjun Xie, Fei Huang, Lei Liang, Zhiqiang Zhang, Xiaowei Zhu, Jun Zhou, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown extraordinary capabilities in
understanding and generating text that closely mirrors human communication.
However, a primary limitation lies in the significant computational demands
during training, arising from their extensive parameterization. This challenge
is further intensified by the dynamic nature of the world, necessitating
frequent updates to LLMs to correct outdated information or integrate new
knowledge, thereby ensuring their continued relevance. Note that many
applications demand continual model adjustments post-training to address
deficiencies or undesirable behaviors. There is an increasing interest in
efficient, lightweight methods for on-the-fly model modifications. To this end,
recent years have seen a burgeoning in the techniques of knowledge editing for
LLMs, which aim to efficiently modify LLMs' behaviors within specific domains
while preserving overall performance across various inputs. In this paper, we
first define the knowledge editing problem and then provide a comprehensive
review of cutting-edge approaches. Drawing inspiration from educational and
cognitive research theories, we propose a unified categorization criterion that
classifies knowledge editing methods into three groups: resorting to external
knowledge, merging knowledge into the model, and editing intrinsic knowledge.
Furthermore, we introduce a new benchmark, KnowEdit, for a comprehensive
empirical evaluation of representative knowledge editing approaches.
Additionally, we provide an in-depth analysis of knowledge location, which can
provide a deeper understanding of the knowledge structures inherent within
LLMs. Finally, we discuss several potential applications of knowledge editing,
outlining its broad and impactful implications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Ongoing work; 50 pages, 265 citations; benchmark is available at
  https://huggingface.co/datasets/zjunlp/KnowEdit; code is available at
  https://github.com/zjunlp/EasyEdit; paper list is available at
  https://github.com/zjunlp/KnowledgeEditingPapers</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MOC-RVQ: Multilevel Codebook-assisted Digital Generative Semantic
  Communication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01272v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01272v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingbin Zhou, Yaping Sun, Guanying Chen, Xiaodong Xu, Hao Chen, Binhong Huang, Shuguang Cui, Ping Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vector quantization-based image semantic communication systems have
successfully boosted transmission efficiency, but face a challenge with
conflicting requirements between codebook design and digital constellation
modulation. Traditional codebooks need a wide index range, while modulation
favors few discrete states. To address this, we propose a multilevel generative
semantic communication system with a two-stage training framework. In the first
stage, we train a high-quality codebook, using a multi-head octonary codebook
(MOC) to compress the index range. We also integrate a residual vector
quantization (RVQ) mechanism for effective multilevel communication. In the
second stage, a noise reduction block (NRB) based on Swin Transformer is
introduced, coupled with the multilevel codebook from the first stage, serving
as a high-quality semantic knowledge base (SKB) for generative feature
restoration. Experimental results highlight MOC-RVQ's superior performance over
methods like BPG or JPEG, even without channel error correction coding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VideoDrafter: Content-Consistent Multi-Scene Video Generation with LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuchen Long, Zhaofan Qiu, Ting Yao, Tao Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent innovations and breakthroughs in diffusion models have
significantly expanded the possibilities of generating high-quality videos for
the given prompts. Most existing works tackle the single-scene scenario with
only one video event occurring in a single background. Extending to generate
multi-scene videos nevertheless is not trivial and necessitates to nicely
manage the logic in between while preserving the consistent visual appearance
of key content across video scenes. In this paper, we propose a novel
framework, namely VideoDrafter, for content-consistent multi-scene video
generation. Technically, VideoDrafter leverages Large Language Models (LLM) to
convert the input prompt into comprehensive multi-scene script that benefits
from the logical knowledge learnt by LLM. The script for each scene includes a
prompt describing the event, the foreground/background entities, as well as
camera movement. VideoDrafter identifies the common entities throughout the
script and asks LLM to detail each entity. The resultant entity description is
then fed into a text-to-image model to generate a reference image for each
entity. Finally, VideoDrafter outputs a multi-scene video by generating each
scene video via a diffusion process that takes the reference images, the
descriptive prompt of the event and camera movement into account. The diffusion
model incorporates the reference images as the condition and alignment to
strengthen the content consistency of multi-scene videos. Extensive experiments
demonstrate that VideoDrafter outperforms the SOTA video generation models in
terms of visual quality, content consistency, and user preference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://videodrafter.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning-Based Computational Model for Disease Identification in
  Cocoa Pods (Theobroma cacao L.) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Darlyn Buenaño Vera, Byron Oviedo, Washington Chiriboga Casanova, Cristian Zambrano-Vega
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The early identification of diseases in cocoa pods is an important task to
guarantee the production of high-quality cocoa. The use of artificial
intelligence techniques such as machine learning, computer vision and deep
learning are promising solutions to help identify and classify diseases in
cocoa pods. In this paper we introduce the development and evaluation of a deep
learning computational model applied to the identification of diseases in cocoa
pods, focusing on "monilia" and "black pod" diseases. An exhaustive review of
state-of-the-art of computational models was carried out, based on scientific
articles related to the identification of plant diseases using computer vision
and deep learning techniques. As a result of the search, EfficientDet-Lite4, an
efficient and lightweight model for object detection, was selected. A dataset,
including images of both healthy and diseased cocoa pods, has been utilized to
train the model to detect and pinpoint disease manifestations with considerable
accuracy. Significant enhancements in the model training and evaluation
demonstrate the capability of recognizing and classifying diseases through
image analysis. Furthermore, the functionalities of the model were integrated
into an Android native mobile with an user-friendly interface, allowing to
younger or inexperienced farmers a fast and accuracy identification of health
status of cocoa pods
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal Adaptive RGBT Tracking with Modality Prompt 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01244v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01244v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyu Wang, Xiaotao Liu, Yifan Li, Meng Sun, Dian Yuan, Jing Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  RGBT tracking has been widely used in various fields such as robotics,
surveillance processing, and autonomous driving. Existing RGBT trackers fully
explore the spatial information between the template and the search region and
locate the target based on the appearance matching results. However, these RGBT
trackers have very limited exploitation of temporal information, either
ignoring temporal information or exploiting it through online sampling and
training. The former struggles to cope with the object state changes, while the
latter neglects the correlation between spatial and temporal information. To
alleviate these limitations, we propose a novel Temporal Adaptive RGBT Tracking
framework, named as TATrack. TATrack has a spatio-temporal two-stream structure
and captures temporal information by an online updated template, where the
two-stream structure refers to the multi-modal feature extraction and
cross-modal interaction for the initial template and the online update template
respectively. TATrack contributes to comprehensively exploit spatio-temporal
information and multi-modal information for target localization. In addition,
we design a spatio-temporal interaction (STI) mechanism that bridges two
branches and enables cross-modal interaction to span longer time scales.
Extensive experiments on three popular RGBT tracking benchmarks show that our
method achieves state-of-the-art performance, while running at real-time speed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IdentiFace : A VGG Based Multimodal Facial Biometric System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01227v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01227v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahmoud Rabea, Hanya Ahmed, Sohaila Mahmoud, Nourhan Sayed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of facial biometric systems has contributed greatly to the
development of the computer vision field. Nowadays, there's always a need to
develop a multimodal system that combines multiple biometric traits in an
efficient, meaningful way. In this paper, we introduce "IdentiFace" which is a
multimodal facial biometric system that combines the core of facial recognition
with some of the most important soft biometric traits such as gender, face
shape, and emotion. We also focused on developing the system using only VGG-16
inspired architecture with minor changes across different subsystems. This
unification allows for simpler integration across modalities. It makes it
easier to interpret the learned features between the tasks which gives a good
indication about the decision-making process across the facial modalities and
potential connection. For the recognition problem, we acquired a 99.2% test
accuracy for five classes with high intra-class variations using data collected
from the FERET database[1]. We achieved 99.4% on our dataset and 95.15% on the
public dataset[2] in the gender recognition problem. We were also able to
achieve a testing accuracy of 88.03% in the face-shape problem using the
celebrity face-shape dataset[3]. Finally, we achieved a decent testing accuracy
of 66.13% in the emotion task which is considered a very acceptable accuracy
compared to related work on the FER2013 dataset[4].
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 22 figures and 9 images</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Noise-NeRF: Hide Information in Neural Radiance Fields using Trainable
  Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01216v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01216v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinglong Huang, Yong Liao, Yanbin Hao, Pengyuan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural radiance fields (NeRF) have been proposed as an innovative 3D
representation method. While attracting lots of attention, NeRF faces critical
issues such as information confidentiality and security. Steganography is a
technique used to embed information in another object as a means of protecting
information security. Currently, there are few related studies on NeRF
steganography, facing challenges in low steganography quality, model weight
damage, and a limited amount of steganographic information. This paper proposes
a novel NeRF steganography method based on trainable noise: Noise-NeRF.
Furthermore, we propose the Adaptive Pixel Selection strategy and Pixel
Perturbation strategy to improve the steganography quality and efficiency. The
extensive experiments on open-source datasets show that Noise-NeRF provides
state-of-the-art performances in both steganography quality and rendering
quality, as well as effectiveness in super-resolution image steganography.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ YOLO algorithm with hybrid attention feature pyramid network for solder
  joint defect detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Ang, Siti Khatijah Nor Abdul Rahim, Raseeda Hamzah, Raihah Aminuddin, Gao Yousheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional manual detection for solder joint defect is no longer applied
during industrial production due to low efficiency, inconsistent evaluation,
high cost and lack of real-time data. A new approach has been proposed to
address the issues of low accuracy, high false detection rates and
computational cost of solder joint defect detection in surface mount technology
of industrial scenarios. The proposed solution is a hybrid attention mechanism
designed specifically for the solder joint defect detection algorithm to
improve quality control in the manufacturing process by increasing the accuracy
while reducing the computational cost. The hybrid attention mechanism comprises
a proposed enhanced multi-head self-attention and coordinate attention
mechanisms increase the ability of attention networks to perceive contextual
information and enhances the utilization range of network features. The
coordinate attention mechanism enhances the connection between different
channels and reduces location information loss. The hybrid attention mechanism
enhances the capability of the network to perceive long-distance position
information and learn local features. The improved algorithm model has good
detection ability for solder joint defect detection, with mAP reaching 91.5%,
4.3% higher than the You Only Look Once version 5 algorithm and better than
other comparative algorithms. Compared to other versions, mean Average
Precision, Precision, Recall, and Frame per Seconds indicators have also
improved. The improvement of detection accuracy can be achieved while meeting
real-time detection requirements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FGENet: Fine-Grained Extraction Network for Congested Crowd Counting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao-Yuan Ma, Li Zhang, Xiang-Yi Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crowd counting has gained significant popularity due to its practical
applications. However, mainstream counting methods ignore precise individual
localization and suffer from annotation noise because of counting from
estimating density maps. Additionally, they also struggle with high-density
images.To address these issues, we propose an end-to-end model called
Fine-Grained Extraction Network (FGENet). Different from methods estimating
density maps, FGENet directly learns the original coordinate points that
represent the precise localization of individuals.This study designs a fusion
module, named Fine-Grained Feature Pyramid(FGFP), that is used to fuse feature
maps extracted by the backbone of FGENet. The fused features are then passed to
both regression and classification heads, where the former provides predicted
point coordinates for a given image, and the latter determines the confidence
level for each predicted point being an individual. At the end, FGENet
establishes correspondences between prediction points and ground truth points
by employing the Hungarian algorithm. For training FGENet, we design a robust
loss function, named Three-Task Combination (TTC), to mitigate the impact of
annotation noise. Extensive experiments are conducted on four widely used crowd
counting datasets. Experimental results demonstrate the effectiveness of
FGENet. Notably, our method achieves a remarkable improvement of 3.14 points in
Mean Absolute Error (MAE) on the ShanghaiTech Part A dataset, showcasing its
superiority over the existing state-of-the-art methods. Even more impressively,
FGENet surpasses previous benchmarks on the UCF\_CC\_50 dataset with an
astounding enhancement of 30.16 points in MAE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 30th International Conference on MultiMedia Modeling</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Simultaneous and Granular Identity-Expression Control in
  Personalized Face Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01207v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01207v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renshuai Liu, Bowen Ma, Wei Zhang, Zhipeng Hu, Changjie Fan, Tangjie Lv, Yu Ding, Xuan Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In human-centric content generation, the pre-trained text-to-image models
struggle to produce user-wanted portrait images, which retain the identity of
individuals while exhibiting diverse expressions. This paper introduces our
efforts towards personalized face generation. To this end, we propose a novel
multi-modal face generation framework, capable of simultaneous
identity-expression control and more fine-grained expression synthesis. Our
expression control is so sophisticated that it can be specialized by the
fine-grained emotional vocabulary. We devise a novel diffusion model that can
undertake the task of simultaneously face swapping and reenactment. Due to the
entanglement of identity and expression, it's nontrivial to separately and
precisely control them in one framework, thus has not been explored yet. To
overcome this, we propose several innovative designs in the conditional
diffusion model, including balancing identity and expression encoder, improved
midpoint sampling, and explicitly background conditioning. Extensive
experiments have demonstrated the controllability and scalability of the
proposed framework, in comparison with state-of-the-art text-to-image, face
swapping, and face reenactment methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Whole-examination AI estimation of fetal biometrics from 20-week
  ultrasound scans 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01201v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01201v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Venturini, Samuel Budd, Alfonso Farruggia, Robert Wright, Jacqueline Matthew, Thomas G. Day, Bernhard Kainz, Reza Razavi, Jo V. Hajnal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The current approach to fetal anomaly screening is based on biometric
measurements derived from individually selected ultrasound images. In this
paper, we introduce a paradigm shift that attains human-level performance in
biometric measurement by aggregating automatically extracted biometrics from
every frame across an entire scan, with no need for operator intervention. We
use a convolutional neural network to classify each frame of an ultrasound
video recording. We then measure fetal biometrics in every frame where
appropriate anatomy is visible. We use a Bayesian method to estimate the true
value of each biometric from a large number of measurements and
probabilistically reject outliers. We performed a retrospective experiment on
1457 recordings (comprising 48 million frames) of 20-week ultrasound scans,
estimated fetal biometrics in those scans and compared our estimates to the
measurements sonographers took during the scan. Our method achieves human-level
performance in estimating fetal biometrics and estimates well-calibrated
credible intervals in which the true biometric value is expected to lie.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 16 figures. Submitted to NPJ digital medicine. For
  associated video file, see
  http://wp.doc.ic.ac.uk/ifind/wp-content/uploads/sites/79/2023/12/realtime.gif</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Skin cancer diagnosis using NIR spectroscopy data of skin lesions in
  vivo using machine learning algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Flavio P. Loss, Pedro H. da Cunha, Matheus B. Rocha, Madson Poltronieri Zanoni, Leandro M. de Lima, Isadora Tavares Nascimento, Isabella Rezende, Tania R. P. Canuto, Luciana de Paula Vieira, Renan Rossoni, Maria C. S. Santos, Patricia Lyra Frasson, Wanderson Romão, Paulo R. Filgueiras, Renato A. Krohling
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skin lesions are classified in benign or malignant. Among the malignant,
melanoma is a very aggressive cancer and the major cause of deaths. So, early
diagnosis of skin cancer is very desired. In the last few years, there is a
growing interest in computer aided diagnostic (CAD) using most image and
clinical data of the lesion. These sources of information present limitations
due to their inability to provide information of the molecular structure of the
lesion. NIR spectroscopy may provide an alternative source of information to
automated CAD of skin lesions. The most commonly used techniques and
classification algorithms used in spectroscopy are Principal Component Analysis
(PCA), Partial Least Squares - Discriminant Analysis (PLS-DA), and Support
Vector Machines (SVM). Nonetheless, there is a growing interest in applying the
modern techniques of machine and deep learning (MDL) to spectroscopy. One of
the main limitations to apply MDL to spectroscopy is the lack of public
datasets. Since there is no public dataset of NIR spectral data to skin
lesions, as far as we know, an effort has been made and a new dataset named
NIR-SC-UFES, has been collected, annotated and analyzed generating the
gold-standard for classification of NIR spectral data to skin cancer. Next, the
machine learning algorithms XGBoost, CatBoost, LightGBM, 1D-convolutional
neural network (1D-CNN) were investigated to classify cancer and non-cancer
skin lesions. Experimental results indicate the best performance obtained by
LightGBM with pre-processing using standard normal variate (SNV), feature
extraction providing values of 0.839 for balanced accuracy, 0.851 for recall,
0.852 for precision, and 0.850 for F-score. The obtained results indicate the
first steps in CAD of skin lesions aiming the automated triage of patients with
skin lesions in vivo using NIR spectral data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JMA: a General Algorithm to Craft Nearly Optimal Targeted Adversarial
  Example 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benedetta Tondi, Wei Guo, Mauro Barni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most of the approaches proposed so far to craft targeted adversarial examples
against Deep Learning classifiers are highly suboptimal and typically rely on
increasing the likelihood of the target class, thus implicitly focusing on
one-hot encoding settings. In this paper, we propose a more general,
theoretically sound, targeted attack that resorts to the minimization of a
Jacobian-induced MAhalanobis distance (JMA) term, taking into account the
effort (in the input space) required to move the latent space representation of
the input sample in a given direction. The minimization is solved by exploiting
the Wolfe duality theorem, reducing the problem to the solution of a
Non-Negative Least Square (NNLS) problem. The proposed algorithm provides an
optimal solution to a linearized version of the adversarial example problem
originally introduced by Szegedy et al. \cite{szegedy2013intriguing}. The
experiments we carried out confirm the generality of the proposed attack which
is proven to be effective under a wide variety of output encoding schemes.
Noticeably, the JMA attack is also effective in a multi-label classification
scenario, being capable to induce a targeted modification of up to half the
labels in a complex multilabel classification scenario with 20 labels, a
capability that is out of reach of all the attacks proposed so far. As a
further advantage, the JMA attack usually requires very few iterations, thus
resulting more efficient than existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Query-Based Knowledge Sharing for Open-Vocabulary Multi-Label
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuelin Zhu, Jian Liu, Dongqi Tang, Jiawei Ge, Weijia Liu, Bo Liu, Jiuxin Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying labels that did not appear during training, known as multi-label
zero-shot learning, is a non-trivial task in computer vision. To this end,
recent studies have attempted to explore the multi-modal knowledge of
vision-language pre-training (VLP) models by knowledge distillation, allowing
to recognize unseen labels in an open-vocabulary manner. However, experimental
evidence shows that knowledge distillation is suboptimal and provides limited
performance gain in unseen label prediction. In this paper, a novel query-based
knowledge sharing paradigm is proposed to explore the multi-modal knowledge
from the pretrained VLP model for open-vocabulary multi-label classification.
Specifically, a set of learnable label-agnostic query tokens is trained to
extract critical vision knowledge from the input image, and further shared
across all labels, allowing them to select tokens of interest as visual clues
for recognition. Besides, we propose an effective prompt pool for robust label
embedding, and reformulate the standard ranking learning into a form of
classification to allow the magnitude of feature vectors for matching, which
both significantly benefit label recognition. Experimental results show that
our framework significantly outperforms state-of-the-art methods on zero-shot
task by 5.9% and 4.5% in mAP on the NUS-WIDE and Open Images, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accurate and Efficient Urban Street Tree Inventory with Deep Learning on
  Mobile Phone Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01180v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01180v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Asim Khan, Umair Nawaz, Anwaar Ulhaq, Iqbal Gondal, Sajid Javed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deforestation, a major contributor to climate change, poses detrimental
consequences such as agricultural sector disruption, global warming, flash
floods, and landslides. Conventional approaches to urban street tree inventory
suffer from inaccuracies and necessitate specialised equipment. To overcome
these challenges, this paper proposes an innovative method that leverages deep
learning techniques and mobile phone imaging for urban street tree inventory.
Our approach utilises a pair of images captured by smartphone cameras to
accurately segment tree trunks and compute the diameter at breast height (DBH).
Compared to traditional methods, our approach exhibits several advantages,
including superior accuracy, reduced dependency on specialised equipment, and
applicability in hard-to-reach areas. We evaluated our method on a
comprehensive dataset of 400 trees and achieved a DBH estimation accuracy with
an error rate of less than 2.5%. Our method holds significant potential for
substantially improving forest management practices. By enhancing the accuracy
and efficiency of tree inventory, our model empowers urban management to
mitigate the adverse effects of deforestation and climate change.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 Pages, 7 figures and 5 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Freeze the backbones: A Parameter-Efficient Contrastive Approach to
  Robust Medical Vision-Language Pre-training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiuming Qin, Che Liu, Sibo Cheng, Yike Guo, Rossella Arcucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern healthcare often utilises radiographic images alongside textual
reports for diagnostics, encouraging the use of Vision-Language Self-Supervised
Learning (VL-SSL) with large pre-trained models to learn versatile medical
vision representations. However, most existing VL-SSL frameworks are trained
end-to-end, which is computation-heavy and can lose vital prior information
embedded in pre-trained encoders. To address both issues, we introduce the
backbone-agnostic Adaptor framework, which preserves medical knowledge in
pre-trained image and text encoders by keeping them frozen, and employs a
lightweight Adaptor module for cross-modal learning. Experiments on medical
image classification and segmentation tasks across three datasets reveal that
our framework delivers competitive performance while cutting trainable
parameters by over 90% compared to current pre-training approaches. Notably,
when fine-tuned with just 1% of data, Adaptor outperforms several
Transformer-based methods trained on full datasets in medical image
segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GBSS:a global building semantic segmentation <span class="highlight-title">dataset</span> for large-scale
  remote sensing building extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuping Hu, Xin Huang, Jiayi Li, Zhen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation techniques for extracting building footprints from
high-resolution remote sensing images have been widely used in many fields such
as urban planning. However, large-scale building extraction demands higher
diversity in training samples. In this paper, we construct a Global Building
Semantic Segmentation (GBSS) dataset (The dataset will be released), which
comprises 116.9k pairs of samples (about 742k buildings) from six continents.
There are significant variations of building samples in terms of size and
style, so the dataset can be a more challenging benchmark for evaluating the
generalization and robustness of building semantic segmentation models. We
validated through quantitative and qualitative comparisons between different
datasets, and further confirmed the potential application in the field of
transfer learning by conducting experiments on subsets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages,6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Surface Scattering Parameters From SAR Images Using
  Differentiable Ray Tracing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01175v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01175v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiangtao Wei, Yixiang Luomei, Xu Zhang, Feng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulating high-resolution Synthetic Aperture Radar (SAR) images in complex
scenes has consistently presented a significant research challenge. The
development of a microwave-domain surface scattering model and its
reversibility are poised to play a pivotal role in enhancing the authenticity
of SAR image simulations and facilitating the reconstruction of target
parameters. Drawing inspiration from the field of computer graphics, this paper
proposes a surface microwave rendering model that comprehensively considers
both Specular and Diffuse contributions. The model is analytically represented
by the coherent spatially varying bidirectional scattering distribution
function (CSVBSDF) based on the Kirchhoff approximation (KA) and the
perturbation method (SPM). And SAR imaging is achieved through the synergistic
combination of ray tracing and fast mapping projection techniques. Furthermore,
a differentiable ray tracing (DRT) engine based on SAR images was constructed
for CSVBSDF surface scattering parameter learning. Within this SAR image
simulation engine, the use of differentiable reverse ray tracing enables the
rapid estimation of parameter gradients from SAR images. The effectiveness of
this approach has been validated through simulations and comparisons with real
SAR images. By learning the surface scattering parameters, substantial
enhancements in SAR image simulation performance under various observation
conditions have been demonstrated.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ En3D: An Enhanced Generative Model for Sculpting 3D Humans from 2D
  Synthetic Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01173v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01173v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifang Men, Biwen Lei, Yuan Yao, Miaomiao Cui, Zhouhui Lian, Xuansong Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present En3D, an enhanced generative scheme for sculpting high-quality 3D
human avatars. Unlike previous works that rely on scarce 3D datasets or limited
2D collections with imbalanced viewing angles and imprecise pose priors, our
approach aims to develop a zero-shot 3D generative scheme capable of producing
visually realistic, geometrically accurate and content-wise diverse 3D humans
without relying on pre-existing 3D or 2D assets. To address this challenge, we
introduce a meticulously crafted workflow that implements accurate physical
modeling to learn the enhanced 3D generative model from synthetic 2D data.
During inference, we integrate optimization modules to bridge the gap between
realistic appearances and coarse 3D shapes. Specifically, En3D comprises three
modules: a 3D generator that accurately models generalizable 3D humans with
realistic appearance from synthesized balanced, diverse, and structured human
images; a geometry sculptor that enhances shape quality using multi-view normal
constraints for intricate human anatomy; and a texturing module that
disentangles explicit texture maps with fidelity and editability, leveraging
semantical UV partitioning and a differentiable rasterizer. Experimental
results show that our approach significantly outperforms prior works in terms
of image quality, geometry accuracy and content diversity. We also showcase the
applicability of our generated avatars for animation and editing, as well as
the scalability of our approach for content-style free adaptation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://menyifang.github.io/projects/En3D/index.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large-scale data extraction from the UNOS organ donor documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15752v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15752v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marek Rychlik, Bekir Tanriover, Yan Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scope of our study is all UNOS data of the USA organ donors since 2008.
The data is not analyzable in a large scale in the past because it was captured
in PDF documents known as "Attachments", whereby every donor is represented by
dozens of PDF documents in heterogenous formats. To make the data analyzable,
one needs to convert the content inside these PDFs to an analyzable data
format, such as a standard SQL database. In this paper we will focus on 2022
UNOS data comprised of $\approx 400,000$ PDF documents spanning millions of
pages. The totality of UNOS data covers 15 years (2008--20022) and our results
will be quickly extended to the entire data. Our method captures a portion of
the data in DCD flowsheets, kidney perfusion data, and data captured during
patient hospital stay (e.g. vital signs, ventilator settings, etc.). The
current paper assumes that the reader is familiar with the content of the UNOS
data. The overview of the types of data and challenges they present is a
subject of another paper. Here we focus on demonstrating that the goal of
building a comprehensive, analyzable database from UNOS documents is an
attainable task, and we provide an overview of our methodology. The project
resulted in datasets by far larger than previously available even in this
preliminary phase.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Test Statistic Estimation-based Approach for Establishing
  Self-interpretable CNN-based Binary Classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06876v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06876v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sourya Sengupta, Mark A. Anastasio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interpretability is highly desired for deep neural network-based classifiers,
especially when addressing high-stake decisions in medical imaging. Commonly
used post-hoc interpretability methods have the limitation that they can
produce plausible but different interpretations of a given model, leading to
ambiguity about which one to choose. To address this problem, a novel
decision-theory-inspired approach is investigated to establish a
self-interpretable model, given a pre-trained deep binary black-box medical
image classifier. This approach involves utilizing a self-interpretable
encoder-decoder model in conjunction with a single-layer fully connected
network with unity weights. The model is trained to estimate the test statistic
of the given trained black-box deep binary classifier to maintain a similar
accuracy. The decoder output image, referred to as an equivalency map, is an
image that represents a transformed version of the to-be-classified image that,
when processed by the fixed fully connected layer, produces the same test
statistic value as the original classifier. The equivalency map provides a
visualization of the transformed image features that directly contribute to the
test statistic value and, moreover, permits quantification of their relative
contributions. Unlike the traditional post-hoc interpretability methods, the
proposed method is self-interpretable, quantitative. Detailed quantitative and
qualitative analyses have been performed with three different medical image
binary classification tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.05134v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.05134v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Grimal, Hervé Le Borgne, Olivier Ferret, Julien Tourille
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The progress in the generation of synthetic images has made it crucial to
assess their quality. While several metrics have been proposed to assess the
rendering of images, it is crucial for Text-to-Image (T2I) models, which
generate images based on a prompt, to consider additional aspects such as to
which extent the generated image matches the important content of the prompt.
Moreover, although the generated images usually result from a random starting
point, the influence of this one is generally not considered. In this article,
we propose a new metric based on prompt templates to study the alignment
between the content specified in the prompt and the corresponding generated
images. It allows us to better characterize the alignment in terms of the type
of the specified objects, their number, and their color. We conducted a study
on several recent T2I models about various aspects. An additional interesting
result we obtained with our approach is that image quality can vary drastically
depending on the noise used as a seed for the images. We also quantify the
influence of the number of concepts in the prompt, their order as well as their
(color) attributes. Finally, our method allows us to identify some seeds that
produce better images than others, opening novel directions of research on this
understudied topic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Consensus and Subjectivity of Skin Tone Annotation for ML Fairness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.09073v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.09073v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Candice Schumann, Gbolahan O. Olanubi, Auriel Wright, Ellis Monk Jr., Courtney Heldreth, Susanna Ricco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding different human attributes and how they affect model behavior
may become a standard need for all model creation and usage, from traditional
computer vision tasks to the newest multimodal generative AI systems. In
computer vision specifically, we have relied on datasets augmented with
perceived attribute signals (e.g., gender presentation, skin tone, and age) and
benchmarks enabled by these datasets. Typically labels for these tasks come
from human annotators. However, annotating attribute signals, especially skin
tone, is a difficult and subjective task. Perceived skin tone is affected by
technical factors, like lighting conditions, and social factors that shape an
annotator's lived experience. This paper examines the subjectivity of skin tone
annotation through a series of annotation experiments using the Monk Skin Tone
(MST) scale, a small pool of professional photographers, and a much larger pool
of trained crowdsourced annotators. Along with this study we release the Monk
Skin Tone Examples (MST-E) dataset, containing 1515 images and 31 videos spread
across the full MST scale. MST-E is designed to help train human annotators to
annotate MST effectively. Our study shows that annotators can reliably annotate
skin tone in a way that aligns with an expert in the MST scale, even under
challenging environmental conditions. We also find evidence that annotators
from different geographic regions rely on different mental models of MST
categories resulting in annotations that systematically vary across regions.
Given this, we advise practitioners to use a diverse set of annotators and a
higher replication count for each image when annotating skin tone for fairness
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-Age Contrastive Learning for Age-Invariant Face Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11195v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11195v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyi Wang, Victor Sanchez, Chang-Tsun Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-age facial images are typically challenging and expensive to collect,
making noise-free age-oriented datasets relatively small compared to
widely-used large-scale facial datasets. Additionally, in real scenarios,
images of the same subject at different ages are usually hard or even
impossible to obtain. Both of these factors lead to a lack of supervised data,
which limits the versatility of supervised methods for age-invariant face
recognition, a critical task in applications such as security and biometrics.
To address this issue, we propose a novel semi-supervised learning approach
named Cross-Age Contrastive Learning (CACon). Thanks to the identity-preserving
power of recent face synthesis models, CACon introduces a new contrastive
learning method that leverages an additional synthesized sample from the input
image. We also propose a new loss function in association with CACon to perform
contrastive learning on a triplet of samples. We demonstrate that our method
not only achieves state-of-the-art performance in homogeneous-dataset
experiments on several age-invariant face recognition benchmarks but also
outperforms other methods by a large margin in cross-dataset experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICASSP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KeDuSR: Real-World Dual-Lens Super-Resolution via Kernel-Free Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17050v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17050v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huanjing Yue, Zifan Cui, Kun Li, Jingyu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dual-lens super-resolution (SR) is a practical scenario for reference (Ref)
based SR by utilizing the telephoto image (Ref) to assist the super-resolution
of the low-resolution wide-angle image (LR input). Different from general
RefSR, the Ref in dual-lens SR only covers the overlapped field of view (FoV)
area. However, current dual-lens SR methods rarely utilize these specific
characteristics and directly perform dense matching between the LR input and
Ref. Due to the resolution gap between LR and Ref, the matching may miss the
best-matched candidate and destroy the consistent structures in the overlapped
FoV area. Different from them, we propose to first align the Ref with the
center region (namely the overlapped FoV area) of the LR input by combining
global warping and local warping to make the aligned Ref be sharp and
consistent. Then, we formulate the aligned Ref and LR center as value-key
pairs, and the corner region of the LR is formulated as queries. In this way,
we propose a kernel-free matching strategy by matching between the LR-corner
(query) and LR-center (key) regions, and the corresponding aligned Ref (value)
can be warped to the corner region of the target. Our kernel-free matching
strategy avoids the resolution gap between LR and Ref, which makes our network
have better generalization ability. In addition, we construct a DuSR-Real
dataset with (LR, Ref, HR) triples, where the LR and HR are well aligned.
Experiments on three datasets demonstrate that our method outperforms the
second-best method by a large margin. Our code and dataset are available at
https://github.com/ZifanCui/KeDuSR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 10 figures. Accepted by AAAI-2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tuned Compositional Feature Replays for Efficient Stream Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.02206v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.02206v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Morgan B. Talbot, Rushikesh Zawar, Rohil Badkundri, Mengmi Zhang, Gabriel Kreiman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Our brains extract durable, generalizable knowledge from transient
experiences of the world. Artificial neural networks come nowhere close to this
ability. When tasked with learning to classify objects by training on
non-repeating video frames in temporal order (online stream learning), models
that learn well from shuffled datasets catastrophically forget old knowledge
upon learning new stimuli. We propose a new continual learning algorithm,
Compositional Replay Using Memory Blocks (CRUMB), which mitigates forgetting by
replaying feature maps reconstructed by combining generic parts. CRUMB
concatenates trainable and re-usable "memory block" vectors to compositionally
reconstruct feature map tensors in convolutional neural networks. Storing the
indices of memory blocks used to reconstruct new stimuli enables memories of
the stimuli to be replayed during later tasks. This reconstruction mechanism
also primes the neural network to minimize catastrophic forgetting by biasing
it towards attending to information about object shapes more than information
about image textures, and stabilizes the network during stream learning by
providing a shared feature-level basis for all training examples. These
properties allow CRUMB to outperform an otherwise identical algorithm that
stores and replays raw images, while occupying only 3.6% as much memory. We
stress-tested CRUMB alongside 13 competing methods on 7 challenging datasets.
To address the limited number of existing online stream learning datasets, we
introduce 2 new benchmarks by adapting existing datasets for stream learning.
With only 3.7-4.1% as much memory and 15-43% as much runtime, CRUMB mitigates
catastrophic forgetting more effectively than the state-of-the-art. Our code is
available at https://github.com/MorganBDT/crumb.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Copyright 2023 IEEE. The journal version of this article is hosted at
  https://ieeexplore.ieee.org/document/10373937 and
  https://klab.tch.harvard.edu/publications/PDFs/gk8019.pdf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Recovering 3D Human Mesh from Monocular Images: A Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.01923v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.01923v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yating Tian, Hongwen Zhang, Yebin Liu, Limin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating human pose and shape from monocular images is a long-standing
problem in computer vision. Since the release of statistical body models, 3D
human mesh recovery has been drawing broader attention. With the same goal of
obtaining well-aligned and physically plausible mesh results, two paradigms
have been developed to overcome challenges in the 2D-to-3D lifting process: i)
an optimization-based paradigm, where different data terms and regularization
terms are exploited as optimization objectives; and ii) a regression-based
paradigm, where deep learning techniques are embraced to solve the problem in
an end-to-end fashion. Meanwhile, continuous efforts are devoted to improving
the quality of 3D mesh labels for a wide range of datasets. Though remarkable
progress has been achieved in the past decade, the task is still challenging
due to flexible body motions, diverse appearances, complex environments, and
insufficient in-the-wild annotations. To the best of our knowledge, this is the
first survey that focuses on the task of monocular 3D human mesh recovery. We
start with the introduction of body models and then elaborate recovery
frameworks and training objectives by providing in-depth analyses of their
strengths and weaknesses. We also summarize datasets, evaluation metrics, and
benchmark results. Open issues and future directions are discussed in the end,
hoping to motivate researchers and facilitate their research in this area. A
regularly updated project page can be found at
https://github.com/tinatiansjz/hmr-survey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in IEEE TPAMI, Survey on monocular 3D human mesh recovery,
  Project page: https://github.com/tinatiansjz/hmr-survey</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ArtGPT-4: Towards Artistic-understanding Large Vision-Language Models
  with Enhanced Adapter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.07490v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.07490v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengqing Yuan, Xinyi Wang, Kun Wang, Lichao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, advancements in large language models have been remarkable,
with models such as ChatGPT demonstrating exceptional proficiency in diverse
linguistic tasks. The pre-training of large models with billions of parameters,
poses a formidable challenge, primarily due to the scarcity of datasets of a
commensurate scale for effective training. Nevertheless, innovative strategies
have emerged, including methods to fine-tune these pre-trained models using
fewer parameters set, as evidenced by models like MiniGPT-4 and LLaVA. Despite
their potential in various domains, these models remain limited in their
understanding of artistic imagery. They have yet to fully grasp the intricate
nuances of art images or to provide an objective articulation of the emotions
they evoke, in a manner akin to human perception. This work introduces
ArtGPT-4, a pioneering large vision-language model tailored to address the
deficiencies of contemporary models in artistic comprehension. ArtGPT-4
underwent training on image-text pairs utilizing a Tesla A100 device in a mere
2 hours, with a dataset comprising approximately 0.52M entries. Impressively,
the model can render images with an artistic-understanding and convey the
emotions they inspire, mirroring human interpretation. Additionally, this work
presents a unique dataset designed to evaluate the efficacy of vision-language
models. In subsequent evaluations, ArtGPT-4 not only achieved state-of-the-art
performance on the ArtEmis and ArtEmis-v2.0 datasets but also exceeded the
established benchmarks introduced in This study, lagging behind professional
artists' descriptions by a negligible 0.15 points on a 6-point scale. The code
and the pre-trained model are accessible in
https://huggingface.co/Tyrannosaurus/ArtGPT-4.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-Efficient Multimodal Fusion on a Single GPU 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10144v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10144v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noël Vouitsis, Zhaoyan Liu, Satya Krishna Gorti, Valentin Villecroze, Jesse C. Cresswell, Guangwei Yu, Gabriel Loaiza-Ganem, Maksims Volkovs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of multimodal alignment is to learn a single latent space that is
shared between multimodal inputs. The most powerful models in this space have
been trained using massive datasets of paired inputs and large-scale
computational resources, making them prohibitively expensive to train in many
practical scenarios. We surmise that existing unimodal encoders pre-trained on
large amounts of unimodal data should provide an effective bootstrap to create
multimodal models from unimodal ones at much lower costs. We therefore propose
FuseMix, a multimodal augmentation scheme that operates on the latent spaces of
arbitrary pre-trained unimodal encoders. Using FuseMix for multimodal
alignment, we achieve competitive performance -- and in certain cases
outperform state-of-the art methods -- in both image-text and audio-text
retrieval, with orders of magnitude less compute and data: for example, we
outperform CLIP on the Flickr30K text-to-image retrieval task with $\sim \!
600\times$ fewer GPU days and $\sim \! 80\times$ fewer image-text pairs.
Additionally, we show how our method can be applied to convert pre-trained
text-to-image generative models into audio-to-image ones. Code is available at:
https://github.com/layer6ai-labs/fusemix.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lossy Image Compression with Conditional Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.06950v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.06950v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruihan Yang, Stephan Mandt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper outlines an end-to-end optimized lossy image compression framework
using diffusion generative models. The approach relies on the transform coding
paradigm, where an image is mapped into a latent space for entropy coding and,
from there, mapped back to the data space for reconstruction. In contrast to
VAE-based neural compression, where the (mean) decoder is a deterministic
neural network, our decoder is a conditional diffusion model. Our approach thus
introduces an additional ``content'' latent variable on which the reverse
diffusion process is conditioned and uses this variable to store information
about the image. The remaining ``texture'' variables characterizing the
diffusion process are synthesized at decoding time. We show that the model's
performance can be tuned toward perceptual metrics of interest. Our extensive
experiments involving multiple datasets and image quality assessment metrics
show that our approach yields stronger reported FID scores than the GAN-based
model, while also yielding competitive performance with VAE-based models in
several distortion metrics. Furthermore, training the diffusion with
$\mathcal{X}$-parameterization enables high-quality reconstructions in only a
handful of decoding steps, greatly affecting the model's practicality. Our code
is available at: \url{https://github.com/buggyyang/CDC_compression}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PointDC:Unsupervised Semantic Segmentation of 3D Point Clouds via
  Cross-modal Distillation and Super-Voxel Clustering <span class="chip">ICCV</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.08965v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.08965v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zisheng Chen, Hongbin Xu, Weitao Chen, Zhipeng Zhou, Haihong Xiao, Baigui Sun, Xuansong Xie, Wenxiong Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic segmentation of point clouds usually requires exhausting efforts of
human annotations, hence it attracts wide attention to the challenging topic of
learning from unlabeled or weaker forms of annotations. In this paper, we take
the first attempt for fully unsupervised semantic segmentation of point clouds,
which aims to delineate semantically meaningful objects without any form of
annotations. Previous works of unsupervised pipeline on 2D images fails in this
task of point clouds, due to: 1) Clustering Ambiguity caused by limited
magnitude of data and imbalanced class distribution; 2) Irregularity Ambiguity
caused by the irregular sparsity of point cloud. Therefore, we propose a novel
framework, PointDC, which is comprised of two steps that handle the
aforementioned problems respectively: Cross-Modal Distillation (CMD) and
Super-Voxel Clustering (SVC). In the first stage of CMD, multi-view visual
features are back-projected to the 3D space and aggregated to a unified point
feature to distill the training of the point representation. In the second
stage of SVC, the point features are aggregated to super-voxels and then fed to
the iterative clustering process for excavating semantic classes. PointDC
yields a significant improvement over the prior state-of-the-art unsupervised
methods, on both the ScanNet-v2 (+18.4 mIoU) and S3DIS (+11.5 mIoU) semantic
segmentation benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by International Conference on Computer Vision (ICCV) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploiting Causality Signals in Medical Images: A Pilot Study with
  Empirical Results 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.10399v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.10399v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gianluca Carloni, Sara Colantonio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel technique to discover and exploit weak causal signals
directly from images via neural networks for classification purposes. This way,
we model how the presence of a feature in one part of the image affects the
appearance of another feature in a different part of the image. Our method
consists of a convolutional neural network backbone and a causality-factors
extractor module, which computes weights to enhance each feature map according
to its causal influence in the scene. We develop different architecture
variants and empirically evaluate all the models on two public datasets of
prostate MRI images and breast histopathology slides for cancer diagnosis. We
study the effectiveness of our module both in fully-supervised and few-shot
learning, we assess its addition to existing attention-based solutions, we
conduct ablation studies, and investigate the explainability of our models via
class activation maps. Our findings show that our lightweight block extracts
meaningful information and improves the overall classification, together with
producing more robust predictions that focus on relevant parts of the image.
That is crucial in medical imaging, where accurate and reliable classifications
are essential for effective diagnosis and treatment planning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added experiments in which we integrate our Mulcat module to existing
  models using Bottleneck Attention Modules, and added experiments in Few-Shot
  Learning; 19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RS5M and GeoRSCLIP: A Large Scale Vision-Language <span class="highlight-title">Dataset</span> and A Large
  Vision-Language Model for Remote Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11300v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11300v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zilun Zhang, Tiancheng Zhao, Yulong Guo, Jianwei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained Vision-Language Models (VLMs) utilizing extensive image-text
paired data have demonstrated unprecedented image-text association
capabilities, achieving remarkable results across various downstream tasks. A
critical challenge is how to make use of existing large-scale pre-trained VLMs,
which are trained on common objects, to perform the domain-specific transfer
for accomplishing domain-related downstream tasks. A critical challenge is how
to make use of existing large-scale pre-trained VLMs, which are trained on
common objects, to perform the domain-specific transfer for accomplishing
domain-related downstream tasks. In this paper, we propose a new framework that
includes the Domain pre-trained Vision-Language Model (DVLM), bridging the gap
between the General Vision-Language Model (GVLM) and domain-specific downstream
tasks. Moreover, we present an image-text paired dataset in the field of remote
sensing (RS), RS5M, which has 5 million RS images with English descriptions.
The dataset is obtained from filtering publicly available image-text paired
datasets and captioning label-only RS datasets with pre-trained VLM. These
constitute the first large-scale RS image-text paired dataset. Additionally, we
fine-tuned the CLIP model and tried several Parameter-Efficient Fine-Tuning
methods on RS5M to implement the DVLM. Experimental results show that our
proposed dataset is highly effective for various tasks, and our model GeoRSCLIP
improves upon the baseline or previous state-of-the-art model by $3\%\sim20\%$
in Zero-shot Classification (ZSC), $3\%\sim6\%$ in Remote Sensing Cross-Modal
Text-Image Retrieval (RSCTIR) and $4\%\sim5\%$ in Semantic Localization (SeLo)
tasks. Dataset and models have been released in:
\url{https://github.com/om-ai-lab/RS5M}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>RS5M dataset v5</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Application of Efficient Neural Mapping to Real-Time Indoor
  Localisation for Unmanned Ground Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.04718v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.04718v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher J. Holder, Muhammad Shafique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Global localisation from visual data is a challenging problem applicable to
many robotics domains. Prior works have shown that neural networks can be
trained to map images of an environment to absolute camera pose within that
environment, learning an implicit neural mapping in the process. In this work
we evaluate the applicability of such an approach to real-world robotics
scenarios, demonstrating that by constraining the problem to 2-dimensions and
significantly increasing the quantity of training data, a compact model capable
of real-time inference on embedded platforms can be used to achieve
localisation accuracy of several centimetres. We deploy our trained model
onboard a UGV platform, demonstrating its effectiveness in a waypoint
navigation task, wherein it is able to localise with a mean accuracy of 9cm at
a rate of 6fps running on the UGV onboard CPU, 35fps on an embedded GPU, or
220fps on a desktop GPU. Along with this work we will release a novel
localisation dataset comprising simulated and real environments, each with
training samples numbering in the tens of thousands.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>\c{opyright} 2024 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GD^2-NeRF: Generative Detail Compensation via GAN and Diffusion for
  One-shot Generalizable Neural Radiance Fields 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00616v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00616v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Pan, Zongxin Yang, Shuai Bai, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we focus on the One-shot Novel View Synthesis (O-NVS) task
which targets synthesizing photo-realistic novel views given only one reference
image per scene. Previous One-shot Generalizable Neural Radiance Fields
(OG-NeRF) methods solve this task in an inference-time finetuning-free manner,
yet suffer the blurry issue due to the encoder-only architecture that highly
relies on the limited reference image. On the other hand, recent
diffusion-based image-to-3d methods show vivid plausible results via distilling
pre-trained 2D diffusion models into a 3D representation, yet require tedious
per-scene optimization. Targeting these issues, we propose the GD$^2$-NeRF, a
Generative Detail compensation framework via GAN and Diffusion that is both
inference-time finetuning-free and with vivid plausible details. In detail,
following a coarse-to-fine strategy, GD$^2$-NeRF is mainly composed of a
One-stage Parallel Pipeline (OPP) and a 3D-consistent Detail Enhancer
(Diff3DE). At the coarse stage, OPP first efficiently inserts the GAN model
into the existing OG-NeRF pipeline for primarily relieving the blurry issue
with in-distribution priors captured from the training dataset, achieving a
good balance between sharpness (LPIPS, FID) and fidelity (PSNR, SSIM). Then, at
the fine stage, Diff3DE further leverages the pre-trained image diffusion
models to complement rich out-distribution details while maintaining decent 3D
consistency. Extensive experiments on both the synthetic and real-world
datasets show that GD$^2$-NeRF noticeably improves the details while without
per-scene finetuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Reading with Macbook Preview is recommended for best quality;
  Submitted to Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ImFace++: A Sophisticated Nonlinear 3D Morphable Face Model with
  Implicit Neural Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04028v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04028v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingwu Zheng, Haiyu Zhang, Hongyu Yang, Liming Chen, Di Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate representations of 3D faces are of paramount importance in various
computer vision and graphics applications. However, the challenges persist due
to the limitations imposed by data discretization and model linearity, which
hinder the precise capture of identity and expression clues in current studies.
This paper presents a novel 3D morphable face model, named ImFace++, to learn a
sophisticated and continuous space with implicit neural representations.
ImFace++ first constructs two explicitly disentangled deformation fields to
model complex shapes associated with identities and expressions, respectively,
which simultaneously facilitate the automatic learning of correspondences
across diverse facial shapes. To capture more sophisticated facial details, a
refinement displacement field within the template space is further
incorporated, enabling a fine-grained learning of individual-specific facial
details. Furthermore, a Neural Blend-Field is designed to reinforce the
representation capabilities through adaptive blending of an array of local
fields. In addition to ImFace++, we have devised an improved learning strategy
to extend expression embeddings, allowing for a broader range of expression
variations. Comprehensive qualitative and quantitative evaluations demonstrate
that ImFace++ significantly advances the state-of-the-art in terms of both face
reconstruction fidelity and correspondence accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page:
  https://github.com/MingwuZheng/ImFace/tree/imface%2B%2B. arXiv admin note:
  text overlap with arXiv:2203.14510</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tomato Maturity Recognition with Convolutional Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.01530v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.01530v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Asim Khan, Taimur Hassan, Muhammad Shafay, Israa Fahmy, Naoufel Werghi, Lakmal Seneviratne, Irfan Hussain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tomatoes are a major crop worldwide, and accurately classifying their
maturity is important for many agricultural applications, such as harvesting,
grading, and quality control. In this paper, the authors propose a novel method
for tomato maturity classification using a convolutional transformer. The
convolutional transformer is a hybrid architecture that combines the strengths
of convolutional neural networks (CNNs) and transformers. Additionally, this
study introduces a new tomato dataset named KUTomaData, explicitly designed to
train deep-learning models for tomato segmentation and classification.
KUTomaData is a compilation of images sourced from a greenhouse in the UAE,
with approximately 700 images available for training and testing. The dataset
is prepared under various lighting conditions and viewing perspectives and
employs different mobile camera sensors, distinguishing it from existing
datasets. The contributions of this paper are threefold:Firstly, the authors
propose a novel method for tomato maturity classification using a modular
convolutional transformer. Secondly, the authors introduce a new tomato image
dataset that contains images of tomatoes at different maturity levels. Lastly,
the authors show that the convolutional transformer outperforms
state-of-the-art methods for tomato maturity classification. The effectiveness
of the proposed framework in handling cluttered and occluded tomato instances
was evaluated using two additional public datasets, Laboro Tomato and Rob2Pheno
Annotated Tomato, as benchmarks. The evaluation results across these three
datasets demonstrate the exceptional performance of our proposed framework,
surpassing the state-of-the-art by 58.14%, 65.42%, and 66.39% in terms of mean
average precision scores for KUTomaData, Laboro Tomato, and Rob2Pheno Annotated
Tomato, respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 6 figures and 8 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tensor PCA from basis in tensor space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.02803v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.02803v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Claudio Turchetti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The aim of this paper is to present a mathematical framework for tensor PCA.
The proposed approach is able to overcome the limitations of previous methods
that extract a low dimensional subspace by iteratively solving an optimization
problem. The core of the proposed approach is the derivation of a basis in
tensor space from a real self-adjoint tensor operator, thus reducing the
problem of deriving a basis to an eigenvalue problem. Three different cases
have been studied to derive: i) a basis from a self-adjoint tensor operator;
ii) a rank-1 basis; iii) a basis in a subspace. In particular, the equivalence
between eigenvalue equation for a real self-adjoint tensor operator and
standard matrix eigenvalue equation has been proven. For all the three cases
considered, a subspace approach has been adopted to derive a tensor PCA.
Experiments on image datasets validate the proposed mathematical framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Long- and Short-Range Temporal Information for Learned Video
  Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.03754v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.03754v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huairui Wang, Zhenzhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learned video compression methods have gained a variety of interest in the
video coding community since they have matched or even exceeded the
rate-distortion (RD) performance of traditional video codecs. However, many
current learning-based methods are dedicated to utilizing short-range temporal
information, thus limiting their performance. In this paper, we focus on
exploiting the unique characteristics of video content and further exploring
temporal information to enhance compression performance. Specifically, for
long-range temporal information exploitation, we propose temporal prior that
can update continuously within the group of pictures (GOP) during inference. In
that case temporal prior contains valuable temporal information of all decoded
images within the current GOP. As for short-range temporal information, we
propose a progressive guided motion compensation to achieve robust and
effective compensation. In detail, we design a hierarchical structure to
achieve multi-scale compensation. More importantly, we use optical flow
guidance to generate pixel offsets between feature maps at each scale, and the
compensation results at each scale will be used to guide the following scale's
compensation. Sufficient experimental results demonstrate that our method can
obtain better RD performance than state-of-the-art video compression
approaches. The code is publicly available on:
https://github.com/Huairui/LSTVC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2207.04589</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attention Based Encoder Decoder Model for Video Captioning in Nepali
  (2023) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.07418v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.07418v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kabita Parajuli, Shashidhar Ram Joshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video captioning in Nepali, a language written in the Devanagari script,
presents a unique challenge due to the lack of existing academic work in this
domain. This work develops a novel encoder-decoder paradigm for Nepali video
captioning to tackle this difficulty. LSTM and GRU sequence-to-sequence models
are used in the model to produce related textual descriptions based on features
retrieved from video frames using CNNs. Using Google Translate and manual
post-editing, a Nepali video captioning dataset is generated from the Microsoft
Research Video Description Corpus (MSVD) dataset created using Google
Translate, and manual post-editing work. The efficacy of the model for
Devanagari-scripted video captioning is demonstrated by BLEU, METOR, and ROUGE
measures, which are used to assess its performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Result are wrong and took some time</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ YOLO and Mask R-CNN for Vehicle Number Plate Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.13165v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.13165v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Ganjoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  License plate scanners have grown in popularity in parking lots during the
past few years. In order to quickly identify license plates, traditional plate
recognition devices used in parking lots employ a fixed source of light and
shooting angles. For skewed angles, such as license plate images taken with
ultra-wide angle or fisheye lenses, deformation of the license plate
recognition plate can also be quite severe, impairing the ability of standard
license plate recognition systems to identify the plate. Mask RCNN gadget that
may be utilised for oblique pictures and various shooting angles. The results
of the experiments show that the suggested design will be capable of
classifying license plates with bevel angles larger than 0/60. Character
recognition using the suggested Mask R-CNN approach has advanced significantly
as well. The proposed Mask R-CNN method has also achieved significant progress
in character recognition, which is tilted more than 45 degrees as compared to
the strategy of employing the YOLOv2 model. Experiment results also suggest
that the methodology presented in the open data plate collecting is better than
other techniques (known as the AOLP dataset).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>changes to be done</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">18</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human Leading or Following Preferences: Effects on Human Perception of
  the Robot and the Human-Robot Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01466v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01466v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Noormohammadi-Asl, Kevin Fan, Stephen L. Smith, Kerstin Dautenhahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving effective and seamless human-robot collaboration requires two key
outcomes: enhanced team performance and fostering a positive human perception
of both the robot and the collaboration. This paper investigates the capability
of the proposed task planning framework to realize these objectives by
integrating human leading/following preference and performance into its task
allocation and scheduling processes. We designed a collaborative scenario
wherein the robot autonomously collaborates with participants. The outcomes of
the user study indicate that the proactive task planning framework successfully
attains the aforementioned goals. We also explore the impact of participants'
leadership and followership styles on their collaboration. The results reveal
intriguing relationships between these factors, which warrant further
investigation in future studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Indoor Obstacle Discovery on Reflective Ground via Monocular Camera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Xue, Yicong Chang, Tianxi Wang, Yu Zhou, Anlong Ming
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual obstacle discovery is a key step towards autonomous navigation of
indoor mobile robots. Successful solutions have many applications in multiple
scenes. One of the exceptions is the reflective ground. In this case, the
reflections on the floor resemble the true world, which confuses the obstacle
discovery and leaves navigation unsuccessful. We argue that the key to this
problem lies in obtaining discriminative features for reflections and
obstacles. Note that obstacle and reflection can be separated by the ground
plane in 3D space. With this observation, we firstly introduce a
pre-calibration based ground detection scheme that uses robot motion to predict
the ground plane. Due to the immunity of robot motion to reflection, this
scheme avoids failed ground detection caused by reflection. Given the detected
ground, we design a ground-pixel parallax to describe the location of a pixel
relative to the ground. Based on this, a unified appearance-geometry feature
representation is proposed to describe objects inside rectangular boxes.
Eventually, based on segmenting by detection framework, an appearance-geometry
fusion regressor is designed to utilize the proposed feature to discover the
obstacles. It also prevents our model from concentrating too much on parts of
obstacles instead of whole obstacles. For evaluation, we introduce a new
dataset for Obstacle on Reflective Ground (ORG), which comprises 15 scenes with
various ground reflections, a total of more than 200 image sequences and 3400
RGB images. The pixel-wise annotations of ground and obstacle provide a
comparison to our method and other methods. By reducing the misdetection of the
reflection, the proposed approach outperforms others. The source code and the
dataset will be available at
https://github.com/XuefengBUPT/IndoorObstacleDiscovery-RG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Journal of Computer Vision (IJCV) 2023. Project Page:
  https://xuefeng-cvr.github.io/IODRG</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Off-Road LiDAR Intensity Based Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kasi Viswanath, Peng Jiang, Sujit PB, Srikanth Saripalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR is used in autonomous driving to provide 3D spatial information and
enable accurate perception in off-road environments, aiding in obstacle
detection, mapping, and path planning. Learning-based LiDAR semantic
segmentation utilizes machine learning techniques to automatically classify
objects and regions in LiDAR point clouds. Learning-based models struggle in
off-road environments due to the presence of diverse objects with varying
colors, textures, and undefined boundaries, which can lead to difficulties in
accurately classifying and segmenting objects using traditional geometric-based
features. In this paper, we address this problem by harnessing the LiDAR
intensity parameter to enhance object segmentation in off-road environments.
Our approach was evaluated in the RELLIS-3D data set and yielded promising
results as a preliminary analysis with improved mIoU for classes "puddle" and
"grass" compared to more complex deep learning-based benchmarks. The
methodology was evaluated for compatibility across both Velodyne and Ouster
LiDAR systems, assuring its cross-platform applicability. This analysis
advocates for the incorporation of calibrated intensity as a supplementary
input, aiming to enhance the prediction accuracy of learning based semantic
segmentation frameworks.
https://github.com/MOONLABIISERB/lidar-intensity-predictor/tree/main
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ISER 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SwapTransformer: highway overtaking tactical planner model via imitation
  learning on OSHA <span class="highlight-title">dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Shamsoshoara, Safin B Salih, Pedram Aghazadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the high-level decision-making problem in highway
scenarios regarding lane changing and over-taking other slower vehicles. In
particular, this paper aims to improve the Travel Assist feature for automatic
overtaking and lane changes on highways. About 9 million samples including lane
images and other dynamic objects are collected in simulation. This data;
Overtaking on Simulated HighwAys (OSHA) dataset is released to tackle this
challenge. To solve this problem, an architecture called SwapTransformer is
designed and implemented as an imitation learning approach on the OSHA dataset.
Moreover, auxiliary tasks such as future points and car distance network
predictions are proposed to aid the model in better understanding the
surrounding environment. The performance of the proposed solution is compared
with a multi-layer perceptron (MLP) and multi-head self-attention networks as
baselines in a simulation environment. We also demonstrate the performance of
the model with and without auxiliary tasks. All models are evaluated based on
different metrics such as time to finish each lap, number of overtakes, and
speed difference with speed limit. The evaluation shows that the
SwapTransformer model outperforms other models in different traffic densities
in the inference phase.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 12 Figures, 1 Algorithm, 2 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Design, Manufacturing and Open-Loop Control of a Soft Pneumatic Arm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jorge Francisco García-Samartín, Adrián Rieker, Antonio Barrientos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soft Robots distinguish themselves from traditional robots by embracing
flexible kinematics. Because of their recent emergence, there exist numerous
uncharted territories, including novel actuators, manufacturing processes, and
advanced control methods. This research is centred on the design, fabrication,
and control of a pneumatic soft robot. The principal objective is to develop a
modular soft robot featuring with multiple segments, each one of three degrees
of freedom. This yields to tubular structure with five independent degrees of
freedom, enabling motion across three spatial dimensions. Physical construction
leverages tin-cured silicone and a wax casting method, refined through
iterative processes. 3D-printed PLA moulds, filled with silicone, yield the
desired model, while bladder-like structures, are formed within using
solidified paraffin wax positive moulds. For control, an empirically fine-tuned
open-loop system is adopted. The project culminates in rigorous testing bending
ability and weight carrying capacity and possible applications are discussed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NID-SLAM: Neural Implicit Representation-based RGB-D SLAM in dynamic
  environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01189v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01189v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziheng Xu, Jianwei Niu, Qingfeng Li, Tao Ren, Chen Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural implicit representations have been explored to enhance visual SLAM
algorithms, especially in providing high-fidelity dense map. Existing methods
operate robustly in static scenes but struggle with the disruption caused by
moving objects. In this paper we present NID-SLAM, which significantly improves
the performance of neural SLAM in dynamic environments. We propose a new
approach to enhance inaccurate regions in semantic masks, particularly in
marginal areas. Utilizing the geometric information present in depth images,
this method enables accurate removal of dynamic objects, thereby reducing the
probability of camera drift. Additionally, we introduce a keyframe selection
strategy for dynamic scenes, which enhances camera tracking robustness against
large-scale objects and improves the efficiency of mapping. Experiments on
publicly available RGB-D datasets demonstrate that our method outperforms
competitive neural SLAM approaches in tracking accuracy and mapping quality in
dynamic environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Symbolic Manipulation Planning with Discovered Object and Relational
  Predicates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alper Ahmetoglu, Erhan Oztop, Emre Ugur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discovering the symbols and rules that can be used in long-horizon planning
from a robot's unsupervised exploration of its environment and continuous
sensorimotor experience is a challenging task. The previous studies proposed
learning symbols from single or paired object interactions and planning with
these symbols. In this work, we propose a system that learns rules with
discovered object and relational symbols that encode an arbitrary number of
objects and the relations between them, converts those rules to Planning Domain
Description Language (PDDL), and generates plans that involve affordances of
the arbitrary number of objects to achieve tasks. We validated our system with
box-shaped objects in different sizes and showed that the system can develop a
symbolic knowledge of pick-up, carry, and place operations, taking into account
object compounds in different configurations, such as boxes would be carried
together with a larger box that they are placed on. We also compared our method
with the state-of-the-art methods and showed that planning with the operators
defined over relational symbols gives better planning performance compared to
the baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PLE-SLAM: A Visual-Inertial SLAM Based on Point-Line Features and
  Efficient IMU Initialization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01081v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01081v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaming He, Mingrui Li, Yangyang Wang, Hongyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual-inertial SLAM is essential in various fields, such as AR/VR, uncrewed
aerial vehicles, industrial robots, and autonomous driving. The fusion of a
camera and inertial measurement unit (IMU) can make up for the shortcomings of
a signal sensor, which significantly improves the accuracy and robustness of
localization in challenging environments. Robust tracking and accurate inertial
parameter estimation are the basis for the stable operation of the system. This
article presents PLE-SLAM, an entirely precise and real-time visual-inertial
SLAM algorithm based on point-line features and efficient IMU initialization.
First, we introduce line features in a point-based visual-inertial SLAM system.
We use parallel computing methods to extract features and compute descriptors
to ensure real-time performance. Second, the proposed system estimates
gyroscope bias with rotation pre-integration and point and line observations.
Accelerometer bias and gravity direction are solved by an analytical method.
After initialization, all inertial parameters are refined through maximum a
posteriori (MAP) estimation. Moreover, we open a dynamic feature elimination
thread to improve the adaptability to dynamic environments and use CNN,
bag-of-words and GNN to detect loops and match features. Excellent wide
baseline matching capability of DNN-based matching method and illumination
robustness significantly improve loop detection recall and loop inter-frame
pose estimation. The front-end and back-end are designed for hardware
acceleration. The experiments are performed on public datasets, and the results
show that the proposed system is one of the state-of-the-art methods in complex
scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeXtreme: Transfer of Agile In-hand Manipulation from Simulation to
  Reality <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.13702v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.13702v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ankur Handa, Arthur Allshire, Viktor Makoviychuk, Aleksei Petrenko, Ritvik Singh, Jingzhou Liu, Denys Makoviichuk, Karl Van Wyk, Alexander Zhurkevich, Balakumar Sundaralingam, Yashraj Narang, Jean-Francois Lafleche, Dieter Fox, Gavriel State
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work has demonstrated the ability of deep reinforcement learning (RL)
algorithms to learn complex robotic behaviours in simulation, including in the
domain of multi-fingered manipulation. However, such models can be challenging
to transfer to the real world due to the gap between simulation and reality. In
this paper, we present our techniques to train a) a policy that can perform
robust dexterous manipulation on an anthropomorphic robot hand and b) a robust
pose estimator suitable for providing reliable real-time information on the
state of the object being manipulated. Our policies are trained to adapt to a
wide range of conditions in simulation. Consequently, our vision-based policies
significantly outperform the best vision policies in the literature on the same
reorientation task and are competitive with policies that are given privileged
state information via motion capture systems. Our work reaffirms the
possibilities of sim-to-real transfer for dexterous manipulation in diverse
kinds of hardware and simulator setups, and in our case, with the Allegro Hand
and Isaac Gym GPU-based simulation. Furthermore, it opens up possibilities for
researchers to achieve such results with commonly-available, affordable robot
hands and cameras. Videos of the resulting policy and supplementary
information, including experiments and demos, can be found at
https://dextreme.org/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages. A smaller version of this paper is accepted to ICRA 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Path Planning Through Large Collections of Safe Boxes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.01072v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.01072v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobia Marcucci, Parth Nobel, Russ Tedrake, Stephen Boyd
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a fast algorithm for the design of smooth paths (or trajectories)
that are constrained to lie in a collection of axis-aligned boxes. We consider
the case where the number of these safe boxes is large, and basic preprocessing
of them (such as finding their intersections) can be done offline. At runtime
we quickly generate a smooth path between given initial and terminal positions.
Our algorithm designs trajectories that are guaranteed to be safe at all times,
and detects infeasibility whenever such a trajectory does not exist. Our
algorithm is based on two subproblems that we can solve very efficiently:
finding a shortest path in a weighted graph, and solving (multiple) convex
optimal-control problems. We demonstrate the proposed path planner on
large-scale numerical examples, and we provide an efficient open-source
software implementation, fastpathplanning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sample-Efficient Safety Assurances using Conformal Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.14082v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.14082v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rachel Luo, Shengjia Zhao, Jonathan Kuck, Boris Ivanovic, Silvio Savarese, Edward Schmerling, Marco Pavone
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When deploying machine learning models in high-stakes robotics applications,
the ability to detect unsafe situations is crucial. Early warning systems can
provide alerts when an unsafe situation is imminent (in the absence of
corrective action). To reliably improve safety, these warning systems should
have a provable false negative rate; i.e. of the situations that are unsafe,
fewer than $\epsilon$ will occur without an alert. In this work, we present a
framework that combines a statistical inference technique known as conformal
prediction with a simulator of robot/environment dynamics, in order to tune
warning systems to provably achieve an $\epsilon$ false negative rate using as
few as $1/\epsilon$ data points. We apply our framework to a driver warning
system and a robotic grasping application, and empirically demonstrate
guaranteed false negative rate while also observing low false detection
(positive) rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Journal of Robotics Research, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Independence in the Home: A Wearable Interface for a Person with
  Quadriplegia to Teleoperate a Mobile Manipulator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15071v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15071v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akhil Padmanabha, Janavi Gupta, Chen Chen, Jehan Yang, Vy Nguyen, Douglas J. Weber, Carmel Majidi, Zackory Erickson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Teleoperation of mobile manipulators within a home environment can
significantly enhance the independence of individuals with severe motor
impairments, allowing them to regain the ability to perform self-care and
household tasks. There is a critical need for novel teleoperation interfaces to
offer effective alternatives for individuals with impairments who may encounter
challenges in using existing interfaces due to physical limitations. In this
work, we iterate on one such interface, HAT (Head-Worn Assistive
Teleoperation), an inertial-based wearable integrated into any head-worn
garment. We evaluate HAT through a 7-day in-home study with Henry Evans, a
non-speaking individual with quadriplegia who has participated extensively in
assistive robotics studies. We additionally evaluate HAT with a proposed shared
control method for mobile manipulators termed Driver Assistance and demonstrate
how the interface generalizes to other physical devices and contexts. Our
results show that HAT is a strong teleoperation interface across key metrics
including efficiency, errors, learning curve, and workload. Code and videos are
located on our project website.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging the Gaps: Learning Verifiable Model-Free Quadratic Programming
  Controllers Inspired by Model Predictive Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05332v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05332v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwen Lu, Zishuo Li, Yihan Zhou, Na Li, Yilin Mo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a new class of parameterized controllers, drawing
inspiration from Model Predictive Control (MPC). The controller resembles a
Quadratic Programming (QP) solver of a linear MPC problem, with the parameters
of the controller being trained via Deep Reinforcement Learning (DRL) rather
than derived from system models. This approach addresses the limitations of
common controllers with Multi-Layer Perceptron (MLP) or other general neural
network architecture used in DRL, in terms of verifiability and performance
guarantees, and the learned controllers possess verifiable properties like
persistent feasibility and asymptotic stability akin to MPC. On the other hand,
numerical examples illustrate that the proposed controller empirically matches
MPC and MLP controllers in terms of control performance and has superior
robustness against modeling uncertainty and noises. Furthermore, the proposed
controller is significantly more computationally efficient compared to MPC and
requires fewer parameters to learn than MLP controllers. Real-world experiments
on vehicle drift maneuvering task demonstrate the potential of these
controllers for robotics and other demanding control tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Application of Efficient Neural Mapping to Real-Time Indoor
  Localisation for Unmanned Ground Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.04718v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.04718v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher J. Holder, Muhammad Shafique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Global localisation from visual data is a challenging problem applicable to
many robotics domains. Prior works have shown that neural networks can be
trained to map images of an environment to absolute camera pose within that
environment, learning an implicit neural mapping in the process. In this work
we evaluate the applicability of such an approach to real-world robotics
scenarios, demonstrating that by constraining the problem to 2-dimensions and
significantly increasing the quantity of training data, a compact model capable
of real-time inference on embedded platforms can be used to achieve
localisation accuracy of several centimetres. We deploy our trained model
onboard a UGV platform, demonstrating its effectiveness in a waypoint
navigation task, wherein it is able to localise with a mean accuracy of 9cm at
a rate of 6fps running on the UGV onboard CPU, 35fps on an embedded GPU, or
220fps on a desktop GPU. Along with this work we will release a novel
localisation dataset comprising simulated and real environments, each with
training samples numbering in the tens of thousands.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>\c{opyright} 2024 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toward Certifying Maps for Safe Registration-based Localization Under
  Adverse Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.04251v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.04251v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johann Laconte, Daniil Lisus, Timothy D. Barfoot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a way to model the resilience of the Iterative
Closest Point (ICP) algorithm in the presence of corrupted measurements. In the
context of autonomous vehicles, certifying the safety of the localization
process poses a significant challenge. As robots evolve in a complex world,
various types of noise can impact the measurements. Conventionally, this noise
has been assumed to be distributed according to a zero-mean Gaussian
distribution. However, this assumption does not hold in numerous scenarios,
including adverse weather conditions, occlusions caused by dynamic obstacles,
or long-term changes in the map. In these cases, the measurements are instead
affected by large and deterministic faults. This paper introduces a closed-form
formula approximating the pose error resulting from an ICP algorithm when
subjected to the most detrimental adverse measurements. Using this formula, we
develop a metric to certify and pinpoint specific regions within the
environment where the robot is more vulnerable to localization failures in the
presence of faults in the measurements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>in IEEE Robotics and Automation Letters, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inertial Line-Of-Sight Stabilization Using a 3-DOF Spherical Parallel
  Manipulator with Coaxial Input Shafts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.02641v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.02641v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Le, Guillaume Rance, Fabrice Rouillier, Damien Chablat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article dives into the use of a 3-RRR Spherical Parallel Manipulator
(SPM) for the purpose of inertial Line Of Sight (LOS) stabilization. Such a
parallel robot provides three Degrees of Freedom (DOF) in orientation and is
studied from the kinematic point of view. In particular, one guarantees that
the singular loci (with the resulting numerical instabilities and inappropriate
behavior of the mechanism) are far away from the prescribed workspace. Once the
kinematics of the device is certified, a control strategy needs to be
implemented in order to stabilize the LOS through the upper platform of the
mechanism. Such a work is done with MATLAB Simulink using a SimMechanics model
of our robot.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>OPTRO Conference 2024 (11th International Symposium on Optronics in
  Defense & Security, 2024), 11 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision-based Learning for Drones: A Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.05019v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.05019v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaping Xiao, Rangya Zhang, Yuhang Zhang, Mir Feroskhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drones as advanced cyber-physical systems are undergoing a transformative
shift with the advent of vision-based learning, a field that is rapidly gaining
prominence due to its profound impact on drone autonomy and functionality.
Different from existing task-specific surveys, this review offers a
comprehensive overview of vision-based learning in drones, emphasizing its
pivotal role in enhancing their operational capabilities under various
scenarios. We start by elucidating the fundamental principles of vision-based
learning, highlighting how it significantly improves drones' visual perception
and decision-making processes. We then categorize vision-based control methods
into indirect, semi-direct, and end-to-end approaches from the
perception-control perspective. We further explore various applications of
vision-based drones with learning capabilities, ranging from single-agent
systems to more complex multi-agent and heterogeneous system scenarios, and
underscore the challenges and innovations characterizing each area. Finally, we
explore open questions and potential solutions, paving the way for ongoing
research and development in this dynamic and rapidly evolving field. With
growing large language models (LLMs) and embodied intelligence, vision-based
learning for drones provides a promising but challenging road towards
artificial general intelligence (AGI) in 3D physical world.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hybrid Internal Model: Learning Agile Legged Locomotion with Simulated
  Robot Response 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11460v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11460v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junfeng Long, Zirui Wang, Quanyi Li, Jiawei Gao, Liu Cao, Jiangmiao Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust locomotion control depends on accurate state estimations. However, the
sensors of most legged robots can only provide partial and noisy observations,
making the estimation particularly challenging, especially for external states
like terrain frictions and elevation maps. Inspired by the classical Internal
Model Control principle, we consider these external states as disturbances and
introduce Hybrid Internal Model (HIM) to estimate them according to the
response of the robot. The response, which we refer to as the hybrid internal
embedding, contains the robot's explicit velocity and implicit stability
representation, corresponding to two primary goals for locomotion tasks:
explicitly tracking velocity and implicitly maintaining stability. We use
contrastive learning to optimize the embedding to be close to the robot's
successor state, in which the response is naturally embedded. HIM has several
appealing benefits: It only needs the robot's proprioceptions, i.e., those from
joint encoders and IMU as observations. It innovatively maintains consistent
observations between simulation reference and reality that avoids information
loss in mimicking learning. It exploits batch-level information that is more
robust to noises and keeps better sample efficiency. It only requires 1 hour of
training on an RTX 4090 to enable a quadruped robot to traverse any terrain
under any disturbances. A wealth of real-world experiments demonstrates its
agility, even in high-difficulty tasks and cases never occurred during the
training process, revealing remarkable open-world generalizability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Use 1 hour to train a quadruped robot capable of traversing any
  terrain under any disturbances in the open world, Project Page:
  https://github.com/OpenRobotLab/HIMLoco</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NU-Class Net: A Novel Deep Learning-based Approach for Video Quality
  Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.01163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.01163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parham Zilouchian Moghaddam, Mehdi Modarressi, MohammadAmin Sadeghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video content has experienced a surge in popularity, asserting its dominance
over internet traffic and Internet of Things (IoT) networks. Video compression
has long been regarded as the primary means of efficiently managing the
substantial multimedia traffic generated by video-capturing devices.
Nevertheless, video compression algorithms entail significant computational
demands in order to achieve substantial compression ratios. This complexity
presents a formidable challenge when implementing efficient video coding
standards in resource-constrained embedded systems, such as IoT edge node
cameras. To tackle this challenge, this paper introduces NU-Class Net, an
innovative deep-learning model designed to mitigate compression artifacts
stemming from lossy compression codecs. This enhancement significantly elevates
the perceptible quality of low-bit-rate videos. By employing the NU-Class Net,
the video encoder within the video-capturing node can reduce output quality,
thereby generating low-bit-rate videos and effectively curtailing both
computation and bandwidth requirements at the edge. On the decoder side, which
is typically less encumbered by resource limitations, NU-Class Net is applied
after the video decoder to compensate for artifacts and approximate the quality
of the original video. Experimental results affirm the efficacy of the proposed
model in enhancing the perceptible quality of videos, especially those streamed
at low bit rates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RS5M and GeoRSCLIP: A Large Scale Vision-Language <span class="highlight-title">Dataset</span> and A Large
  Vision-Language Model for Remote Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.11300v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.11300v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zilun Zhang, Tiancheng Zhao, Yulong Guo, Jianwei Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained Vision-Language Models (VLMs) utilizing extensive image-text
paired data have demonstrated unprecedented image-text association
capabilities, achieving remarkable results across various downstream tasks. A
critical challenge is how to make use of existing large-scale pre-trained VLMs,
which are trained on common objects, to perform the domain-specific transfer
for accomplishing domain-related downstream tasks. A critical challenge is how
to make use of existing large-scale pre-trained VLMs, which are trained on
common objects, to perform the domain-specific transfer for accomplishing
domain-related downstream tasks. In this paper, we propose a new framework that
includes the Domain pre-trained Vision-Language Model (DVLM), bridging the gap
between the General Vision-Language Model (GVLM) and domain-specific downstream
tasks. Moreover, we present an image-text paired dataset in the field of remote
sensing (RS), RS5M, which has 5 million RS images with English descriptions.
The dataset is obtained from filtering publicly available image-text paired
datasets and captioning label-only RS datasets with pre-trained VLM. These
constitute the first large-scale RS image-text paired dataset. Additionally, we
fine-tuned the CLIP model and tried several Parameter-Efficient Fine-Tuning
methods on RS5M to implement the DVLM. Experimental results show that our
proposed dataset is highly effective for various tasks, and our model GeoRSCLIP
improves upon the baseline or previous state-of-the-art model by $3\%\sim20\%$
in Zero-shot Classification (ZSC), $3\%\sim6\%$ in Remote Sensing Cross-Modal
Text-Image Retrieval (RSCTIR) and $4\%\sim5\%$ in Semantic Localization (SeLo)
tasks. Dataset and models have been released in:
\url{https://github.com/om-ai-lab/RS5M}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>RS5M dataset v5</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MusER: Musical Element-Based Regularization for Generating Symbolic
  Music with Emotion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.10307v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.10307v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shulei Ji, Xinyu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating music with emotion is an important task in automatic music
generation, in which emotion is evoked through a variety of musical elements
(such as pitch and duration) that change over time and collaborate with each
other. However, prior research on deep learning-based emotional music
generation has rarely explored the contribution of different musical elements
to emotions, let alone the deliberate manipulation of these elements to alter
the emotion of music, which is not conducive to fine-grained element-level
control over emotions. To address this gap, we present a novel approach
employing musical element-based regularization in the latent space to
disentangle distinct elements, investigate their roles in distinguishing
emotions, and further manipulate elements to alter musical emotions.
Specifically, we propose a novel VQ-VAE-based model named MusER. MusER
incorporates a regularization loss to enforce the correspondence between the
musical element sequences and the specific dimensions of latent variable
sequences, providing a new solution for disentangling discrete sequences.
Taking advantage of the disentangled latent vectors, a two-level decoding
strategy that includes multiple decoders attending to latent vectors with
different semantics is devised to better predict the elements. By visualizing
latent space, we conclude that MusER yields a disentangled and interpretable
latent space and gain insights into the contribution of distinct elements to
the emotional dimensions (i.e., arousal and valence). Experimental results
demonstrate that MusER outperforms the state-of-the-art models for generating
emotional music in both objective and subjective evaluation. Besides, we
rearrange music through element transfer and attempt to alter the emotion of
music by transferring emotion-distinguishable elements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-01-01T00:00:00Z">2024-01-01</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">26</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph-Convolutional Autoencoder Ensembles for the Humanities,
  Illustrated with a Study of the American Slave Trade 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00824v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00824v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom Lippincott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a graph-aware autoencoder ensemble framework, with associated
formalisms and tooling, designed to facilitate deep learning for scholarship in
the humanities. By composing sub-architectures to produce a model isomorphic to
a humanistic domain we maintain interpretability while providing function
signatures for each sub-architectural choice, allowing both traditional and
computational researchers to collaborate without disrupting established
practices. We illustrate a practical application of our approach to a
historical study of the American post-Atlantic slave trade, and make several
specific technical contributions: a novel hybrid graph-convolutional
autoencoder mechanism, batching policies for common graph topologies, and
masking techniques for particular use-cases. The effectiveness of the framework
for broadening participation of diverse domains is demonstrated by a growing
suite of two dozen studies, both collaborations with humanists and established
tasks from machine learning literature, spanning a variety of fields and data
modalities. We make performance comparisons of several different architectural
choices and conclude with an ambitious list of imminent next steps for this
research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>More in-depth technical companion to "A general neural ensemble
  technique to support traditional scholarship", Digital Humanities 2020</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Computational Framework for Behavioral Assessment of LLM Therapists 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Ying Chiu, Ashish Sharma, Inna Wanyin Lin, Tim Althoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of ChatGPT and other large language models (LLMs) has greatly
increased interest in utilizing LLMs as therapists to support individuals
struggling with mental health challenges. However, due to the lack of
systematic studies, our understanding of how LLM therapists behave, i.e., ways
in which they respond to clients, is significantly limited. Understanding their
behavior across a wide range of clients and situations is crucial to accurately
assess their capabilities and limitations in the high-risk setting of mental
health, where undesirable behaviors can lead to severe consequences. In this
paper, we propose BOLT, a novel computational framework to study the
conversational behavior of LLMs when employed as therapists. We develop an
in-context learning method to quantitatively measure the behavior of LLMs based
on 13 different psychotherapy techniques including reflections, questions,
solutions, normalizing, and psychoeducation. Subsequently, we compare the
behavior of LLM therapists against that of high- and low-quality human therapy,
and study how their behavior can be modulated to better reflect behaviors
observed in high-quality therapy. Our analysis of GPT and Llama-variants
reveals that these LLMs often resemble behaviors more commonly exhibited in
low-quality therapy rather than high-quality therapy, such as offering a higher
degree of problem-solving advice when clients share emotions, which is against
typical recommendations. At the same time, unlike low-quality therapy, LLMs
reflect significantly more upon clients' needs and strengths. Our analysis
framework suggests that despite the ability of LLMs to generate anecdotal
examples that appear similar to human therapists, LLM therapists are currently
not fully consistent with high-quality care, and thus require additional
research to ensure quality care.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ If LLM Is the Wizard, Then Code Is the Wand: A Survey on How Code
  Empowers Large Language Models to Serve as Intelligent Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ke Yang, Jiateng Liu, John Wu, Chaoqi Yang, Yi R. Fung, Sha Li, Zixuan Huang, Xu Cao, Xingyao Wang, Yiquan Wang, Heng Ji, Chengxiang Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prominent large language models (LLMs) of today differ from past language
models not only in size, but also in the fact that they are trained on a
combination of natural language and formal language (code). As a medium between
humans and computers, code translates high-level goals into executable steps,
featuring standard syntax, logical consistency, abstraction, and modularity. In
this survey, we present an overview of the various benefits of integrating code
into LLMs' training data. Specifically, beyond enhancing LLMs in code
generation, we observe that these unique properties of code help (i) unlock the
reasoning ability of LLMs, enabling their applications to a range of more
complex natural language tasks; (ii) steer LLMs to produce structured and
precise intermediate steps, which can then be connected to external execution
ends through function calls; and (iii) take advantage of code compilation and
execution environment, which also provides diverse feedback for model
improvement. In addition, we trace how these profound capabilities of LLMs,
brought by code, have led to their emergence as intelligent agents (IAs) in
situations where the ability to understand instructions, decompose goals, plan
and execute actions, and refine from feedback are crucial to their success on
downstream tasks. Finally, we present several key challenges and future
directions of empowering LLMs with code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PerSHOP -- A Persian <span class="highlight-title">dataset</span> for shopping dialogue systems modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keyvan Mahmoudi, Heshaam Faili
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays, dialogue systems are used in many fields of industry and research.
There are successful instances of these systems, such as Apple Siri, Google
Assistant, and IBM Watson. Task-oriented dialogue system is a category of
these, that are used in specific tasks. They can perform tasks such as booking
plane tickets or making restaurant reservations. Shopping is one of the most
popular areas on these systems. The bot replaces the human salesperson and
interacts with the customers by speaking. To train the models behind the scenes
of these systems, annotated data is needed. In this paper, we developed a
dataset of dialogues in the Persian language through crowd-sourcing. We
annotated these dialogues to train a model. This dataset contains nearly 22k
utterances in 15 different domains and 1061 dialogues. This is the largest
Persian dataset in this field, which is provided freely so that future
researchers can use it. Also, we proposed some baseline models for natural
language understanding (NLU) tasks. These models perform two tasks for NLU:
intent classification and entity extraction. The F-1 score metric obtained for
intent classification is around 91% and for entity extraction is around 93%,
which can be a baseline for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SecFormer: Towards Fast and Accurate Privacy-Preserving Inference for
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00793v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00793v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinglong Luo, Yehong Zhang, Jiaqi Zhang, Xin Mu, Hui Wang, Yue Yu, Zenglin Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing use of large language models hosted on cloud platforms to
offer inference services, privacy concerns are escalating, especially
concerning sensitive data like investment plans and bank account details.
Secure Multi-Party Computing (SMPC) emerges as a promising solution to protect
the privacy of inference data and model parameters. However, the application of
SMPC in Privacy-Preserving Inference (PPI) for large language models,
particularly those based on the Transformer architecture, often leads to
considerable slowdowns or declines in performance. This is largely due to the
multitude of nonlinear operations in the Transformer architecture, which are
not well-suited to SMPC and are difficult to circumvent or optimize
effectively. To address this concern, we introduce an advanced optimization
framework called SecFormer, designed to strike an optimal balance between
performance and efficiency in PPI for Transformer models. By implementing
knowledge distillation techniques, we successfully eliminate the high-cost
exponential and maximum operations in PPI without sacrificing model
performance. Additionally, we have developed a suite of efficient SMPC
protocols that utilize segmented polynomials and Goldschmidt's method to handle
other complex nonlinear functions within PPI, such as GeLU, LayerNorm, and
Softmax. Our extensive experiments reveal that SecFormer outperforms MPCFormer
in performance, showing improvements of $5.6\%$ and $24.2\%$ for
BERT$_{\text{BASE}}$ and BERT$_{\text{LARGE}}$, respectively. In terms of
efficiency, SecFormer is 3.4 and 3.2 times faster than Puma, demonstrating its
effectiveness and speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12pages, 15figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Astraios: Parameter-Efficient Instruction Tuning Code Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Terry Yue Zhuo, Armel Zebaze, Nitchakarn Suppattarachai, Leandro von Werra, Harm de Vries, Qian Liu, Niklas Muennighoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The high cost of full-parameter fine-tuning (FFT) of Large Language Models
(LLMs) has led to a series of parameter-efficient fine-tuning (PEFT) methods.
However, it remains unclear which methods provide the best cost-performance
trade-off at different model scales. We introduce Astraios, a suite of 28
instruction-tuned OctoCoder models using 7 tuning methods and 4 model sizes up
to 16 billion parameters. Through investigations across 5 tasks and 8 different
datasets encompassing both code comprehension and code generation tasks, we
find that FFT generally leads to the best downstream performance across all
scales, and PEFT methods differ significantly in their efficacy based on the
model scale. LoRA usually offers the most favorable trade-off between cost and
performance. Further investigation into the effects of these methods on both
model robustness and code security reveals that larger models tend to
demonstrate reduced robustness and less security. At last, we explore the
relationships among updated parameters, cross-entropy loss, and task
performance. We find that the tuning effectiveness observed in small models
generalizes well to larger models, and the validation loss in instruction
tuning can be a reliable indicator of overall downstream performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages (12 main), 19 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Temporal Validity Change Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georg Wenzel, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal validity is an important property of text that is useful for many
downstream applications, such as recommender systems, conversational AI, or
story understanding. Existing benchmarking tasks often require models to
identify the temporal validity duration of a single statement. However, in many
cases, additional contextual information, such as sentences in a story or posts
on a social media profile, can be collected from the available text stream.
This contextual information may greatly alter the duration for which a
statement is expected to be valid. We propose Temporal Validity Change
Prediction, a natural language processing task benchmarking the capability of
machine learning models to detect contextual statements that induce such
change. We create a dataset consisting of temporal target statements sourced
from Twitter and crowdsource sample context statements. We then benchmark a set
of transformer-based language models on our dataset. Finally, we experiment
with temporal validity duration prediction as an auxiliary task to improve the
performance of the state-of-the-art model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 9 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ New Job, New Gender? Measuring the Social Bias in Image Generation
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00763v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00763v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Wang, Haonan Bai, Jen-tse Huang, Yuxuan Wan, Youliang Yuan, Haoyi Qiu, Nanyun Peng, Michael R. Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image generation models can generate or edit images from a given text. Recent
advancements in image generation technology, exemplified by DALL-E and
Midjourney, have been groundbreaking. These advanced models, despite their
impressive capabilities, are often trained on massive Internet datasets, making
them susceptible to generating content that perpetuates social stereotypes and
biases, which can lead to severe consequences. Prior research on assessing bias
within image generation models suffers from several shortcomings, including
limited accuracy, reliance on extensive human labor, and lack of comprehensive
analysis. In this paper, we propose BiasPainter, a novel metamorphic testing
framework that can accurately, automatically and comprehensively trigger social
bias in image generation models. BiasPainter uses a diverse range of seed
images of individuals and prompts the image generation models to edit these
images using gender, race, and age-neutral queries. These queries span 62
professions, 39 activities, 57 types of objects, and 70 personality traits. The
framework then compares the edited images to the original seed images, focusing
on any changes related to gender, race, and age. BiasPainter adopts a testing
oracle that these characteristics should not be modified when subjected to
neutral prompts. Built upon this design, BiasPainter can trigger the social
bias and evaluate the fairness of image generation models. To evaluate the
effectiveness of BiasPainter, we use BiasPainter to test five widely-used
commercial image generation software and models, such as stable diffusion and
Midjourney. Experimental results show that 100\% of the generated test cases
can successfully trigger social bias in image generation models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Earth is Flat? Unveiling Factual Errors in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Wang, Juluan Shi, Zhaopeng Tu, Youliang Yuan, Jen-tse Huang, Wenxiang Jiao, Michael R. Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) like ChatGPT are foundational in various
applications due to their extensive knowledge from pre-training and
fine-tuning. Despite this, they are prone to generating factual and commonsense
errors, raising concerns in critical areas like healthcare, journalism, and
education to mislead users. Current methods for evaluating LLMs' veracity are
limited by test data leakage or the need for extensive human labor, hindering
efficient and accurate error detection. To tackle this problem, we introduce a
novel, automatic testing framework, FactChecker, aimed at uncovering factual
inaccuracies in LLMs. This framework involves three main steps: First, it
constructs a factual knowledge graph by retrieving fact triplets from a
large-scale knowledge database. Then, leveraging the knowledge graph,
FactChecker employs a rule-based approach to generates three types of questions
(Yes-No, Multiple-Choice, and WH questions) that involve single-hop and
multi-hop relations, along with correct answers. Lastly, it assesses the LLMs'
responses for accuracy using tailored matching strategies for each question
type. Our extensive tests on six prominent LLMs, including text-davinci-002,
text-davinci-003, ChatGPT~(gpt-3.5-turbo, gpt-4), Vicuna, and LLaMA-2, reveal
that FactChecker can trigger factual errors in up to 45\% of questions in these
models. Moreover, we demonstrate that FactChecker's test cases can improve
LLMs' factual accuracy through in-context learning and fine-tuning (e.g.,
llama-2-13b-chat's accuracy increase from 35.3\% to 68.5\%). We are making all
code, data, and results available for future research endeavors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A & B == B & A: Triggering Logical Reasoning Failures in Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Wan, Wenxuan Wang, Yiliu Yang, Youliang Yuan, Jen-tse Huang, Pinjia He, Wenxiang Jiao, Michael R. Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have propelled Artificial
Intelligence (AI) to new heights, enabling breakthroughs in various tasks such
as writing assistance, code generation, and machine translation. A significant
distinction of advanced LLMs, such as ChatGPT, is their demonstrated ability to
"reason." However, evaluating the reasoning ability of LLMs remains a challenge
as most existing evaluations focus on their accuracy on the downstream tasks
rather than directly assessing their reasoning processes. Efforts have been
made to develop benchmarks and metrics to assess reasoning in LLMs, but they
suffer from data leakage or limited scope. In this paper, we introduce
LogicAsker, an automatic approach that comprehensively evaluates and improves
the logical reasoning abilities of LLMs under a set of atomic reasoning skills
based on propositional and predicate logic. The results provide insights into
LLMs' reasoning abilities and reveal the logical rules the LLMs did not learn
well. We evaluate LogicAsker on six widely deployed LLMs, including GPT-3,
ChatGPT, GPT-4, Bard, Vicuna, and Guanaco. The results show that test cases
from LogicAsker can find logical reasoning failures in different LLMs with a
rate of 25\% - 94\%. In addition, the test cases of LogicAsker can be further
used to design demonstration examples for in-context learning, which
effectively improves the logical reasoning ability of LLMs, e.g., 10\% for
GPT-4. As far as we know, our work is the first to create prompts based on
testing results to improve LLMs' formal reasoning ability effectively. All the
code, data, and results will be released for reproduction and future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Translation Testing via Syntactic Tree Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quanjun Zhang, Juan Zhai, Chunrong Fang, Jiawei Liu, Weisong Sun, Haichuan Hu, Qingyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine translation systems have been widely adopted in our daily life,
making life easier and more convenient. Unfortunately, erroneous translations
may result in severe consequences, such as financial losses. This requires to
improve the accuracy and the reliability of machine translation systems.
However, it is challenging to test machine translation systems because of the
complexity and intractability of the underlying neural models. To tackle these
challenges, we propose a novel metamorphic testing approach by syntactic tree
pruning (STP) to validate machine translation systems. Our key insight is that
a pruned sentence should have similar crucial semantics compared with the
original sentence. Specifically, STP (1) proposes a core semantics-preserving
pruning strategy by basic sentence structure and dependency relations on the
level of syntactic tree representation; (2) generates source sentence pairs
based on the metamorphic relation; (3) reports suspicious issues whose
translations break the consistency property by a bag-of-words model. We further
evaluate STP on two state-of-the-art machine translation systems (i.e., Google
Translate and Bing Microsoft Translator) with 1,200 source sentences as inputs.
The results show that STP can accurately find 5,073 unique erroneous
translations in Google Translate and 5,100 unique erroneous translations in
Bing Microsoft Translator (400% more than state-of-the-art techniques), with
64.5% and 65.4% precision, respectively. The reported erroneous translations
vary in types and more than 90% of them cannot be found by state-of-the-art
techniques. There are 9,393 erroneous translations unique to STP, which is
711.9% more than state-of-the-art techniques. Moreover, STP is quite effective
to detect translation errors for the original sentences with a recall reaching
74.0%, improving state-of-the-art techniques by 55.1% on average.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM Transactions on Software Engineering and Methodology
  2024 (TOSEM'24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ToolEyes: Fine-Grained Evaluation for Tool Learning Capabilities of
  Large Language Models in Real-world Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00741v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00741v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Ye, Guanyu Li, Songyang Gao, Caishuang Huang, Yilong Wu, Sixian Li, Xiaoran Fan, Shihan Dou, Qi Zhang, Tao Gui, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing evaluations of tool learning primarily focus on validating the
alignment of selected tools for large language models (LLMs) with expected
outcomes. However, these approaches rely on a limited set of scenarios where
answers can be pre-determined, diverging from genuine needs. Furthermore, a
sole emphasis on outcomes disregards the intricate capabilities essential for
LLMs to effectively utilize tools. To tackle this issue, we propose ToolEyes, a
fine-grained system tailored for the evaluation of the LLMs' tool learning
capabilities in authentic scenarios. The system meticulously examines seven
real-world scenarios, analyzing five dimensions crucial to LLMs in tool
learning: format alignment, intent comprehension, behavior planning, tool
selection, and answer organization. Additionally, ToolEyes incorporates a tool
library boasting approximately 600 tools, serving as an intermediary between
LLMs and the physical world. Evaluations involving ten LLMs across three
categories reveal a preference for specific scenarios and limited cognitive
abilities in tool learning. Intriguingly, expanding the model size even
exacerbates the hindrance to tool learning. These findings offer instructive
insights aimed at advancing the field of tool learning. The data is available
att https://github.com/Junjie-Ye/ToolEyes.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models aren't all that you need 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00698v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00698v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kiran Voderhobli Holla, Chaithanya Kumar, Aryan Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes the architecture and systems built towards solving the
SemEval 2023 Task 2: MultiCoNER II (Multilingual Complex Named Entity
Recognition) [1]. We evaluate two approaches (a) a traditional Conditional
Random Fields model and (b) a Large Language Model (LLM) fine-tuned with a
customized head and compare the two approaches. The novel ideas explored are:
1) Decaying auxiliary loss (with residual) - where we train the model on an
auxiliary task of Coarse-Grained NER and include this task as a part of the
loss function 2) Triplet token blending - where we explore ways of blending the
embeddings of neighboring tokens in the final NER layer prior to prediction 3)
Task-optimal heads - where we explore a variety of custom heads and learning
rates for the final layer of the LLM. We also explore multiple LLMs including
GPT-3 and experiment with a variety of dropout and other hyperparameter
settings before arriving at our final model which achieves micro & macro f1 of
0.85/0.84 (on dev) and 0.67/0.61 on the test data . We show that while
pre-trained LLMs, by themselves, bring about a large improvement in scores as
compared to traditional models, we also demonstrate that tangible improvements
to the Macro-F1 score can be made by augmenting the LLM with additional
feature/loss/model engineering techniques described above.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Benchmark</span>ing Large Language Models on Controllable Generation under
  Diversified Instructions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00690v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00690v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihan Chen, Benfeng Xu, Quan Wang, Yi Liu, Zhendong Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models (LLMs) have exhibited impressive
instruction-following capabilities, it is still unclear whether and to what
extent they can respond to explicit constraints that might be entailed in
various instructions. As a significant aspect of LLM alignment, it is thus
important to formulate such a specialized set of instructions as well as
investigate the resulting behavior of LLMs. To address this vacancy, we propose
a new benchmark CoDI-Eval to systematically and comprehensively evaluate LLMs'
responses to instructions with various constraints. We construct a large
collection of constraints-attributed instructions as a test suite focused on
both generalization and coverage. Specifically, we advocate an instruction
diversification process to synthesize diverse forms of constraint expression
and also deliberate the candidate task taxonomy with even finer-grained
sub-categories. Finally, we automate the entire evaluation process to
facilitate further developments. Different from existing studies on
controllable text generation, CoDI-Eval extends the scope to the prevalent
instruction-following paradigm for the first time. We provide extensive
evaluations of representative LLMs (e.g., ChatGPT, Vicuna) on CoDI-Eval,
revealing their limitations in following instructions with specific constraints
and there is still a significant gap between open-source and commercial
closed-source LLMs. We believe this benchmark will facilitate research into
improving the controllability of LLMs' responses to instructions. Our data and
code are available at https://github.com/Xt-cyh/CoDI-Eval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large language model for Bible sentiment analysis: Sermon on the Mount 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahek Vora, Tom Blau, Vansh Kachhwal, Ashu M. G. Solo, Rohitash Chandra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The revolution of natural language processing via large language models has
motivated its use in multidisciplinary areas that include social sciences and
humanities and more specifically, comparative religion. Sentiment analysis
provides a mechanism to study the emotions expressed in text. Recently,
sentiment analysis has been used to study and compare translations of the
Bhagavad Gita, which is a fundamental and sacred Hindu text. In this study, we
use sentiment analysis for studying selected chapters of the Bible. These
chapters are known as the Sermon on the Mount. We utilize a pre-trained
language model for sentiment analysis by reviewing five translations of the
Sermon on the Mount, which include the King James version, the New
International Version, the New Revised Standard Version, the Lamsa Version, and
the Basic English Version. We provide a chapter-by-chapter and verse-by-verse
comparison using sentiment and semantic analysis and review the major
sentiments expressed. Our results highlight the varying sentiments across the
chapters and verses. We found that the vocabulary of the respective
translations is significantly different. We detected different levels of
humour, optimism, and empathy in the respective chapters that were used by
Jesus to deliver his message.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Digger: Detecting Copyright Content Mis-usage in Large Language Model
  Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00676v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00676v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haodong Li, Gelei Deng, Yi Liu, Kailong Wang, Yuekang Li, Tianwei Zhang, Yang Liu, Guoai Xu, Guosheng Xu, Haoyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training, which utilizes extensive and varied datasets, is a critical
factor in the success of Large Language Models (LLMs) across numerous
applications. However, the detailed makeup of these datasets is often not
disclosed, leading to concerns about data security and potential misuse. This
is particularly relevant when copyrighted material, still under legal
protection, is used inappropriately, either intentionally or unintentionally,
infringing on the rights of the authors.
  In this paper, we introduce a detailed framework designed to detect and
assess the presence of content from potentially copyrighted books within the
training datasets of LLMs. This framework also provides a confidence estimation
for the likelihood of each content sample's inclusion. To validate our
approach, we conduct a series of simulated experiments, the results of which
affirm the framework's effectiveness in identifying and addressing instances of
content misuse in LLM training processes. Furthermore, we investigate the
presence of recognizable quotes from famous literary works within these
datasets. The outcomes of our study have significant implications for ensuring
the ethical use of copyrighted materials in the development of LLMs,
highlighting the need for more transparent and responsible data management
practices in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting Anti-microbial Resistance using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunwoo Yoo, Bahrad Sokhansanj, James R. Brown, Gail Rosen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  During times of increasing antibiotic resistance and the spread of infectious
diseases like COVID-19, it is important to classify genes related to antibiotic
resistance. As natural language processing has advanced with transformer-based
language models, many language models that learn characteristics of nucleotide
sequences have also emerged. These models show good performance in classifying
various features of nucleotide sequences. When classifying nucleotide
sequences, not only the sequence itself, but also various background knowledge
is utilized. In this study, we use not only a nucleotide sequence-based
language model but also a text language model based on PubMed articles to
reflect more biological background knowledge in the model. We propose a method
to fine-tune the nucleotide sequence language model and the text language model
based on various databases of antibiotic resistance genes. We also propose an
LLM-based augmentation technique to supplement the data and an ensemble method
to effectively combine the two models. We also propose a benchmark for
evaluating the model. Our method achieved better performance than the
nucleotide sequence language model in the drug resistance class prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast and Optimal Weight Update for Pruned Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.02938v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.02938v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vladimír Boža
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pruning large language models (LLMs) is a challenging task due to their
enormous size. The primary difficulty is fine-tuning the model after pruning,
which is needed to recover the lost performance caused by dropping weights.
Recent approaches have either ignored fine-tuning entirely, focusing on
efficient pruning criteria, or attempted layer-wise weight updates, preserving
the behavior of each layer. However, even layer-wise weight updates can be
costly for LLMs, and previous works have resorted to various approximations.
  In our paper, we propose a fast and optimal weight update algorithm for
pruned layers based on the Alternating Direction Method of Multipliers (ADMM).
Coupled with a simple iterative pruning mask selection, our algorithm achieves
state-of-the-art pruning performance across a wide range of LLMs. Code is
available at https://github.com/fmfi-compbio/admm-pruning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ViCrop: Perceiving Small Visual Details in Zero-shot Visual Question
  Answering with Multimodal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16033v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16033v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarui Zhang, Mahyar Khayatkhoei, Prateek Chhikara, Filip Ilievski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have recently achieved promising
zero-shot accuracy on visual question answering (VQA) -- a fundamental task
affecting various downstream applications and domains. Given the great
potential for the broad use of these models, it is important to investigate
their limitations in dealing with different image and question properties. In
this work, we investigate whether MLLMs can perceive details as well as larger
components in images. In particular, we show that their zero-shot accuracy in
answering visual questions is very sensitive to the size of the visual subject
related to the question, declining up to $45.91\%$ with size. Furthermore, we
show that this effect is causal by observing that human visual cropping can
significantly mitigate their sensitivity to size. To scale up the usefulness of
human cropping, we propose ViCrop, a general framework that utilizes automatic
visual cropping to enhance zero-shot VQA of MLLMs. We construct five variants
of ViCrop leveraging either external localization models or the decision
process of the given MLLM itself. Our results show that ViCrop improves MLLMs'
zero-shot accuracy across different VQA datasets, for example, enhances
BLIP2-T5's performance by $32.23\%$ on the TextVQA test set. To facilitate
further investigation of MLLMs' behaviors, our code is publicly released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 11 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Cambridge Law Corpus: A <span class="highlight-title">Dataset</span> for Legal AI Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.12269v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.12269v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Östling, Holli Sargeant, Huiyuan Xie, Ludwig Bull, Alexander Terenin, Leif Jonsson, Måns Magnusson, Felix Steffek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the Cambridge Law Corpus (CLC), a dataset for legal AI research.
It consists of over 250 000 court cases from the UK. Most cases are from the
21st century, but the corpus includes cases as old as the 16th century. This
paper presents the first release of the corpus, containing the raw text and
meta-data. Together with the corpus, we provide annotations on case outcomes
for 638 cases, done by legal experts. Using our annotated data, we have trained
and evaluated case outcome extraction with GPT-3, GPT-4 and RoBERTa models to
provide benchmarks. We include an extensive legal and ethical discussion to
address the potentially sensitive nature of this material. As a consequence,
the corpus will only be released for research purposes under certain
restrictions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Learnability of Watermarks for Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04469v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04469v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenchen Gu, Xiang Lisa Li, Percy Liang, Tatsunori Hashimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Watermarking of language model outputs enables statistical detection of
model-generated text, which has many applications in the responsible deployment
of language models. Existing watermarking strategies operate by altering the
decoder of an existing language model, and the ability for a language model to
directly learn to generate the watermark would have significant implications
for the real-world deployment of watermarks. First, learned watermarks could be
used to build open models that naturally generate watermarked text, allowing
for open models to benefit from watermarking. Second, if watermarking is used
to determine the provenance of generated text, an adversary can hurt the
reputation of a victim model by spoofing its watermark and generating damaging
watermarked text. To investigate the learnability of watermarks, we propose
watermark distillation, which trains a student model to behave like a teacher
model that uses decoding-based watermarking. We test our approach on three
distinct decoding-based watermarking strategies and various hyperparameter
settings, finding that models can learn to generate watermarked text with high
detectability. We also find limitations to learnability, including the loss of
watermarking capabilities under fine-tuning on normal text and high sample
complexity when learning low-distortion watermarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Models are Bounded Pragmatic Speakers: Understanding RLHF from
  a Bayesian Cognitive Modeling Perspective <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17760v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17760v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khanh Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How do language models "think"? This paper formulates a probabilistic
cognitive model called the bounded pragmatic speaker, which can characterize
the operation of different variations of language models. Specifically, we
demonstrate that large language models fine-tuned with reinforcement learning
from human feedback (Ouyang et al., 2022) embody a model of thought that
conceptually resembles a fast-and-slow model (Kahneman, 2011), which
psychologists have attributed to humans. We discuss the limitations of
reinforcement learning from human feedback as a fast-and-slow model of thought
and propose avenues for expanding this framework. In essence, our research
highlights the value of adopting a cognitive probabilistic modeling approach to
gain insights into the comprehension, evaluation, and advancement of language
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the First Workshop on Theory of Mind in Communicating
  Agents at (TOM @ ICML 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aurora:Activating Chinese chat capability for Mixtral-8x7B sparse
  Mixture-of-Experts through Instruction-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.14557v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.14557v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongsheng Wang, Haoming Chen, Ruizhe Zhou, Yaofei Duan, Kunyan Cai, Han Ma, Jiaxi Cui, Jian Li, Patrick Cheong-Iao Pang, Yapeng Wang, Tao Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing research has demonstrated that refining large language models (LLMs)
through the utilization of machine-generated instruction-following data
empowers these models to exhibit impressive zero-shot capabilities for novel
tasks, without requiring human-authored instructions. In this paper, we
systematically investigate, preprocess, and integrate three Chinese
instruction-following datasets with the aim of enhancing the Chinese
conversational capabilities of Mixtral-8x7B sparse Mixture-of-Experts model.
Through instruction fine-tuning on this carefully processed dataset, we
successfully construct the Mixtral-8x7B sparse Mixture-of-Experts model named
"Aurora." To assess the performance of Aurora, we utilize three widely
recognized benchmark tests: C-Eval, MMLU, and CMMLU. Empirical studies validate
the effectiveness of instruction fine-tuning applied to Mixtral-8x7B sparse
Mixture-of-Experts model. This work is pioneering in the execution of
instruction fine-tuning on a sparse expert-mixed model, marking a significant
breakthrough in enhancing the capabilities of this model architecture. Our
code, data and model are publicly available at
  https://github.com/WangRongsheng/Aurora
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Normalization of Lithuanian Text Using Regular Expressions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.17660v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.17660v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pijus Kasparaitis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text Normalization is an integral part of any text-to-speech synthesis
system. In a natural language text, there are elements such as numbers, dates,
abbreviations, etc. that belong to other semiotic classes. They are called
non-standard words (NSW) and need to be expanded into ordinary words. For this
purpose, it is necessary to identify the semiotic class of each NSW. The
taxonomy of semiotic classes adapted to the Lithuanian language is presented in
the work. Sets of rules are created for detecting and expanding NSWs based on
regular expressions. Experiments with three completely different data sets were
performed and the accuracy was assessed. Causes of errors are explained and
recommendations are given for the development of text normalization rules.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring AI-Generated Text in Student Writing: How Does AI Help? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.02478v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.02478v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David James Woo, Hengky Susanto, Chi Ho Yeung, Kai Guo, April Ka Yeng Fung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  English as foreign language_EFL_students' use of text generated from
artificial intelligence_AI_natural language generation_NLG_tools may improve
their writing quality. However, it remains unclear to what extent AI-generated
text in these students' writing might lead to higher-quality writing. We
explored 23 Hong Kong secondary school students' attempts to write stories
comprising their own words and AI-generated text. Human experts scored the
stories for dimensions of content, language and organization. We analyzed the
basic organization and structure and syntactic complexity of the stories'
AI-generated text and performed multiple linear regression and cluster
analyses. The results show the number of human words and the number of
AI-generated words contribute significantly to scores. Besides, students can be
grouped into competent and less competent writers who use more AI-generated
text or less AI-generated text compared to their peers. Comparisons of clusters
reveal some benefit of AI-generated text in improving the quality of both
high-scoring students' and low-scoring students' writing. The findings can
inform pedagogical strategies to use AI-generated text for EFL students'
writing and to address digital divides. This study contributes designs of NLG
tools and writing activities to implement AI-generated text in schools.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages, 11 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BSpell: A CNN-Blended BERT Based Bangla Spell Checker 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.09709v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.09709v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chowdhury Rafeed Rahman, MD. Hasibur Rahman, Samiha Zakir, Mohammad Rafsan, Mohammed Eunus Ali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bangla typing is mostly performed using English keyboard and can be highly
erroneous due to the presence of compound and similarly pronounced letters.
Spelling correction of a misspelled word requires understanding of word typing
pattern as well as the context of the word usage. A specialized BERT model
named BSpell has been proposed in this paper targeted towards word for word
correction in sentence level. BSpell contains an end-to-end trainable CNN
sub-model named SemanticNet along with specialized auxiliary loss. This allows
BSpell to specialize in highly inflected Bangla vocabulary in the presence of
spelling errors. Furthermore, a hybrid pretraining scheme has been proposed for
BSpell that combines word level and character level masking. Comparison on two
Bangla and one Hindi spelling correction dataset shows the superiority of our
proposed approach. BSpell is available as a Bangla spell checking tool via
GitHub: https://github.com/Hasiburshanto/Bangla-Spell-Checker
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GenH2R: Learning Generalizable Human-to-Robot Handover via Scalable
  Simulation, Demonstration, and Imitation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00929v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00929v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zifan Wang, Junyu Chen, Ziqing Chen, Pengwei Xie, Rui Chen, Li Yi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents GenH2R, a framework for learning generalizable
vision-based human-to-robot (H2R) handover skills. The goal is to equip robots
with the ability to reliably receive objects with unseen geometry handed over
by humans in various complex trajectories. We acquire such generalizability by
learning H2R handover at scale with a comprehensive solution including
procedural simulation assets creation, automated demonstration generation, and
effective imitation learning. We leverage large-scale 3D model repositories,
dexterous grasp generation methods, and curve-based 3D animation to create an
H2R handover simulation environment named \simabbns, surpassing the number of
scenes in existing simulators by three orders of magnitude. We further
introduce a distillation-friendly demonstration generation method that
automatically generates a million high-quality demonstrations suitable for
learning. Finally, we present a 4D imitation learning method augmented by a
future forecasting objective to distill demonstrations into a visuo-motor
handover policy. Experimental evaluations in both simulators and the real world
demonstrate significant improvements (at least +10\% success rate) over
baselines in all cases. The project page is https://GenH2R.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The project page is https://GenH2R.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Edge Computing based Human-Robot Cognitive Fusion: A Medical Case Study
  in the Autism Spectrum Disorder Therapy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00776v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00776v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, edge computing has served as a paradigm that enables many
future technologies like AI, Robotics, IoT, and high-speed wireless sensor
networks (like 5G) by connecting cloud computing facilities and services to the
end users. Especially in medical and healthcare applications, it provides
remote patient monitoring and increases voluminous multimedia. From the
robotics angle, robot-assisted therapy (RAT) is an active-assistive robotic
technology in rehabilitation robotics, attracting many researchers to study and
benefit people with disability like autism spectrum disorder (ASD) children.
However, the main challenge of RAT is that the model capable of detecting the
affective states of ASD people exists and can recall individual preferences.
Moreover, involving expert diagnosis and recommendations to guide robots in
updating the therapy approach to adapt to different statuses and scenarios is a
crucial part of the ASD therapy process. This paper proposes the architecture
of edge cognitive computing by combining human experts and assisted robots
collaborating in the same framework to help ASD patients with long-term
support. By integrating the real-time computing and analysis of a new cognitive
robotic model for ASD therapy, the proposed architecture can achieve a seamless
remote diagnosis, round-the-clock symptom monitoring, emergency warning,
therapy alteration, and advanced assistance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was accepted by the 38th AAAI 2024 workshop: "Cooperative
  Multi-Agent Systems Decision-Making and Learning: From Individual Needs to
  Swarm Intelligence"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast and Continual Learning for Hybrid Control Policies using
  Generalized Benders Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00917v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00917v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hybrid model predictive control with both continuous and discrete variables
is widely applicable to robotic control tasks, especially those involving
contact with the environment. Due to the combinatorial complexity, the solving
speed of hybrid MPC can be insufficient for real-time applications. In this
paper, we proposed a hybrid MPC solver based on Generalized Benders
Decomposition (GBD). The algorithm enumerates and stores cutting planes online
inside a finite buffer. After a short cold-start phase, the stored cuts provide
warm-starts for the new problem instances to enhance the solving speed. Despite
the disturbance and randomly changing environment, the solving speed maintains.
Leveraging on the sparsity of feasibility cuts, we also propose a fast
algorithm for Benders master problems. Our solver is validated through
controlling a cart-pole system with randomly moving soft contact walls, and a
free-flying robot navigating around obstacles. The results show that with
significantly less data than previous works, the solver reaches competitive
speeds to the off-the-shelf solver Gurobi despite the Python overhead.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A more complete version of the previous paper "Generalized Benders
  Decomposition with Continual Learning for Hybrid Model Predictive Control in
  Dynamic Environment". arXiv admin note: substantial text overlap with
  arXiv:2310.03344</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ General-purpose foundation models for increased autonomy in
  robot-assisted surgery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Schmidgall, Ji Woong Kim, Alan Kuntz, Ahmed Ezzat Ghazi, Axel Krieger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The dominant paradigm for end-to-end robot learning focuses on optimizing
task-specific objectives that solve a single robotic problem such as picking up
an object or reaching a target position. However, recent work on high-capacity
models in robotics has shown promise toward being trained on large collections
of diverse and task-agnostic datasets of video demonstrations. These models
have shown impressive levels of generalization to unseen circumstances,
especially as the amount of data and the model complexity scale. Surgical robot
systems that learn from data have struggled to advance as quickly as other
fields of robot learning for a few reasons: (1) there is a lack of existing
large-scale open-source data to train models, (2) it is challenging to model
the soft-body deformations that these robots work with during surgery because
simulation cannot match the physical and visual complexity of biological
tissue, and (3) surgical robots risk harming patients when tested in clinical
trials and require more extensive safety measures. This perspective article
aims to provide a path toward increasing robot autonomy in robot-assisted
surgery through the development of a multi-modal, multi-task,
vision-language-action model for surgical robots. Ultimately, we argue that
surgical robots are uniquely positioned to benefit from general-purpose models
and provide three guiding actions toward increased autonomy in robot-assisted
surgery.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nonlinear vibration of a dipteran flight robot system with rotational
  geometric nonlinearity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanwei Han, Zijian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The dipteran flight mechanism of the insects is commonly used to design the
nonlinear flight robot system. However, the dynamic response of the click
mechanism of the nonlinear robot system with multiple stability still unclear.
In this paper, a novel dipteran robot model with click mechanism proposed based
on the multiple stability of snap-through buckling. The motion of equation of
the nonlinear flight robot system is obtained by using the Euler-Lagrange
equation. The nonlinear potential energy, the elastic force, equilibrium
bifurcation, as well as equilibrium stability are investigated to show the
multiple stability characteristics. The transient sets of bifurcation and
persistent set of regions in the system parameter plane and the corresponding
phase portraits are obtained with multiple stability of single and double well
behaviors. Then, the periodic free vibration response are defined by the
analytical solution of three kinds of elliptical functions, as well as the
amplitude frequency responses are investigated by numerical integration. Based
on the topological equivalent method, the chaotic thresholds of the homo-clinic
orbits for the chaotic vibration of harmonic forced robot system are derived to
show the chaotic parametric condition. Finally, the prototype of nonlinear
flapping robot is manufactured and the experimental system is setup. The
nonlinear static moment of force curves, periodic response and dynamic flight
vibration of dipteran robot system are carried out. It is shown that the test
results are agree well with the theoretical analysis and numerical simulation.
Those result have the potential application for the structure design of the
efficient flight robot.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 24 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ParisLuco3D: A high-quality target <span class="highlight-title">dataset</span> for domain generalization of
  LiDAR perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.16542v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.16542v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jules Sanchez, Louis Soum-Fontez, Jean-Emmanuel Deschaud, Francois Goulette
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR is an essential sensor for autonomous driving by collecting precise
geometric information regarding a scene. As the performance of various LiDAR
perception tasks has improved, generalizations to new environments and sensors
has emerged to test these optimized models in real-world conditions.
Unfortunately, the various annotation strategies of data providers complicate
the computation of cross-domain performances.
  This paper provides a novel dataset, ParisLuco3D, specifically designed for
cross-domain evaluation to make it easier to evaluate the performance utilizing
various source datasets. Alongside the dataset, online benchmarks for LiDAR
semantic segmentation, LiDAR object detection, and LiDAR tracking are provided
to ensure a fair comparison across methods.
  The ParisLuco3D dataset, evaluation scripts, and links to benchmarks can be
found at the following website: https://npm3d.fr/parisluco3d
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UDTIRI: An Online Open-Source Intelligent Road Inspection <span class="highlight-title">Benchmark</span>
  Suite 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.08842v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.08842v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sicen Guo, Jiahang Li, Yi Feng, Dacheng Zhou, Denghuang Zhang, Chen Chen, Shuai Su, Xingyi Zhu, Qijun Chen, Rui Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the nascent domain of urban digital twins (UDT), the prospects for
leveraging cutting-edge deep learning techniques are vast and compelling.
Particularly within the specialized area of intelligent road inspection (IRI),
a noticeable gap exists, underscored by the current dearth of dedicated
research efforts and the lack of large-scale well-annotated datasets. To foster
advancements in this burgeoning field, we have launched an online open-source
benchmark suite, referred to as UDTIRI. Along with this article, we introduce
the road pothole detection task, the first online competition published within
this benchmark suite. This task provides a well-annotated dataset, comprising
1,000 RGB images and their pixel/instance-level ground-truth annotations,
captured in diverse real-world scenarios under different illumination and
weather conditions. Our benchmark provides a systematic and thorough evaluation
of state-of-the-art object detection, semantic segmentation, and instance
segmentation networks, developed based on either convolutional neural networks
or Transformers. We anticipate that our benchmark will serve as a catalyst for
the integration of advanced UDT techniques into IRI. By providing algorithms
with a more comprehensive understanding of diverse road conditions, we seek to
unlock their untapped potential and foster innovation in this critical domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Database webpage: https://www.udtiri.com/, Kaggle webpage:
  https://www.kaggle.com/datasets/jiahangli617/udtiri</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ New Job, New Gender? Measuring the Social Bias in Image Generation
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00763v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00763v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Wang, Haonan Bai, Jen-tse Huang, Yuxuan Wan, Youliang Yuan, Haoyi Qiu, Nanyun Peng, Michael R. Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image generation models can generate or edit images from a given text. Recent
advancements in image generation technology, exemplified by DALL-E and
Midjourney, have been groundbreaking. These advanced models, despite their
impressive capabilities, are often trained on massive Internet datasets, making
them susceptible to generating content that perpetuates social stereotypes and
biases, which can lead to severe consequences. Prior research on assessing bias
within image generation models suffers from several shortcomings, including
limited accuracy, reliance on extensive human labor, and lack of comprehensive
analysis. In this paper, we propose BiasPainter, a novel metamorphic testing
framework that can accurately, automatically and comprehensively trigger social
bias in image generation models. BiasPainter uses a diverse range of seed
images of individuals and prompts the image generation models to edit these
images using gender, race, and age-neutral queries. These queries span 62
professions, 39 activities, 57 types of objects, and 70 personality traits. The
framework then compares the edited images to the original seed images, focusing
on any changes related to gender, race, and age. BiasPainter adopts a testing
oracle that these characteristics should not be modified when subjected to
neutral prompts. Built upon this design, BiasPainter can trigger the social
bias and evaluate the fairness of image generation models. To evaluate the
effectiveness of BiasPainter, we use BiasPainter to test five widely-used
commercial image generation software and models, such as stable diffusion and
Midjourney. Experimental results show that 100\% of the generated test cases
can successfully trigger social bias in image generation models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion Models, Image Super-Resolution And Everything: A Survey 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brian B. Moser, Arundhati S. Shanbhag, Federico Raue, Stanislav Frolov, Sebastian Palacio, Andreas Dengel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Models (DMs) represent a significant advancement in image
Super-Resolution (SR), aligning technical image quality more closely with human
preferences and expanding SR applications. DMs address critical limitations of
previous methods, enhancing overall realism and details in SR images. However,
DMs suffer from color-shifting issues, and their high computational costs call
for efficient sampling alternatives, underscoring the challenge of balancing
computational efficiency and image quality. This survey gives an overview of
DMs applied to image SR and offers a detailed analysis that underscores the
unique characteristics and methodologies within this domain, distinct from
broader existing reviews in the field. It presents a unified view of DM
fundamentals and explores research directions, including alternative input
domains, conditioning strategies, guidance, corruption spaces, and zero-shot
methods. This survey provides insights into the evolution of image SR with DMs,
addressing current trends, challenges, and future directions in this rapidly
evolving field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Point Cloud in the Air 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.00658v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.00658v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulin Shao, Chenghong Bian, Li Yang, Qianqian Yang, Zhaoyang Zhang, Deniz Gunduz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Acquisition and processing of point clouds (PCs) is a crucial enabler for
many emerging applications reliant on 3D spatial data, such as robot
navigation, autonomous vehicles, and augmented reality. In most scenarios, PCs
acquired by remote sensors must be transmitted to an edge server for fusion,
segmentation, or inference. Wireless transmission of PCs not only puts on
increased burden on the already congested wireless spectrum, but also confronts
a unique set of challenges arising from the irregular and unstructured nature
of PCs. In this paper, we meticulously delineate these challenges and offer a
comprehensive examination of existing solutions while candidly acknowledging
their inherent limitations. In response to these intricacies, we proffer four
pragmatic solution frameworks, spanning advanced techniques, hybrid schemes,
and distributed data aggregation approaches. In doing so, our goal is to chart
a path toward efficient, reliable, and low-latency wireless PC transmission.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Q-Bench: A <span class="highlight-title">Benchmark</span> for General-Purpose Foundation Models on Low-level
  Vision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14181v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14181v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoning Wu, Zicheng Zhang, Erli Zhang, Chaofeng Chen, Liang Liao, Annan Wang, Chunyi Li, Wenxiu Sun, Qiong Yan, Guangtao Zhai, Weisi Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid evolution of Multi-modality Large Language Models (MLLMs) has
catalyzed a shift in computer vision from specialized models to general-purpose
foundation models. Nevertheless, there is still an inadequacy in assessing the
abilities of MLLMs on low-level visual perception and understanding. To address
this gap, we present Q-Bench, a holistic benchmark crafted to systematically
evaluate potential abilities of MLLMs on three realms: low-level visual
perception, low-level visual description, and overall visual quality
assessment. a) To evaluate the low-level perception ability, we construct the
LLVisionQA dataset, consisting of 2,990 diverse-sourced images, each equipped
with a human-asked question focusing on its low-level attributes. We then
measure the correctness of MLLMs on answering these questions. b) To examine
the description ability of MLLMs on low-level information, we propose the
LLDescribe dataset consisting of long expert-labelled golden low-level text
descriptions on 499 images, and a GPT-involved comparison pipeline between
outputs of MLLMs and the golden descriptions. c) Besides these two tasks, we
further measure their visual quality assessment ability to align with human
opinion scores. Specifically, we design a softmax-based strategy that enables
MLLMs to predict quantifiable quality scores, and evaluate them on various
existing image quality assessment (IQA) datasets. Our evaluation across the
three abilities confirms that MLLMs possess preliminary low-level visual
skills. However, these skills are still unstable and relatively imprecise,
indicating the need for specific enhancements on MLLMs towards these abilities.
We hope that our benchmark can encourage the research community to delve deeper
to discover and enhance these untapped potentials of MLLMs. Project Page:
https://q-future.github.io/Q-Bench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 11 tables, with updated results</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-01-09T05:27:08.215655284Z">
            2024-01-09 05:27:08 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
